<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>计算机网络实验完成思路</title>
    <url>/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/</url>
    <content><![CDATA[<p>计算机网络实验完成思路</p>
<h2 id="2025-4-14-实验二">2025/4/14 实验二</h2>
<p>实验1：</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%B8%80%E6%8E%A5%E7%BA%BF.jpg" alt="实验一接线"></p>
<p>一开始先啥也不用做，因为此时电脑都默认处于vlan1，只需要待连接的一些电脑防火墙都关闭，ipv4的地址前三位相同后一位不同，然后互相ping，截图即可。</p>
<p>随后，某一台电脑连接交换机S1，打开超级终端，选择com2，波特率9600其他不用管，进去之后enter就行，用sys（tab补全）进入系统设置状态</p>
<p>随后输入vlan 2，port Ethernet 1/0/1 to 1/0/2，让两个接口进入vlan 2，然后用display curr（tab补全）查看情况。随后quit。</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%B8%80%E5%88%86vlan%E5%89%8D.jpg" alt="实验一分vlan前"></p>
<p>同理输入vlan 3，port Ethernet 1/0/3 to 1/0/4，display curr（tab补全）查看情况。这时12与34（就是路由器上PC对应链接SIL的数值，这个就是端口号）对应的主机无法ping，说明vlan配置成功。</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%B8%80%E5%88%86vlan%E5%90%8E.jpg" alt="实验一分vlan后"></p>
<p>实验2：</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%BA%8C%E6%8E%A5%E7%BA%BF.jpg" alt="实验二接线"></p>
<p>另一台电脑接入交换机S2，按照S1的配置来安排S2，随后按照如图所示接线接好（主要是两台交换机的端口4相连，同一vlan下的端口就要连到各自交换机的相同端口）。</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E7%AB%AF%E5%8F%A3%E9%85%8D%E7%BD%AE%E6%88%90%E5%8A%9F%E5%9B%BE.jpg" alt="端口配置成功图"></p>
<p>然后输入interface Ethernet 1/0/4,port link-type trunk,port trunk permit vlan2,随后就可以两台交换机对应的电脑相互通信，发现可以就完成了。</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%BA%8C%E4%B8%8D%E5%90%8C%E4%BA%A4%E6%8D%A2%E6%9C%BA%E7%9B%B8%E5%90%8Cvlan.jpg" alt="实验二不同交换机相同vlan"></p>
<h2 id="2025-4-21-实验三">2025.4.21 实验三</h2>
<p>1).同实验二 只是将四个电脑中的两台分到不同网段 人工设置网关为X.X.X.1</p>
<p>接线：</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%B8%80%E7%BA%A7%E8%81%94%E5%89%8D.jpg" alt="实验一级联前"></p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E4%BA%8C%E5%B1%82%E4%B8%8E%E4%B8%89%E5%B1%82%E7%BA%A7%E8%81%94%E5%89%8D.jpg" alt="二层与三层级联前"></p>
<p>结果：同一网段能传不同不能传</p>
<p>加入三层交换机并将二三层级联</p>
<p>连线：</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%B8%80%E7%BA%A7%E8%81%94%E5%90%8E.jpg" alt="实验一级联后"></p>
<p>手动设置s1的主从网关</p>
<p>[Switch] interface vlan-interface 1</p>
<p>[Switch-Vlan-interface1] ip address 192.168.0.1 255.255.255.0</p>
<p>[Switch-Vlan-interface1] ip address 202.202.0.1 255.255.255.0 sub</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E4%B8%89%E5%B1%82%E4%BA%A4%E6%8D%A2%E6%9C%BA%E9%85%8D%E7%BD%AE%E8%B7%AF%E7%94%B1%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%B8%BB%E4%BB%8EIP.jpg" alt="三层交换机配置路由接口的主从IP"></p>
<p>结果 联通 包括电脑到电脑 电脑到节点</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E4%BA%8C%E5%B1%82%E4%B8%8E%E4%B8%89%E5%B1%82%E7%BA%A7%E8%81%94%E5%90%8E.jpg" alt="二层与三层级联后"></p>
<p>实验二 ： 验证同1交换机 不同网段不同VLAN能否互联</p>
<p>连线</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%BA%8C.jpg" alt="实验二"></p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%88%86%E9%85%8D%E7%AB%AF%E5%8F%A3.jpg" alt="分配端口"></p>
<p>如实验二分配端口</p>
<p>display curr….</p>
<p>vlan ？</p>
<p>port Ethernet 1/0/1 to 1/0/2</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E8%AE%BE%E7%BD%AEip%E5%92%8CVLAN%E5%86%85%E7%9A%84%E8%AE%A1%E7%AE%97%E6%9C%BAip%E5%9C%B0%E5%9D%80%E5%A4%84%E4%BA%8E%E5%90%8C%E4%B8%80%E7%BD%91%E6%AE%B5%E5%86%85%E3%80%82.jpg" alt="设置ip和VLAN内的计算机ip地址处于同一网段内。"></p>
<p>建立VLAN 100和VLAN 200的路由接口，并分别设置ip地址，该ip应和VLAN内的计算机ip地址处于同一网段内。</p>
<p>三层交换机上的配置</p>
<p>[S1] vlan 100       # 创建 vlan 100 并进入其视图</p>
<p>[S1-vlan100] port e1/0/1 to e1/0/3    # 将1-3号以太口加入VLAN100</p>
<p>[S1-vlan100] vlan 200      # 创建 vlan 200 并进入其视图</p>
<p>[S1-vlan200] port e1/0/4 to e1/0/6    #将4-6号以太口加入VLAN200</p>
<p>[S1] int vlan 100       # 创建vlan100的路由接口</p>
<p>[S1-vlan-interface100] ip address 192.168.0.1 255.255.255.0  # 配置IP地址</p>
<p>[S1-vlan-interface100] int vlan 200  # 创建vlan200的路由接口</p>
<p>[S1-vlan-interface200] ip address 202.0.0.1 255.255.255.0</p>
<p>结果：显然联通</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C%E4%BA%8C%E7%BB%93%E6%9E%9C.jpg" alt="实验二结果"></p>
<h2 id="2025-4-28实验四">2025.4.28实验四</h2>
<p>连线：</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/a13db8cbfb38eb34ec2e117ff084c74.jpg" alt="a13db8cbfb38eb34ec2e117ff084c74"></p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/b336387fe7521d1e9169f1b51058ee5.jpg" alt="b336387fe7521d1e9169f1b51058ee5"></p>
<p>首先，交换机正常情况会把所有IP放入vlan1 检查所有IP是否在vlan1中</p>
<p>后将两台电脑分别配置路由器1和路由器2</p>
<p>先配置一台路由器做消融实验看看能联通到哪一个位置</p>
<p>路由器R1的设置：通过超级终端，进入路由器控制口。</p>
<p>进入系统视图后，进入以太网0口的接口视图，设置ip地址：</p>
<p><H3C>system-view</H3C></p>
<p>[H3C]sysname R1</p>
<p>[R1]interface Ethernet 0/0</p>
<p>[R1-Ethernet0/0]ip address 202.0.0.1 24</p>
<p>再进入串口的1/0口，设置ip地址：</p>
<p>[R1-Ethernet0/0]interface serial 1/0</p>
<p>[R1-Serial1/0]ip address 192.0.0.1 24</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C1%E9%85%8D%E7%BD%AE-1745845804411-3.jpg" alt="实验1配置"></p>
<p>配置完成后检查以太口和串口的状态 如图 可以发现均转为up</p>
<p>display ip interface brief</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%AE%9E%E9%AA%8C1%E4%BB%A5%E5%A4%AA%E5%8F%A3%E4%B8%8E%E4%B8%B2%E5%8F%A3%E7%8A%B6%E6%80%81.jpg" alt="实验1以太口与串口状态"></p>
<p>设置静态路由</p>
<p>静态路由（Static Routing）是在路由器中设置固定的路由表。由网络管理员管理路由表。由于静态路由不能对网络的改变作出反映，一般用于网络规模不大、拓扑结构固定的网络中。优点是简单、高效、可靠。在所有的路由中，静态路由优先级最高。当动态路由与静态路由发生冲突时，以静态路由为准。</p>
<p>命令 ip route-static <em>ip</em>*-address* { <em>mask</em> | <em>mask-length</em> } [ <em>interface-type interface-number</em> ] [ <em>nexthop</em>*-address* ] [ preference <em>preference-value</em> ] [ reject | blackhole ] [ tag <em>tag-value</em> ] [ description <em>string</em> ]</p>
<p>R1上的配置：</p>
<p>[R1] ip route-static 202.0.1.0 255.255.255.0 192.0.0.2</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1%E8%B7%AF%E7%94%B1%E8%A1%A8-1745846184681-7.jpg" alt="静态路由路由表"></p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C.jpg" alt="实验1配置"></p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-04-28%20211107.png" alt="屏幕截图 2025-04-28 211107"></p>
<p>检测到只能ping通到第一个路由器位置</p>
<p>后按之前步骤配置第二台交换机 继续测试联通位置</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C.jpg" alt="最终结果"></p>
<p>发现所有接口均可联通</p>
<p>可知试验成功</p>
<p>将之前的路由改为默认路由 测试结果</p>
<p>默认路由是静态路由的一种，当目标网络为0.0.0.0时，表示任何发往外网的数据包找不到匹配路由项时将发往指定的下一跳地址</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/0000%E7%BB%93%E6%9E%9C.jpg" alt="0000结果"></p>
<p>结果如上</p>
<p>最后删除所有静态路由</p>
<p>实验结束</p>
<h2 id="2025-05-11实验五">2025.05.11实验五</h2>
<p>动态路由协议的配置</p>
<p>如图所示搭建网络</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/image-20250519235551495.png" alt="image-20250519235551495"></p>
<p>师承实验四的路由器配置，设置以太网和串口的ip地址，然后ping网关或者查看display ip routing-table，可以发现已经加入进去对方网关</p>
<p>随后设置动态路由：</p>
<p>先用命令delete static-routes all将所有静态路由删除。</p>
<p>进入系统视图，用rip命令启用RIP协议，然后用network命令宣告连接在R1上的网段</p>
<p>[R1] rip</p>
<p>[R1-rip-1] network 192.0.0.0</p>
<p>[R1-rip-1] network 202.0.0.0</p>
<p>R2上的配置：</p>
<p>同R1的配置，宣告网段注意应该上连接到自己接口上的网段</p>
<p>[R2] rip</p>
<p>[R2-rip-1] network 192.0.0.0</p>
<p>[R2-rip-1] network 202.0.1.0</p>
<p>配置完成后，路由器将会把连接到自己的网段的路由信息发送给相邻的路由器，用display ip routing-table查看路由表，因该能看到对方网段的路由信息。</p>
<p>可以用display rip来显示rip协议配置信息。</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%7B40CCDF6B-6040-4427-B687-D348E2E5E521%7D.png.jpg" alt="{40CCDF6B-6040-4427-B687-D348E2E5E521}.png"></p>
<p>设置动态路由OSPF。</p>
<p>R1上的配置：</p>
<p>[R1] router id 1.1.1.1</p>
<p>[R1] ospf 1</p>
<p>[R1-ospf-1] area 1</p>
<p>[R1-ospf-1-aeR1-0.0.0.1] network 202.0.0.0 0.0.0.255</p>
<p>[R1-ospf-1] area 0</p>
<p>[R1-ospf-1-aeR1-0.0.0.0] network 192.0.0.0 0.0.0.255</p>
<p>R2上的配置：</p>
<p>同R1的配置，宣告网段注意应该是连接到自己接口上的网段</p>
<p>[[R2] router id 2.2.2.1</p>
<p>[R2] ospf 1</p>
<p>[R2-ospf-1] area 2</p>
<p>[R2-ospf-1-aeR1-0.0.0.2] network 202.0.1.0 0.0.0.255</p>
<p>[R2-ospf-1] area 0</p>
<p>[R2-ospf-1-aeR1-0.0.0.0] network 192.0.0.0 0.0.0.255</p>
<p>配置完成后，路由器将会把连接到自己的网段的路由信息发送给相邻的路由器，用display ip routing-table查看路由表，因该能看到对方网段的路由信息。</p>
<p>在PC下用ping命令测试，应该能ping通对放的网段。</p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%7B0C1E3B07-103F-4BCB-B4D2-B35F1850C431%7D.png.jpg" alt="{0C1E3B07-103F-4BCB-B4D2-B35F1850C431}.png"></p>
<p><img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%7B7DC3C540-91F0-4632-BE68-DBBE19E78677%7D.png.jpg" alt="{7DC3C540-91F0-4632-BE68-DBBE19E78677}.png"></p>
<h2 id="2025-05-19-实验六">2025.05.19 实验六</h2>
<p>按照实验四先配置交换机和路由器</p>
<p>在R1上设置：</p>
<p>acl <strong>num 3000</strong></p>
<p><strong>rule deny</strong> <strong>ip</strong> <strong>source</strong> 202.0.0.0 0.0.0.255 <strong>destination</strong> 202.0.1.0 0.0.0.255</p>
<p><strong>interface</strong> e 0</p>
<p>packet-filter** 3000 <strong>inbound</strong></p>
<p><strong>interface</strong> s 0</p>
<p>packet-filter** 3000 <strong>outbound</strong></p>
<p>禁止202.0.0.0 网段发送信息到202.0.1.0网段</p>
<p>结果：<img src="/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%AA%8C%E5%AE%8C%E6%88%90%E6%80%9D%E8%B7%AF/%7B7898914C-7DC4-4B94-8F8D-9EBDBAA2395F%7D.png.jpg" alt="{7898914C-7DC4-4B94-8F8D-9EBDBAA2395F}.png"></p>
<p>以上是无防火墙的情况下</p>
<p>以下是有防火墙的情况下</p>
<p>先配置防火墙</p>
<p>l<strong>打开或者关闭防火墙</strong></p>
<p>à<strong>firewall { enable | disable }</strong></p>
<p>l<strong>设置防火墙的缺省过滤模式</strong></p>
<p>à<strong>firewall default {</strong> <strong>permit|deny</strong> <strong>}</strong></p>
<p>l<strong>显示防火墙的统计信息</strong></p>
<p>à<strong>display firewall-statistics { all | interface</strong> <em>interface-name</em> <strong>|</strong> <strong>fragments-inspect }</strong></p>
<p>l<strong>打开防火墙包过滤调试信息开关</strong></p>
<p>à<strong>debugging firewall</strong> <strong>{ all |</strong> <strong>icmp</strong> <strong>|</strong> <strong>tcp</strong> <strong>|</strong> <strong>udp</strong> <strong>| others } [ interface</strong> <em>interface-name</em> <strong>]</strong></p>
<p>在R1上输入</p>
<p><strong>acl</strong> <strong>num 3000</strong></p>
<p><strong>rule deny</strong> <strong>ip</strong> <strong>source</strong> 202.0.0.0 0.0.0.255 <strong>destination</strong> 202.0.1.0 0.0.0.255</p>
<p><strong>firewall enable</strong></p>
<p><strong>interface</strong> e 0</p>
<p><strong>firewall packet-filter</strong> 3000 <strong>inbound</strong></p>
<p><strong>interface</strong> s 0</p>
<p><strong>firewall packet-filter</strong> 3000 <strong>outbound</strong></p>
<p>结果同上</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库原理与应用</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/</url>
    <content><![CDATA[<p>数据库原理与应用</p>
<p>目录：</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/4056449bbc39c95e5e5283fcd35fd06.png" alt="4056449bbc39c95e5e5283fcd35fd06"></p>
<h2 id="第一章-绪论">第一章 绪论</h2>
<blockquote>
<p>重点</p>
<p>1.数据模型：组成要素（三要素）是数据结构，数据操作和完整性约束条件</p>
<blockquote>
<p>[!success]</p>
<ul>
<li>数据结构：描述数据库的组成对象以及对象之间的关系</li>
<li>数据操纵：对数据库中各种对象（型）的实例（值）允许执行的操作的集合，包括操作及有关的操作规则。主要操作是查询和更新（包括插入、删除和修改）。</li>
<li>完整性约束：是一组完整性规则（即给定的数据模型中数据及其联系所具有的制约和依存规则）。限定了符合数据模型的数据库状态以及状态的变化，以保证数据的正确、 有效和相容。</li>
</ul>
</blockquote>
<p>2.数据、数据库、数据库管理系统和数据库系统</p>
<blockquote>
<p>[!success]</p>
<ul>
<li>数据（data）是数据库存储的基本对象</li>
<li>数据库（database简称DB）是长期存储在计算机内有组织、可共享的大量数据的集合</li>
<li>数据库管理系统（DBMS）：是位于用户与操作系统之间的一层数据管理软件，是计算机的基础软件，是一个大型复杂的软件系统</li>
<li>数据库系统由数据库（DB）、数据库管理系统（及外围的应用开发工具）（DBMS）、应用程序与数据库管理员构成</li>
</ul>
</blockquote>
<p>3.数据库系统特点：模式、三级模式结构和二级映像功能与数据独立性</p>
<blockquote>
<p>[!success]</p>
<p>1.数据库模式（即全局逻辑结构）</p>
<p>①是数据库的中心和关键</p>
<p>②独立于数据库的其他层次</p>
<p>③设计数据库模式时应首先确定数据库逻辑模式</p>
<p>2.三级模式结构：模式，内模式，外模式</p>
<ul>
<li>数据库的内模式</li>
</ul>
<p>①依赖于模式（全局逻辑结构）</p>
<p>②独立于数据库的用户视图，即外模式</p>
<p>③独立于具体的存储设备</p>
<p>④将全局逻辑结构中所定义的数据结构及其联系按照一定的物理存储策略进行组织，以达到较好的时间和空间效率</p>
<ul>
<li>数据库的外模式</li>
</ul>
<p>①面向具体的应用程序</p>
<p>②定义在逻辑模式之上</p>
<p>③独立于存储模式和存储设备</p>
<p>④当应用需求发生较大变化，相应外模式不能满足其视图要求时，该外模式就得做相应改动</p>
<p>⑤设计外模式时应充分考虑应用的扩充性</p>
<p>3.数据库的二级映像功能（外模式/模式映像，模式/内模式映像）</p>
<p>①保证了数据库外模式的稳定性</p>
<p>②从底层保证了应用程序的稳定性，除非应用需求本身发生变化，否则应用程序一般不需要修改</p>
<p>4.数据独立性：指数据与程序之间的独立性，使得数据的定义和描述可以从应用程序中分离出去</p>
<ul>
<li>外模式/模式映像的作用是：保持数据的逻辑独立性</li>
</ul>
<p>①当模式改变时，数据库管理员对外模式/模式映像作相应改变，使外模式保持不变。应用程序是依据数据的外模式编写的，应用程序不必修改，保证了数据与程序的逻辑独立性，简称数据的逻辑独立性</p>
<ul>
<li>模式/内模式映像的作用是：保持数据的物理独立性</li>
</ul>
<p>②当数据库的存储结构改变时（例如选用了另一种存储结构），数据库管理员修改模式/内模式映像，使模式保持不变。模式保持不变，则应用程序不必改变。保证了数据与程序的物理独立性，简称数据的物理独立性</p>
</blockquote>
</blockquote>
<h3 id="数据库的4个基本概念">数据库的4个基本概念</h3>
<p>数据（Data）、数据库（DataBase，DB）、数据库管理系统（DataBase Management System，DBMS）、数据库系统（DataBase System，DBS）</p>
<ul>
<li>数据（data）是数据库存储的基本对象</li>
<li>数据库（database简称DB）是长期存储在计算机内有组织、可共享的大量数据的集合</li>
</ul>
<blockquote>
<p>数据库的基本特征</p>
<p>1.数据按一定的数据模型组织、描述和储存</p>
<p>2.较小的冗余度</p>
<p>3.较高的数据独立性</p>
<p>4.可扩展性</p>
</blockquote>
<ul>
<li>数据库管理系统（DBMS）：是位于用户与操作系统之间的一层数据管理软件，是计算机的基础软件，是一个大型复杂的软件系统</li>
</ul>
<blockquote>
<p>数据库管理系统的主要功能</p>
<p>1.数据定义功能</p>
<p>①提供数据定义语言（DDL）</p>
<p>②定义数据库中的数据对象的组成和结构</p>
<p>2.数据组织、存储和管理功能</p>
<p>①分类组织、存储和管理各种数据</p>
<p>②确定组织数据的文件结构和存取方式</p>
<p>③实现数据之间的联系</p>
<p>④提供多种存取方式提高存取效率</p>
<p>3.数据操纵功能</p>
<p>①提供数据操纵语言（DML）</p>
<p>②实现对数据库的基本操作（查询、插入、删除和修改）</p>
<p>4.数据库的事务管理与运行功能</p>
<p>①数据库在建立、运行和维护时由数据库管理系统统一管理和控制</p>
<p>②<strong>保证数据的安全性、完整性</strong></p>
<p>③多用户对数据的并发使用及发生故障后的系统恢复</p>
<p>5.数据库的建立与维护功能</p>
<p>①<strong>数据库初始数据的输入和转换</strong></p>
<p>②<strong>数据库转储和恢复功能</strong></p>
<p>③<strong>数据库的重组织、性能监视和数据分析等</strong></p>
<p>6.其他功能</p>
<p>①<strong>数据库管理系统与网络中其它软件系统的通信</strong></p>
<p>②<strong>数据库管理系统系统之间或与文件系统的数据转换</strong></p>
<p>③<strong>异构数据库之间的互访和互操作</strong></p>
</blockquote>
<ul>
<li>数据库系统由数据库（DB）、数据库管理系统（及外围的应用开发工具）（DBMS）、应用程序与数据库管理员构成</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404162717301.png" alt="image-20250404162717301"></p>
<h3 id="数据模型">数据模型</h3>
<ul>
<li>定义</li>
</ul>
<blockquote>
<p>1.数据模型是对现实数据特征的抽象，是现实世界的模拟</p>
<p>2.数据模型是用来描述数据、组织数据和对数据进行操作的</p>
<p>3.数据模型应满足三方面要求</p>
<p>①能比较真实地模拟现实世界</p>
<p>②容易为人所理解</p>
<p>③便于在计算机上实现</p>
<p>4.数据模型是数据库系统的核心与基础</p>
</blockquote>
<ul>
<li>信息世界中的基本概念</li>
</ul>
<blockquote>
<p>1.信息世界的基本概念</p>
<p>①实体（entity）：客观存在并可相互区别的事物</p>
<p>②属性（attribute）：尸体所具有的某一特性，一个实体可以由若干属性刻画</p>
<p>③码（key）：唯一标识实体的属性集</p>
<p>④实体型（entity type）：用实体名及其属性名集合来抽象和刻画同类实体称为实体型</p>
<p>⑤实体集（entity set）：同一类型实体的集合</p>
<p>⑥联系（relationship）：现实世界中事物内部以及事物之间的联系在信息世界中反映为实体（型）内部的联系和实体（型）之间的联系</p>
<p>a.实体内部的联系指组成实体的各属性之间的联系</p>
<p>b.实体之间的联系指不同实体集之间的联系</p>
<p>c.实体之间的联系可以是一对一、一对多、多对多等</p>
</blockquote>
<ul>
<li>概念模型的一种表示方法：实体-联系模型（Entity-Relationship，简称E-R）</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404170719519.png" alt="image-20250404170719519"></p>
<blockquote>
<ul>
<li>
<p><strong>抽象了学校中的学生和课程两个客观事物：学生实体和课程实体</strong></p>
</li>
<li>
<p><strong>抽象了现实世界中事物之间的联系：</strong></p>
</li>
</ul>
<p><strong>一门课程可以有多名学生选修，一个学生可以选修多门课程</strong></p>
<p>用课程实体与学生实体多对多（m:n）联系来描述</p>
</blockquote>
<p><mark>数据建模的两个层次：概念模型与数据模型</mark></p>
<ul>
<li>概念模型：按照用户的观点对数据和信息建模</li>
<li>数据模型：按照计算机系统的观点对数据建模</li>
</ul>
<h4 id="数据模型的三要素：数据结构，数据操纵和完整性约束">数据模型的三要素：数据结构，数据操纵和完整性约束</h4>
<ul>
<li>数据结构：描述数据库的组成对象以及对象之间的关系</li>
</ul>
<blockquote>
<p>描述的内容：与对象的类型，内容和性质有关，也与数据之间的联系有关</p>
<p>数据结构是对系统静态特性的描述</p>
</blockquote>
<ul>
<li>数据操纵：对数据库中各种对象（型）的实例（值）允许执行的操作的集合，包括操作及有关的操作规则。主要操作是查询和更新（包括插入、删除和修改）。</li>
</ul>
<blockquote>
<p>数据模型必须定义：</p>
<p>①操作的确切含义</p>
<p>②操作符号</p>
<p>③操作规则（如优先级）</p>
<p>④实现操作的语言</p>
<p>数据操纵是对系统动态特性的描述</p>
</blockquote>
<ul>
<li>完整性约束是一组完整性规则（即给定的数据模型中数据及其联系所具有的制约和依存规则）。限定了符合数据模型的数据库状态以及状态的变化，以保证数据的正确、 有效和相容。</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404172215011.png" alt="image-20250404172215011"></p>
<h4 id="层次模型">层次模型</h4>
<p>层次模型是数据库系统中最早出现的数据库模型，用<mark>树形结构</mark>来表示各类实体以及实体间的联系</p>
<blockquote>
<p>①实体用记录表示</p>
<p>②实体的属性对应记录的数据项（或字段）</p>
<p>③实体之间的联系转换成记录之间的两两关系</p>
<p>④数据结构的单位是基本层次关系：指两个记录以及他们之间的一对多（包括一对一）的联系</p>
</blockquote>
<p>1.层次模型的数据结构</p>
<blockquote>
<p>①有且只有一个结点没有双亲结点，这个结点称为根节点</p>
<p>②根以外的其他结点有且只有一个双亲结点</p>
</blockquote>
<p>一个结点对应一个记录类型（实体），字段对应属性，联系对应连线</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404174542552.png" alt="image-20250404174542552"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404173242304.png" alt="image-20250404173242304"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404173301724.png" alt="image-20250404173301724"></p>
<p>2.层次模型的数据操纵与完整性约束</p>
<p>①数据操纵：查询，插入，删除，更新</p>
<p>②完整性约束：</p>
<p>a.无相应的双亲结点值就不能插入子女结点值</p>
<p>b.如果删除双亲结点值，则相应子女结点值也被同时删除</p>
<p>c.更新操作时，应更新所有相应记录，以保证数据一致性</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404174230327.png" alt="image-20250404174230327"></p>
<h4 id="网状模型">网状模型</h4>
<p>1.网状模型的数据结构</p>
<p>①定义</p>
<p>a.允许一个以上的结点无双亲</p>
<p>b.一个结点可以有多于一个双亲</p>
<p>②表示方法</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404174904185.png" alt="image-20250404174904185"></p>
<p>③网状模型与层次模型的区别</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404174938134.png" alt="image-20250404174938134"></p>
<p>2.网状模型的数据操纵与完整性约束</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404175138249.png" alt="image-20250404175138249"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404175152885.png" alt="image-20250404175152885"></p>
<h4 id="关系模型">关系模型</h4>
<p>1.关系模型中的数据结构</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404180404369.png" alt="image-20250404180404369"></p>
<p>①基本概念</p>
<blockquote>
<p>1.关系（relation）：一个关系对应一个表</p>
<p>2.元组（tuple）：表中的一行即为一个元组</p>
<p>3.属性（attribute）：表中的一列即为一个属性，给每个属性起一个名称即属性名</p>
<p>4.码（key）：又称码键或键。表中的某一个属性或一组属性，其值可以唯一确定一个元组。</p>
<p>5.域（domain）：是一组具有相同数据类型的值的集合。属性的取值范围来自某个域。</p>
<p>6.分量：元组中的一个属性值</p>
<p>7.关系模式：即对关系的描述（表头）。如关系名（属性1，属性2，…, 属性n），学生（学号，姓名，性别，出生日期，主修专业）</p>
</blockquote>
<p>②关系必须是规范化的，满足一定的规范条件。最基本的规范条件：关系的每一个分量必须是一个不可分的数据项，不允许表中还有表。如下图的联系方式是可分的数据项，不符合关系模型要求（也就是说每一个分量都是下面没有子标题的标题）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404182607053.png" alt="image-20250404182607053"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404182642359.png" alt="image-20250404182642359"></p>
<p>2.关系模型的数据操纵与完整性约束</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404183001650.png" alt="image-20250404183001650"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404183008482.png" alt="image-20250404183008482"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404183020132.png" alt="image-20250404183020132"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404183030136.png" alt="image-20250404183030136"></p>
<h3 id="数据库系统的三级模式结构">数据库系统的三级模式结构</h3>
<h4 id="数据库系统中模式的概念">数据库系统中模式的概念</h4>
<p>1.型和值的概念</p>
<p>①型type：对某一类数据的结构和属性的说明</p>
<p>②值value：是型的一个具体赋值</p>
<p>例如：学生记录（型，也就是表头）：（学号，姓名，性别，出生日期，主修专业）。一个记录值：（20180003，王敏，女，2001-8-1，计算机科学与技术）</p>
<p>2.模式（schema）</p>
<blockquote>
<p>①是数据库中全体数据的逻辑结构和特征的描述</p>
<p>②是型（表头）的描述，不涉及具体值</p>
<p>③反映的是数据结构及其联系</p>
<p>④模式是相对稳定的</p>
</blockquote>
<p>3.实例（instance）</p>
<p>①是模式的一个具体值</p>
<p>②反映数据库某一时刻的状态</p>
<p>③同一个模式可以有很多实例</p>
<p>④实例随数据库中数据的更新而变动</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404225033947.png" alt="image-20250404225033947"></p>
<h4 id="数据库系统的三级模式结构-2">数据库系统的三级模式结构</h4>
<p>数据库系统三级模式结构：模式、外模式和内模式</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404225239363.png" alt="image-20250404225239363"></p>
<p>1.模式（schema）</p>
<blockquote>
<p>1.内容（也称逻辑模式）</p>
<p>①是数据库中全体数据的逻辑结构和特征描述</p>
<p>②所有用户的公共数据视图</p>
<p><mark>一个数据库只有一个模式</mark></p>
<p>2.模式的地位：是数据库系统结构的中间层</p>
<p>①与数据的物理存储细节和硬件环境无关</p>
<p>②与具体的应用程序、开发工具和高级程序设计语言无关</p>
<p>3.模式的定义</p>
<p>①数据的逻辑结构（数据项的名字、类型、取值范围等）</p>
<p>②数据之间的联系</p>
<p>③数据有关的安全性、完整性要求</p>
<p><mark>数据库管理系统提供模式数据定义语言（模式DDL）严格定义模式</mark></p>
</blockquote>
<p>2.外模式（external schema）</p>
<blockquote>
<p>1.外模式定义（也称子模式或用户模式）</p>
<p>①数据库用户（包括应用程序员和最终用户）能够看见和使用的局部数据的逻辑结构和特征的描述</p>
<p>②数据库用户的数据视图，是与某一应用有关的数据的逻辑表示</p>
<p>2.一个数据库可以有多个外模式：不同在用户的应用需求、看待数据的方式、对数据保密的要求等方面存在差异</p>
<p>3.对模式中的同一数据，在外模式中的结构、类型、长度、保密级别等都可以不同</p>
<p>4.同一外模式也可以为某一用户的多个应用系统使用，但一个应用程序只能使用一个外模式</p>
<p>5.外模式的用途</p>
<p>①保证数据库安全性的一个有力措施</p>
<p>②每个用户只能看见和访问所对应的外模式中的数据</p>
<p>6.数据库管理系统提供外模式数据定义语言（外模式DDL）来严格定义外模式</p>
</blockquote>
<p>3.内模式（internal schema）（也称物理模式或存储模式）</p>
<blockquote>
<p>1.定义</p>
<p>①是数据物理结构和组织方式的描述</p>
<p>②是数据在数据库内部的表现方式</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250404231037023.png" alt="image-20250404231037023"></p>
</blockquote>
<h4 id="数据库的两级映像与数据独立性">数据库的两级映像与数据独立性</h4>
<p>三级模式是对数据的三个抽象级别，而两级映像在数据库管理系统内部实现三个抽象层次的联系和转换：外模式/模式映像，模式/内模式映像</p>
<p>1.外模式/模式映像</p>
<blockquote>
<p>1.模式：描述的是数据的全局逻辑结构</p>
<p>2.外模式：描述的是数据的局部逻辑结构</p>
<p>3.同一个模式可以有任意多个外模式</p>
<p>4.对于每一个外模式，数据库系统都有一个外模式/模式映像用于定义外模式与模式之间的对应关系</p>
<p>5.映像定义通常包含在各自外模式描述中</p>
<p>6.外模式/模式映像的作用是：保持数据的逻辑独立性</p>
<p>①当模式改变时，数据库管理员对外模式/模式映像作相应改变，使外模式保持不变</p>
<p>②应用程序是依据数据的外模式编写的，应用程序不必修改，保证了数据与程序的逻辑独立性，简称数据的逻辑独立性</p>
</blockquote>
<p>2.模式/内模式映像</p>
<blockquote>
<p>1.模式/内模式映像定义了数据全局逻辑结构与存储结构之间的对应关系</p>
<p>例如：说明逻辑记录和字段在内部是如何表示的</p>
<p>2.数据库中模式/内模式映像是唯一的</p>
<p>3.该映像定义通常包含在模式描述中</p>
<p>4.保证数据的物理独立性</p>
<p>①当数据库的存储结构改变时（例如选用了另一种存储结构），数据库管理员修改模式/内模式映像，使模式保持不变</p>
<p>②模式保持不变，应用程序不必改变。保证了数据与程序的物理独立性，简称数据的物理独立性</p>
</blockquote>
<h3 id="数据库系统的组成">数据库系统的组成</h3>
<p>数据库+数据库管理系统（及其应用开发工具）+应用系统+数据库管理员</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405002036004.png" alt="image-20250405002036004"></p>
<h2 id="关系模型-2">关系模型</h2>
<blockquote>
<p>重点</p>
<p>1.关系的完整性：实体完整性，参照完整性，用户定义完整性</p>
<p>2.什么是关系</p>
<p>3.关系的性质</p>
<p>4.实体标识符</p>
<p>5.码的定义</p>
<p>6.关系代数</p>
<p>4种传统集合运算：并，差，交，广义笛卡尔积</p>
<p>4种专门关系运算：选择，投影，连接（掌握），除</p>
<p>其中基本运算有5个：并，差，笛卡尔积，投影，选择</p>
<p>连接、外连接</p>
<p>会使用关系代数表达查询</p>
</blockquote>
<h3 id="关系模型的数据结构及形式化定义">关系模型的数据结构及形式化定义</h3>
<h4 id="关系">关系</h4>
<blockquote>
<p>1.单一的数据结构——关系：现实世界的实体以及实体间的各种联系均用关系来表示</p>
<p>2.逻辑结构——二维表：从用户角度，关系模型中数据的逻辑结构是一张二维表</p>
<p>3.分为域（domain）、笛卡尔积（Cartesian product）和关系（relation）</p>
<p>4.域（domain）</p>
<p>①域是一组具有相同数据类型的值的集合</p>
<p>例：整数，实数，介于某个取值范围的整数，长度小于25B的变长字符串集合，{男，女}……</p>
<p>5.笛卡尔积</p>
<p>①是所有域的所有取值的一个不能重复的组合</p>
<p>给定一组域D1，D2，…,Dn，允许其中某些域是相同的。笛卡尔积为：</p>
<p>$D_1\times D_2\times ... \times D_n={(d_1,d_2,...,d_n)\ |d_i\in D_i,\ i=1,2,...,\ n }$</p>
<p>②元组（tuple）：笛卡尔积中每一个元素$(d_1,d_2,...,d_n)$叫做一个n元组（n-tuple）或简称元组。如{a同学，计算机科学与技术，a老师}</p>
<p>③分组（Component）：笛卡尔积元素$(d_1,d_2,...,d_n)$中的每一个值$d_i$都叫做一个分量，如a同学，计算机科学与技术，a老师</p>
<p>④基数（cardinal number）：是一个域允许的不同取值个数。</p>
<p>若$D_i(i=1,2,...,n)$为有限集，其基数为$m_i(i=1,2,...,n)$，则$D_1\times D_2 \times ... \times D_n$的基数$M$为：$M=\prod_{i=1}^{n}m_i $</p>
<p>⑤笛卡尔积的表示方法：二维表，表中每行对应一个元组，每列对应一个域</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405015935005.png" alt="image-20250405015935005"><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405015951495.png" alt="image-20250405015951495"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405020015334.png" alt="image-20250405020015334"></p>
<p>6.关系</p>
<p>①关系模型中$D_1,D_2,...,D_n$的笛卡尔积一般没有实际语义，只有某个真子集才有实际含义</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405160947912.png" alt="image-20250405160947912"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405161034771.png" alt="image-20250405161034771"></p>
<p>②关系的定义：$D_1 \times D_2 \times ...\times D_n$的子集叫做在域$D_1,D_2,...,D_n$上的关系，表示为$R(D_1,D_2,...,D_n)$。其中R是关系名，n是关系的度</p>
<p>③元组：关系中的每个元素是关系中的元组，通常用t表示</p>
<p>④单元关系与二元关系</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405161325718.png" alt="image-20250405161325718"></p>
<p>⑤关系的表示：关系也是一个二维表，表的每行对应一个元组，表的每列对应一个域</p>
<p>⑥属性</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405161444475.png" alt="image-20250405161444475"></p>
<p>⑦三类关系</p>
<p>a.基本关系（基本表或基表）：实际存在的表，是实际存储数据的逻辑表示</p>
<p>b.查询结果：查询执行产生的结果对应的临时表</p>
<p>c.视图表：由基本表或其他视图表导出的虚表，不存储实际数据</p>
<p>⑧基本关系的性质</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405161951203.png" alt="image-20250405161951203"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405162010473.png" alt="image-20250405162010473"></p>
</blockquote>
<h4 id="关系模式">关系模式</h4>
<p>1.关系模式的定义：关系模式是对关系的描述，描述了关系元组集合的结构：如属性构成，属性来自的域，属性与域之间的映像关系；也描述了关系的完整性约束</p>
<p><mark>关系模式是型，关系是值</mark></p>
<p>2.定义关系模式</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405163551604-1743842152769-2.png" alt="image-20250405163551604"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405163713204.png" alt="image-20250405163713204"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405163813526.png" alt="image-20250405163813526"></p>
<p>3.关系模式中的各种码</p>
<p>（1）候选码（candidate key）：关系模式中的某一个属性或者一组属性的值能唯一地标识一个元组，而它的真子集不能唯一地标识一个元组，则称该属性或属性组为候选码。最简单的情况就是候选码只包含一个属性</p>
<p>（2）全码（all-key）：最极端的情况下关系模式中的所有属性才能成为这个关系模式的候选码，这个时候的码就是全码（all-key）</p>
<p>（3）主码：若一个关系有多个候选码，则选定其中一个为主码（primary key）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405164901862.png" alt="image-20250405164901862"></p>
<p>（4）主属性：候选码的主属性称为主属性（prime attribute）</p>
<p>相对应的不包含在任何候选码中的属性称为非主属性（non-prime attribute）或非码属性（non-key attribute）</p>
<p>4.关系模式与关系</p>
<p>①关系模式是对关系的描述，是静态的、稳定的</p>
<p>②关系是关系模式在某一时刻的状态或内容，是动态的、随时间不断变化的</p>
<p>③关系模式和关系往往笼统称为关系，具体由上下文区别</p>
<h4 id="关系数据库">关系数据库</h4>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405165521667.png" alt="image-20250405165521667"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405165748572.png" alt="image-20250405165748572"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405165811644.png" alt="image-20250405165811644"></p>
<h4 id="关系模型的存储结构">关系模型的存储结构</h4>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405165835910.png" alt="image-20250405165835910"></p>
<h3 id="关系操作">关系操作</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405165937879.png" alt="image-20250405165937879"></p>
<h3 id="关系的完整性">关系的完整性</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405170043731-1743843644613-4.png" alt="image-20250405170043731"></p>
<h4 id="实体完整性">实体完整性</h4>
<p>1.实体完整性规则（entity integrity）：若属性（指一个或一组属性）A是基本关系R的主属性，则A不能取空值（空值就是“不知道”不存在”或“无意义”的值）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405170410765.png" alt="image-20250405170410765"></p>
<p>2.实体完整性规则说明</p>
<p>①实体完整性规则是针对基本关系而言的。一个基本表通常对应现实世界的一个实体集</p>
<p>②现实世界中的实体是可区分的，具有某种唯一性标识</p>
<p>③关系模型中以主码作为唯一性标识</p>
<p>④主码中的属性不能去空值（如果取了空值，就说明存在某个不可标识的实体，即存在不可区分的实体，而②中实体是可区分的也就矛盾了，因此规则命名为实体完整性）</p>
<h4 id="参照完整性">参照完整性</h4>
<p>1.关系间的引用</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405170953706.png" alt="image-20250405170953706"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405171008694.png" alt="image-20250405171008694"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405171057891.png" alt="image-20250405171057891"></p>
<p>2.外码（foreign key）</p>
<p>①外码的定义：设F是基本关系R的一个或一组属性，但不是关系R的码，Ks是基本关系S的主码。如果F与Ks对应，则称F是R的外码</p>
<blockquote>
<p>基本关系R称为参照关系（referencing relation）</p>
<p>基本关系S称为被参照关系（referenced relation）或目标关系（target relation）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405171900690.png" alt="image-20250405171900690"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405172601006.png" alt="image-20250405172601006"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405172621179.png" alt="image-20250405172621179"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405172749648.png" alt="image-20250405172749648"></p>
<p><mark>总结</mark></p>
<blockquote>
<p>[!success]</p>
<p>1.参照关系、被参照关系就看谁是模仿者，谁是被模仿者，如果一个关系中某个属性模仿了另一个属性那ta既是参照关系也是非参照关系</p>
<p>2.外码不一定要与主码同名，也就是模仿者不一定啥都要跟被模仿者一样，比如先修课（模仿者，外码）和课程（被模仿者，主码）</p>
</blockquote>
</blockquote>
<p>3.参照完整性约束</p>
<p>若属性（或属性组）F是基本关系R的外码，ta与基本关系S的主码Ks相对于（R和S可以是同一个关系也可以是不同关系），则对于R中每个元组在F上的值必须为：空值（F的每个属性值均为空值）或等于S中某个元组的主码值</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405173850068.png" alt="image-20250405173850068"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405173900305.png" alt="image-20250405173900305"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405173957448.png" alt="image-20250405173957448"></p>
<h4 id="用户定义的完整性">用户定义的完整性</h4>
<p>1.定义：针对某一具体关系数据库的约束条件，反映某一具体应用所涉及的数据必须满足的语义要求。关系模型应提供定义和检验这类完整性的机制，以便用统一的系统的方法处理它们，而不需由应用程序承担这一功能。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405174436294.png" alt="image-20250405174436294"></p>
<h3 id="关系代数">关系代数</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405180148534-1743847309555-9.png" alt="image-20250405180148534"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405180208489.png" alt="image-20250405180208489"></p>
<h4 id="传统的集合运算">传统的集合运算</h4>
<p>1.并（union）</p>
<p>（1）R和S要求：具有相同的目n（即两个关系都有n个属性）且相应的属性取自同一个域</p>
<blockquote>
<p>属性与域的区别</p>
<ul>
<li><strong>属性</strong>：用于描述实体的特性 。比如学生实体，有学号、姓名、年龄等属性，体现学生不同方面特征。</li>
<li><strong>域</strong>：是属性的取值范围 。像学号属性，域可能是特定长度的数字集合；性别的域是 {男，女} 。</li>
</ul>
<p>简言之，属性是实体特征描述，域限定属性取值，是属性的规则。</p>
</blockquote>
<p>（2）$R \cup S$：仍为n目关系，由属于R或属于S的元组组成（取属于R或S的元组，且<strong>去重</strong>）。</p>
<p>​			$R \cup S={t \ | \ t \in R \ \vee t \in S}$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405181640903.png" alt="image-20250405181640903"></p>
<p>去重是以一行一行来看的，只有S第二行是R中没出现过的。</p>
<p>2.差（difference）</p>
<p>（1）R和S要求：具有相同的目n（即两个关系都有n个属性）且相应的属性取自同一个域</p>
<p>（2）$R - S$：仍为n目关系，由属于R或属于S的元组组成（去掉R中同属于R和S的元组）。</p>
<p>​			$R - S={t \ | \ t \in R \ \vee t \notin S}$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405182248727-1743848569474-12.png" alt="image-20250405182248727"></p>
<p>3.交（intersection）</p>
<p>（1）R和S要求：具有相同的目n（即两个关系都有n个属性）且相应的属性取自同一个域</p>
<p>（2）$R \cap S$：仍为n目关系，由属于R或属于S的元组组成（取R中同属于R和S的元组）。</p>
<p>​			$R \cap S={t \ | \ t \in R \ \wedge t \in S}$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405182511113.png" alt="image-20250405182511113"></p>
<p>4.（广义）笛卡尔积（（extended） Cartesian product）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405182825587.png" alt="image-20250405182825587"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405183007933.png" alt="image-20250405183007933"></p>
<h4 id="专门的关系运算">专门的关系运算</h4>
<p>1.记号的引入</p>
<p>（1）$R,t\in R,t[A_i]$</p>
<p>设关系模式$R(A_1,A_2,...,A_n)$</p>
<p>一个关系设为R，则$t\in R$表示$t$是R的一个元组，$t[A_i]$则表示元组t中相应于属性$A_i$的一个分量</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405183613324-1743849376124-19.png" alt="image-20250405183613324"></p>
<p>（2）$A,t[A],\overline{A}$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405183821828.png" alt="image-20250405183821828"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405183907897.png" alt="image-20250405183907897"></p>
<p>（3）$\widehat{t_r t_s} $</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405184442674.png" alt="image-20250405184442674"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405184515362.png" alt="image-20250405184515362"></p>
<p>（4）象集$Z_x$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405184546205.png" alt="image-20250405184546205"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250405184639198.png" alt="image-20250405184639198"></p>
<p>相当于已知一个分量，把另一个基于该分量得到的分量结果输出出来（相当于筛选）</p>
<p>2.选择</p>
<p>选择又称为限制（restriction）。在关系R中选择满足给定条件的诸元组$\sigma_F(R)={t\ |t \in R \ \wedge \ F(t)='真'}$</p>
<p>其中F是选择条件，是一个逻辑表达式，取值为‘真’或‘假’</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406170418856.png" alt="image-20250406170418856"></p>
<p>选择运算是从关系R中选取使逻辑表达式F为真的元组，是从行（hang）的角度进行的运算</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406171001965.png" alt="image-20250406171001965"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406170602656.png" alt="image-20250406170602656"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406170936471.png" alt="image-20250406170936471"></p>
<p>3.投影</p>
<p>从R中选择出若干属性组成新的关系$\pi_A(R)={t[A]\ | \ t \in \ R}$</p>
<p>A是R中的属性列。投影操作主要是从列的角度进行运算。投影之后不仅取消了原关系中的某些列，而且还可能取消某些元组（避免重复行）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406171449993.png" alt="image-20250406171449993"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406171506097.png" alt="image-20250406171506097"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406171527749.png" alt="image-20250406171527749"></p>
<p>4.连接（join）</p>
<p>①定义</p>
<blockquote>
<p>连接也成为θ连接，是从两个关系的笛卡尔积中选取属性间满足一定条件的元组</p>
<p>$R \mathbin{\underset{A \theta B}{\bowtie}} S = { \widehat{t_r t_s} \mid t_r \in R \land t_s \in S \land t_r[A] \mathbin{\theta} t_s[B] }$</p>
<p>A和B：分别为R和S上度数相等且可比的属性组，θ是比较运算符</p>
<p>连接运算从R和S的广义笛卡尔积中选取R关系在A属性组上的值与S关系在B属性组上的值满足比较关系θ的元组</p>
</blockquote>
<p>②两种常用连接运算</p>
<blockquote>
<p>1.等值连接（equijoin）</p>
<p>θ为“=”的连接运算称为等值连接。从关系R与S的广义笛卡尔积中选取A、B属性值相等的那些元组，即等值连接为$R \mathbin{\underset{A = B}{\bowtie}} S = { \widehat{t_r t_s} \mid t_r \in R \land t_s \in S \land t_r[A] = t_s[B] }$</p>
<p>2.自然连接（natural join）</p>
<p>自然连接是一种特殊等值连接，两个关系中进行比较的分量必须是同名的属性组，同时在结果中把重复的属性列去掉。其含义为：R和S具有相同的属性组B，U为R和S的全体属性集合。</p>
<p>$R \mathbin{\Join} S = { \widehat{t_r t_s}[U - B] \mid t_r \in R \land t_s \in S \land t_r[B] = t_s[B] }$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406173740728.png" alt="image-20250406173740728"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406173859636.png" alt="image-20250406173859636"></p>
<p>S第五行b2 2应为b5 2</p>
<p><mark>连接理由：保证C的值小于E</mark></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406173905848.png" alt="image-20250406173905848"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406173952683.png" alt="image-20250406173952683"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406174028759.png" alt="image-20250406174028759"></p>
<p><mark>也就是进一步把B列合并而不分R.B和S.B</mark></p>
</blockquote>
<p>③悬浮元组（dangling tuple）</p>
<p>两个关系R和S在做自然连接时，关系R中某些元组有可能在S中不存在公共属性上值相等的元组，从而造成R中这些元组在操作上被舍弃了，这些被舍弃的元组称为悬浮元组。</p>
<p>④外连接（outer join）</p>
<p>如果把悬浮元组也保存在结果关系中，其他属性上填空值（NULL），就叫做外连接。</p>
<p>左外连接（left outer join或left join）：只保留左边关系R中的悬浮元组</p>
<p>右外连接（right outer join或right join）：只保留右边关系S中的悬浮元组</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406180321425.png" alt="image-20250406180321425"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406180514123.png" alt="image-20250406180514123"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406181040664.png" alt="image-20250406181040664"></p>
<p><mark>也就是外连接强行合并同类项并把所有信息聚合起来，左外连接就是只根据左边的合并同类项聚合信息而不管左边没有右边有的信息</mark></p>
<p>5.除</p>
<p>给定关系R(X,Y)和S(Y,Z)，其中X，Y，Z为属性组。R中的Y与S中的Y可以有不同属性名，但必须出自相同的域集.R与S的除运算得到一个新的关系P（X），P是R中满足下列条件的元组在X属性列上的投影：元组在X上分量值x的象集Yx包含在S在Y上投影的集合，记作：</p>
<p>$R /  S = { t_r[X] \mid t_r \in R \land \pi_Y(S) \subseteq Y_x }$</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406182029068.png" alt="image-20250406182029068"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406182039779.png" alt="image-20250406182039779"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406182434308.png" alt="image-20250406182434308"></p>
<blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406182535168.png" alt="image-20250406182535168"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406182557789.png" alt="image-20250406182557789"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406182651245.png" alt="image-20250406182651245"></p>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406183042122-1743935443016-1.png" alt="image-20250406183042122"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406183117876.png" alt="image-20250406183117876"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250406183130102.png" alt="image-20250406183130102"></p>
<h2 id="SQL语言">SQL语言</h2>
<blockquote>
<p>重点</p>
<p>1.数据定义：基本表的定义（完整性约束的定义）、修改和删除、索引的建立和删除、索引建立的目的</p>
<p>2.数据更新：insert，delete，update</p>
<p>3.视图：create view</p>
<p>4.查询：写SQL语句及理解其含义</p>
<p>①单表查询：distinct，between and,in，like，is，and，or，order，集函数，group by，having的用法</p>
<p>②连接查询：max，min，average，sum，count</p>
<p>③嵌套查询：带in谓词的，带比较运算符的</p>
</blockquote>
<h3 id="SQL概述">SQL概述</h3>
<p>1.SQL（Structured Query Language）结构化查询语言，是关系数据库的标准语言，包括数据查询、数据库模式创建、数据库数据的增删改、数据库安全性和完整性定义与控制等</p>
<p>2.SQL的特点</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250409175029242.png" alt="image-20250409175029242"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250409175040422.png" alt="image-20250409175040422"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250409175051346.png" alt="image-20250409175051346"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250409175111969.png" alt="image-20250409175111969"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410000142106.png" alt="image-20250410000142106"></p>
<p>3.SQL的基本概念</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410000432437.png" alt="image-20250410000432437"></p>
<p>（1）基本表的概念</p>
<p>①是本身独立存在的表</p>
<p>②关系数据库管理系统中一个关系就对应一个基本表</p>
<p>③一个或多个基本表对应一个存储文件</p>
<p>④一个表可以带若干索引</p>
<p>（2）存储文件</p>
<p>①逻辑结构和物理结构组成了关系数据库的内模式</p>
<p>②物理文件结构是由数据库管理系统设计决定的</p>
<p>（3）视图</p>
<p>①是从基本表或其他视图中导出的表</p>
<p>②数据库中只存放视图的定义而不存放视图对应的视图</p>
<p>③视图是一个虚表</p>
<p>④用户可以在视图上再定义视图</p>
<h3 id="数据定义">数据定义</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410001314151.png" alt="image-20250410001314151"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410001414817.png" alt="image-20250410001414817"></p>
<p>1.模式的定义与删除</p>
<p>（1）定义模式</p>
<p>①为用户WANG定义一个“学生选课”模式S-C-SC</p>
<p><code>create schema &quot;S-C-SC&quot; authorization WANG;</code></p>
<p>②下面的模式没有指定模式名，模式名隐含为用户名WANG</p>
<p><code>create schema autorization WANG;</code></p>
<p>（2）定义模式实际上定义了一个<mark>命名空间</mark>。在这个空间中可以定义该模式包含的数据库对象，例如基本表，视图，索引等。也就是说</p>
<p>在<code>create schema</code>中可以接受<code>create table,create view和grant</code>子句</p>
<p><code>create schema &lt;模式名&gt; authorization &lt;用户名&gt;[&lt;表定义子句&gt;|&lt;视图定义子句&gt;|&lt;授权定义子句&gt;]</code></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410002103901.png" alt="image-20250410002103901"></p>
<p><code>create chema Test authorization Zhang</code></p>
<p><code>create table Tab1(Col1 smallint,</code></p>
<p><code>                   Col2 int,</code></p>
<p><code>                   Col3 char(20)</code></p>
<p><code>                   Col4 numeric(10,3)</code></p>
<p><code>                   Col5 decimal(5,2)</code></p>
<p><code>                   );</code></p>
<p>（2）删除模式</p>
<p><code>drop schema &lt;模式名&gt; &lt;cascade|restrict&gt;</code></p>
<p>①cascade（级联）——连坐</p>
<p>删除模式的同时把该模式中所有的数据库对象全部删除。</p>
<p>②restrict（限制）</p>
<p>如果该模式中定义了数据库对象（如表、视图等），则拒绝该删除语句的执行。</p>
<blockquote>
<p>仅当该模式中没有任何下属的对象时才能执行</p>
</blockquote>
<p>例3.4 <code>drop schema Test cascade;</code></p>
<p>删除模式Test，同时该模式中定义的表Tab1也被删除</p>
<p>2.基本表的定义、删除和修改</p>
<p>（1）定义基本表</p>
<p><code>create table &lt;表名&gt;</code></p>
<p><code>(&lt;列名&gt;&lt;数据类型&gt;[&lt;列级完整性约束&gt;])</code></p>
<p><code>[,&lt;列名&gt;&lt;数据类型&gt;[&lt;列级完整性约束&gt;]]</code></p>
<p><code>...</code></p>
<p><code>[,&lt;表级完整性约束&gt;]);</code></p>
<p>&lt;表名&gt;：所要定义的基本表的名字。</p>
<p>&lt;列名&gt;：组成该表的各个属性（列）。</p>
<p>&lt;列级完整性约束&gt;：涉及相应属性列的完整性约束。</p>
<p>&lt;表级完整性约束&gt;：涉及一个或多个属性列的完整性约束。</p>
<p>如果完整性约束涉及该表的多个属性列，则必须定义在表级上，否则可以定义在列级也可以定义在表级</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005115914.png" alt="image-20250410005115914"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005145247.png" alt="image-20250410005145247"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005205364.png" alt="image-20250410005205364"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005236240.png" alt="image-20250410005236240"></p>
<p>（2）数据类型</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005656210.png" alt="image-20250410005656210"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005733716-1744217854547-1.png" alt="image-20250410005733716"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410005753009-1744217874069-3.png" alt="image-20250410005753009"></p>
<p>（3）模式与表</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410010344197.png" alt="image-20250410010344197"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410010406573.png" alt="image-20250410010406573"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410010550322.png" alt="image-20250410010550322"></p>
<p>（4）修改基本表</p>
<p><code>alter table &lt;表名&gt;</code></p>
<p><code>[add [column] &lt;新列名&gt; &lt;数据类型&gt; [完整性约束]]</code></p>
<p><code>[add &lt;表级完整性约束&gt;]</code></p>
<p><code>[drop [column] &lt;列名&gt; [cascade|restrict]]</code></p>
<p><code>[drop constraint &lt;完整性约束名&gt; [restrict|cascade]]</code></p>
<p><code>rename column &lt;列名&gt; to &lt;新列名&gt;</code></p>
<p><code>[alter column &lt;列名&gt; type &lt;数据类型&gt;];</code></p>
<blockquote>
<p>1.&lt;表名&gt;是要修改的基本表，</p>
<p>2.add子句用于增加新列、新的列级完整性约束和新的表级完整性约束</p>
<p>3.drop column子句用于删除表中的列</p>
<p>①如果指定了cascade短语，则自动删除引用了该列的其他对象</p>
<p>②如果指定了restrict短语，则如果该列被其它对象引用，关系数据库系统将拒绝删除该列</p>
<p>4.drop constraint子句用于删除指定的完整性约束</p>
<p>5.rename column子句用于修改列名</p>
<p>6.alter column子句用于修改列的数据类型</p>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410012810819.png" alt="image-20250410012810819"></p>
<p><code>alter（改变） table Student add Semail varchar(30);</code></p>
<blockquote>
<p>不论基本表中原来是否已有数据，新增加的列一律为空值</p>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410013222456.png" alt="image-20250410013222456"></p>
<p><code>alter table Student alter column Sbirthdate type varchar(20);</code></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410013308722.png" alt="image-20250410013308722"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410013325353.png" alt="image-20250410013325353"></p>
<p><code>alter table Course add unique(Cname);</code></p>
<p>（5）删除基本表</p>
<p><code>drop table &lt;表名&gt; [restrict|cascade]</code></p>
<blockquote>
<p>restrict：删除表是有限制的</p>
<ul>
<li>想要删除的基本表不能被其他表的约束所引用。如果存在依赖该表的对象，则此表不能被删除</li>
</ul>
<p>cascade：删除该表没有限制</p>
<ul>
<li>在删除表的同时，相关依赖对象也一并删除</li>
</ul>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410013757166-1744220278095-5.png" alt="image-20250410013757166"></p>
<p><code>drop table Student cascade</code></p>
<blockquote>
<ul>
<li>基本表定义被删除，数据被删除</li>
<li>表上建立的索引，视图，触发器等一般也被删除</li>
</ul>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410015012345.png" alt="image-20250410015012345"></p>
<p><code>create view CS_Student</code>/<em>Student表上建立计科学生视图</em>/</p>
<p><code>as</code></p>
<p><code>select Sno,Sname,Ssex,Sbirthdate,Smajor</code></p>
<p><code>from Student</code></p>
<p><code>where Smajor='计算机科学与技术'</code></p>
<p><code>drop table Student restrict;</code>/<em>删除Student表</em>/</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410015259123.png" alt="image-20250410015259123"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410015320408.png" alt="image-20250410015320408"></p>
<p>3.索引的建立与删除</p>
<p>（1）建立索引目的：加快查询速度</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410015419701.png" alt="image-20250410015419701"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410015430980.png" alt="image-20250410015430980"></p>
<p>（1）建立索引</p>
<p><code>create [unique] [cluster] index &lt;索引名&gt; on &lt;表名&gt;（&lt;列名&gt;[&lt;次序&gt;][，&lt;列名&gt;[&lt;次序&gt;]]...）;</code></p>
<blockquote>
<p>&lt;表名&gt;：要建立索引的基本表的名字</p>
<p>索引：可以建立在该表的一列或多列上，各列明之间用逗号分隔</p>
<p>&lt;次序&gt;：指定索引值的排列顺序，升序asc，降序desc，默认升序asc</p>
<p>unique：此索引的每一个索引值只对应唯一的数据记录</p>
<p>cluster：建立的索引是聚簇索引</p>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410020918720.png" alt="image-20250410020918720"></p>
<p><code>create unique index Idx_StuSname on Student(Sname);</code></p>
<p>/<em>保证了Sname取唯一值的约束</em>****/</p>
<p><code>create unique index Idx_CouCname on Course(Cname);</code></p>
<p>/*加上Cname取唯一值的约束*****/</p>
<p><code>create unique index Idx_SCCno on sc(Sno asc,Cno desc);</code></p>
<p>（2）修改索引</p>
<p><code>alter index &lt;旧索引名&gt; rename to &lt;新索引名&gt;</code></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410021305935.png" alt="image-20250410021305935"></p>
<p><code>alter index Idx_SCCno rename to Idx_SCSnoCno;</code></p>
<p>（3）删除索引</p>
<p><code>drop index &lt;索引名&gt;</code></p>
<p><strong>删除索引时，系统会从数据字典中删去有关该索引的描述</strong></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410021423470.png" alt="image-20250410021423470"></p>
<p>（4）数据字典</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410021449305.png" alt="image-20250410021449305"></p>
<h3 id="数据查询">数据查询</h3>
<h2 id="数据库安全性">数据库安全性</h2>
<blockquote>
<p>重点</p>
<p>1.数据库安全性是指保护数据库，防止因用户非法使用数据库造成数据泄露、更改或破坏。作用是防止恶意的破坏和非法的存取</p>
<p>2.视图的作用</p>
<p>3.GRANT和REVOKE</p>
</blockquote>
<h2 id="数据库完整性">数据库完整性</h2>
<blockquote>
<p>重点</p>
<p>1.数据库完整性是指数据的正确性和相容性；数据库的完整性是为了防止数据库中存在不符合语义的数据，也就是防止数据库中存在不正确的数据</p>
<p>2.如何定义几种完整性</p>
<p>3.插入、更新、修改能否被执行的判断</p>
</blockquote>
<h2 id="关系数据理论">关系数据理论</h2>
<blockquote>
<p>重点</p>
<p>1.1NF的关系模式存在的问题：插入异常，删除异常，数据冗余度高，修改异常（复杂）</p>
<p>2.函数依赖，能写出函数依赖</p>
<p>3.<mark>规范化理论</mark></p>
<p>4.范式（各级范式的特征，会求范式级别）</p>
<p>5.属性集关于函数依赖集的闭包</p>
<p>6.码的求解方法</p>
</blockquote>
<h2 id="数据库设计">数据库设计</h2>
<blockquote>
<p>重点</p>
<p>1.六个步骤</p>
<p>①需求分析：数据字典及其内容</p>
<p>②概念结构设计</p>
<p>③逻辑结构设计：E-R图转换成关系模型</p>
<p>④数据库物理设计</p>
<p>⑤数据库实施</p>
<p>⑥数据库运行与维护：重组织与重构造</p>
<p><mark>2.概念设计中E-R模型设计方法（掌握）</mark></p>
<p><mark>3.逻辑设计中E-R模型向关系模型的转换方法（重点掌握）</mark></p>
</blockquote>
<h3 id="数据库设计概述">数据库设计概述</h3>
<p>1.数据库设计定义：是指对于一个给定的应用环境，构造优化的数据库<mark>逻辑模式</mark>和<mark>物理结构</mark>，并据此建立数据库及其应用系统，是指能够有效存储管理数据，满足各种用户的应用需求，包括信息管理要求和数据操作要求。</p>
<blockquote>
<p>信息管理要求：在数据库中应该存储和管理哪些数据对象</p>
<p>数据操作要求：对数据对象需要进行哪些操作，如查询、增加、删除、修改、统计和分析等操作</p>
</blockquote>
<p>2.数据库设计目标：是为用户和各种应用系统提供一个信息基础设施和高效的运行环境</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410173525324.png" alt="image-20250410173525324"></p>
<p>3.数据库设计的特点</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410173541637.png" alt="image-20250410173541637"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410173601241.png" alt="image-20250410173601241"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410173631717.png" alt="image-20250410173631717"></p>
<p>4.数据库设计的方法</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410173720154.png" alt="image-20250410173720154"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410173730325.png" alt="image-20250410173730325"></p>
<p>5.数据库设计的基本步骤</p>
<blockquote>
<p><mark>六个阶段</mark></p>
<p>1.需求分析</p>
<p>2.概念结构设计</p>
<p>3.逻辑结构设计</p>
<p>4.物理结构设计</p>
<p>5.数据库实施</p>
<p>6.数据库运行与维护</p>
</blockquote>
<blockquote>
<ul>
<li>需求分析和概念设计独立于任何数据库管理系统</li>
<li>逻辑设计和物理设计与选用的数据库管理系统密切相关</li>
</ul>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410174213489.png" alt="image-20250410174213489"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410174437433.png" alt="image-20250410174437433"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410174448375.png" alt="image-20250410174448375"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410174509029.png" alt="image-20250410174509029"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410174536299.png" alt="image-20250410174536299"></p>
<h3 id="需求分析">需求分析</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410175038255.png" alt="image-20250410175038255"></p>
<p>1.数据字典</p>
<p>（1）数据字典是关于数据库中数据的描述，即元数据，不是数据本身</p>
<p>（2）数据字典在需求分析阶段建立，在数据库设计过程中不断修改、充实、完善</p>
<p>（3）数据字典是进行详细的数据收集和数据分析所获得的主要成果</p>
<p>（4）数据字典的内容</p>
<p>①数据项</p>
<p>②数据结构</p>
<p>③数据流</p>
<p>④数据存储</p>
<p>⑤处理过程</p>
<p>（5）数据项是数据的最小组成单位</p>
<p>（6）若干个数据项可以组成一个数据结构</p>
<p>（7）数据字典通过对数据项和数据结构的定义来描述数据流、数据存储的逻辑内容</p>
<p>2.数据项</p>
<p>（1）数据项是不可再分的数据单位</p>
<p>（2）对数据项的描述</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410175538252.png" alt="image-20250410175538252"></p>
<p>3.数据结构</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410175606172.png" alt="image-20250410175606172"></p>
<p>4.数据流</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410175908297.png" alt="image-20250410175908297"></p>
<p>5.数据存储</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180018091.png" alt="image-20250410180018091"></p>
<p>6.处理过程</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180037245.png" alt="image-20250410180037245"></p>
<h3 id="概念结构设计">概念结构设计</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180443644.png" alt="image-20250410180443644"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180458614.png" alt="image-20250410180458614"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180518302.png" alt="image-20250410180518302"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180543655.png" alt="image-20250410180543655"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180556728.png" alt="image-20250410180556728"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180611694.png" alt="image-20250410180611694"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180839689.png" alt="image-20250410180839689"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410180929727.png" alt="image-20250410180929727"></p>
<p><mark>实体型就是具体的一个个主体，联系/关系就是要执行的事件</mark></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181251730-1744279972524-1.png" alt="image-20250410181251730"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181307306.png" alt="image-20250410181307306"></p>
<blockquote>
<p><mark>E-R图</mark>：提供了表示实体型、属性和联系的方法</p>
<p>（1）实体型：用矩形表示，矩形框内写明实体名</p>
<p>（2）属性：用椭圆形表示，并用无向边将其与相应的实体型连接起来</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181459893.png" alt="image-20250410181459893"></p>
<p>（3）联系：用菱形表示，菱形框内写明联系名，并用无向边分别与有关实体型连接起来，同时在无向边旁标上联系的类型（1：1，1：n或m：n）</p>
<ul>
<li>联系可以具有属性</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181726356.png" alt="image-20250410181726356"></p>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181752329.png" alt="image-20250410181752329"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181812575.png" alt="image-20250410181812575"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181826760.png" alt="image-20250410181826760"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181843432-1744280323897-3.png" alt="image-20250410181843432"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410181903772-1744280344218-5.png" alt="image-20250410181903772"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182306654.png" alt="image-20250410182306654"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182452409.png" alt="image-20250410182452409"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182522589.png" alt="image-20250410182522589"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182802410.png" alt="image-20250410182802410"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182830736.png" alt="image-20250410182830736"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182844357.png" alt="image-20250410182844357"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182857171.png" alt="image-20250410182857171"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182915963.png" alt="image-20250410182915963"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182929350.png" alt="image-20250410182929350"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410182943908.png" alt="image-20250410182943908"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410183139220.png" alt="image-20250410183139220"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410183157549-1744281118293-7.png" alt="image-20250410183157549"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410193207482.png" alt="image-20250410193207482"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410193304086.png" alt="image-20250410193304086"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410193437629.png" alt="image-20250410193437629"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410193515852-1744284916784-11.png" alt="image-20250410193515852"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410193535211.png" alt="image-20250410193535211"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410193609199-1744284970353-13.png" alt="image-20250410193609199"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410194055648.png" alt="image-20250410194055648"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410194118303.png" alt="image-20250410194118303"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410194159810.png" alt="image-20250410194159810"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410194211064.png" alt="image-20250410194211064"></p>
<h3 id="逻辑结构设计">逻辑结构设计</h3>
<p>1.E-R图向关系模型的转换</p>
<blockquote>
<p>（1）转换内容</p>
<p>①E-R图由实体型，实体的属性和实体型之间的联系三个要素组成</p>
<p>②关系模型的逻辑结构也是一组关系模式的集合</p>
<p>③将E-R图转换为关系模型：将实体型、实体的属性和实体型之间的联系转化为关系模式</p>
<p>（2）转换原则</p>
<p>①一个实体型转换为一个关系模式：关系的属性&lt;-实体的属性，关系的码&lt;-实体的码</p>
<p>②实体型间的联系</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195145797.png" alt="image-20250410195145797"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195222766.png" alt="image-20250410195222766"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195239542.png" alt="image-20250410195239542"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195350974.png" alt="image-20250410195350974"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195734365-1744286255456-15.png" alt="image-20250410195734365"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195819274-1744286300236-17.png" alt="image-20250410195819274"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195846700.png" alt="image-20250410195846700"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410195943041.png" alt="image-20250410195943041"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410200009109.png" alt="image-20250410200009109"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410200040076.png" alt="image-20250410200040076"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410200133562.png" alt="image-20250410200133562"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410200153466.png" alt="image-20250410200153466"></p>
</blockquote>
<h3 id="数据库物理设计">数据库物理设计</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/image-20250410201102658.png" alt="image-20250410201102658"></p>
<p><mark>数据库管理系统常用存取办法</mark></p>
<p>B+树索引、哈希索引、聚簇的存取方法</p>
<h2 id="（第十一章）数据库恢复技术">（第十一章）数据库恢复技术</h2>
<blockquote>
<p>重点</p>
<p>1.事务概念及ACID特性、commit rollback</p>
<p>2.几种故障，各自恢复方法</p>
<p>3.数据转储和日志文件，登记日志文件原则</p>
<p>4.事务日志</p>
</blockquote>
<h2 id="（第十二章）并发控制">（第十二章）并发控制</h2>
<blockquote>
<p>重点</p>
<p>1.并发控制：就是用正确的方法调度并发操作，以免不同事务的执行会互相带来干扰</p>
<p>2.主要解决那四类数据不一致的问题</p>
<p>3.事务隔离级别</p>
<p>4.并发控制基本方法：封锁（何为封锁）：X锁，S锁</p>
<p>5.封锁协议及其作用</p>
<p>6.死锁</p>
<p>7.两段锁协议</p>
</blockquote>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>算法基础课(持续更新中)</title>
    <url>/algorithm_learning/</url>
    <content><![CDATA[<p>算法基础课</p>
<h2 id="基础算法（一）">基础算法（一）</h2>
<h3 id="快速排序">快速排序</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> i=l<span class="number">-1</span>,j=r<span class="number">+1</span>,x=q[l+r&gt;&gt;<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;j)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">do</span> i++;<span class="keyword">while</span>(q[i]&lt;x);</span><br><span class="line">        <span class="keyword">do</span> j--;<span class="keyword">while</span>(q[j]&gt;x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">quick_sort</span>(q,l,j);</span><br><span class="line">    <span class="built_in">quick_sort</span>(q,j<span class="number">+1</span>,r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="归并排序">归并排序</h3>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> mid=l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">merge_sort</span>(q,l,mid),<span class="built_in">merge_sort</span>(q,mid<span class="number">+1</span>,r)；</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> k=<span class="number">0</span>,i=l,j=mid<span class="number">+1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid&amp;&amp;j&lt;=r)</span><br><span class="line">        <span class="keyword">if</span>(q[i]&lt;=q[j])tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">else</span> tmp[k++]=q[j++];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid)tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">while</span>(j&lt;=r)tmp[k++]=q[j++];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i=l,j=<span class="number">0</span>;i&lt;=r;i++,j++)q[i]=tmp[j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="二分">二分</h3>
<h4 id="整数二分">整数二分</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_1</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;    <span class="comment">// check()判断mid是否满足性质</span></span><br><span class="line">        <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_2</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) l = mid;</span><br><span class="line">        <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="浮点数二分">浮点数二分</h4>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">double</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">bsearch_3</span><span class="params">(<span class="type">double</span> l, <span class="type">double</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> eps = <span class="number">1e-6</span>;   <span class="comment">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class="line">    <span class="keyword">while</span> (r - l &gt; eps)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">double</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;</span><br><span class="line">        <span class="keyword">else</span> l = mid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="基础算法（二）">基础算法（二）</h2>
<h3 id="高精度">高精度</h3>
<h4 id="高精度加法">高精度加法</h4>
<p>本质上是模拟人进行列竖式加法的过程，即$a_1$+…+$a_n$+t，其中当$a$的退一位加起来不超过10则$t=0$否则$t=1$，以此类推</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt;<span class="built_in">add</span>(vector&lt;<span class="type">int</span>&gt;&amp;A,vector&lt;<span class="type">int</span>&gt;&amp;B)</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;C;</span><br><span class="line">    <span class="type">int</span> t=<span class="number">0</span>;<span class="comment">//存储每一位相加后的数，同时完成是否需要进位的考量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>()||i&lt;B.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i&lt;A.<span class="built_in">size</span>())t+=A[i];</span><br><span class="line">        <span class="keyword">if</span>(i&lt;B.<span class="built_in">size</span>())t+=B[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>(t%<span class="number">10</span>);</span><br><span class="line">        t/=<span class="number">10</span>;</span><br><span class="line">	&#125;    </span><br><span class="line">    <span class="keyword">if</span>(t)C.<span class="built_in">push_back</span>(<span class="number">1</span>);<span class="comment">//最后一个t代表最高位，因此等for循环运行完后再用判断看看进位情况，是的话就在vector后面再加一个1</span></span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string a,b;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;A,B;</span><br><span class="line">    cin&gt;&gt;a&gt;&gt;b;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=a.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)A.<span class="built_in">push_back</span>(a[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=b.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)B.<span class="built_in">push_back</span>(b[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line"><span class="comment">//A、B元素进栈都是逆序进栈，比如a=&#x27;123456&#x27;,那A=[6.5,4,3,2,1],目的是方便进位多了1的时候整体数组挪动空间不大，否则要是最高位在前边ta进位后面数组所有数字都要往后挪</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">auto</span> C=<span class="built_in">add</span>(A,B);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=C.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)cout&lt;&lt;C[i];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="vector相关知识点">vector相关知识点</h5>
<ul>
<li>
<p><strong>vector定义</strong> <code>vector</code> 是 C++ 标准模板库（STL）中的一个容器，它可以看作是一个动态数组。它能存储一系列相同类型的元素，并且可以根据需要自动调整大小。使用 <code>vector</code> 需要包含 <code>&lt;vector&gt;</code> 头文件。</p>
</li>
<li>
<p><strong>vector数组相关操作</strong>（最后面）增加用<code>.push_back(一个数)</code>，（最后面）删除<code>.pop_back(一个数)</code>，指定位置插入<code>.insert(向量,指定位置)</code>，指定位置删除<code>.erase(向量，指定位置)</code>，清空<code>.clear()</code>，查看大小<code>.size()</code></p>
</li>
<li>
<p><strong>vector元素的索引和数组一样，也是从 0 开始存储的</strong></p>
</li>
<li>
<p><strong>vector和普通数组区别</strong></p>
<p>1.大小灵活性</p>
<ul>
<li><strong>普通数组</strong>：大小在定义时就必须确定，且在程序运行过程中不能改变。例如 <code>int arr[10];</code> 定义了一个大小为 10 的整数数组，之后无法再改变其大小。</li>
<li><strong>vector</strong>：可以动态调整大小。可以使用 <code>push_back()</code> 方法在末尾添加元素，当空间不足时，<code>vector</code> 会自动分配更大的内存空间来存储元素。</li>
</ul>
<p>2.内存管理</p>
<ul>
<li><strong>普通数组</strong>：由程序员手动管理内存。如果数组在栈上分配，其生命周期受限于所在的代码块；如果在堆上分配（使用 <code>new</code>），则需要手动使用 <code>delete</code> 释放内存，否则会造成内存泄漏。</li>
<li><strong>vector</strong>：自动管理内存。当 <code>vector</code> 不再使用时，其占用的内存会自动释放，无需程序员手动干预。</li>
</ul>
<p>3.功能丰富度</p>
<ul>
<li><strong>普通数组</strong>：只提供基本的元素访问功能，操作相对有限。</li>
<li><strong>vector</strong>：提供了丰富的成员函数，如 <code>size()</code> 获取元素数量、<code>empty()</code> 判断是否为空、<code>clear()</code> 清空元素等，使用起来更加方便。</li>
</ul>
</li>
<li>
<p><strong>使用 vector 而不用数组的原因</strong></p>
<p>在大整数相加的场景中，输入的数字长度是不确定的。如果使用普通数组，需要预先定义一个足够大的数组来存储数字的每一位，但这样可能会浪费大量的内存空间。而 <code>vector</code> 可以根据输入数字的实际长度动态调整大小，避免了内存的浪费，并且其提供的 <code>push_back()</code> 方法可以方便地将数字的每一位添加到容器中，同时在处理进位时也更加方便。因此，使用 <code>vector</code> 能更好地适应这种动态长度的需求。</p>
</li>
</ul>
<h5 id="add函数运行示例">add函数运行示例</h5>
<p>假设我们要计算两个四位数 <code>a = 1234</code> 和 <code>b = 5678</code> 的和。</p>
<p>在代码中，输入的数字以字符串形式存储，然后将其逆序存储到 <code>vector&lt;int&gt;</code> 中，这样做是为了方便处理进位。对于 <code>a = 1234</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>A = &#123;4, 3, 2, 1&#125;</code>；对于 <code>b = 5678</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>B = &#123;8, 7, 6, 5&#125;</code>。</p>
<p>具体步骤：</p>
<ul>
<li>
<p>[x] 初始化</p>
</li>
<li>
<p><code>vector&lt;int&gt; C;</code>：用于存储相加结果的向量，初始为空。</p>
</li>
<li>
<p><code>int t = 0;</code>：用于存储每一位相加后的结果以及进位信息，初始值为 0。</p>
</li>
<li>
<p>[x] 循环处理每一位</p>
</li>
</ul>
<p>循环条件为 <code>i &lt; A.size() || i &lt; B.size()</code>，这意味着只要 <code>A</code> 或 <code>B</code> 还有未处理的位，就会继续循环。</p>
<ul>
<li>
<p>[ ] 第一次循环（<code>i = 0</code>）</p>
</li>
<li>
<p><code>if(i &lt; A.size()) t += A[i];</code>：因为 <code>i = 0</code> 小于 <code>A.size()</code>（4），所以 <code>t = t + A[0] = 0 + 4 = 4</code>。</p>
</li>
<li>
<p><code>if(i &lt; B.size()) t += B[i];</code>：因为 <code>i = 0</code> 小于 <code>B.size()</code>（4），所以 <code>t = t + B[0] = 4 + 8 = 12</code>。</p>
</li>
<li>
<p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 2）添加到 <code>C</code> 中，此时 <code>C = &#123;2&#125;</code>。</p>
</li>
<li>
<p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li>
<p>[ ] 第二次循环（<code>i = 1</code>）</p>
</li>
<li>
<p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[1] = 1 + 3 = 4</code>。</p>
</li>
<li>
<p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[1] = 4 + 7 = 11</code>。</p>
</li>
<li>
<p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 1）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1&#125;</code>。</p>
</li>
<li>
<p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li>
<p>[ ] 第三次循环（<code>i = 2</code>）</p>
</li>
<li>
<p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[2] = 1 + 2 = 3</code>。</p>
</li>
<li>
<p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[2] = 3 + 6 = 9</code>。</p>
</li>
<li>
<p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 9）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9&#125;</code>。</p>
</li>
<li>
<p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li>
<p>[ ] 第四次循环（<code>i = 3</code>）</p>
</li>
<li>
<p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[3] = 0 + 1 = 1</code>。</p>
</li>
<li>
<p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[3] = 1 + 5 = 6</code>。</p>
</li>
<li>
<p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 6）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9, 6&#125;</code>。</p>
</li>
<li>
<p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li>
<p>[x] 处理最后可能的进位</p>
</li>
</ul>
<p><code>if(t) C.push_back(1);</code></p>
<p>因为此时 <code>t = 0</code>，所以不需要添加额外的进位，<code>C</code> 仍然为 <code>&#123;2, 1, 9, 6&#125;</code>。</p>
<ul>
<li>[x] 返回结果</li>
</ul>
<p><code>return C;</code>：返回存储相加结果的向量 <code>C</code>。</p>
<p>在 <code>main</code> 函数中，将 <code>C</code> 逆序输出，得到最终结果 <code>6912</code>，即 <code>1234 + 5678 = 6912</code>。</p>
<h5 id="auto的用法">auto的用法</h5>
<p><code>auto</code>是C++11的新特性，可以自动识别变量属性。比如<code>auto C=add(A,B);</code>就自动识别了add返回的类型，因此<code>auto C &lt;=&gt; vector&lt;int&gt;c</code></p>
<h5 id="a-i-0-的作用：将字符串每一位由ASCII码转为整数"><code>a[i] - '0'</code> 的作用：将字符串每一位由ASCII码转为整数</h5>
<p>在 C++ 中，当你从输入读取一个数字字符串时，比如 <code>&quot;123&quot;</code>，字符串中的每个字符（如 <code>'1'</code>、<code>'2'</code>、<code>'3'</code>）实际上存储的是字符的 ASCII 码值，而不是对应的数值。字符 <code>'0'</code> 到 <code>'9'</code> 的 ASCII 码是连续的，<code>'0'</code> 的 ASCII 码值是 48，<code>'1'</code> 是 49，以此类推，<code>'9'</code> 是 57。</p>
<p><code>b[i] - '0'</code> 的作用就是将字符形式的数字转换为对应的整数值。例如，当 <code>b[i]</code> 为 <code>'1'</code> 时，<code>'1'</code> 的 ASCII 码值是 49，<code>'0'</code> 的 ASCII 码值是 48，那么 <code>'1' - '0'</code> 就等于 <code>49 - 48 = 1</code>，这样就把字符 <code>'1'</code> 转换为了整数 1。</p>
<p>如果没有 <code>b[i] - '0'</code> 这个操作，直接将字符存储到 <code>vector&lt;int&gt;</code> 中，那么存储的是字符的 ASCII 码值，而不是对应的数值，这会导致后续的计算出现错误。</p>
<h4 id="高精度减法">高精度减法</h4>
<h4 id="高精度乘法">高精度乘法</h4>
<h4 id="高精度除法">高精度除法</h4>
]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>ACWing</tag>
      </tags>
  </entry>
  <entry>
    <title>如何用hexo新建文件并上传/如何用hexo插入图片（next主题）</title>
    <url>/hexo_maintanance/</url>
    <content><![CDATA[<p>hexo（博主采用next主题）日常维护教程</p>
<h1>修改scaffolds/post.md（默认标题文件）</h1>
<p>注意修改时不要把中文加进去，在此只起到注释作用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:([1,2]设置同时属于不同类别可以这样)</span><br><span class="line">categories:</span><br><span class="line">top:(表示置顶情况，不置顶不填即可数字大小代表置顶顺序，数字越大排序越前)</span><br></pre></td></tr></table></figure>
<h1>新建md文件</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new/hexo n &quot;文件名&quot;</span><br></pre></td></tr></table></figure>
<h1>查看新建文件</h1>
<p>进入目录是source/_posts</p>
<h1>完善标题对应信息，填写md</h1>
<p>也就是刚才上面那堆东西</p>
<h1>在blog目录下（因人而异）打开git bash</h1>
<p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure>
<p>即可上传</p>
<p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>
<p>即可实时查看网页</p>
<h1>插入图片操作（图片和md文件最好均为英文名）</h1>
<h2 id="下插件">下插件</h2>
<p>见<a href="https://github.com/yiyungent/hexo-asset-img">yiyungent/hexo-asset-img：🍰 Hexo 本地图片插件。|Hexo 本地图片插件：转换图片相对路径为asset_img</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-img --save</span><br><span class="line">或者</span><br><span class="line">npm install git://github.com/yiyungent/hexo-asset-img.git#main</span><br></pre></td></tr></table></figure>
<h2 id="修改host-config-yml">修改host/_config.yml</h2>
<p>permalink控制了永久域名的样式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.post_asset_folder: true</span><br><span class="line">2.permalink: :title/（我的自带了日期导致图片一直不行:year/:month/:date/:title/）</span><br></pre></td></tr></table></figure>
<h2 id="直接粘贴图片">直接粘贴图片</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![1](./hexo_maintanance/1.png)</span><br></pre></td></tr></table></figure>
<p>上面是下面图片的路径。能看到下面的图片就能说明这个方法就是成功的。</p>
<p><img src="/hexo_maintanance/1.png" alt="1"></p>
<h1>实现侧边栏标题全展开</h1>
<p>有些文件目录很长，不全展开不方便看。可以修改</p>
<p><code>blog\themes\next\source\css\_common\outline\sidebar\sidebar-toc.styl</code>文件</p>
<p>查找修改<code>.nav-child</code>对应代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">.<span class="property">nav</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (not hexo-<span class="title function_">config</span>(<span class="string">&#x27;toc.expand_all&#x27;</span>)) &#123;</span><br><span class="line">    .<span class="property">nav</span>-child &#123;</span><br><span class="line">      --<span class="attr">height</span>: auto;          <span class="comment">/* 取消高度限制 */</span></span><br><span class="line">      <span class="attr">height</span>: auto;            <span class="comment">/* 启用自动高度适应内容 */</span></span><br><span class="line">      <span class="attr">opacity</span>: <span class="number">1</span>;              <span class="comment">/* 取消透明度隐藏 */</span></span><br><span class="line">      <span class="attr">overflow</span>: visible;       <span class="comment">/* 允许内容溢出显示 */</span></span><br><span class="line">      transition-<span class="attr">property</span>: opacity;  <span class="comment">/* 仅保留透明度过渡 */</span></span><br><span class="line">      <span class="attr">visibility</span>: visible;     <span class="comment">/* 确保元素可见 */</span></span><br><span class="line">      <span class="attr">transition</span>: $transition-ease;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>原配置为：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">原配置--<span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">opacity</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">overflow</span>: hidden;</span><br><span class="line">transition-<span class="attr">property</span>: height, opacity, visibility;</span><br><span class="line"><span class="attr">transition</span>: $transition-ease;</span><br><span class="line"><span class="attr">visibility</span>: hidden;</span><br></pre></td></tr></table></figure>
<h1>实现文字高亮</h1>
<p>网上帖子都是交怎么实现代码块高亮，但是迁移到网页之后typora本来能实现的文字高亮效果就无了。修改方法如下：</p>
<p><mark>先换个markdown编译器</mark></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save  # 卸载默认解析器</span><br><span class="line">npm install hexo-renderer-markdown-it --save</span><br></pre></td></tr></table></figure>
<p>然后在根目录下<mark>source/_data/__styles.styl</mark>中添加如下内容</p>
<p>（我的颜色和typora已经保持一致，用的是simplehappy主题）</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">// ==高亮内容样式==</span></span><br><span class="line">mark &#123;</span><br><span class="line">  background-<span class="attr">color</span>: #<span class="title class_">FBE598</span>;  <span class="comment">// 黄色背景</span></span><br><span class="line">  <span class="attr">color</span>: inherit;             <span class="comment">// 文字颜色继承</span></span><br><span class="line">  <span class="attr">padding</span>: <span class="number">0.</span>1em <span class="number">0.</span>3em;</span><br><span class="line">  border-<span class="attr">radius</span>: 3px;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后在<mark>根目录下的_config.yml</mark>中添加如下内容：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">markdown</span>:</span><br><span class="line">  <span class="attr">preset</span>: <span class="string">&quot;commonmark&quot;</span></span><br><span class="line">  <span class="attr">plugins</span>:</span><br><span class="line">    - markdown-it-mark  # 启用高亮语法支持（==内容==）</span><br></pre></td></tr></table></figure>
<p>成功之后就应该能看到上述步骤一样的高亮了~</p>
]]></content>
      <categories>
        <category>博客维护</category>
      </categories>
      <tags>
        <tag>博客维护</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕OCC-NeRF:Occlusion-Free Scene Recovery via Neural Radiance Fields</title>
    <url>/%E6%89%8B%E6%92%95nerfmm/</url>
    <content><![CDATA[<p>链接：<a href="https://freebutuselesssoul.github.io/occnerf/">OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields</a></p>
<h2 id="文件夹目录">文件夹目录</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ-nerf/</span><br><span class="line">├── .gitignore</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── environment.yml</span><br><span class="line">├── local1.txt</span><br><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save.py</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br><span class="line">├── models/</span><br><span class="line">│   ├── depth_decoder.py</span><br><span class="line">│   ├── intrinsics.py</span><br><span class="line">│   ├── layers.py</span><br><span class="line">│   ├── nerf_feature.py</span><br><span class="line">│   ├── nerf_mask.py</span><br><span class="line">│   ├── nerf_models.py</span><br><span class="line">│   └── poses.py</span><br><span class="line">├── utils/</span><br><span class="line">│   ├── align_traj.py</span><br><span class="line">│   ├── comp_ate.py</span><br><span class="line">│   ├── comp_ray_dir.py</span><br><span class="line">│   ├── lie_group_helper.py</span><br><span class="line">│   ├── pos_enc.py</span><br><span class="line">│   ├── pose_utils.py</span><br><span class="line">│   ├── split_dataset.py</span><br><span class="line">│   ├── training_utils.py</span><br><span class="line">│   ├── vgg.py</span><br><span class="line">│   ├── vis_cam_traj.py</span><br><span class="line">│   └── volume_op.py</span><br><span class="line">├── tasks/</span><br><span class="line">│   └── ...</span><br><span class="line">└── third_party/</span><br><span class="line">    ├── ATE/</span><br><span class="line">    │   └── README.md</span><br><span class="line">    └── pytorch_ssim/</span><br></pre></td></tr></table></figure>
<h2 id="DEBUG-代码">DEBUG 代码</h2>
<h3 id="dataloader">dataloader</h3>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save.py</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br></pre></td></tr></table></figure>
<h4 id="any-folder-py">any_folder.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os                                       <span class="comment"># 操作系统接口模块</span></span><br><span class="line"><span class="keyword">import</span> torch                                    <span class="comment"># PyTorch 深度学习框架</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                              <span class="comment"># 科学计算库</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm                           <span class="comment"># 进度条显示模块</span></span><br><span class="line"><span class="keyword">import</span> imageio                                  <span class="comment"># 图像 IO 处理库</span></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs  <span class="comment"># 自定义图像缩放函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 获取并排序目录下所有文件名</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:                                 <span class="comment"># 从 start 开始按间隔 skip 取图</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 取 start 到 end 区间按间隔 skip 取图</span></span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:                           <span class="comment"># 是否打乱图像顺序</span></span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):          <span class="comment"># 检查请求数量是否超出范围</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;图像请求数<span class="subst">&#123;num_img_to_load&#125;</span>超过可用数<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>&#x27;</span>)</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:                   <span class="comment"># 加载全部可用图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;加载全部<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张图像&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 截取指定数量的图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;从<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张中加载<span class="subst">&#123;num_img_to_load&#125;</span>张&#x27;</span>)</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line">    </span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]  <span class="comment"># 构建完整文件路径</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)                         <span class="comment"># 计算实际加载数量</span></span><br><span class="line">    </span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:                                     <span class="comment"># 实际加载图像数据</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]        <span class="comment"># 读取 RGB 三通道图像</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        img_list = np.stack(img_list)                <span class="comment"># 堆叠为 4D 数组</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 转换为浮点张量并归一化</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]  <span class="comment"># 获取图像尺寸</span></span><br><span class="line">    <span class="keyword">else</span>:                                            <span class="comment"># 仅获取图像尺寸</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])</span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;                                         <span class="comment"># 返回结构化数据</span></span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,        <span class="comment"># 图像张量 (N, H, W, 3)</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># 图像文件名数组</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,        <span class="comment"># 总图像数</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,                  <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,                  <span class="comment"># 图像宽度</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, </span></span><br><span class="line"><span class="params">                 start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):  <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir                  <span class="comment"># 数据根目录</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name              <span class="comment"># 场景名称</span></span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio                <span class="comment"># 分辨率缩放比例</span></span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load    <span class="comment"># 最大加载数量</span></span><br><span class="line">        <span class="variable language_">self</span>.start = start                        <span class="comment"># 起始索引</span></span><br><span class="line">        <span class="variable language_">self</span>.end = end                            <span class="comment"># 结束索引</span></span><br><span class="line">        <span class="variable language_">self</span>.skip = skip                          <span class="comment"># 采样间隔</span></span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted            <span class="comment"># 是否保持顺序</span></span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img                  <span class="comment"># 是否实际加载图像</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)  <span class="comment"># 构建图像目录路径</span></span><br><span class="line">        </span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load,  <span class="comment"># 加载图像数据</span></span><br><span class="line">                              <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                              <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]             <span class="comment"># 图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]   <span class="comment"># 文件名列表</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]         <span class="comment"># 图像总数</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]               <span class="comment"># 原始高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]               <span class="comment"># 原始宽度</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span>                            <span class="comment"># 近裁剪面(NDC 坐标系)</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span>                             <span class="comment"># 远裁剪面(NDC 坐标系)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:                     <span class="comment"># 计算实际使用分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:                          <span class="comment"># 执行图像缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span>                   <span class="comment"># 数据根目录配置示例</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span>                <span class="comment"># 场景路径配置示例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span>                               <span class="comment"># 缩放比例配置</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span>                           <span class="comment"># 加载全部图像</span></span><br><span class="line">    start, end, skip = <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>                    <span class="comment"># 采样参数初始化</span></span><br><span class="line">    load_sorted, load_img = <span class="literal">True</span>, <span class="literal">True</span>             <span class="comment"># 加载配置参数</span></span><br><span class="line">    </span><br><span class="line">    scene = DataLoaderAnyFolder(                   <span class="comment"># 创建数据加载实例</span></span><br><span class="line">        base_dir=base_dir,</span><br><span class="line">        scene_name=scene_name,</span><br><span class="line">        res_ratio=resize_ratio,</span><br><span class="line">        num_img_to_load=num_img_to_load,</span><br><span class="line">        start=start,</span><br><span class="line">        end=end,</span><br><span class="line">        skip=skip,</span><br><span class="line">        load_sorted=load_sorted,</span><br><span class="line">        load_img=load_img)</span><br></pre></td></tr></table></figure>
<h4 id="local-save-py">local_save.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vgg19</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, requires_grad=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 加载预训练的 VGG19 模型的特征提取部分</span></span><br><span class="line">        <span class="variable language_">self</span>.vgg_pretrained_features = models.vgg19(pretrained=<span class="literal">True</span>).features</span><br><span class="line">        <span class="comment"># 如果不需要计算梯度，则将模型参数的 requires_grad 属性设置为 False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> requires_grad:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">                param.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 初始化特征图的形状为 None</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_shape = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, indices=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 记录输入特征图的形状</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_shape = X.shape</span><br><span class="line">        <span class="comment"># 如果没有指定索引，则默认使用 [7, 25]</span></span><br><span class="line">        <span class="keyword">if</span> indices <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            indices = [<span class="number">7</span>,<span class="number">25</span>]</span><br><span class="line">        <span class="comment"># 存储提取的特征图</span></span><br><span class="line">        out = []</span><br><span class="line">        <span class="comment"># 遍历到最后一个索引位置</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(indices[-<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 通过 VGG19 的第 i 层进行特征提取</span></span><br><span class="line">            X = <span class="variable language_">self</span>.vgg_pretrained_features[i](X)</span><br><span class="line">            <span class="comment"># 如果当前层的索引加 1 在指定的索引列表中</span></span><br><span class="line">            <span class="keyword">if</span> (i+<span class="number">1</span>) <span class="keyword">in</span> indices:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.feature_shape <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="comment"># 如果特征图形状为 None，则记录当前特征图的形状</span></span><br><span class="line">                    <span class="variable language_">self</span>.feature_shape = X.shape</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 对特征图进行双线性插值，使其尺寸与输入特征图的尺寸一致</span></span><br><span class="line">                    X = F.interpolate(X,<span class="variable language_">self</span>.feature_shape[-<span class="number">2</span>:],mode=<span class="string">&#x27;bilinear&#x27;</span>,align_corners=<span class="literal">True</span>)</span><br><span class="line">                <span class="comment"># 将处理后的特征图添加到输出列表中</span></span><br><span class="line">                out.append(X)</span><br><span class="line">        <span class="comment"># 将所有提取的特征图在通道维度上拼接起来</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像的文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir))) <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 时间域下采样：根据 start、end 和 skip 参数选择图像</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:</span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查要加载的图像数量是否超过可用图像数量</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个图像文件名</span></span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建图像文件的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储加载的图像</span></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载进度</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            <span class="comment"># 读取图像并截取前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>] <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            <span class="comment"># 将图像添加到列表中</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list) <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span> <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>]) <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储加载图像的相关信息</span></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list, <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names, <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">    self.c2ws: (N_imgs, 4, 4) torch.float32</span></span><br><span class="line"><span class="string">    self.imgs (N_imgs, H, W, 4) torch.float32</span></span><br><span class="line"><span class="string">    self.ray_dir_cam (H, W, 3) torch.float32</span></span><br><span class="line"><span class="string">    self.H scalar</span></span><br><span class="line"><span class="string">    self.W scalar</span></span><br><span class="line"><span class="string">    self.N_imgs scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start, end, skip, load_sorted, load_img=<span class="literal">True</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 False：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用 load_imgs 函数加载图像</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                               <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        <span class="comment"># 加载的图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>] <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 图像的文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>] <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 初始化 VGG19 编码器</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = Vgg19()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W) <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="comment"># 存储图像的特征</span></span><br><span class="line">            <span class="variable language_">self</span>.features = []</span><br><span class="line">            <span class="comment"># 使用 tqdm 显示处理进度</span></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(<span class="variable language_">self</span>.imgs):</span><br><span class="line">                <span class="comment"># 对图像进行通道维度的调整，并通过编码器提取特征</span></span><br><span class="line">                <span class="variable language_">self</span>.features.append(<span class="variable language_">self</span>.encoder(img.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)[<span class="literal">None</span>,...]))</span><br><span class="line">            <span class="comment"># 将所有图像的特征在批次维度上拼接起来</span></span><br><span class="line">            <span class="variable language_">self</span>.features = torch.cat(<span class="variable language_">self</span>.features,<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 特征图的尺寸</span></span><br><span class="line">            <span class="variable language_">self</span>.feature_size = (<span class="variable language_">self</span>.features.shape[-<span class="number">2</span>],<span class="variable language_">self</span>.features.shape[-<span class="number">1</span>]) <span class="comment"># (H,W)</span></span><br><span class="line">            <span class="comment"># 打印特征图的形状</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="variable language_">self</span>.features.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 DataLoaderAnyFolder 类</span></span><br><span class="line">    scene = DataLoaderAnyFolder(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img)</span><br></pre></td></tr></table></figure>
<h4 id="with-colmap-py">with_colmap.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可以用来处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 可以在 CPU 或 GPU 上进行高效的数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可以进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 可以在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_imgs</span>(<span class="params">imgs, new_h, new_w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param imgs:    (N, H, W, 3)            torch.float32 RGB</span></span><br><span class="line"><span class="string">    :param new_h:   int/torch int</span></span><br><span class="line"><span class="string">    :param new_w:   int/torch int</span></span><br><span class="line"><span class="string">    :return:        (N, new_H, new_W, 3)    torch.float32 RGB</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将图像张量从 (N, H, W, 3) 转换为 (N, 3, H, W) 以适应 F.interpolate 函数的输入要求</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (N, 3, H, W)</span></span><br><span class="line">    <span class="comment"># 使用双线性插值方法将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    imgs = F.interpolate(imgs, size=(new_h, new_w), mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># (N, 3, new_H, new_W)</span></span><br><span class="line">    <span class="comment"># 将图像张量从 (N, 3, new_H, new_W) 转换回 (N, new_H, new_W, 3)</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># (N, new_H, new_W, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs  <span class="comment"># (N, new_H, new_W, 3) torch.float32 RGB</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, img_ids, new_h, new_w</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line">    <span class="comment"># 根据给定的图像索引筛选出需要的图像文件名</span></span><br><span class="line">    img_names = img_names[img_ids]  <span class="comment"># image name for this split</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">        <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">        img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">        img_list.append(img)</span><br><span class="line">    <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">    img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">    img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">    <span class="comment"># 调用 resize_imgs 函数将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    img_list = resize_imgs(img_list, new_h, new_w)</span><br><span class="line">    <span class="keyword">return</span> img_list, img_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Read the poses_bounds.npy file produced by LLFF imgs2poses.py.</span></span><br><span class="line"><span class="string">    This function is modified from https://github.com/kwea123/nerf_pl.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))  <span class="comment"># (N_images, 17)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># (N_images, 3, 5)</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># (N_images, 2)</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;down right back&quot; 改为 &quot;right up back&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws -&gt; centred c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># (N_images, 3, 4), (4, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 获取最近深度值</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 计算缩放因子，将最近深度调整到稍大于 1.0 的位置</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  <span class="comment"># 0.75 is the default parameter</span></span><br><span class="line">        <span class="comment"># 对深度边界进行缩放</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        <span class="comment"># 对相机位姿的平移部分进行缩放</span></span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># (N, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) np</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) np</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) np</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderWithCOLMAP</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, data_type, res_ratio, num_img_to_load, skip, use_ndc, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir:</span></span><br><span class="line"><span class="string">        :param scene_name:</span></span><br><span class="line"><span class="string">        :param data_type:   &#x27;train&#x27; or &#x27;val&#x27;.</span></span><br><span class="line"><span class="string">        :param res_ratio:   int [1, 2, 4] etc to resize images to a lower resolution.</span></span><br><span class="line"><span class="string">        :param num_img_to_load/skip: control frame loading in temporal domain.</span></span><br><span class="line"><span class="string">        :param use_ndc      True/False, just centre the poses and scale them.</span></span><br><span class="line"><span class="string">        :param load_img:    True/False. If set to false: only count number of images, get H and W,</span></span><br><span class="line"><span class="string">                            but do not load imgs. Useful when vis poses or debug etc.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.data_type = data_type</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建场景目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = os.path.join(<span class="variable language_">self</span>.scene_dir, <span class="string">&#x27;images&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.scene_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = meta[<span class="string">&#x27;c2ws&#x27;</span>]  <span class="comment"># (N, 4, 4) all camera pose</span></span><br><span class="line">        <span class="comment"># 提取图像高度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.H = meta[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取图像宽度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.W = meta[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像高度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像宽度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对焦距进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 加载图像并调整到指定的高度和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.img_names = load_imgs(<span class="variable language_">self</span>.img_dir, np.arange(num_img_to_load), <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[:num_img_to_load]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = <span class="variable language_">self</span>.c2ws.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成相机坐标系下的光线方向</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = comp_ray_dir_cam(<span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.focal)  <span class="comment"># (H, W, 3) torch.float32</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相机位姿从 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.from_numpy(<span class="variable language_">self</span>.c2ws).<span class="built_in">float</span>()  <span class="comment"># (N, 4, 4) torch.float32</span></span><br><span class="line">        <span class="comment"># 将光线方向张量转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = <span class="variable language_">self</span>.ray_dir_cam.<span class="built_in">float</span>()  <span class="comment"># (H, W, 3) torch.float32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern&#x27;</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 注意：需要将 /your/data/path 替换为实际的数据路径，</span></span><br><span class="line">    <span class="comment"># 这里创建了一个 DataLoaderWithCOLMAP 类的实例，用于加载指定场景的数据</span></span><br><span class="line">    scene = DataLoaderWithCOLMAP(base_dir=<span class="string">&#x27;/your/data/path&#x27;</span>,</span><br><span class="line">                                 scene_name=scene_name,</span><br><span class="line">                                 data_type=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">                                 res_ratio=<span class="number">8</span>,</span><br><span class="line">                                 num_img_to_load=-<span class="number">1</span>,</span><br><span class="line">                                 skip=<span class="number">1</span>,</span><br><span class="line">                                 use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>
<h4 id="with-feature-colmap-py">with_feature_colmap.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="comment"># 从 dataloader.with_colmap 模块导入 resize_imgs 函数，</span></span><br><span class="line"><span class="comment"># 该函数用于调整图像的尺寸。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="comment"># torchvision 是 PyTorch 中用于计算机视觉任务的库，</span></span><br><span class="line"><span class="comment"># models 子模块提供了预训练的深度学习模型。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 例如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.vgg <span class="keyword">import</span> Vgg19</span><br><span class="line"><span class="comment"># 从 utils.vgg 模块导入 Vgg19 类，可能用于特征提取。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在时间域上对帧进行下采样</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:</span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载下采样后的图像</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])  <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    result = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Read the poses_bounds.npy file produced by LLFF imgs2poses.py.</span></span><br><span class="line"><span class="string">    This function is modified from https://github.com/kwea123/nerf_pl.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;../poses_bounds.npy&#x27;</span>))  <span class="comment"># (N_images, 17)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># (N_images, 3, 5)</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># (N_images, 2)</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;down right back&quot; 改为 &quot;right up back&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws -&gt; centred c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># (N_images, 3, 4), (4, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 修正尺度，使最近的深度略大于 1.0</span></span><br><span class="line">        <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 0.75 是默认参数</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  </span><br><span class="line">        <span class="comment"># 最近的深度约为 1/0.75 = 1.33</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># (N, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) np</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) np</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) np</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataloader_feature_n_colmap</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start=<span class="number">0</span>, end=-<span class="number">1</span>, skip=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 load_sorted=<span class="literal">True</span>, load_img=<span class="literal">True</span>, use_ndc=<span class="literal">True</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 false：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息并转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.Tensor(meta[<span class="string">&#x27;c2ws&#x27;</span>])  <span class="comment"># (N, 4, 4) all camera pose</span></span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line">        <span class="comment"># 根据 start、end 和 skip 参数对相机位姿进行筛选</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.end == -<span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[<span class="variable language_">self</span>.start::<span class="variable language_">self</span>.skip]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[<span class="variable language_">self</span>.start:<span class="variable language_">self</span>.end:<span class="variable language_">self</span>.skip]</span><br><span class="line">        <span class="comment"># 加载图像数据</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                                <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        <span class="comment"># 提取加载的图像数据</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 提取图像文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 初始化 Vgg19 编码器并将其移动到指定设备</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = Vgg19().to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC）</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 计算调整后的图像高度</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 计算调整后的图像宽度</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        <span class="comment"># 调整焦距</span></span><br><span class="line">        <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率并将其移动到指定设备</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W).to(device)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="variable language_">self</span>.features = []</span><br><span class="line">            <span class="comment"># 使用 tqdm 显示处理进度</span></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(<span class="variable language_">self</span>.imgs):</span><br><span class="line">                <span class="comment"># 对图像进行通道维度的调整，并通过编码器提取特征</span></span><br><span class="line">                <span class="variable language_">self</span>.features.append(<span class="variable language_">self</span>.encoder(img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)[<span class="literal">None</span>, ...]))</span><br><span class="line">            <span class="comment"># 这里注释掉了特征拼接的代码，可根据需要取消注释</span></span><br><span class="line">            <span class="comment"># self.features = torch.cat(self.features, 0)</span></span><br><span class="line">            <span class="comment"># print(self.features.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否使用归一化设备坐标（NDC）</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 Dataloader_feature_n_colmap 类</span></span><br><span class="line">    scene = Dataloader_feature_n_colmap(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img,</span><br><span class="line">                                use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>
<h4 id="with-feature-py">with_feature.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 例如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_imgs</span>(<span class="params">imgs, new_h, new_w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param imgs:    (N, H, W, 3)            torch.float32 格式的 RGB 图像</span></span><br><span class="line"><span class="string">    :param new_h:   整数或 torch 整数类型，表示新的图像高度</span></span><br><span class="line"><span class="string">    :param new_w:   整数或 torch 整数类型，表示新的图像宽度</span></span><br><span class="line"><span class="string">    :return:        (N, new_H, new_W, 3)    torch.float32 格式的 RGB 图像</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将图像张量的维度从 (N, H, W, 3) 调整为 (N, 3, H, W)，以适配 F.interpolate 函数的输入要求</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 变为 (N, 3, H, W)</span></span><br><span class="line">    <span class="comment"># 使用双线性插值方法将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    imgs = F.interpolate(imgs, size=(new_h, new_w), mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># 变为 (N, 3, new_H, new_W)</span></span><br><span class="line">    <span class="comment"># 将图像张量的维度从 (N, 3, new_H, new_W) 调整回 (N, new_H, new_W, 3)</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 变为 (N, new_H, new_W, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs  <span class="comment"># 返回 (N, new_H, new_W, 3) 格式的 torch.float32 类型 RGB 图像</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, img_ids, new_h, new_w</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 得到所有图像文件名</span></span><br><span class="line">    <span class="comment"># 根据给定的图像索引筛选出本次需要的图像文件名</span></span><br><span class="line">    img_names = img_names[img_ids]  <span class="comment"># 得到本次分割所需的图像文件名</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">        <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">        img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># 得到 (H, W, 3) 格式的 np.uint8 类型图像</span></span><br><span class="line">        img_list.append(img)</span><br><span class="line">    <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">    img_list = np.stack(img_list)  <span class="comment"># 变为 (N, H, W, 3) 格式</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">    img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 变为 (N, H, W, 3) 格式的 torch.float32 类型</span></span><br><span class="line">    <span class="comment"># 调用 resize_imgs 函数将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    img_list = resize_imgs(img_list, new_h, new_w)</span><br><span class="line">    <span class="keyword">return</span> img_list, img_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取由 LLFF 的 imgs2poses.py 生成的 poses_bounds.npy 文件。</span></span><br><span class="line"><span class="string">    此函数改编自 https://github.com/kwea123/nerf_pl。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))  <span class="comment"># 得到 (N_images, 17) 格式的数组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># 变为 (N_images, 3, 5) 格式</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># 变为 (N_images, 2) 格式</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;下 右 后&quot; 改为 &quot;右 上 后&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws 得到中心化后的 c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># 分别得到 (N_images, 3, 4) 和 (4, 4) 格式的数组</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 获取最近深度值</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 计算缩放因子，使最近深度调整到稍大于 1.0 的位置</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  <span class="comment"># 0.75 是默认参数</span></span><br><span class="line">        <span class="comment"># 现在最近深度约为 1/0.75 = 1.33</span></span><br><span class="line">        <span class="comment"># 对深度边界进行缩放</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        <span class="comment"># 对相机位姿的平移部分进行缩放</span></span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># 变为 (N, 4, 4) 格式</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) 格式的 numpy 数组</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) 格式的 numpy 数组</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># 标量，图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># 标量，图像宽度</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># 标量，焦距</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) 格式的 numpy 数组</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderWithCOLMAP</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    最有用的字段：</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4) 格式的 torch.float32 类型张量，表示相机位姿</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4) 格式的 torch.float32 类型张量，表示图像</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3) 格式的 torch.float32 类型张量，表示相机坐标系下的光线方向</span></span><br><span class="line"><span class="string">        self.H              标量，图像高度</span></span><br><span class="line"><span class="string">        self.W              标量，图像宽度</span></span><br><span class="line"><span class="string">        self.N_imgs         标量，图像数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, data_type, res_ratio, num_img_to_load, skip, use_ndc, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param data_type: 数据类型，&#x27;train&#x27; 或 &#x27;val&#x27;。</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param num_img_to_load/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param use_ndc: 布尔值，是否对相机位姿进行中心化和缩放。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 False：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.data_type = data_type</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建场景目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = os.path.join(<span class="variable language_">self</span>.scene_dir, <span class="string">&#x27;images&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.scene_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = meta[<span class="string">&#x27;c2ws&#x27;</span>]  <span class="comment"># (N, 4, 4) 格式的 numpy 数组，表示所有相机位姿</span></span><br><span class="line">        <span class="comment"># 提取图像高度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.H = meta[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取图像宽度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.W = meta[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像高度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像宽度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对焦距进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 加载图像并调整到指定的高度和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.img_names = load_imgs(<span class="variable language_">self</span>.img_dir, np.arange(num_img_to_load), <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[:num_img_to_load]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = <span class="variable language_">self</span>.c2ws.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成相机坐标系下的光线方向</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = comp_ray_dir_cam(<span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.focal)  <span class="comment"># (H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相机位姿从 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.from_numpy(<span class="variable language_">self</span>.c2ws).<span class="built_in">float</span>()  <span class="comment"># (N, 4, 4) 格式的 torch.float32 类型张量</span></span><br><span class="line">        <span class="comment"># 将光线方向张量转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = <span class="variable language_">self</span>.ray_dir_cam.<span class="built_in">float</span>()  <span class="comment"># (H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern&#x27;</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 注意：需要将 /your/data/path 替换为实际的数据路径，</span></span><br><span class="line">    <span class="comment"># 这里创建了一个 DataLoaderWithCOLMAP 类的实例，用于加载指定场景的数据</span></span><br><span class="line">    scene = DataLoaderWithCOLMAP(base_dir=<span class="string">&#x27;/your/data/path&#x27;</span>,</span><br><span class="line">                                 scene_name=scene_name,</span><br><span class="line">                                 data_type=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">                                 res_ratio=<span class="number">8</span>,</span><br><span class="line">                                 num_img_to_load=-<span class="number">1</span>,</span><br><span class="line">                                 skip=<span class="number">1</span>,</span><br><span class="line">                                 use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>
<h4 id="with-mask-py">with_mask.py</h4>
<p>他们的mask其实是掩码文件，有没有可能只基于掩码文件去做呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="comment"># 从 dataloader.with_colmap 模块导入 resize_imgs 函数，</span></span><br><span class="line"><span class="comment"># 该函数用于调整图像的尺寸。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, mask_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在时间域上对帧进行下采样</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(os.listdir(mask_dir)) == <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="comment"># 若 end 为 -1 且掩码目录和图像目录文件数量相同，则按 skip 间隔选取</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 取 end 和掩码目录文件数量的最小值，避免越界</span></span><br><span class="line">        end = <span class="built_in">min</span>(end, <span class="built_in">len</span>(os.listdir(mask_dir)))</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载下采样后的图像</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 构建每个掩码图像的完整路径，假设掩码图像为 png 格式，且文件名和图像文件名对应</span></span><br><span class="line">    mask_paths = [os.path.join(mask_dir, n[:-<span class="number">4</span>]+<span class="string">&#x27;.png&#x27;</span>) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    img_list, mask_list = [], []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(img_paths)):</span><br><span class="line">            <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">            <span class="comment"># 读取对应的掩码图像，只取第一个通道</span></span><br><span class="line">            img = imageio.imread(mask_paths[i])[:, :, [<span class="number">0</span>]]  <span class="comment"># (H, W, 1)</span></span><br><span class="line">            mask_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将掩码列表转换为 numpy 数组</span></span><br><span class="line">        mask_list = np.stack(mask_list)</span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        mask_list = torch.from_numpy(mask_list).<span class="built_in">float</span>() / <span class="number">255</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])  <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;masks&#x27;</span>: mask_list,  <span class="comment"># 掩码图像张量</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 false：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建掩码目录的完整路径，假设掩码目录在图像目录的上一级的 mask 文件夹下</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_dir = os.path.join(<span class="variable language_">self</span>.imgs_dir, <span class="string">&#x27;../mask/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用 load_imgs 函数加载图像和掩码数据</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.mask_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                               <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取加载的图像数据</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 提取图像文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 提取掩码数据</span></span><br><span class="line">        <span class="variable language_">self</span>.masks = image_data[<span class="string">&#x27;masks&#x27;</span>]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC），设置近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC），设置远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 计算调整后的图像高度</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 计算调整后的图像宽度</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="comment"># 调整掩码图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.masks = resize_imgs(<span class="variable language_">self</span>.masks, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 DataLoaderAnyFolder 类</span></span><br><span class="line">    scene = DataLoaderAnyFolder(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img)</span><br></pre></td></tr></table></figure>
<h3 id="models">models</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── models/  # 模型文件夹</span><br><span class="line">│   ├── depth_decoder.py  # 深度解码器脚本文件</span><br><span class="line">│   ├── intrinsics.py  # 内参相关脚本文件</span><br><span class="line">│   ├── layers.py  # 层相关脚本文件</span><br><span class="line">│   ├── nerf_feature.py  # NeRF特征相关脚本文件</span><br><span class="line">│   ├── nerf_mask.py  # NeRF掩码相关脚本文件</span><br><span class="line">│   ├── nerf_models.py  # NeRF模型相关脚本文件</span><br><span class="line">│   └── poses.py  # 位姿相关脚本文件</span><br></pre></td></tr></table></figure>
<h4 id="depth-decoder-py">depth_decoder.py</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 版权所有 Niantic 2019。专利申请中。保留所有权利。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本软件遵循 Monodepth2 许可证的条款，</span></span><br><span class="line"><span class="comment"># 该许可证仅允许非商业用途，完整条款可在 LICENSE 文件中获取。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> models.layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积层，输入通道数为 in_planes，输出通道数为 out_planes，卷积核大小为 kernel_size</span></span><br><span class="line"><span class="comment"># 如果 instancenorm 为 True，则使用实例归一化；否则使用批量归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv</span>(<span class="params">in_planes, out_planes, kernel_size, instancenorm=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> instancenorm:</span><br><span class="line">        <span class="comment"># 构建一个包含卷积层、实例归一化层和 LeakyReLU 激活函数的序列</span></span><br><span class="line">        m = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层，使用指定的输入和输出通道数、卷积核大小，步长为 1，填充为 (kernel_size - 1) // 2，无偏置</span></span><br><span class="line">            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,</span><br><span class="line">                      stride=<span class="number">1</span>, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            <span class="comment"># 实例归一化层</span></span><br><span class="line">            nn.InstanceNorm2d(out_planes),</span><br><span class="line">            <span class="comment"># LeakyReLU 激活函数，负斜率为 0.1，原地操作</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 构建一个包含卷积层、批量归一化层和 LeakyReLU 激活函数的序列</span></span><br><span class="line">        m = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层，使用指定的输入和输出通道数、卷积核大小，步长为 1，填充为 (kernel_size - 1) // 2，无偏置</span></span><br><span class="line">            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,</span><br><span class="line">                      stride=<span class="number">1</span>, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            <span class="comment"># 批量归一化层</span></span><br><span class="line">            nn.BatchNorm2d(out_planes),</span><br><span class="line">            <span class="comment"># LeakyReLU 激活函数，负斜率为 0.1，原地操作</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度解码器类，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DepthDecoder</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 将元组转换为字符串，用于作为字典的键</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tuple_to_str</span>(<span class="params">self, key_tuple</span>):</span><br><span class="line">        key_str = <span class="string">&#x27;-&#x27;</span>.join(<span class="built_in">str</span>(key_tuple))</span><br><span class="line">        <span class="keyword">return</span> key_str</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_ch_enc, embedder, embedder_out_dim,</span></span><br><span class="line"><span class="params">                 use_alpha=<span class="literal">False</span>, scales=<span class="built_in">range</span>(<span class="params"><span class="number">4</span></span>), num_output_channels=<span class="number">4</span>,</span></span><br><span class="line"><span class="params">                 use_skips=<span class="literal">True</span>, sigma_dropout_rate=<span class="number">0.0</span>, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(DepthDecoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_output_channels = num_output_channels</span><br><span class="line">        <span class="comment"># 是否使用跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.use_skips = use_skips</span><br><span class="line">        <span class="comment"># 上采样模式</span></span><br><span class="line">        <span class="variable language_">self</span>.upsample_mode = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">        <span class="comment"># 要处理的尺度</span></span><br><span class="line">        <span class="variable language_">self</span>.scales = scales</span><br><span class="line">        <span class="comment"># 是否使用 alpha</span></span><br><span class="line">        <span class="variable language_">self</span>.use_alpha = use_alpha</span><br><span class="line">        <span class="comment"># sigma 的丢弃率</span></span><br><span class="line">        <span class="variable language_">self</span>.sigma_dropout_rate = sigma_dropout_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 嵌入器</span></span><br><span class="line">        <span class="variable language_">self</span>.embedder = embedder</span><br><span class="line">        <span class="comment"># 嵌入器的输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.E = embedder_out_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器最后一层的输出通道数</span></span><br><span class="line">        final_enc_out_channels = num_ch_enc[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 最大池化层，用于下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.downsample = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最近邻上采样层，用于上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.upsample = nn.UpsamplingNearest2d(scale_factor=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第一个下采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_down1 = conv(final_enc_out_channels, <span class="number">512</span>, <span class="number">1</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个下采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_down2 = conv(<span class="number">512</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第一个上采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_up1 = conv(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个上采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_up2 = conv(<span class="number">256</span>, final_enc_out_channels, <span class="number">1</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器各层的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_enc = num_ch_enc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;num_ch_enc=&quot;</span>, num_ch_enc)</span><br><span class="line">        <span class="comment"># 将编码器各层的通道数加上嵌入器的输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_enc = [x + <span class="variable language_">self</span>.E <span class="keyword">for</span> x <span class="keyword">in</span> <span class="variable language_">self</span>.num_ch_enc]</span><br><span class="line">        <span class="comment"># 解码器各层的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_dec = np.array([<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>])</span><br><span class="line">        <span class="comment"># self.num_ch_enc = np.array([64, 64, 128, 256, 512])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器的卷积层，使用 nn.ModuleDict 存储</span></span><br><span class="line">        <span class="variable language_">self</span>.convs = nn.ModuleDict()</span><br><span class="line">        <span class="comment"># 从 4 到 0 遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 上卷积层 0</span></span><br><span class="line">            <span class="comment"># 如果 i 为 4，则输入通道数为编码器最后一层的通道数；否则为解码器上一层的通道数</span></span><br><span class="line">            num_ch_in = <span class="variable language_">self</span>.num_ch_enc[-<span class="number">1</span>] <span class="keyword">if</span> i == <span class="number">4</span> <span class="keyword">else</span> <span class="variable language_">self</span>.num_ch_dec[i + <span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 输出通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_out = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 创建卷积块并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">0</span>))] = ConvBlock(num_ch_in, num_ch_out)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;upconv_&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="number">0</span>), num_ch_in, num_ch_out)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 上卷积层 1</span></span><br><span class="line">            <span class="comment"># 输入通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_in = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 如果使用跳跃连接且 i 大于 0，则输入通道数加上编码器上一层的通道数</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_skips <span class="keyword">and</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                num_ch_in += <span class="variable language_">self</span>.num_ch_enc[i - <span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 输出通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_out = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 创建卷积块并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">1</span>))] = ConvBlock(num_ch_in, num_ch_out)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;upconv_&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="number">1</span>), num_ch_in, num_ch_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历要处理的尺度</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.scales:</span><br><span class="line">            <span class="comment"># 创建一个 3x3 的卷积层并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;dispconv&quot;</span>, s))] = Conv3x3(<span class="variable language_">self</span>.num_ch_dec[s], <span class="variable language_">self</span>.num_output_channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sigmoid 激活函数，用于将输出映射到 [0, 1] 范围</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_features, disparity</span>):</span><br><span class="line">        <span class="comment"># 获取输入视差的批次大小和序列长度</span></span><br><span class="line">        B, S = disparity.size()</span><br><span class="line">        <span class="comment"># 对输入视差进行嵌入操作，然后增加两个维度</span></span><br><span class="line">        disparity = <span class="variable language_">self</span>.embedder(disparity.reshape(B * S, <span class="number">1</span>)).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line">tensor_1d 是一个一维张量，直接使用 torch.tensor 创建，其形状为 (<span class="number">3</span>,)。</span><br><span class="line">tensor_2d 是通过对 tensor_1d 使用 unsqueeze(<span class="number">1</span>) 增加一个维度得到的，形状为 (<span class="number">3</span>, <span class="number">1</span>)。</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为 (3,) 的一维张量</span></span><br><span class="line">tensor_1d = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tensor_1d 是否定义:&quot;</span>, <span class="string">&#x27;tensor_1d&#x27;</span> <span class="keyword">in</span> <span class="built_in">locals</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tensor_1d 的值:&quot;</span>, tensor_1d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制一维张量</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(tensor_1d)), tensor_1d, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Shape (3,) Tensor&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为 (3, 1) 的二维张量</span></span><br><span class="line">tensor_2d = tensor_1d.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制二维张量</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(tensor_2d)), tensor_2d.flatten(), width=<span class="number">0.5</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Shape (3, 1) Tensor&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line">        <span class="comment"># 扩展编码器的输出以增加感受野</span></span><br><span class="line">        <span class="comment"># 获取编码器最后一层的输出</span></span><br><span class="line">        encoder_out = input_features[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 对编码器输出进行下采样，然后通过第一个下采样卷积层</span></span><br><span class="line">        conv_down1 = <span class="variable language_">self</span>.conv_down1(<span class="variable language_">self</span>.downsample(encoder_out))</span><br><span class="line">        <span class="comment"># 对第一个下采样卷积层的输出进行下采样，然后通过第二个下采样卷积层</span></span><br><span class="line">        conv_down2 = <span class="variable language_">self</span>.conv_down2(<span class="variable language_">self</span>.downsample(conv_down1))</span><br><span class="line">        <span class="comment"># 对第二个下采样卷积层的输出进行上采样，然后通过第一个上采样卷积层</span></span><br><span class="line">        conv_up1 = <span class="variable language_">self</span>.conv_up1(<span class="variable language_">self</span>.upsample(conv_down2))</span><br><span class="line">        <span class="comment"># 对第一个上采样卷积层的输出进行上采样，然后通过第二个上采样卷积层</span></span><br><span class="line">        conv_up2 = <span class="variable language_">self</span>.conv_up2(<span class="variable language_">self</span>.upsample(conv_up1))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复 / 重塑特征</span></span><br><span class="line">        <span class="comment"># 获取第二个上采样卷积层输出的通道数、高度和宽度</span></span><br><span class="line">        _, C_feat, H_feat, W_feat = conv_up2.size()</span><br><span class="line">        <span class="comment"># 对第二个上采样卷积层的输出进行扩展和重塑</span></span><br><span class="line">        feat_tmp = conv_up2.unsqueeze(<span class="number">1</span>).expand(B, S, C_feat, H_feat, W_feat) \</span><br><span class="line">            .contiguous().view(B * S, C_feat, H_feat, W_feat)</span><br><span class="line">        <span class="comment"># 对视差进行重复操作以匹配特征图的大小</span></span><br><span class="line">        disparity_BsCHW = disparity.repeat(<span class="number">1</span>, <span class="number">1</span>, H_feat, W_feat)</span><br><span class="line">        <span class="comment"># 将扩展后的特征和视差拼接在一起</span></span><br><span class="line">        conv_up2 = torch.cat((feat_tmp, disparity_BsCHW), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复 / 重塑输入特征</span></span><br><span class="line">        <span class="keyword">for</span> i, feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(input_features):</span><br><span class="line">            <span class="comment"># 获取输入特征的通道数、高度和宽度</span></span><br><span class="line">            _, C_feat, H_feat, W_feat = feat.size()</span><br><span class="line">            <span class="comment"># 对输入特征进行扩展和重塑</span></span><br><span class="line">            feat_tmp = feat.unsqueeze(<span class="number">1</span>).expand(B, S, C_feat, H_feat, W_feat) \</span><br><span class="line">                .contiguous().view(B * S, C_feat, H_feat, W_feat)</span><br><span class="line">            <span class="comment"># 对视差进行重复操作以匹配特征图的大小</span></span><br><span class="line">            disparity_BsCHW = disparity.repeat(<span class="number">1</span>, <span class="number">1</span>, H_feat, W_feat)</span><br><span class="line">            <span class="comment"># 将扩展后的特征和视差拼接在一起</span></span><br><span class="line">            input_features[i] = torch.cat((feat_tmp, disparity_BsCHW), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        <span class="comment"># 存储输出结果的字典</span></span><br><span class="line">        outputs = &#123;&#125;</span><br><span class="line">        <span class="comment"># 初始输入为扩展后的第二个上采样卷积层的输出</span></span><br><span class="line">        x = conv_up2</span><br><span class="line">        <span class="comment"># 从 4 到 0 遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 通过上卷积层 0</span></span><br><span class="line">            x = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">0</span>))](x)</span><br><span class="line">            <span class="comment"># 进行上采样</span></span><br><span class="line">            x = [upsample(x)]</span><br><span class="line">            <span class="comment"># 如果使用跳跃连接且 i 大于 0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_skips <span class="keyword">and</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 将编码器上一层的特征添加到列表中</span></span><br><span class="line">                x += [input_features[i - <span class="number">1</span>]]</span><br><span class="line">            <span class="comment"># 将列表中的特征拼接在一起</span></span><br><span class="line">            x = torch.cat(x, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 通过上卷积层 1</span></span><br><span class="line">            x = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">1</span>))](x)</span><br><span class="line">            <span class="comment"># 如果当前尺度在要处理的尺度列表中</span></span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.scales:</span><br><span class="line">                <span class="comment"># 通过视差卷积层得到输出</span></span><br><span class="line">                output = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;dispconv&quot;</span>, i))](x)</span><br><span class="line">                <span class="comment"># 获取输出的高度和宽度</span></span><br><span class="line">                H_mpi, W_mpi = output.size(<span class="number">2</span>), output.size(<span class="number">3</span>)</span><br><span class="line">                <span class="comment"># 调整输出的维度</span></span><br><span class="line">                mpi = output.view(B, S, <span class="number">4</span>, H_mpi, W_mpi)</span><br><span class="line">                <span class="comment"># 对 RGB 通道应用 Sigmoid 激活函数</span></span><br><span class="line">                mpi_rgb = <span class="variable language_">self</span>.sigmoid(mpi[:, :, <span class="number">0</span>:<span class="number">3</span>, :, :])</span><br><span class="line">                <span class="comment"># 如果不使用 alpha，则取绝对值并加上一个小的常数；否则应用 Sigmoid 激活函数</span></span><br><span class="line">                mpi_sigma = torch.<span class="built_in">abs</span>(mpi[:, :, <span class="number">3</span>:, :, :]) + <span class="number">1e-4</span> \</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.use_alpha \</span><br><span class="line">                        <span class="keyword">else</span> <span class="variable language_">self</span>.sigmoid(mpi[:, :, <span class="number">3</span>:, :, :])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果 sigma 丢弃率大于 0 且处于训练模式</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.sigma_dropout_rate &gt; <span class="number">0.0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.training:</span><br><span class="line">                    <span class="comment"># 对 sigma 通道应用 2D Dropout</span></span><br><span class="line">                    mpi_sigma = F.dropout2d(mpi_sigma, p=<span class="variable language_">self</span>.sigma_dropout_rate)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将 RGB 和 sigma 通道拼接在一起，并存储到输出字典中</span></span><br><span class="line">                outputs[(<span class="string">&quot;disp&quot;</span>, i)] = torch.cat((mpi_rgb, mpi_sigma), dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<h4 id="intrinsics-py">intrinsics.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个用于学习焦距的神经网络模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearnFocal</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, H, W, req_grad, fx_only, order=<span class="number">2</span>, init_focal=<span class="literal">None</span>, learn_distortion=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 调用父类 nn.Module 的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(LearnFocal, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.H = H</span><br><span class="line">        <span class="comment"># 图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="comment"># 一个布尔值，如果为 True，只输出 [fx, fx]；如果为 False，输出 [fx, fy]</span></span><br><span class="line">        <span class="variable language_">self</span>.fx_only = fx_only  </span><br><span class="line">        <span class="comment"># 焦距初始化的阶数，检查我们的补充部分有相关说明</span></span><br><span class="line">        <span class="variable language_">self</span>.order = order  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 畸变相关</span></span><br><span class="line">        <span class="comment"># 是否学习畸变参数</span></span><br><span class="line">        <span class="variable language_">self</span>.learn_distortion = learn_distortion</span><br><span class="line">        <span class="keyword">if</span> learn_distortion:</span><br><span class="line">            <span class="comment"># 第一个畸变系数，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">            <span class="variable language_">self</span>.k1 = nn.Parameter(torch.tensor(<span class="number">0.0</span>, dtype=torch.float32), requires_grad=req_grad)</span><br><span class="line">            <span class="comment"># 第二个畸变系数，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">            <span class="variable language_">self</span>.k2 = nn.Parameter(torch.tensor(<span class="number">0.0</span>, dtype=torch.float32), requires_grad=req_grad)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.fx_only:</span><br><span class="line">            <span class="keyword">if</span> init_focal <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fx 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * W = fx 计算系数 a，即 a**2 = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(W)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.order == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a * W = fx 计算系数 a，即 a = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(init_focal / <span class="built_in">float</span>(W), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;焦距初始化阶数需要为 1 或 2。退出&#x27;</span>)</span><br><span class="line">                    exit()</span><br><span class="line">                <span class="comment"># 将计算得到的系数作为 fx，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(coe_x, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> init_focal <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fx 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fy 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fy = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * W = fx 计算 x 方向的系数 a，即 a**2 = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(W)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * H = fy 计算 y 方向的系数 a，即 a**2 = fy / H</span></span><br><span class="line">                    coe_y = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(H)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.order == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a * W = fx 计算 x 方向的系数 a，即 a = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(init_focal / <span class="built_in">float</span>(W), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                    <span class="comment"># 根据公式 a * H = fy 计算 y 方向的系数 a，即 a = fy / H</span></span><br><span class="line">                    coe_y = torch.tensor(init_focal / <span class="built_in">float</span>(H), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;焦距初始化阶数需要为 1 或 2。退出&#x27;</span>)</span><br><span class="line">                    exit()</span><br><span class="line">                <span class="comment"># 将计算得到的 x 方向系数作为 fx，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(coe_x, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">                <span class="comment"># 将计算得到的 y 方向系数作为 fy，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fy = nn.Parameter(coe_y, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, i=<span class="literal">None</span></span>):  <span class="comment"># 参数 i=None 只是为了支持多 GPU 训练</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.fx_only:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy，因为 fx_only 为 True，所以 fy 等于 fx</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx ** <span class="number">2</span> * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fx ** <span class="number">2</span> * <span class="variable language_">self</span>.W])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy，因为 fx_only 为 True，所以 fy 等于 fx</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx**<span class="number">2</span> * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fy**<span class="number">2</span> * <span class="variable language_">self</span>.H])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fy * <span class="variable language_">self</span>.H])</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.learn_distortion:</span><br><span class="line">            <span class="comment"># 如果要学习畸变参数，返回焦距和畸变系数</span></span><br><span class="line">            <span class="keyword">return</span> fxfy, <span class="variable language_">self</span>.k1, <span class="variable language_">self</span>.k2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则只返回焦距</span></span><br><span class="line">            <span class="keyword">return</span> fxfy</span><br></pre></td></tr></table></figure>
<h4 id="layers-py">layers.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 版权所有 Niantic 2019。专利申请中。保留所有权利。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本软件遵循 Monodepth2 许可证的条款，</span></span><br><span class="line"><span class="comment"># 该许可证仅允许非商业用途，完整条款可在 LICENSE 文件中获取。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络的 sigmoid 输出转换为深度预测</span></span><br><span class="line"><span class="comment"># 此转换公式在论文的“额外考虑”部分给出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">disp_to_depth</span>(<span class="params">disp, min_depth, max_depth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将网络的 sigmoid 输出转换为深度预测</span></span><br><span class="line"><span class="string">    该转换公式在论文的“额外考虑”部分给出。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 最小视差，为最大深度的倒数</span></span><br><span class="line">    min_disp = <span class="number">1</span> / max_depth</span><br><span class="line">    <span class="comment"># 最大视差，为最小深度的倒数</span></span><br><span class="line">    max_disp = <span class="number">1</span> / min_depth</span><br><span class="line">    <span class="comment"># 缩放后的视差</span></span><br><span class="line">    scaled_disp = min_disp + (max_disp - min_disp) * disp</span><br><span class="line">    <span class="comment"># 深度值，为缩放后视差的倒数</span></span><br><span class="line">    depth = <span class="number">1</span> / scaled_disp</span><br><span class="line">    <span class="keyword">return</span> scaled_disp, depth</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络输出的 (轴角, 平移) 转换为 4x4 矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transformation_from_parameters</span>(<span class="params">axisangle, translation, invert=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将网络的 (轴角, 平移) 输出转换为 4x4 矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从轴角表示转换为旋转矩阵</span></span><br><span class="line">    R = rot_from_axisangle(axisangle)</span><br><span class="line">    <span class="comment"># 克隆平移向量</span></span><br><span class="line">    t = translation.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        <span class="comment"># 如果需要反转，对旋转矩阵进行转置</span></span><br><span class="line">        R = R.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 平移向量取负</span></span><br><span class="line">        t *= -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line">    T = get_translation_matrix(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        <span class="comment"># 如果需要反转，先旋转再平移</span></span><br><span class="line">        M = torch.matmul(R, T)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 正常情况下，先平移再旋转</span></span><br><span class="line">        M = torch.matmul(T, R)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_translation_matrix</span>(<span class="params">translation_vector</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化一个全零的 4x4 矩阵，形状为 (batch_size, 4, 4)</span></span><br><span class="line">    T = torch.zeros(translation_vector.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>).to(device=translation_vector.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将平移向量调整为 (batch_size, 3, 1) 的形状</span></span><br><span class="line">    t = translation_vector.contiguous().view(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置矩阵的对角元素为 1</span></span><br><span class="line">    T[:, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">1</span>, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">2</span>, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将平移向量添加到矩阵的最后一列</span></span><br><span class="line">    T[:, :<span class="number">3</span>, <span class="number">3</span>, <span class="literal">None</span>] = t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将轴角旋转表示转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="comment"># （改编自 https://github.com/Wallacoloo/printipi）</span></span><br><span class="line"><span class="comment"># 输入 &#x27;vec&#x27; 必须是 Bx1x3 的形状</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rot_from_axisangle</span>(<span class="params">vec</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将轴角旋转表示转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="string">    （改编自 https://github.com/Wallacoloo/printipi）</span></span><br><span class="line"><span class="string">    输入 &#x27;vec&#x27; 必须是 Bx1x3 的形状</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算轴角的模长</span></span><br><span class="line">    angle = torch.norm(vec, <span class="number">2</span>, <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 计算单位轴向量</span></span><br><span class="line">    axis = vec / (angle + <span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算角度的余弦值</span></span><br><span class="line">    ca = torch.cos(angle)</span><br><span class="line">    <span class="comment"># 计算角度的正弦值</span></span><br><span class="line">    sa = torch.sin(angle)</span><br><span class="line">    <span class="comment"># 计算 1 - cos(angle)</span></span><br><span class="line">    C = <span class="number">1</span> - ca</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取轴向量的 x 分量，并增加一个维度</span></span><br><span class="line">    x = axis[..., <span class="number">0</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取轴向量的 y 分量，并增加一个维度</span></span><br><span class="line">    y = axis[..., <span class="number">1</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取轴向量的 z 分量，并增加一个维度</span></span><br><span class="line">    z = axis[..., <span class="number">2</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 x * sin(angle)</span></span><br><span class="line">    xs = x * sa</span><br><span class="line">    <span class="comment"># 计算 y * sin(angle)</span></span><br><span class="line">    ys = y * sa</span><br><span class="line">    <span class="comment"># 计算 z * sin(angle)</span></span><br><span class="line">    zs = z * sa</span><br><span class="line">    <span class="comment"># 计算 x * (1 - cos(angle))</span></span><br><span class="line">    xC = x * C</span><br><span class="line">    <span class="comment"># 计算 y * (1 - cos(angle))</span></span><br><span class="line">    yC = y * C</span><br><span class="line">    <span class="comment"># 计算 z * (1 - cos(angle))</span></span><br><span class="line">    zC = z * C</span><br><span class="line">    <span class="comment"># 计算 x * y * (1 - cos(angle))</span></span><br><span class="line">    xyC = x * yC</span><br><span class="line">    <span class="comment"># 计算 y * z * (1 - cos(angle))</span></span><br><span class="line">    yzC = y * zC</span><br><span class="line">    <span class="comment"># 计算 z * x * (1 - cos(angle))</span></span><br><span class="line">    zxC = z * xC</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化一个全零的 4x4 旋转矩阵，形状为 (batch_size, 4, 4)</span></span><br><span class="line">    rot = torch.zeros((vec.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>)).to(device=vec.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置旋转矩阵的元素</span></span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">0</span>] = torch.squeeze(x * xC + ca)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">1</span>] = torch.squeeze(xyC - zs)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">2</span>] = torch.squeeze(zxC + ys)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">0</span>] = torch.squeeze(xyC + zs)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">1</span>] = torch.squeeze(y * yC + ca)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">2</span>] = torch.squeeze(yzC - xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">0</span>] = torch.squeeze(zxC - ys)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">1</span>] = torch.squeeze(yzC + xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">2</span>] = torch.squeeze(z * zC + ca)</span><br><span class="line">    rot[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积块，包含卷积层、批量归一化层和 ELU 激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;执行卷积后接 ELU 的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3x3 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = Conv3x3(in_channels, out_channels)</span><br><span class="line">        <span class="comment"># ELU 激活函数，原地操作</span></span><br><span class="line">        <span class="variable language_">self</span>.nonlin = nn.ELU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 批量归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.bn = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 通过卷积层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        <span class="comment"># 通过批量归一化层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.bn(out)</span><br><span class="line">        <span class="comment"># 通过 ELU 激活函数</span></span><br><span class="line">        out = <span class="variable language_">self</span>.nonlin(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个 3x3 卷积层，包含填充操作</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对输入进行填充和卷积的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, use_refl=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv3x3, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_refl:</span><br><span class="line">            <span class="comment"># 使用反射填充</span></span><br><span class="line">            <span class="variable language_">self</span>.pad = nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用零填充</span></span><br><span class="line">            <span class="variable language_">self</span>.pad = nn.ZeroPad2d(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(<span class="built_in">int</span>(in_channels), <span class="built_in">int</span>(out_channels), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 进行填充操作</span></span><br><span class="line">        out = <span class="variable language_">self</span>.pad(x)</span><br><span class="line">        <span class="comment"># 进行卷积操作</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将深度图像转换为点云的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BackprojectDepth</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将深度图像转换为点云的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width</span>):</span><br><span class="line">        <span class="built_in">super</span>(BackprojectDepth, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 批量大小</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="variable language_">self</span>.height = height</span><br><span class="line">        <span class="comment"># 图像宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.width = width</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成二维网格坐标</span></span><br><span class="line">        meshgrid = np.meshgrid(<span class="built_in">range</span>(<span class="variable language_">self</span>.width), <span class="built_in">range</span>(<span class="variable language_">self</span>.height), indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line">        <span class="comment"># 将网格坐标堆叠在一起，并转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.id_coords = np.stack(meshgrid, axis=<span class="number">0</span>).astype(np.float32)</span><br><span class="line">        <span class="comment"># 将网格坐标转换为 PyTorch 张量，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.id_coords = nn.Parameter(torch.from_numpy(<span class="variable language_">self</span>.id_coords),</span><br><span class="line">                                      requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一个全为 1 的张量，形状为 (batch_size, 1, height * width)，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.ones = nn.Parameter(torch.ones(<span class="variable language_">self</span>.batch_size, <span class="number">1</span>, <span class="variable language_">self</span>.height * <span class="variable language_">self</span>.width),</span><br><span class="line">                                 requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调整网格坐标的形状，并重复 batch_size 次</span></span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = torch.unsqueeze(torch.stack(</span><br><span class="line">            [<span class="variable language_">self</span>.id_coords[<span class="number">0</span>].view(-<span class="number">1</span>), <span class="variable language_">self</span>.id_coords[<span class="number">1</span>].view(-<span class="number">1</span>)], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = <span class="variable language_">self</span>.pix_coords.repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将网格坐标和全 1 张量拼接在一起，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = nn.Parameter(torch.cat([<span class="variable language_">self</span>.pix_coords, <span class="variable language_">self</span>.ones], <span class="number">1</span>),</span><br><span class="line">                                       requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, depth, inv_K</span>):</span><br><span class="line">        <span class="comment"># 将逆相机内参矩阵与像素坐标相乘</span></span><br><span class="line">        cam_points = torch.matmul(inv_K[:, :<span class="number">3</span>, :<span class="number">3</span>], <span class="variable language_">self</span>.pix_coords)</span><br><span class="line">        <span class="comment"># 将深度值与相机坐标相乘</span></span><br><span class="line">        cam_points = depth.view(<span class="variable language_">self</span>.batch_size, <span class="number">1</span>, -<span class="number">1</span>) * cam_points</span><br><span class="line">        <span class="comment"># 将相机坐标和全 1 张量拼接在一起</span></span><br><span class="line">        cam_points = torch.cat([cam_points, <span class="variable language_">self</span>.ones], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cam_points</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 3D 点投影到具有内参 K 和位置 T 的相机中的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Project3D</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将 3D 点投影到具有内参 K 和位置 T 的相机中的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width, eps=<span class="number">1e-7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Project3D, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 批量大小</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="variable language_">self</span>.height = height</span><br><span class="line">        <span class="comment"># 图像宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.width = width</span><br><span class="line">        <span class="comment"># 防止除零的小常数</span></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, points, K, T</span>):</span><br><span class="line">        <span class="comment"># 计算投影矩阵 P</span></span><br><span class="line">        P = torch.matmul(K, T)[:, :<span class="number">3</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将投影矩阵 P 与 3D 点相乘</span></span><br><span class="line">        cam_points = torch.matmul(P, points)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算像素坐标</span></span><br><span class="line">        pix_coords = cam_points[:, :<span class="number">2</span>, :] / (cam_points[:, <span class="number">2</span>, :].unsqueeze(<span class="number">1</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line">        <span class="comment"># 调整像素坐标的形状</span></span><br><span class="line">        pix_coords = pix_coords.view(<span class="variable language_">self</span>.batch_size, <span class="number">2</span>, <span class="variable language_">self</span>.height, <span class="variable language_">self</span>.width)</span><br><span class="line">        <span class="comment"># 交换维度</span></span><br><span class="line">        pix_coords = pix_coords.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 归一化像素坐标</span></span><br><span class="line">        pix_coords[..., <span class="number">0</span>] /= <span class="variable language_">self</span>.width - <span class="number">1</span></span><br><span class="line">        pix_coords[..., <span class="number">1</span>] /= <span class="variable language_">self</span>.height - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将像素坐标映射到 [-1, 1] 范围</span></span><br><span class="line">        pix_coords = (pix_coords - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> pix_coords</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入张量上采样 2 倍</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upsample</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将输入张量上采样 2 倍</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> F.interpolate(x, scale_factor=<span class="number">2</span>, mode=<span class="string">&quot;nearest&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算视差图像的平滑损失</span></span><br><span class="line"><span class="comment"># 彩色图像用于边缘感知平滑</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_smooth_loss</span>(<span class="params">disp, img</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算视差图像的平滑损失</span></span><br><span class="line"><span class="string">    彩色图像用于边缘感知平滑</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算视差在 x 方向的梯度</span></span><br><span class="line">    grad_disp_x = torch.<span class="built_in">abs</span>(disp[:, :, :, :-<span class="number">1</span>] - disp[:, :, :, <span class="number">1</span>:])</span><br><span class="line">    <span class="comment"># 计算视差在 y 方向的梯度</span></span><br><span class="line">    grad_disp_y = torch.<span class="built_in">abs</span>(disp[:, :, :-<span class="number">1</span>, :] - disp[:, :, <span class="number">1</span>:, :])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算图像在 x 方向的平均梯度</span></span><br><span class="line">    grad_img_x = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :, :-<span class="number">1</span>] - img[:, :, :, <span class="number">1</span>:]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 计算图像在 y 方向的平均梯度</span></span><br><span class="line">    grad_img_y = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :-<span class="number">1</span>, :] - img[:, :, <span class="number">1</span>:, :]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据图像梯度对视差梯度进行加权</span></span><br><span class="line">    grad_disp_x *= torch.exp(-grad_img_x)</span><br><span class="line">    grad_disp_y *= torch.exp(-grad_img_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回视差梯度的平均值</span></span><br><span class="line">    <span class="keyword">return</span> grad_disp_x.mean() + grad_disp_y.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算一对图像之间 SSIM 损失的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SSIM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算一对图像之间 SSIM 损失的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SSIM, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算均值</span></span><br><span class="line">        <span class="variable language_">self</span>.mu_x_pool   = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mu_y_pool   = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算方差</span></span><br><span class="line">        <span class="variable language_">self</span>.sig_x_pool  = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sig_y_pool  = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算协方差</span></span><br><span class="line">        <span class="variable language_">self</span>.sig_xy_pool = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反射填充层</span></span><br><span class="line">        <span class="variable language_">self</span>.refl = nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 常数 C1</span></span><br><span class="line">        <span class="variable language_">self</span>.C1 = <span class="number">0.01</span> ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 常数 C2</span></span><br><span class="line">        <span class="variable language_">self</span>.C2 = <span class="number">0.03</span> ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 对输入图像进行反射填充</span></span><br><span class="line">        x = <span class="variable language_">self</span>.refl(x)</span><br><span class="line">        y = <span class="variable language_">self</span>.refl(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算图像 x 的均值</span></span><br><span class="line">        mu_x = <span class="variable language_">self</span>.mu_x_pool(x)</span><br><span class="line">        <span class="comment"># 计算图像 y 的均值</span></span><br><span class="line">        mu_y = <span class="variable language_">self</span>.mu_y_pool(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算图像 x 的方差</span></span><br><span class="line">        sigma_x  = <span class="variable language_">self</span>.sig_x_pool(x ** <span class="number">2</span>) - mu_x ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 计算图像 y 的方差</span></span><br><span class="line">        sigma_y  = <span class="variable language_">self</span>.sig_y_pool(y ** <span class="number">2</span>) - mu_y ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 计算图像 x 和 y 的协方差</span></span><br><span class="line">        sigma_xy = <span class="variable language_">self</span>.sig_xy_pool(x * y) - mu_x * mu_y</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 SSIM 分子</span></span><br><span class="line">        SSIM_n = (<span class="number">2</span> * mu_x * mu_y + <span class="variable language_">self</span>.C1) * (<span class="number">2</span> * sigma_xy + <span class="variable language_">self</span>.C2)</span><br><span class="line">        <span class="comment"># 计算 SSIM 分母</span></span><br><span class="line">        SSIM_d = (mu_x ** <span class="number">2</span> + mu_y ** <span class="number">2</span> + <span class="variable language_">self</span>.C1) * (sigma_x + sigma_y + <span class="variable language_">self</span>.C2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 SSIM 损失，并将结果限制在 [0, 1] 范围内</span></span><br><span class="line">        <span class="keyword">return</span> torch.clamp((<span class="number">1</span> - SSIM_n / SSIM_d) / <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测深度和真实深度之间的误差指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_depth_errors</span>(<span class="params">gt, pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测深度和真实深度之间的误差指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算预测深度和真实深度的比值的最大值</span></span><br><span class="line">    thresh = torch.<span class="built_in">max</span>((gt / pred), (pred / gt))</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25 的比例</span></span><br><span class="line">    a1 = (thresh &lt; <span class="number">1.25</span>     ).<span class="built_in">float</span>().mean()</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25^2 的比例</span></span><br><span class="line">    a2 = (thresh &lt; <span class="number">1.25</span> ** <span class="number">2</span>).<span class="built_in">float</span>().mean()</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25^3 的比例</span></span><br><span class="line">    a3 = (thresh &lt; <span class="number">1.25</span> ** <span class="number">3</span>).<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算均方误差</span></span><br><span class="line">    rmse = (gt - pred) ** <span class="number">2</span></span><br><span class="line">    rmse = torch.sqrt(rmse.mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算对数均方误差</span></span><br><span class="line">    rmse_log = (torch.log(gt) - torch.log(pred)) ** <span class="number">2</span></span><br><span class="line">    rmse_log = torch.sqrt(rmse_log.mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算绝对相对误差</span></span><br><span class="line">    abs_rel = torch.mean(torch.<span class="built_in">abs</span>(gt - pred) / gt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平方相对误差</span></span><br><span class="line">    sq_rel = torch.mean((gt - pred) ** <span class="number">2</span> / gt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> abs_rel, sq_rel, rmse, rmse_log, a</span><br></pre></td></tr></table></figure>
<h4 id="nerf-feature-py">nerf_feature.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个名为 NerfWFeatures 的神经网络模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NerfWFeatures</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D:           标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类 nn.Module 的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络块，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络块，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims + <span class="number">32</span>, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义用于计算密度的全连接层，最后使用 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(</span><br><span class="line">            nn.Linear(D, <span class="number">1</span>), nn.Softplus()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义用于提取特征的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Sequential(</span><br><span class="line">            nn.Linear(D, D)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义用于处理特征和方向信息以生成中间特征的层</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义用于从中间特征生成 RGB 颜色的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc, cost_volume</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络块处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果、原始编码位置和代价体进行拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc, cost_volume], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, D+pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络块处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将提取的特征和编码后的方向进行拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, D+dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 rgb_layers 层处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># (H, W, N_sample, D/2)</span></span><br><span class="line">        <span class="comment"># 生成 RGB 颜色</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 颜色和密度进行拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br></pre></td></tr></table></figure>
<h4 id="nerf-mask-py">nerf_mask.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个名为 OfficialNerf 的神经网络模块，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OfficialNerf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D: 标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(OfficialNerf, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络序列，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络序列，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义掩码网络，包含两个线性层，中间有 ReLU 激活函数，最后有 Sigmoid 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_net = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, <span class="number">1</span>), nn.Sigmoid())</span><br><span class="line">        <span class="comment"># 定义密度预测网络，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(nn.Linear(D, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 定义特征提取的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Linear(D, D)</span><br><span class="line">        <span class="comment"># 定义 RGB 处理的神经网络序列，包含一个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义 RGB 预测的神经网络序列，包含一个线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络序列处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果和原始编码位置在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过掩码网络得到掩码概率</span></span><br><span class="line">        mask_prob = <span class="variable language_">self</span>.mask_net(x)</span><br><span class="line">        <span class="comment"># 通过密度预测网络得到密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过特征提取线性层得到特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将特征和编码后的方向在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 处理神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D / 2)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 预测神经网络序列得到 RGB 值</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 值、密度和掩码概率在第3维拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density, mask_prob], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br></pre></td></tr></table></figure>
<h4 id="nerf-models-py">nerf_models.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 OfficialNerf 类，继承自 nn.Module，用于实现官方的 NeRF 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OfficialNerf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D: 标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(OfficialNerf, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络序列，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络序列，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义密度预测网络，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(nn.Linear(D, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 定义特征提取的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Linear(D, D)</span><br><span class="line">        <span class="comment"># 定义 RGB 处理的神经网络序列，包含一个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义 RGB 预测的神经网络序列，包含一个线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络序列处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果和原始编码位置在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过密度预测网络得到密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过特征提取线性层得到特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将特征和编码后的方向在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 处理神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D / 2)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 预测神经网络序列得到 RGB 值</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 值和密度在第 3 维拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 fullNeRF 类，继承自 nn.Module，用于实现完整的 NeRF 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">fullNeRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels_xyz, in_channels_dir, W, D=<span class="number">8</span>, skips=[<span class="number">4</span>]</span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 存储网络的深度</span></span><br><span class="line">        <span class="variable language_">self</span>.D = D</span><br><span class="line">        <span class="comment"># 存储隐藏层的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="comment"># 存储跳跃连接的位置</span></span><br><span class="line">        <span class="variable language_">self</span>.skips = skips</span><br><span class="line">        <span class="comment"># 存储输入位置编码的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.in_channels_xyz = in_channels_xyz</span><br><span class="line">        <span class="comment"># 存储输入方向编码的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.in_channels_dir = in_channels_dir</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义位置编码层</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(D):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 第一层，输入维度为输入位置编码的通道数，输出维度为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(in_channels_xyz, W)</span><br><span class="line">            <span class="keyword">elif</span> i <span class="keyword">in</span> skips:</span><br><span class="line">                <span class="comment"># 跳跃连接层，输入维度为隐藏层宽度加上输入位置编码的通道数，输出维度为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(W + in_channels_xyz, W)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 普通层，输入和输出维度均为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(W, W)</span><br><span class="line">            <span class="comment"># 为每个层添加 ReLU 激活函数</span></span><br><span class="line">            layer = nn.Sequential(layer, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">            <span class="comment"># 将层添加到模型中</span></span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;xyz_encoding_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>, layer)</span><br><span class="line">        <span class="comment"># 定义位置编码的最终线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.xyz_encoding_final = nn.Linear(W, W)</span><br><span class="line">        <span class="comment"># 定义方向编码层</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_encoding = nn.Sequential(</span><br><span class="line">            nn.Linear(W + in_channels_dir, W), nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(W, W // <span class="number">2</span>), nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义静态输出层</span></span><br><span class="line">        <span class="comment"># 静态密度预测层，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.static_sigma = nn.Sequential(nn.Linear(W, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 静态 RGB 预测层，包含两个线性层，中间有 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.static_rgb = nn.Sequential(nn.Linear(W // <span class="number">2</span>, W // <span class="number">2</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Linear(W // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_xyz, input_dir_a</span>):</span><br><span class="line">        <span class="comment"># 存储输入的位置编码</span></span><br><span class="line">        xyz_ = input_xyz</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.D):</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.skips:</span><br><span class="line">                <span class="comment"># 如果是跳跃连接位置，将输入的位置编码和当前处理结果拼接</span></span><br><span class="line">                xyz_ = torch.cat([input_xyz, xyz_], -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 通过相应的位置编码层处理</span></span><br><span class="line">            xyz_ = <span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;xyz_encoding_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>)(xyz_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过静态密度预测层得到静态密度</span></span><br><span class="line">        static_sigma = <span class="variable language_">self</span>.static_sigma(xyz_)  <span class="comment"># 输出形状为 (B, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过位置编码的最终线性层得到最终位置编码</span></span><br><span class="line">        xyz_encoding_final = <span class="variable language_">self</span>.xyz_encoding_final(xyz_)</span><br><span class="line">        <span class="comment"># 将最终位置编码和输入的方向编码拼接</span></span><br><span class="line">        dir_encoding_input = torch.cat([xyz_encoding_final, input_dir_a], -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 通过方向编码层处理拼接后的结果</span></span><br><span class="line">        dir_encoding = <span class="variable language_">self</span>.dir_encoding(dir_encoding_input)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过静态 RGB 预测层得到静态 RGB 值</span></span><br><span class="line">        static_rgb = <span class="variable language_">self</span>.static_rgb(dir_encoding)</span><br><span class="line">        <span class="comment"># 将静态 RGB 值和静态密度拼接</span></span><br><span class="line">        static = torch.cat([static_rgb, static_sigma], -<span class="number">1</span>)  <span class="comment"># 输出形状为 (B, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> static</span><br></pre></td></tr></table></figure>
<h4 id="poses-py">poses.py</h4>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> make_c2w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LearnPose 类，继承自 nn.Module，用于学习相机位姿</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearnPose</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_cams, learn_R, learn_t, init_c2w=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param num_cams: 相机的数量</span></span><br><span class="line"><span class="string">        :param learn_R: 是否学习旋转部分，布尔值</span></span><br><span class="line"><span class="string">        :param learn_t: 是否学习平移部分，布尔值</span></span><br><span class="line"><span class="string">        :param init_c2w: (N, 4, 4) 的 torch 张量，表示初始的相机到世界的变换矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(LearnPose, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 存储相机的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.num_cams = num_cams</span><br><span class="line">        <span class="comment"># 初始化初始相机到世界的变换矩阵为 None</span></span><br><span class="line">        <span class="variable language_">self</span>.init_c2w = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> init_c2w <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果提供了初始变换矩阵，将其作为不可训练的参数存储</span></span><br><span class="line">            <span class="variable language_">self</span>.init_c2w = nn.Parameter(init_c2w, requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义旋转参数，初始化为全零，是否可训练由 learn_R 决定</span></span><br><span class="line">        <span class="variable language_">self</span>.r = nn.Parameter(torch.zeros(size=(num_cams, <span class="number">3</span>), dtype=torch.float32), requires_grad=learn_R)  <span class="comment"># (N, 3)</span></span><br><span class="line">        <span class="comment"># 定义平移参数，初始化为全零，是否可训练由 learn_t 决定</span></span><br><span class="line">        <span class="variable language_">self</span>.t = nn.Parameter(torch.zeros(size=(num_cams, <span class="number">3</span>), dtype=torch.float32), requires_grad=learn_t)  <span class="comment"># (N, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, cam_id</span>):</span><br><span class="line">        <span class="comment"># 根据相机 ID 提取对应的旋转参数，形状为 (3, )，表示轴角</span></span><br><span class="line">        r = <span class="variable language_">self</span>.r[cam_id]  <span class="comment"># (3, ) 轴角</span></span><br><span class="line">        <span class="comment"># 根据相机 ID 提取对应的平移参数，形状为 (3, )</span></span><br><span class="line">        t = <span class="variable language_">self</span>.t[cam_id]  <span class="comment"># (3, )</span></span><br><span class="line">        <span class="comment"># 使用 make_c2w 函数将轴角和平移参数转换为相机到世界的变换矩阵，形状为 (4, 4)</span></span><br><span class="line">        c2w = make_c2w(r, t)  <span class="comment"># (4, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果提供了初始变换矩阵，学习初始位姿和目标位姿之间的增量位姿</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.init_c2w <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 将当前计算得到的变换矩阵与初始变换矩阵相乘</span></span><br><span class="line">            c2w = c2w @ <span class="variable language_">self</span>.init_c2w[cam_id]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> c2w</span><br></pre></td></tr></table></figure>
<h2 id="utils">utils</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── utils/  # 工具文件夹</span><br><span class="line">│   ├── align_traj.py  # 轨迹对齐脚本文件，用于对不同轨迹数据进行对齐操作</span><br><span class="line">│   ├── comp_ate.py  # 计算绝对轨迹误差（Absolute Trajectory Error, ATE）的脚本文件</span><br><span class="line">│   ├── comp_ray_dir.py  # 计算光线方向的脚本文件，常用于计算机视觉和三维重建中的光线追踪等场景</span><br><span class="line">│   ├── lie_group_helper.py  # 李群相关辅助函数的脚本文件，李群在机器人运动学、计算机视觉中的位姿表示等方面有应用</span><br><span class="line">│   ├── pos_enc.py  # 位置编码脚本文件，在深度学习模型（如NeRF）中用于对位置信息进行编码</span><br><span class="line">│   ├── pose_utils.py  # 位姿处理工具脚本文件，包含处理相机位姿或物体位姿的相关函数</span><br><span class="line">│   ├── split_dataset.py  # 数据集划分脚本文件，用于将数据集划分为训练集、验证集和测试集等</span><br><span class="line">│   ├── training_utils.py  # 训练辅助工具脚本文件，包含训练模型时常用的工具函数，如学习率调整、损失函数计算等</span><br><span class="line">│   ├── vgg.py  # VGG网络相关脚本文件，可能包含VGG模型的定义、加载预训练权重等操作</span><br><span class="line">│   ├── vis_cam_traj.py  # 可视化相机轨迹的脚本文件，用于将相机在三维空间中的运动轨迹进行可视化展示</span><br><span class="line">│   └── volume_op.py  # 体操作脚本文件，在三维重建、体渲染等场景中对三维体数据进行操作</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕论文知识库</title>
    <url>/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/</url>
    <content><![CDATA[<p>手撕论文知识库</p>
<h2 id="深度学习项目代码目录全览及解析">深度学习项目代码目录全览及解析</h2>
<blockquote>
<p>常见目录如下：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">project_name/</span><br><span class="line">├── data/                   # 数据集相关（原始/处理后的数据）</span><br><span class="line">├── dataloader/             # 数据加载与预处理模块（核心）</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── dataset.py          # 自定义Dataset类</span><br><span class="line">│   └── transforms.py       # 数据增强操作</span><br><span class="line">├── models/                 # 模型定义（核心）</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── backbone.py         # 主干网络</span><br><span class="line">│   └── layers.py           # 自定义网络层</span><br><span class="line">├── configs/                # 超参数配置文件（如YAML/JSON）</span><br><span class="line">├── utils/                  # 工具函数（如日志、评估指标）</span><br><span class="line">│   ├── data_utils.py</span><br><span class="line">│   ├── model_utils.py</span><br><span class="line">│   ├── visualization_utils.py</span><br><span class="line">│   └──...</span><br><span class="line">├── logs/                   # 记录训练和评估过程中的日志信息</span><br><span class="line">│   ├── training.log</span><br><span class="line">│   ├── validation.log</span><br><span class="line">│   └──...</span><br><span class="line">├── checkpoints/            # 训练保存的模型权重</span><br><span class="line">├── scripts/                # 运行脚本（训练/测试命令）</span><br><span class="line">├── requirements.txt        # 依赖库列表</span><br><span class="line">├── environment.yml         # Conda环境配置</span><br><span class="line">├── README.md               # 项目说明</span><br><span class="line">└── main.py                 # 主程序入口</span><br></pre></td></tr></table></figure>
<blockquote>
<ol>
<li><code>dataloader/</code></li>
</ol>
<p><strong>作用</strong>：数据加载、预处理、增强（如论文项目中的 <code>with_colmap.py</code> 可能与多视图数据对齐相关）</p>
<p><strong>典型内容</strong>：<code>Dataset</code> 类定义、数据增强函数、特征提取工具（如项目中的 <code>with_feature.py</code>）</p>
<ol start="2">
<li><strong><code>models/</code></strong></li>
</ol>
<p><strong>作用</strong>：定义神经网络模型（如项目中的<code>nerf_models.py</code> 可实现NeRF的核心架构）。</p>
<p><strong>典型内容</strong>：模型类继承<code>torch.nn.Module</code>，包含前向传播逻辑（如项目中的<code>depth_decoder.py</code>可用于深度估计解码）。</p>
<ol start="3">
<li><strong><code>utils/</code></strong></li>
</ol>
<p><strong>作用</strong>：辅助工具（如项目中的 <code>pose_utils.py</code> 处理相机位姿，<code>training_utils.py</code> 封装训练逻辑）。</p>
<p><strong>典型内容</strong>：评估指标计算、可视化工具、训练回调函数。</p>
<ol start="4">
<li><strong><code>third_party/</code></strong></li>
</ol>
<p><strong>作用</strong>：第三方库或工具（如项目中的 <code>ATE</code> 可能用于轨迹评估，<code>pytorch_ssim</code> 实现结构相似性损失）。</p>
<ol start="5">
<li>其他关键要素</li>
</ol>
<p><code>logs</code> 文件夹：记录训练和评估过程中的日志信息，如训练损失、验证损失、准确率等指标的变化情况，便于跟踪模型训练过程，分析模型的收敛性和性能表现。</p>
<p><code>checkpoints</code> 文件夹：保存训练过程中的模型检查点，即模型在不同训练阶段的参数文件，用于在训练中断时恢复训练，或者用于选择在验证集上表现最好的模型进行测试和部署。</p>
<p><code>requirement.txt</code>：列出项目所需的 Python 依赖库及其版本号，便于在新环境中快速安装项目所需的所有依赖。</p>
<p><code>environment.yml</code>：Conda环境配置，确保依赖一致性。</p>
<p><code>README.md</code>：项目说明、安装与使用指南（深度学习项目必备）。</p>
<p><strong><code>main.py</code></strong>：项目的主程序入口，通常包含模型训练、评估和预测的主要逻辑，可通过命令行参数来控制程序的运行方式和参数设置。</p>
</blockquote>
<blockquote>
<p>示例如下：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ - nerf/  # 基于占用率的神经辐射场项目根目录</span><br><span class="line">├──.gitignore  # 指定Git不跟踪的文件或文件夹</span><br><span class="line">├── LICENSE  # 项目使用许可条款文件</span><br><span class="line">├── README.md  # 介绍项目背景、功能、使用方法等的说明文档</span><br><span class="line">├── environment.yml  # 定义项目运行所需软件环境</span><br><span class="line">├── local1.txt  # 用途不明，或为本地说明、配置等文件</span><br><span class="line">├── dataloader/  # 存放数据加载与预处理代码</span><br><span class="line">│   ├── any_folder.py  # 从任意文件夹结构加载数据</span><br><span class="line">│   ├── local_save.py  # 负责数据本地保存</span><br><span class="line">│   ├── with_colmap.py  # 从COLMAP处理后格式加载数据</span><br><span class="line">│   ├── with_feature.py  # 加载带特征的数据</span><br><span class="line">│   ├── with_feature_colmap.py  # 结合COLMAP与特征数据加载</span><br><span class="line">│   └── with_mask.py  # 加载带掩码的数据</span><br><span class="line">├── models/  # 存放深度学习模型定义与操作代码</span><br><span class="line">│   ├── depth_decoder.py  # 深度解码，用于深度估计</span><br><span class="line">│   ├── intrinsics.py  # 处理相机内参相关内容</span><br><span class="line">│   ├── layers.py  # 定义深度学习层结构</span><br><span class="line">│   ├── nerf_feature.py  # 处理NeRF特征相关逻辑</span><br><span class="line">│   ├── nerf_mask.py  # 处理NeRF模型中掩码相关内容</span><br><span class="line">│   ├── nerf_models.py  # 定义NeRF模型架构等核心内容</span><br><span class="line">│   └── poses.py  # 处理位姿相关操作</span><br><span class="line">├── utils/  # 包含辅助项目运行的工具函数</span><br><span class="line">│   ├── align_traj.py  # 实现轨迹对齐算法</span><br><span class="line">│   ├── comp_ate.py  # 计算绝对轨迹误差</span><br><span class="line">│   ├── comp_ray_dir.py  # 计算光线方向</span><br><span class="line">│   ├── lie_group_helper.py  # 提供李群相关辅助函数</span><br><span class="line">│   ├── pos_enc.py  # 实现位置编码</span><br><span class="line">│   ├── pose_utils.py  # 提供位姿相关实用工具函数</span><br><span class="line">│   ├── split_dataset.py  # 划分数据集为训练、验证、测试集</span><br><span class="line">│   ├── training_utils.py  # 提供模型训练辅助函数</span><br><span class="line">│   ├── vgg.py  # 与VGG神经网络相关操作</span><br><span class="line">│   ├── vis_cam_traj.py  # 可视化相机轨迹</span><br><span class="line">│   └── volume_op.py  # 操作三维体数据</span><br><span class="line">├── tasks/  # 存放训练、测试等具体任务代码</span><br><span class="line">│   └──...</span><br><span class="line">└── third_party/  # 存放第三方代码或库</span><br><span class="line">    ├── ATE/  # 与绝对轨迹误差计算相关</span><br><span class="line">    │   └── README.md  # 说明该部分功能与用法</span><br><span class="line">    └── pytorch_ssim/  # 计算结构相似性指数的库</span><br></pre></td></tr></table></figure>
<h2 id="深度学习-神经网络架构总览">深度学习/神经网络架构总览</h2>
<h2 id="PyTorch">PyTorch</h2>
<h3 id="什么是torch">什么是torch</h3>
<p>Torch 是 PyTorch 深度学习框架的核心库，具备强大的功能与广泛的用途。它提供了丰富的张量操作，可在 CPU 或 GPU 上高效计算，能轻松处理各类数据；其自动求导机制极大简化了深度学习中梯度计算与反向传播的过程，让模型训练更为便捷。借助<code>torch.nn</code>模块可方便构建如 CNN、RNN 等复杂神经网络架构，<code>torch.optim</code>模块提供多种优化算法用于模型参数更新。此外，Torch 还支持预训练模型的使用与微调，结合可视化工具能助力监控训练过程，广泛应用于图像、自然语言处理、推荐系统等诸多领域。</p>
<h3 id="torch常用函数和功能">torch常用函数和功能</h3>
<h4 id="张量">张量</h4>
<h5 id="什么是张量">什么是张量</h5>
<p>张量是多维数组的泛化表示，可理解为一个多维的数据容器，零维张量是标量，一维张量是向量，二维张量是矩阵，三维及以上则是更高阶的张量。在深度学习里，使用张量是因为它能够高效地表示和处理大量的数据，像图像可表示为三维张量（高度、宽度、通道数），视频可表示为四维张量（帧数、高度、宽度、通道数）。并且，深度学习框架（如 PyTorch）针对张量运算进行了高度优化，能利用 GPU 等硬件加速计算，张量还能自然地支持自动求导机制，方便进行模型训练时的梯度计算和参数更新。</p>
<p>张量是 PyTorch 中最基础的数据结构，类似于 NumPy 的多维数组，但它可以在 GPU 上进行加速计算，并且支持自动求导等深度学习所需的特性。</p>
<h5 id="张量的维度">张量的维度</h5>
<p>维度（也称为轴）是指张量在某个方向上的延伸。可以将维度理解为数据组织的一个方向或一个层次，类似于在地理坐标系统中，经度和纬度分别代表了不同的方向，张量的每个维度也代表了数据的一个特定方向的排列。维度的数量被称为张量的阶（rank），零阶张量是标量（一个单独的数值），一阶张量是向量（一维数组），二阶张量是矩阵（二维数组），三阶及以上的张量则用于表示更复杂的数据结构。</p>
<p><strong>注：维度从0开始算起，比如对于二阶张量，维度0代表行，维度1代表列</strong></p>
<p><strong>另：维度排列遵循（$a_n$, $a_{n-1}$, …, $a_1$）的形式，数字越前代表越高维的堆叠。比如<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起</strong></p>
<p><strong>1.零阶张量（标量）</strong></p>
<p>零阶张量只有一个数值，它没有方向的概念，维度数量为 0。例如这里的 <code>scalar</code> 就是一个零阶张量，它代表一个单一的数值，不涉及方向或多个元素的排列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scalar = torch.tensor(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标量的维度数量:&quot;</span>, scalar.dim()) </span><br></pre></td></tr></table></figure>
<p><strong>2.一阶张量（向量）</strong></p>
<p>一阶张量可以看作是一个向量，它有一个维度。这个维度代表了向量中元素的排列方向，向量的长度就是这个维度的大小。例如<code>vector</code> 是一个一阶张量，维度数量为 1，该维度的大小为 4，表示向量中有 4 个元素。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">vector = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量的维度数量:&quot;</span>, vector.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量在该维度的大小:&quot;</span>, vector.size(<span class="number">0</span>)) </span><br></pre></td></tr></table></figure>
<p><strong>3.二阶张量（矩阵）</strong></p>
<p>二阶张量是一个矩阵，有两个维度，通常称为行和列。第一个维度代表矩阵的行方向，第二个维度代表矩阵的列方向。例如<code>matrix</code> 是一个 2 行 3 列的矩阵，第一个维度的大小为 2 表示有 2 行，第二个维度的大小为 3 表示有 3 列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">matrix = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵的维度数量:&quot;</span>, matrix.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第一个维度（行）的大小:&quot;</span>, matrix.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第二个维度（列）的大小:&quot;</span>, matrix.size(<span class="number">1</span>)) </span><br></pre></td></tr></table></figure>
<p><strong>4.高阶张量（图像、视频等）</strong></p>
<p>对于三阶及以上的张量，维度的含义更加丰富，通常与具体的数据类型和应用场景相关。</p>
<p><strong>图像数据</strong>：在处理图像时，通常使用三阶张量。例如，一张彩色图像可以表示为一个形状为 <code>(高度, 宽度, 通道数)</code> 的三阶张量。这里的第一个维度代表图像的高度方向，第二个维度代表图像的宽度方向，第三个维度代表图像的通道（如 RGB 三个通道）。</p>
<p><code>image = torch.randn(224, 224, 3)</code> 这行代码能够随机生成一个形状为 <code>(224, 224, 3)</code> 的张量来模拟图像的三通道数值。</p>
<p>由于 <code>torch.randn()</code> 生成的是服从标准正态分布的随机数，这些数值可能为负数，也可能超出了常见图像像素值的范围（通常是 0 - 255 或者 0 - 1）。在实际的图像处理任务中，如果需要模拟真实图像，可能需要对这些随机值进行进一步的处理，例如通过归一化或裁剪操作将其限制在合适的范围内。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">image = torch.randn(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像张量的维度数量:&quot;</span>, image.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像高度维度的大小:&quot;</span>, image.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像宽度维度的大小:&quot;</span>, image.size(<span class="number">1</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像通道维度的大小:&quot;</span>, image.size(<span class="number">2</span>)) </span><br></pre></td></tr></table></figure>
<p><img src="/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250219032451598.png" alt="image-20250219032451598"></p>
<p><strong>视频数据</strong>：视频可以看作是一系列的图像帧，因此可以用四阶张量表示，形状通常为 <code>(帧数, 高度, 宽度, 通道数)</code>。第一个维度代表视频中的帧数，其余维度与图像张量的含义相同。</p>
<p>*<strong>5.通道</strong></p>
<p>通道指图像中特定类型信息的集合，图像可含一个或多个通道，各通道存储图像某方面特征数据。像单通道存亮度，RGB 三通道分别存红、绿、蓝颜色信息，四通道还多了透明度通道，以此组合完整呈现图像。</p>
<p>通道能实现颜色表示与混合，如 RGB 三通道通过不同数值组合呈现丰富色彩；可用于特征提取与分析，不同通道提供不同特征，助力图像分析和目标识别；还能用于图像合成与特效制作，借助透明度通道可控制图像透明效果实现合成。</p>
<p>在图片里，灰度图用单通道呈现黑白影像；彩色照片靠 RGB 三通道展示多彩画面；PNG 图片利用四通道含透明度信息实现图像融合。视频是连续的图片帧，同样利用通道来呈现色彩、进行特效处理，如影视中常见的抠图合成场景就借助了通道特性。</p>
<h5 id="张量相关函数">张量相关函数</h5>
<p><strong>1.创建</strong></p>
<ul>
<li>
<p>创建张量<code>torch.tensor()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>当你有现有的数据存储在 Python 列表或 NumPy 数组中，并且需要将其输入到 PyTorch 模型进行计算时使用。例如，在加载数据集后，将数据转换为张量形式以便后续处理。</p>
<p>从数据存储的角度来看，<code>tensor</code> 存储了 Python 列表 <code>data</code> 中的元素 <code>[1, 2, 3]</code>。它将这些数据以一种高效的、适合计算机处理的方式组织起来，存储在内存中。在这个例子中，<code>tensor</code> 是一个一维张量，形状为 <code>(3,)</code>，这意味着它包含 3 个元素。</p>
<p>在数学运算方面，<code>tensor</code> 可以参与各种数学运算，如加法、乘法、矩阵乘法等。PyTorch 为张量提供了丰富的数学运算函数，这些运算可以在 CPU 或 GPU 上高效执行。</p>
<p>在深度学习的上下文中，<code>tensor</code> 是模型输入、输出以及参数的基本表示形式。例如，在一个简单的全连接神经网络中，输入数据会被转换为张量输入到网络中，网络的权重和偏置也是以张量的形式存储和更新的。在上述例子中，<code>tensor</code> 可以作为一个简单的输入数据示例，如果要构建一个神经网络处理这个输入，可能会进行如下操作（以下是一个简单示例）：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 3，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(<span class="number">0</span>)  <span class="comment"># 转换为适合输入模型的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型输出:&quot;</span>, output)</span><br></pre></td></tr></table></figure>
<ul>
<li>
<p>创建全零张量<code>torch.zeros()</code></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">zeros_tensor = torch.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>常用于初始化某些变量，如在初始化神经网络的偏置项时，可使用全零张量。另外，在需要填充零值进行数据预处理或占位时也会用到。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<p>传入的参数 <code>(2, 3)</code>对应创建的 <code>zeros_tensor</code> 是一个 2 行 3 列的二维张量。如果使用 <code>torch.zeros((2, 3, 4))</code> 这样的代码，那么创建的就是一个三维张量，其中 <code>2</code> 表示最外层维度的大小（可以想象成有 2 个二维矩阵堆叠在一起），<code>3</code> 表示每个二维矩阵的行数，<code>4</code> 表示每个二维矩阵的列数。依此类推，对于更高维的张量，每个数字都代表对应维度上的大小。相当于高是2，行是3，列是4</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>如果是<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起，数字越前就代表越高维的堆叠。</p>
<ul>
<li>
<p>创建全一张量**<code>torch.ones()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">ones_tensor = torch.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>与 <code>torch.zeros()</code> 类似，可用于初始化特定变量。在一些归一化操作或需要特定初始值为 1 的场景中会使用。</p>
<ul>
<li>
<p>创建指定形状的随机张量，元素值在[0 , 1)之间**<code>torch.rand()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">random_tensor = torch.rand((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>在初始化神经网络的权重时，随机初始化是常见的做法，可使用 <code>torch.rand()</code> 生成初始权重张量。</p>
<p><strong>2.操作</strong></p>
<ul>
<li>
<p><strong><code>torch.cat()</code></strong>：用于在指定维度上拼接多个张量。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">c = torch.cat((a, b), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>当需要将多个张量合并为一个更大的张量时使用。例如，在处理多模态数据时，将不同模态的特征张量拼接在一起。</p>
<p><code>torch.cat((a, b), dim=1)</code> 表示在维度 1（列方向）上对张量 <code>a</code> 和 <code>b</code> 进行拼接。可以看到，拼接后的张量 <code>c</code> 是将 <code>a</code> 和 <code>b</code> 的列进行了合并，行数不变，列数变为原来两个张量列数之和。在处理多模态数据时，比如一个模态的数据特征用张量 <code>a</code> 表示，另一个模态的数据特征用张量 <code>b</code> 表示，通过这种拼接操作可以将不同模态的特征合并在一起，方便后续的处理。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
<p>在二维张量的语境下，维度 0 代表行方向。<code>torch.cat((a, b), dim=0)</code> 会将张量 <code>b</code> 按行的顺序拼接到张量 <code>a</code> 的下方，拼接后的张量列数不变，行数为原来两个张量行数之和。在实际应用中，若 <code>a</code> 和 <code>b</code> 分别表示两组样本数据，在维度 0 上拼接就相当于将这两组样本合并成一组更大的样本集。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">在维度 <span class="number">0</span> 上拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong><code>torch.reshape()</code></strong>：改变张量的形状。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([1, 2, 3, 4])</span><br><span class="line">reshaped_x = torch.reshape(x, (2, 2))</span><br></pre></td></tr></table></figure>
<p>在神经网络中，不同层之间的数据形状可能需要进行调整，使用 <code>torch.reshape()</code> 可以方便地改变张量形状以满足层的输入要求。</p>
<p><code>torch.reshape(x, (2, 2))</code> 是将一维张量 <code>x</code> 重塑为二维张量 <code>reshaped_x</code>，形状为 <code>(2, 2)</code>。在神经网络中，不同层之间的数据形状可能不匹配，例如某一层的输出是一维向量，而后续层需要二维矩阵作为输入，这时就可以使用 <code>torch.reshape()</code> 来调整数据的形状，使其满足层的输入要求。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">重塑后的张量 reshaped_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><strong><code>torch.transpose()</code></strong>：交换张量的两个维度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[1, 2], [3, 4]])</span><br><span class="line">transposed_x = torch.transpose(x, 0, 1)</span><br></pre></td></tr></table></figure>
<p>在矩阵运算中，有时需要对矩阵进行转置操作。在图像处理中，可能需要调整图像张量的维度顺序。</p>
<p><code>torch.transpose(x, 0, 1)</code> 表示交换张量 <code>x</code> 的第 0 维和第 1 维。在这个二维矩阵的例子中，就是对矩阵进行了转置操作，原来的行变成了列，列变成了行。在矩阵运算中，矩阵转置是一个常见的操作，例如在计算矩阵乘法时可能需要对矩阵进行转置。在图像处理中，图像张量的维度顺序可能需要调整，比如将 <code>(高度, 宽度, 通道数)</code> 调整为 <code>(通道数, 高度, 宽度)</code> 以适应某些模型的输入要求，这时就可以使用 <code>torch.transpose()</code> 来实现。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">转置后的张量 transposed_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li>
<p><code>torch.squeeze()</code>：移除张量中所有维度为 1 的轴（或指定轴）。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]])  <span class="comment"># 形状 [1, 3, 1]</span></span><br><span class="line">y = torch.squeeze(x)                 <span class="comment"># 形状变为 [3]</span></span><br></pre></td></tr></table></figure>
<p>在张量操作中，某些操作（如池化、索引）可能产生冗余的维度为 1 的轴。例如：</p>
<ul>
<li>处理单通道图像时，通道维度可能为 1。</li>
<li>批量处理单个样本时，批量维度可能为 1。</li>
<li>某些神经网络层的输出可能保留不必要的单维度。</li>
</ul>
<p><code>torch.squeeze()</code> 默认移除所有大小为 1 的维度，也可指定 <code>dim</code> 参数移除特定轴（仅当该轴大小为 1 时生效）。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">输入张量 x：</span><br><span class="line">tensor([[[<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>],</span><br><span class="line">         [<span class="number">3</span>]]])</span><br><span class="line">输出张量 y：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">squeeze() 移除了所有大小为 <span class="number">1</span> 的维度（第 <span class="number">0</span> 维和第 <span class="number">2</span> 维），仅保留第 <span class="number">1</span> 维（大小为 <span class="number">3</span>）。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="自动求导">自动求导</h4>
<h5 id="前向传播（Forward-Propagation）">前向传播（Forward Propagation）</h5>
<ol>
<li>定义</li>
</ol>
<p>前向传播是深度学习模型处理输入数据以产生输出的过程。在这个过程中，输入数据从输入层开始，依次经过神经网络的各个隐藏层，每层都会对输入进行特定的数学变换（如加权求和后通过激活函数），最终到达输出层得到预测结果。可以将其看作是信息从输入向输出流动的过程，每一层根据前一层的输出计算本层的输出，逐步传递直至得到最终输出。</p>
<ol start="2">
<li>作用</li>
</ol>
<p>根据当前模型的参数（权重和偏置）对输入数据进行预测。通过一系列的线性和非线性变换，模型能够学习到输入数据中的特征模式，并将其映射到输出空间。例如，在图像分类任务中，前向传播可以将输入的图像转换为不同类别的概率分布，从而判断图像所属的类别。它为后续的反向传播提供了预测结果，是整个深度学习训练过程的基础步骤。</p>
<h5 id="反向传播（Backward-Propagation）">反向传播（Backward Propagation）</h5>
<ol>
<li>定义</li>
</ol>
<p>反向传播是深度学习中用于计算损失函数（Loss Function）关于模型参数（权重和偏置）的梯度的算法。它基于链式法则，从输出层开始，将损失函数的误差沿着计算图反向传播到输入层，依次计算每一层参数的梯度。简单来说，反向传播是在已知前向传播得到的预测结果和真实标签的情况下，计算如何调整模型参数可以减小损失的过程。</p>
<ol start="2">
<li>作用</li>
</ol>
<p>为模型参数的更新提供依据。在深度学习中，通常使用优化算法（如随机梯度下降，Stochastic Gradient Descent，SGD）来更新模型的参数，而这些优化算法需要知道损失函数关于参数的梯度。反向传播通过高效地计算这些梯度，使得模型能够根据预测误差自动调整参数，从而不断优化模型的性能。通过多次迭代前向传播和反向传播，模型可以逐渐学习到输入数据和输出标签之间的映射关系，提高预测的准确性。</p>
<h5 id="计算图与梯度追踪（可略过）">计算图与梯度追踪（可略过）</h5>
<ol>
<li><code>requires_grad</code>设置张量是否需要进行梯度追踪</li>
</ol>
<p>在深度学习模型训练过程中，需要计算损失函数关于模型参数的梯度，以便使用优化算法（如随机梯度下降）更新参数。通过将模型参数张量的 <code>requires_grad</code> 设置为 <code>True</code>，可以利用自动求导机制自动计算梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出 4.0</span></span><br></pre></td></tr></table></figure>
<p><code>requires_grad</code> 是 PyTorch 张量的一个属性，当将其设置为 <code>True</code> 时，PyTorch 会开始追踪该张量的所有操作，构建计算图。计算图是一个有向无环图，它记录了从输入张量到输出张量的所有操作路径。</p>
<p>在上述示例中，我们创建了一个张量 <code>x</code> 并将 <code>requires_grad</code> 设置为 <code>True</code>，然后定义了一个函数 <code>y = x ** 2</code>。PyTorch 会自动记录这个操作，构建相应的计算图。</p>
<p>调用 <code>y.backward()</code> 方法时，PyTorch 会根据链式法则沿着计算图反向传播，计算出 <code>y</code> 关于 <code>x</code> 的梯度，并将梯度存储在 <code>x.grad</code> 属性中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x 的梯度: tensor([4.])</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><code>grad_fn</code> 属性与反向传播链</li>
</ol>
<p><strong><code>grad_fn</code></strong>：PyTorch 张量的属性，指向创建该张量的函数对象，用于记录操作历史以支持反向传播求梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个需要梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义操作 y = x^2</span></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="comment"># 定义操作 z = y * 3</span></span><br><span class="line">z = y * <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个张量的 grad_fn</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y 的 grad_fn:&quot;</span>, y.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;z 的 grad_fn:&quot;</span>, z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># 打印 x 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x 的梯度:&quot;</span>, x.grad)</span><br></pre></td></tr></table></figure>
<p>在深度学习模型训练时，需要计算损失函数关于模型参数的梯度。由于模型中存在大量复杂的运算，通过 <code>grad_fn</code> 记录的操作历史，PyTorch 可以构建反向传播链，从而准确计算出梯度，为参数更新提供依据。</p>
<p>在上述代码中，首先创建了一个开启梯度追踪的张量 <code>x</code>。当执行 $y = x^2$操作时，PyTorch 会创建一个表示平方操作的函数对象，<code>y.grad_fn</code> 就会指向这个函数对象，以此记录下 <code>y</code> 是由 <code>x</code> 经过平方操作得到的。接着执行 $z = 3y$，同样地，<code>z.grad_fn</code> 会指向表示乘法操作的函数对象，记录 <code>z</code> 的计算来源。</p>
<p>当调用 <code>z.backward()</code> 时，PyTorch 会从 <code>z</code> 开始，依据 <code>z.grad_fn</code> 找到创建 <code>z</code> 的操作，然后通过这个操作回溯到 <code>y</code>，再根据 <code>y.grad_fn</code> 回溯到 <code>x</code>。在这个回溯过程中，按照链式法则逐步计算出 <code>z</code> 关于 <code>x</code> 的梯度，并将其存储在 <code>x.grad</code> 中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y 的 grad_fn: &lt;PowBackward0 object at 0x...&gt;</span><br><span class="line">z 的 grad_fn: &lt;MulBackward0 object at 0x...&gt;</span><br><span class="line">x 的梯度: tensor([12.])</span><br></pre></td></tr></table></figure>
<p><code>&lt;PowBackward0 object at 0x...&gt;</code> 是 PyTorch 中用于表示反向传播过程中特定操作的梯度计算函数对象的字符串表示形式。</p>
<p><strong>反向传播函数对象</strong>：在 PyTorch 的自动求导机制里，对张量进行各种运算（如加法、乘法、幂运算等）时，PyTorch 会构建一个计算图来记录这些操作的顺序和依赖关系。每个操作在计算图中都对应一个前向传播函数（用于计算输出结果）和一个反向传播函数（用于计算梯度）。</p>
<p><code>PowBackward0</code> 的意义：<code>PowBackward0</code> 表示的是幂运算的反向传播函数。执行 $y = x^2$这样的幂运算时，<code>y</code> 的 <code>grad_fn</code> 属性就会指向一个 <code>PowBackward0</code> 对象，这个对象负责在反向传播过程中计算关于输入张量 <code>x</code> 的梯度。<strong><code>at 0x...</code></strong>：<code>at 0x...</code> 后面跟着的是该对象在内存中的地址。这个地址是系统为该对象分配的唯一标识符，用于在内存中定位该对象。</p>
<h5 id="梯度控制（可略过）">梯度控制（可略过）</h5>
<p>1.<code>torch.no_grad()</code> 上下文管理器</p>
<p>用于临时禁止 PyTorch 的梯度计算功能，在其作用域内创建或操作的张量不会进行梯度追踪。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个需要梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 torch.no_grad() 上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y 是否进行梯度追踪:&quot;</span>, y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在上下文管理器外进行操作</span></span><br><span class="line">z = x ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;z 是否进行梯度追踪:&quot;</span>, z.requires_grad)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y 是否进行梯度追踪: False</span><br><span class="line">z 是否进行梯度追踪: True（z在上下文管理器外进行操作）</span><br></pre></td></tr></table></figure>
<p>2.<code>detach()</code> 分离计算图</p>
<p><strong><code>detach()</code></strong>：用于将张量从当前计算图中分离，返回一个和原张量数据相同但不记录梯度信息、不参与反向传播的新张量。</p>
<ul>
<li>在模型评估阶段，只需得到预测结果，无需计算梯度，用 <code>detach()</code> 可节省内存和计算资源。</li>
<li>在多模型联合训练时，若某个模型输出用于另一模型但不影响自身梯度计算，可使用 <code>detach()</code> 分离。（核心还是节省内存和计算资源）</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建需梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line"><span class="comment"># 分离计算图</span></span><br><span class="line">y_detached = y.detach()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原张量 y 是否追踪梯度:&quot;</span>, y.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分离后的张量 y_detached 是否追踪梯度:&quot;</span>, y_detached.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    y_detached.backward()</span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;错误信息: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>上述代码里，<code>x</code> 开启梯度追踪，<code>y = 2 * x</code> 会记录在计算图中。调用 <code>y.detach()</code> 后得到 <code>y_detached</code>，它和 <code>y</code> 数据相同，但不追踪梯度。尝试对 <code>y_detached</code> 反向传播会报错，因为它已和计算图分离，无梯度信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原张量 y 是否追踪梯度: True</span><br><span class="line">分离后的张量 y_detached 是否追踪梯度: False</span><br><span class="line">错误信息: element 0 of tensors does not require grad and does not have a grad_fn</span><br></pre></td></tr></table></figure>
<p>3.<code>zero_grad()</code> (梯度清零)</p>
<p>在深度学习模型的训练过程中，通常会按批次（batch）输入数据进行训练。每次反向传播计算得到的梯度会累加到模型参数的 <code>.grad</code> 属性中。如果不进行梯度清零，那么下一次计算的梯度会与之前的梯度累加，导致梯度计算错误。因此，在每个批次训练开始前，需要调用 <code>zero_grad()</code> 方法将梯度清零，以确保每个批次的梯度计算是独立的。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">target = torch.randn(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(input_tensor)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播计算梯度</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;反向传播后第一个参数的梯度:&quot;</span>, model.weight.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度清零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度清零后第一个参数的梯度:&quot;</span>, model.weight.grad)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">反向传播后第一个参数的梯度: tensor([[ 0.1234, -0.5678, ...]])</span><br><span class="line">梯度清零后第一个参数的梯度: tensor([[ 0., 0., ...]])</span><br></pre></td></tr></table></figure>
<h4 id="神经网络层">神经网络层</h4>
<h5 id="核心基类-nn-Module">核心基类 <code>nn.Module</code></h5>
<h6 id="什么是核心基类-nn-Module？">什么是核心基类 <code>nn.Module</code>？</h6>
<p><strong><code>nn.Module</code></strong>：PyTorch 中所有神经网络模块的基类，用于构建自定义的神经网络模型，封装了模型的结构和参数，方便进行前向传播、参数管理和模型保存等操作。</p>
<p>在深度学习中，我们需要构建各种各样的神经网络模型，如卷积神经网络（CNN）、循环神经网络（RNN）等。<code>nn.Module</code> 提供了一个统一的框架，使得我们可以方便地定义和管理这些模型。无论是简单的全连接网络还是复杂的深度网络，都可以通过继承 <code>nn.Module</code> 来构建。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个简单的神经网络模型，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个全连接层，输入维度为 10，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 定义前向传播过程</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型结构：&quot;</span>, model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状：&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状：&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>__init__</code> 方法</strong>：在自定义的模型类中，<code>__init__</code> 方法用于初始化模型的各个层。通过调用 <code>super(SimpleNet, self).__init__()</code> 确保父类 <code>nn.Module</code> 的初始化被正确执行。然后定义了一个全连接层 <code>self.fc</code>。</li>
<li><strong><code>forward</code> 方法</strong>：<code>forward</code> 方法定义了模型的前向传播过程，即输入数据如何经过各个层得到输出。在这个例子中，输入数据 <code>x</code> 经过全连接层 <code>self.fc</code> 得到输出。</li>
<li>模型实例化和前向传播：创建 <code>SimpleNet</code> 的实例 <code>model</code> 后，将输入张量 <code>input_tensor</code> 传递给 <code>model</code> 就相当于调用了 <code>forward</code> 方法进行前向传播，得到输出 <code>output</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">模型结构： SimpleNet(</span><br><span class="line">  (fc): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">输入张量形状： torch.Size([1, 10])</span><br><span class="line">输出张量形状： torch.Size([1, 1])</span><br></pre></td></tr></table></figure>
<h6 id="自定义模型继承方法">自定义模型继承方法</h6>
<ol>
<li>参数管理<code>parameters()</code>，<code>named_parameters()</code></li>
</ol>
<p><strong><code>parameters()</code></strong>：是 <code>nn.Module</code> 类的一个方法，它返回一个包含模型所有可学习参数的迭代器。通过遍历这个迭代器，能依次获取到模型中的各个参数张量，但不会提供参数的名称信息。</p>
<p><strong><code>named_parameters()</code></strong>：同样是 <code>nn.Module</code> 类的方法，它返回一个迭代器，该迭代器会生成模型可学习参数的名称和对应参数张量的元组。借助这个方法，我们不仅能获取参数张量，还能明确每个参数对应的名称，这在模型参数管理和调试时非常有用。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的神经网络模型，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个全连接层，输入维度为 10，输出维度为 5</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义另一个全连接层，输入维度为 5，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 parameters() 方法获取模型参数的迭代器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用 parameters() 获取参数：&quot;</span>)</span><br><span class="line">params = model.parameters()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数形状: <span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 named_parameters() 方法获取模型带名称的参数迭代器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用 named_parameters() 获取参数：&quot;</span>)</span><br><span class="line">named_params = model.named_parameters()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> named_params:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数名称: <span class="subst">&#123;name&#125;</span>, 参数形状: <span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong><code>parameters()</code></strong>：主要用于优化器初始化。在训练模型时，优化器（如 <code>torch.optim.SGD</code>、<code>torch.optim.Adam</code> 等）需要知道模型的可学习参数，以便对这些参数进行梯度更新。由于优化器只关心参数张量本身，不需要参数名称，所以使用 <code>parameters()</code> 即可。</p>
<p><strong><code>named_parameters()</code></strong>：在模型调试、参数分组优化或模型参数的选择性加载时非常有用。例如，你可能只想更新模型中某些特定层的参数，通过参数名称可以方便地筛选出这些参数；或者在加载预训练模型时，根据参数名称选择性地加载部分参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用 parameters() 获取参数：</span><br><span class="line">参数形状: torch.Size([5, 10])</span><br><span class="line">参数形状: torch.Size([5])</span><br><span class="line">参数形状: torch.Size([1, 5])</span><br><span class="line">参数形状: torch.Size([1])</span><br><span class="line"></span><br><span class="line">使用 named_parameters() 获取参数：</span><br><span class="line">参数名称: fc1.weight, 参数形状: torch.Size([5, 10])</span><br><span class="line">参数名称: fc1.bias, 参数形状: torch.Size([5])</span><br><span class="line">参数名称: fc2.weight, 参数形状: torch.Size([1, 5])</span><br><span class="line">参数名称: fc2.bias, 参数形状: torch.Size([1])</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><code>train()</code> 与 <code>eval()</code> 模式切换</li>
</ol>
<p><code>train()</code> 和 <code>eval()</code> 是 <code>nn.Module</code> 类中的方法，用于切换模型的训练和评估模式。<code>train()</code> 方法将模型设置为训练模式，<code>eval()</code> 方法将模型设置为评估模式，不同模式下部分层（如 <code>Dropout</code>、<code>BatchNorm</code>）会有不同的行为。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个包含 Dropout 层的简单神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置为训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式下 Dropout 是否启用:&quot;</span>, model.dropout.training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置为评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式下 Dropout 是否启用:&quot;</span>, model.dropout.training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output_eval = model(input_tensor)</span><br><span class="line">model.train()</span><br><span class="line">output_train = model(input_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式输出:&quot;</span>, output_eval)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式输出:&quot;</span>, output_train)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>训练模式（<code>train()</code>）</strong>：在模型训练阶段使用，此时模型中的 <code>Dropout</code> 层会按照设定的概率随机丢弃部分神经元，<code>BatchNorm</code> 层会根据当前批次的数据更新统计信息（如均值和方差），有助于提高模型的泛化能力。</li>
<li><strong>评估模式（<code>eval()</code>）</strong>：在模型评估、测试或者推理阶段使用。在评估模式下，<code>Dropout</code> 层不再丢弃神经元，<code>BatchNorm</code> 层使用训练阶段统计得到的均值和方差进行归一化操作，保证评估结果的稳定性和一致性。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练模式下 Dropout 是否启用: True</span><br><span class="line">评估模式下 Dropout 是否启用: False</span><br><span class="line">评估模式输出: tensor([[...]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">训练模式输出: tensor([[...]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>这里具体的输出数值会因随机生成的输入数据和模型初始化不同而有所变化，但可以看到在不同模式下 <code>Dropout</code> 层的行为不同，导致输出结果也可能不同。</p>
<h5 id="网络层-Layers">网络层 (Layers)</h5>
<h6 id="基础层">基础层</h6>
<ol>
<li><code>nn.Linear</code>全连接层</li>
</ol>
<p><strong><code>nn.Linear</code></strong>：PyTorch 中用于创建全连接层（也称为线性层）的类，它对输入数据进行线性变换，即执行矩阵乘法和加法操作，可用于构建各种神经网络模型。</p>
<p><strong>神经网络基础构建</strong>：全连接层是神经网络中最基本的组成部分之一，常用于多层感知机（MLP）的构建，可处理各种类型的数据，如图像、文本等特征向量。</p>
<p><strong>特征映射</strong>：可以将输入数据从一个特征空间映射到另一个特征空间，有助于模型学习数据中的复杂模式和特征表示。</p>
<hr>
<p>为什么是全连接层？</p>
<p>1.连接方式：在全连接层中，每一个输入神经元都与每一个输出神经元相连接，这种连接是 “全” 的，即完全连接。对于</p>
<p><code>nn.Linear(in_features, out_features)</code> ，输入的 <code>in_features</code> 个神经元和输出的 <code>out_features</code> 个神经元之间存在着完整的连接关系。</p>
<p>2.数学运算：假设输入向量$X$维度为$n$（即 <code>in_features</code>），输出向量$Y$维度为$m$（即 <code>out_features</code>），全连接层通过权重矩阵$W$（形状为$m × n$）和偏置向量$b$（形状为$m$）进行线性变换$Y=WX+b$。这里的权重矩阵$W$描述了输入神经元和输出神经元之间所有可能的连接强度，每一个输入元素都会影响到每一个输出元素的计算结果，这种全面的连接关系是 “全连接” 概念的核心体现。</p>
<p>3.网络结构对比：在神经网络中，除了全连接层，还有<strong>其他类型的层</strong>，比如卷积层、池化层等。卷积层中，神经元只与输入数据的局部区域相连接，而<strong>不是像全连接层那样与所有输入神经元连接</strong>；池化层则主要进行下采样操作，不存在像全连接层这样全面的神经元连接模式。因此，为了突出这种所有输入和输出神经元之间都有连接的特殊结构，将其称为全连接层，以便和其他类型的层进行区分。</p>
<hr>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个全连接层，输入维度为 10，输出维度为 5</span></span><br><span class="line">linear_layer = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据，假设有 1 个样本，每个样本有 10 个特征</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = linear_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全连接层的权重形状:&quot;</span>, linear_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全连接层的偏置形状:&quot;</span>, linear_layer.bias.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量的形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量的形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>初始化参数</strong>：<code>nn.Linear(in_features, out_features)</code> 中，<code>in_features</code> 表示输入特征的数量，<code>out_features</code> 表示输出特征的数量。在上述代码中，<code>in_features = 10</code>，<code>out_features = 5</code>，意味着输入的每个样本有 10 个特征，经过全连接层后输出的每个样本有 5 个特征。</li>
<li><strong>线性变换</strong>：全连接层的计算过程可以表示为$Y=XW^T+b$ ，其中$X$是输入向量，$W$是权重矩阵，形状为 <code>(out_features, in_features)</code>，$b$是偏置向量，形状为 <code>(out_features,)</code>，$Y$是输出向量。</li>
<li><strong>前向传播</strong>：将输入张量 <code>input_tensor</code> 传递给 <code>linear_layer</code> 时，会自动执行上述线性变换，得到输出张量 <code>output</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">全连接层的权重形状: torch.Size([5, 10])</span><br><span class="line">全连接层的偏置形状: torch.Size([5])</span><br><span class="line">输入张量的形状: torch.Size([1, 10])</span><br><span class="line">输出张量的形状: torch.Size([1, 5])</span><br></pre></td></tr></table></figure>
<ul>
<li>[ ] 权重形状的确定:</li>
</ul>
<p>为了使矩阵乘法$WX$能够得到维度为$m$的输出向量，权重矩阵$W$的形状必须是$m × n$。这是因为在矩阵乘法中，两个矩阵能够相乘的条件是前一个矩阵的列数等于后一个矩阵的行数，并且相乘结果矩阵的行数等于前一个矩阵的行数，列数等于后一个矩阵的列数。即如果$W$是$m × n$矩阵，$X$是$n × 1$向量，那么$WX$的结果就是一个$m × 1$向量，符合输出向量$Y$的维度要求。</p>
<p>在代码示例中，输入特征数量 <code>in_features = 10</code>，输出特征数量 <code>out_features = 5</code>，所以权重矩阵 的形状就是 <code>(5, 10)</code>，即 <code>torch.Size([5, 10])</code>。</p>
<ul>
<li>[ ] 偏置形状的确定</li>
</ul>
<p>偏置向量$b$的作用是在经过矩阵乘法得到的结果上进行偏移。由于输出向量$Y$的维度是$m$，为了能够对$WX$的每一个元素都加上一个偏移量，偏置向量$b$的维度也必须是$m$，即$b∈\R^m$。</p>
<p>在代码示例中，输出特征数量 <code>out_features = 5</code>，所以偏置向量 的形状就是 <code>(5,)</code>，即 <code>torch.Size([5])</code>。</p>
<ol start="2">
<li><code>nn.Bilinear</code>双线性层</li>
</ol>
<p><strong><code>nn.Bilinear</code></strong>：PyTorch 中的一个类，用于创建双线性层。双线性层对两个输入进行双线性变换，能捕捉两个输入之间的交互信息，是一种比普通线性层更复杂的变换形式。</p>
<p><strong>关系建模</strong>：在需要捕捉两个不同特征之间交互关系的任务中非常有用。例如，在推荐系统中，可以用双线性层来建模用户特征和物品特征之间的交互，以预测用户对物品的偏好；在自然语言处理中，可用于处理两个不同句子或不同语义表示之间的关系。</p>
<p><strong>融合多模态信息</strong>：当处理多模态数据（如图像和文本）时，双线性层可以帮助融合不同模态之间的信息，挖掘它们之间的潜在关联。</p>
<p><img src="/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250222212028325.png" alt="image-20250222212028325"></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个双线性层</span></span><br><span class="line"><span class="comment"># 第一个输入维度为 10，第二个输入维度为 20，输出维度为 5</span></span><br><span class="line">bilinear_layer = nn.Bilinear(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟两个输入张量</span></span><br><span class="line"><span class="comment"># 第一个输入：假设有 1 个样本，每个样本有 10 个特征</span></span><br><span class="line">input1 = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 第二个输入：假设有 1 个样本，每个样本有 20 个特征</span></span><br><span class="line">input2 = torch.randn(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = bilinear_layer(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;双线性层的权重形状:&quot;</span>, bilinear_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;双线性层的偏置形状:&quot;</span>, bilinear_layer.bias.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个输入张量的形状:&quot;</span>, input1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第二个输入张量的形状:&quot;</span>, input2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量的形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>
<p>权重张量的形状符合 <code>(out_features, in1_features, in2_features)</code>，偏置向量的长度等于输出特征的数量，两个输入张量经过双线性层后，输出张量的特征数量变为 <code>out_features</code> 设定的值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">双线性层的权重形状: torch.Size([5, 10, 20])</span><br><span class="line">双线性层的偏置形状: torch.Size([5])</span><br><span class="line">第一个输入张量的形状: torch.Size([1, 10])</span><br><span class="line">第二个输入张量的形状: torch.Size([1, 20])</span><br><span class="line">输出张量的形状: torch.Size([1, 5])</span><br></pre></td></tr></table></figure>
<h6 id="卷积层">卷积层</h6>
<hr>
<p>1.什么是卷积层？</p>
<p>卷积层本质上是通过可学习的卷积核（滤波器）在输入数据上进行滑动并执行卷积操作，以提取输入数据中的局部特征模式，同时利用参数共享减少模型参数数量。</p>
<p>卷积操作指的是卷积核（一个小的矩阵）在输入数据（如图像矩阵）上按一定步长滑动，每滑动到一个位置，就将卷积核与该位置对应的输入局部区域元素对应相乘后求和，得到输出特征图的一个值，不断滑动直至覆盖整个输入数据，最终生成完整的输出特征图。</p>
<p>假设输入是一个 4×4 的单通道图像矩阵，使用一个 2×2 的卷积核进行卷积操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入图像矩阵：</span><br><span class="line">[[1 2 3 4]</span><br><span class="line"> [5 6 7 8]</span><br><span class="line"> [9 10 11 12]</span><br><span class="line"> [13 14 15 16]]</span><br><span class="line"></span><br><span class="line">卷积核：</span><br><span class="line">[[1 2]</span><br><span class="line"> [3 4]]</span><br><span class="line"> </span><br><span class="line">第一步：卷积核位于输入图像的左上角，覆盖的局部区域是：</span><br><span class="line">[[1 2]</span><br><span class="line"> [5 6]]</span><br><span class="line">将卷积核与该局部区域对应元素相乘再求和：</span><br><span class="line">(1x1+2x2)+(3x5+4x6)=44</span><br><span class="line">这个 44 就是输出特征图左上角的值。</span><br><span class="line"></span><br><span class="line">第二步：卷积核向右滑动一个步长（假设步长为 1），覆盖的局部区域变为：</span><br><span class="line">[[2 3]</span><br><span class="line"> [6 7]]</span><br><span class="line">同样进行对应元素相乘再求和的操作：</span><br><span class="line">(1x2+2x3)+(3x6+4x7)=54</span><br><span class="line">这是输出特征图中左上角右侧位置的值。</span><br><span class="line"></span><br><span class="line">后续步骤：卷积核继续向右、向下滑动，每次都重复上述相乘求和的操作，直到遍历完整个输入图像矩阵，最终得到一个 3×3 的输出特征图（因为 4×4 的输入矩阵使用 2×2 卷积核，步长为 1 时会得到 3×3 的输出）。</span><br></pre></td></tr></table></figure>
<p>2.卷积层有什么好处？</p>
<p><strong>减少参数数量</strong></p>
<p>卷积层使用参数共享机制，即一个卷积核在整个输入数据上滑动使用，相比于全连接层每个输出神经元都与所有输入相连，大大减少了需要学习的参数数量，降低计算量和存储需求，也减少了过拟合风险。</p>
<p><strong>提取局部特征</strong></p>
<p>卷积核在输入数据的局部区域进行操作，能够有效捕捉数据中的局部特征，如在图像中可检测边缘、纹理等。这些局部特征在不同位置可能具有相似性，卷积层可以很好地学习和利用这种特性。</p>
<p><strong>保留空间结构</strong></p>
<p>卷积操作基于局部连接，能保留输入数据的空间结构信息，这对于处理具有空间结构的数据（如图像、音频）非常重要，有助于模型理解数据中元素之间的相对位置关系。</p>
<p><strong>可构建深层网络</strong></p>
<p>多个卷积层可以堆叠形成深层卷积神经网络，随着网络深度增加，能学习到从简单到复杂、从底层到高层的多层次特征，提升模型在各种任务（如图像分类、目标检测）中的性能。</p>
<hr>
<ol>
<li><code>nn.Conv1d</code> 1D卷积：处理时序数据（如音频、文本）</li>
</ol>
<p><code>nn.Conv1d</code> 是 PyTorch 中用于进行一维卷积操作的模块，主要用于处理时序数据，像音频信号、文本序列等。一维卷积在这些数据上沿着一个维度（通常是时间维度）进行卷积操作，能有效提取数据中的局部特征模式。</p>
<p><strong>音频处理</strong>：可以用于音频特征提取、语音识别等任务，通过一维卷积提取音频信号中的时域特征。</p>
<p><strong>文本处理</strong>：在自然语言处理中，将文本序列看作一维数据，一维卷积可以捕捉文本中的局部语义信息，常用于文本分类、情感分析等任务。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line"><span class="comment"># 输入数据形状：(批量大小, 输入通道数, 序列长度)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一维卷积层</span></span><br><span class="line"><span class="comment"># in_channels: 输入通道数</span></span><br><span class="line"><span class="comment"># out_channels: 输出通道数</span></span><br><span class="line"><span class="comment"># kernel_size: 卷积核大小</span></span><br><span class="line">conv1d_layer = nn.Conv1d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行卷积操作</span></span><br><span class="line">output = conv1d_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>
<p><strong>输入数据</strong></p>
<p>通常是三维张量，形状为 <code>(batch_size, in_channels, sequence_length)</code>。其中 <code>batch_size</code> 表示一次处理的样本数量；<code>in_channels</code> 是输入数据的通道数，例如在音频处理中，单声道音频 <code>in_channels</code> 为 1，立体声音频 <code>in_channels</code> 为 2；<code>sequence_length</code> 是序列的长度，对于音频数据就是音频信号的采样点数，对于文本数据就是文本序列的长度。</p>
<p><strong>卷积层参数</strong></p>
<p><code>in_channels</code>：输入数据的通道数，必须与输入张量的第二维大小一致。</p>
<p><code>out_channels</code>：输出数据的通道数，即卷积层使用的卷积核数量。每个卷积核会提取一种特定的特征，因此不同的卷积核会输出不同的特征图。</p>
<p><code>kernel_size</code>：卷积核的大小，表示在序列维度上卷积核覆盖的元素个数。例如 <code>kernel_size=3</code> 表示卷积核在序列上每次覆盖 3 个元素。</p>
<p><strong>卷积操作过程</strong></p>
<p>卷积核在输入数据的序列维度上滑动，每次覆盖 <code>kernel_size</code> 个元素，并在每个通道上进行卷积操作，然后将各通道的结果相加（如果有多个输入通道），最后加上偏置项得到输出的一个值。卷积核不断滑动，最终得到输出特征图。</p>
<p><strong>输出数据</strong></p>
<p>输出数据同样是三维张量，形状为 <code>(batch_size, out_channels, new_sequence_length)</code>。其中 <code>batch_size</code> 与输入相同；<code>out_channels</code> 是卷积层定义的输出通道数；<code>new_sequence_length</code> 由输入序列长度、卷积核大小、步长（<code>stride</code>，默认为 1）和填充（<code>padding</code>，默认为 0）等因素决定，计算公式为：</p>
<p><img src="/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250222215408498.png" alt="image-20250222215408498"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([16, 3, 100])</span><br><span class="line">输出张量形状: torch.Size([16, 6, 98])</span><br></pre></td></tr></table></figure>
<ol start="2">
<li><code>nn.Conv2d</code> 2D卷积：处理图像数据</li>
</ol>
<p><code>nn.Conv2d</code>：PyTorch 中用于实现二维卷积操作的类，主要处理图像数据。通过可学习的卷积核在输入图像上滑动并进行卷积运算，提取图像局部特征，利用参数共享减少模型参数。</p>
<p><strong>图像分类</strong>：用于提取图像特征，结合后续层将图像映射到不同类别，如区分猫狗图像。</p>
<p><strong>目标检测</strong>：提取物体特征，配合检测算法确定图像中物体的位置和类别，像自动驾驶中检测车辆、行人。</p>
<p><strong>语义分割</strong>：对图像每个像素分类，将图像分割成不同语义区域，例如医学图像中分割肿瘤和正常组织。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据，仅保留第一张图片</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">in_channels = <span class="number">3</span></span><br><span class="line">height = <span class="number">4</span></span><br><span class="line">width = <span class="number">4</span></span><br><span class="line"><span class="comment"># 手动设定输入张量，方便后续手动计算验证</span></span><br><span class="line">input_tensor = torch.tensor([</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">         [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">         [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>]],</span><br><span class="line">        [[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">         [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>],</span><br><span class="line">         [<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]],</span><br><span class="line">        [[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">         [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]</span><br><span class="line">    ]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积层</span></span><br><span class="line">out_channels = <span class="number">2</span></span><br><span class="line">kernel_size = <span class="number">2</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span></span><br><span class="line">conv2d_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,</span><br><span class="line">                         kernel_size=kernel_size, stride=stride, padding=padding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动设定卷积核权重，方便后续手动计算验证</span></span><br><span class="line"><span class="comment"># 这里假设的卷积核和前面理论部分一致</span></span><br><span class="line">conv2d_layer.weight.data = torch.tensor([</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>]],</span><br><span class="line">        [[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">        [[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">    ]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动设定偏置为 0，简化计算</span></span><br><span class="line">conv2d_layer.bias.data = torch.zeros(out_channels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行卷积操作</span></span><br><span class="line">output = conv2d_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;卷积核形状:&quot;</span>, conv2d_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一张图片的输出:&quot;</span>, output[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算第一张图片经过第一个卷积核的左上角输出值</span></span><br><span class="line">first_kernel_channel1 = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line">first_kernel_channel2 = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]], dtype=torch.float32)</span><br><span class="line">first_kernel_channel3 = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个通道的局部卷积</span></span><br><span class="line">local_conv_channel1 = (input_tensor[<span class="number">0</span>, <span class="number">0</span>, :<span class="number">2</span>, :<span class="number">2</span>] * first_kernel_channel1).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 第二个通道的局部卷积</span></span><br><span class="line">local_conv_channel2 = (input_tensor[<span class="number">0</span>, <span class="number">1</span>, :<span class="number">2</span>, :<span class="number">2</span>] * first_kernel_channel2).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 第三个通道的局部卷积</span></span><br><span class="line">local_conv_channel3 = (input_tensor[<span class="number">0</span>, <span class="number">2</span>, :<span class="number">2</span>, :<span class="number">2</span>] * first_kernel_channel3).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多通道结果相加</span></span><br><span class="line">manual_output = local_conv_channel1 + local_conv_channel2 + local_conv_channel3</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;手动计算第一张图片经过第一个卷积核左上角输出值:&quot;</span>, manual_output)</span><br></pre></td></tr></table></figure>
<p>上述代码模拟了处理彩色图像的过程。输入数据是一个四维张量，形状为 <code>(batch_size, in_channels, height, width)</code>。例如，<code>batch_size = 1</code> 表示一次处理 1 张图像；<code>in_channels = 3</code> 对应 RGB 三个通道；<code>height = 4</code> 和 <code>width = 4</code> 是图像的高度和宽度。输入数据可以表示为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入数据形状: (2, 3, 4, 4)</span><br><span class="line">第 1 张图像通道 1:</span><br><span class="line">[[1, 2, 3, 4],</span><br><span class="line"> [5, 6, 7, 8],</span><br><span class="line"> [9, 10, 11, 12],</span><br><span class="line"> [13, 14, 15, 16]]</span><br><span class="line"></span><br><span class="line">第 1 张图像通道 2:</span><br><span class="line">[[2, 3, 4, 5],</span><br><span class="line"> [6, 7, 8, 9],</span><br><span class="line"> [10, 11, 12, 13],</span><br><span class="line"> [14, 15, 16, 17]]</span><br><span class="line"></span><br><span class="line">第 1 张图像通道 3:</span><br><span class="line">[[3, 4, 5, 6],</span><br><span class="line"> [7, 8, 9, 10],</span><br><span class="line"> [11, 12, 13, 14],</span><br><span class="line"> [15, 16, 17, 18]]</span><br><span class="line"></span><br><span class="line">...（其他图片类似）</span><br></pre></td></tr></table></figure>
<p>定义一个二维卷积层，设置参数如下：</p>
<ul>
<li><code>in_channels = 3</code>，与输入图像的通道数一致。</li>
<li><code>out_channels = 2</code>，表示使用 2 个卷积核。</li>
<li><code>kernel_size = 2</code>，卷积核是 2x2 的正方形。</li>
<li><code>stride = 1</code>，卷积核每次滑动 1 个单位。</li>
<li><code>padding = 0</code>，不进行填充。</li>
</ul>
<p>每个卷积核也是一个四维张量，形状为 <code>(out_channels, in_channels, kernel_height, kernel_width)</code>，这里是 <code>(2, 3, 2, 2)</code>。假设两个卷积核分别为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积核 1:</span><br><span class="line">通道 1:</span><br><span class="line">[[1, 0],</span><br><span class="line"> [0, 1]]</span><br><span class="line"></span><br><span class="line">通道 2:</span><br><span class="line">[[0, 1],</span><br><span class="line"> [1, 0]]</span><br><span class="line"></span><br><span class="line">通道 3:</span><br><span class="line">[[1, 1],</span><br><span class="line"> [1, 1]]</span><br><span class="line"></span><br><span class="line">卷积核 2:</span><br><span class="line">通道 1:</span><br><span class="line">[[1, 1],</span><br><span class="line"> [1, 1]]</span><br><span class="line"></span><br><span class="line">通道 2:</span><br><span class="line">[[0, 0],</span><br><span class="line"> [0, 0]]</span><br><span class="line"></span><br><span class="line">通道 3:</span><br><span class="line">[[1, 0],</span><br><span class="line"> [0, 1]]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积操作</span><br><span class="line">以第 1 张图像为例，对于卷积核 1：</span><br><span class="line">通道 1 的卷积：卷积核在通道 1 的输入图像上滑动，计算局部区域与卷积核对应元素相乘再求和。例如，在左上角区域：</span><br><span class="line">通道 2 和通道 3 同样操作：得到各自的局部卷积结果。</span><br><span class="line">多通道结果相加：将三个通道的局部卷积结果相加，得到该位置的最终输出值。</span><br><span class="line">滑动卷积核：卷积核在图像上按步长 1 滑动，重复上述操作，得到卷积核 1 对第 1 张图像的输出特征图。</span><br><span class="line">对于卷积核 2 也进行同样的操作，最终得到两个输出特征图，这两个特征图组成了第 1 张图像经过卷积层后的输出。对于第 2 张图像，也重复上述卷积操作。</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([1, 3, 4, 4])</span><br><span class="line">卷积核形状: torch.Size([2, 3, 2, 2])</span><br><span class="line">输出张量形状: torch.Size([1, 2, 3, 3])</span><br><span class="line">第一张图片的输出:</span><br><span class="line">tensor([[[ 41.,  48.,  55.],</span><br><span class="line">         [ 65.,  72.,  79.],</span><br><span class="line">         [ 89.,  96., 103.]],</span><br><span class="line"></span><br><span class="line">        [[ 26.,  30.,  34.],</span><br><span class="line">         [ 46.,  50.,  54.],</span><br><span class="line">         [ 66.,  70.,  74.]]], grad_fn=&lt;SelectBackward0&gt;)</span><br><span class="line">手动计算第一张图片经过第一个卷积核左上角输出值: tensor(41.)</span><br></pre></td></tr></table></figure>
<hr>
<p>卷积核如何确定？</p>
<p>卷积核的确定需要综合考虑任务需求、数据特点、模型架构和计算资源等因素，并通过实验和调优不断优化，以达到最佳的性能。</p>
<hr>
<ol start="3">
<li><code>nn.Conv3d</code> 3D卷积：处理视频/体数据</li>
</ol>
<p><code>nn.Conv3d</code> 是 PyTorch 中用于实现三维卷积操作的类，主要用于处理视频数据（包含时间维度和空间维度）或体数据（如医学影像中的三维体数据）。它通过可学习的三维卷积核在输入数据上进行滑动并执行卷积运算，提取数据中的三维特征，利用参数共享机制减少模型参数数量，降低计算复杂度。</p>
<p><strong>视频分类</strong>：在视频分类任务中，<code>nn.Conv3d</code> 用于提取视频的时空特征，通过多个卷积层和池化层的组合，将视频特征映射到不同的类别。例如，在识别视频中的动作类型（如跑步、跳舞、打球等）时，三维卷积可以捕捉视频中随时间变化的动作特征。</p>
<p><strong>医学影像分析</strong>：在医学影像分析中，如对 CT 或 MRI 扫描得到的三维体数据进行分析，<code>nn.Conv3d</code> 可以用于肿瘤检测、器官分割等任务。它能够提取体数据中的三维结构信息，帮助医生进行疾病诊断。</p>
<p><strong>自动驾驶场景</strong>：在自动驾驶中处理点云数据（可以看作三维空间中的数据）时，<code>nn.Conv3d</code> 可用于识别道路上的障碍物、车辆、行人等目标，通过分析点云数据的三维特征来辅助决策。</p>
<p><strong>卷积核与卷积操作</strong></p>
<p>三维卷积核是一个三维的矩阵，其元素是可学习的权重参数。在进行卷积操作时，三维卷积核会在输入的三维数据（如视频的多个帧组成的序列或者三维体数据）上按一定的步长进行滑动，每次覆盖一个与卷积核大小相同的三维局部区域。对于这个局部区域，将卷积核与该区域的元素对应相乘后求和，得到一个输出值，这个值就构成了输出特征体的一个元素。随着卷积核不断滑动，遍历整个输入三维数据，最终生成完整的输出特征体。</p>
<hr>
<p><code>nn.Conv1d/2d/3d</code>的共同原理与特征</p>
<p><strong>共同原理</strong></p>
<p>1.卷积操作核心</p>
<p>核心都是卷积运算。卷积核（也称为滤波器）是一组可学习的权重参数，在输入数据上按照一定规则滑动，每次覆盖一个局部区域，将卷积核与该局部区域的元素对应相乘后求和，得到一个输出值。这个过程不断重复，直到卷积核遍历完整个输入数据，从而生成输出特征。</p>
<p>以 <code>nn.Conv1d</code> 为例，输入是一维序列，卷积核也是一维的，在序列上滑动进行卷积；<code>nn.Conv2d</code> 输入是二维图像，卷积核是二维矩阵，在图像上滑动；<code>nn.Conv3d</code> 输入是三维数据（如视频或体数据），卷积核是三维的，在三维空间中滑动。</p>
<p>2.线性变换</p>
<p>卷积操作本质上是一种线性变换。对于输入数据的每个局部区域，卷积运算将其与卷积核进行加权求和，这相当于对输入数据进行了线性组合。这种线性变换使得模型能够学习到输入数据中的特征模式。</p>
<p>3.参数共享</p>
<p>在卷积过程中，卷积核的权重在整个输入数据上是共享的。也就是说，同一个卷积核在不同的位置进行卷积操作时，使用的是相同的权重参数。这大大减少了模型需要学习的参数数量，降低了计算复杂度，同时也提高了模型的泛化能力，使得模型能够在不同位置检测到相同的特征。</p>
<p>4.多通道处理</p>
<p>当输入数据具有多个通道时，每个卷积核也具有对应数量的通道。卷积操作会在每个通道上分别进行卷积，然后将各通道的结果相加，得到最终的输出值。例如，对于 RGB 图像（3 个通道），每个二维卷积核也有 3 个通道，分别对 R、G、B 通道进行卷积后求和。<code>nn.Conv1d</code>、<code>nn.Conv2d</code> 和 <code>nn.Conv3d</code> 都支持多通道输入和输出，通过使用多个卷积核可以得到多个通道的输出特征。</p>
<p><strong>共同特征</strong></p>
<p>1.可学习性</p>
<p>卷积核的权重参数是可学习的，在模型训练过程中，通过反向传播算法和优化器（如随机梯度下降、Adam 等）不断调整卷积核的权重，使得模型能够自动学习到输入数据中的有效特征，以适应不同的任务需求，如分类、检测、分割等。</p>
<p>2.局部感知</p>
<p>都具有局部感知的特性，即卷积核只关注输入数据的局部区域，而不是全局信息。这种局部感知能力使得模型能够捕捉到数据中的局部特征，如边缘、纹理、模式等。通过堆叠多个卷积层，模型可以逐渐学习到更高级、更抽象的特征。</p>
<p>3.平移不变性</p>
<p>由于参数共享的特性，卷积操作具有平移不变性。这意味着如果输入数据中的某个特征在不同位置出现，卷积核都能够检测到该特征，而不依赖于其具体位置。这种平移不变性使得模型在处理具有平移特性的数据时更加有效，例如图像中的物体在不同位置出现，模型都能够正确识别。</p>
<p>4.输出特征的维度调整</p>
<p>通过调整卷积核的大小、步长和填充等参数，可以控制输出特征的维度。步长越大，输出特征的维度越小；填充可以在输入数据的边缘添加额外的元素，从而保持输出特征的维度与输入数据相同或满足特定的要求。这种灵活性使得模型能够根据具体任务和数据特点进行合理的设计。</p>
<hr>
<ol start="4">
<li><code>nn.ConvTranspose1d/2d/3d</code> 转置卷积：反卷积（上采样）</li>
</ol>
<p><code>nn.ConvTranspose1d</code>、<code>nn.ConvTranspose2d</code> 和 <code>nn.ConvTranspose3d</code> 分别是 PyTorch 中用于一维、二维和三维转置卷积（也常被称为反卷积，但实际上并非严格意义的卷积逆运算，主要用于上采样）的类。转置卷积通过在输入数据上进行特殊的卷积操作，使得输出的尺寸比输入尺寸更大，实现数据的上采样，在图像生成、语义分割等任务中经常使用。</p>
<p>转置卷积本质上是标准卷积的一种 “逆向” 操作，但不是严格意义上的逆运算。在标准卷积中，卷积核在输入数据上滑动，将局部区域映射到一个输出值，导致输出尺寸通常小于输入尺寸。而转置卷积则是将输入的每个元素扩展到一个更大的区域，然后与卷积核进行卷积操作，从而实现输出尺寸大于输入尺寸的效果。</p>
<p>具体来说，转置卷积通过在输入数据周围插入零值（称为 “零填充”），并调整卷积核的滑动方式，使得卷积操作能够产生更大的输出。在一维、二维和三维的情况下，分别使用 <code>nn.ConvTranspose1d</code>、<code>nn.ConvTranspose2d</code> 和 <code>nn.ConvTranspose3d</code> 来实现相应维度的转置卷积。</p>
<blockquote>
<p><mark>上采样</mark></p>
<p>指的是将低分辨率的数据转换为高分辨率数据的过程。在深度学习中，输入数据（如图像、特征图等）经过一系列下采样操作（如卷积、池化）后尺寸会变小，为了恢复到原始尺寸或达到特定任务所需的尺寸，就需要进行上采样操作。</p>
<ul>
<li><strong>恢复尺寸</strong>：在一些任务中，如语义分割，模型需要对输入图像的每个像素进行分类，经过下采样后的特征图尺寸变小，需要通过上采样恢复到与输入图像相同的尺寸，以便为每个像素分配类别标签。</li>
<li><strong>增加细节</strong>：上采样可以在一定程度上增加数据的细节信息，使模型能够学习到更丰富的特征，提升模型性能。</li>
</ul>
</blockquote>
<p><strong>图像生成</strong>：在生成对抗网络（GAN）和变分自编码器（VAE）等图像生成模型中，转置卷积用于将低维的特征向量逐步上采样为高分辨率的图像。例如，在生成手写数字图像时，模型从一个随机的低维向量开始，通过一系列的转置卷积层逐渐生成 28x28 像素的手写数字图像。</p>
<p><strong>语义分割</strong>：在语义分割任务中，模型需要将卷积层提取的低分辨率特征图恢复到与输入图像相同的尺寸，以便为每个像素分配一个类别标签。转置卷积可以有效地实现特征图的上采样，帮助模型生成与输入图像大小一致的分割结果。</p>
<p><strong>超分辨率重建</strong>：超分辨率重建任务旨在将低分辨率的图像恢复为高分辨率的图像。转置卷积可以用于逐步增加图像的分辨率，提高图像的清晰度和细节。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟低分辨率图像特征图，形状为 (批量大小, 输入通道数, 高度, 宽度)</span></span><br><span class="line">input_image = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义二维转置卷积层，用于上采样图像</span></span><br><span class="line">conv_transpose2d = nn.ConvTranspose2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">3</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行转置卷积操作</span></span><br><span class="line">output_image = conv_transpose2d(input_image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入图像特征图形状:&quot;</span>, input_image.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出图像特征图形状:&quot;</span>, output_image.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入图像特征图形状: torch.Size([1, 3, 8, 8])</span><br><span class="line">输出图像特征图形状: torch.Size([1, 3, 16, 16])</span><br></pre></td></tr></table></figure>
<ol start="5">
<li><code>nn.Conv1d/2d/3d</code> （设置 <code>dilation</code>）空洞卷积：扩大感受野</li>
</ol>
<p><code>nn.Conv1d</code>、<code>nn.Conv2d</code> 和 <code>nn.Conv3d</code> 在设置 <code>dilation</code> 参数后可实现空洞卷积（也叫扩张卷积）。空洞卷积是对标准卷积的扩展，通过在卷积核元素之间插入空洞，在不增加参数数量的情况下扩大卷积核的感受野，使模型能够捕捉更大范围的上下文信息，常用于语义分割、目标检测等任务。</p>
<p>在标准卷积中，卷积核的元素是连续排列的，在输入数据上进行滑动卷积操作。而空洞卷积通过设置 <code>dilation</code> 参数，在卷积核元素之间插入空洞。例如，当 <code>dilation = 2</code> 时，卷积核元素之间会间隔一个位置，相当于在标准卷积核的基础上每隔一个元素设置为零，然后再进行卷积操作。这样，卷积核在输入数据上滑动时，能够覆盖更大的区域，从而扩大了感受野。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量</span></span><br><span class="line"><span class="comment"># 形状：(批量大小, 输入通道数, 高度, 宽度)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义二维空洞卷积层</span></span><br><span class="line"><span class="comment"># 设置 dilation = 2 来实现空洞卷积</span></span><br><span class="line">conv2d_dilated = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, dilation=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行空洞卷积操作</span></span><br><span class="line">output = conv2d_dilated(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([1, 3, 16, 16])</span><br><span class="line">输出张量形状: torch.Size([1, 6, 16, 16])</span><br></pre></td></tr></table></figure>
<h6 id="循环神经网络层">循环神经网络层</h6>
<p>循环神经网络层（Recurrent Neural Network layer，RNN layer）是一种专门用于处理序列数据的神经网络层结构。在很多实际应用场景中，数据具有序列特性，例如文本（由单词按顺序组成）、语音（音频信号随时间变化）、时间序列数据（如股票价格随时间的波动）等。与传统的前馈神经网络不同，循环神经网络层引入了循环结构，使得网络能够在处理序列数据时保留之前时间步的信息，从而更好地捕捉序列中的上下文关系和时间依赖。</p>
<p>RNN、LSTM 和 GRU 都属于循环神经网络层的范畴，它们在处理序列数据时各有特点。简单 RNN 结构简单但存在梯度问题；LSTM 通过门控机制解决了梯度问题但计算复杂；GRU 则在性能和计算效率之间取得了较好的平衡。根据不同的应用场景和数据特点，可以选择合适的循环神经网络层来构建模型。</p>
<p>1.<code>nn.RNN</code> RNN：基础循环网络</p>
<p><code>nn.RNN</code> 是 PyTorch 中用于构建基础循环神经网络（Recurrent Neural Network，RNN）的模块。RNN 是一种专门处理序列数据的神经网络，它通过在网络中引入循环结构，使得网络能够保存和利用之前时间步的信息，从而对序列中的时间依赖关系进行建模。不过，RNN 存在梯度消失或梯度爆炸问题，在处理长序列时表现不佳。</p>
<p>在每个时间步 ，RNN 接收当前输入$x_t$和上一个时间步的隐藏状态$h_{t-1}$，通过以下公式计算当前时间步的隐藏状态$h_t$：</p>
<p>$h_t=tanh(W_{ih}x_t+W_{hh}h_{t-1}+b_h)$</p>
<p>其中$W_{ih}$是输入到隐藏状态的权重矩阵，$W_{hh}$是隐藏状态到隐藏状态的权重矩阵，$b_h$是偏置，$tanh$是激活函数，用于引入非线性。</p>
<p><strong>自然语言处理</strong>：如文本分类任务，将一段文本看作一个词序列，RNN 可以对文本中的语义信息进行建模，根据之前的词来预测当前词的类别概率；还可用于语言生成，例如生成诗歌、故事等，通过不断根据之前生成的词来预测下一个词。</p>
<p><strong>时间序列预测</strong>：像股票价格预测、天气预报等，把时间序列数据（如每天的股票价格、每小时的气温）作为输入，RNN 可以学习到序列中的趋势和模式，从而预测未来的值。</p>
<p><strong>语音识别</strong>：语音信号是随时间变化的序列，RNN 可以处理语音特征序列，根据之前的语音帧信息来识别当前帧对应的语音内容。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入参数</span></span><br><span class="line">input_size = <span class="number">10</span>  <span class="comment"># 输入特征维度</span></span><br><span class="line">hidden_size = <span class="number">20</span>  <span class="comment"># 隐藏状态维度</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># RNN 层数</span></span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># 批量大小</span></span><br><span class="line">seq_len = <span class="number">5</span>  <span class="comment"># 序列长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 RNN 层</span></span><br><span class="line">rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机输入数据</span></span><br><span class="line">input_data = torch.randn(batch_size, seq_len, input_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态</span></span><br><span class="line">h_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output, h_n = rnn(input_data, h_0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入数据形状:&quot;</span>, input_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出形状:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终隐藏状态形状:&quot;</span>, h_n.shape)</span><br></pre></td></tr></table></figure>
<p>输入数据 <code>input_data</code> 是一个三维张量，当 <code>batch_first = True</code> 时，形状为 <code>(batch_size, seq_len, input_size)</code>，表示批量大小为 32，序列长度为 5，每个时间步的输入特征维度为 10。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">input_data = torch.randn(batch_size, seq_len, input_size)</span><br></pre></td></tr></table></figure>
<p>使用 <code>nn.RNN</code> 创建 RNN 层，设置输入特征维度、隐藏状态维度和层数等参数。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>初始化隐藏状态 <code>h_0</code>，形状为 <code>(num_layers, batch_size, hidden_size)</code>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">h_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br></pre></td></tr></table></figure>
<p>调用 <code>rnn</code> 进行前向传播，得到输出 <code>output</code> 和最终隐藏状态 <code>h_n</code>。输出 <code>output</code> 包含每个时间步的隐藏状态，形状为 <code>(batch_size, seq_len, hidden_size)</code>；最终隐藏状态 <code>h_n</code> 是最后一个时间步的隐藏状态，形状为 <code>(num_layers, batch_size, hidden_size)</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入数据形状: torch.Size([32, 5, 10])</span><br><span class="line">输出形状: torch.Size([32, 5, 20])</span><br><span class="line">最终隐藏状态形状: torch.Size([1, 32, 20])</span><br></pre></td></tr></table></figure>
<p>2.<code>nn.LSTM</code> LSTM：长短期记忆网络</p>
<p><code>nn.LSTM</code> 是 PyTorch 中用于构建长短期记忆网络（Long Short - Term Memory，LSTM）的模块。LSTM 是一种特殊的循环神经网络（RNN），旨在解决传统 RNN 在处理长序列时遇到的梯度消失或梯度爆炸问题。它通过引入门控机制（输入门、遗忘门和输出门），能够有效地捕捉序列中的长距离依赖关系，在处理自然语言处理、时间序列分析等序列数据任务中表现出色。</p>
<blockquote>
<p><mark>梯度消失与梯度爆炸</mark></p>
<p>1.梯度消失</p>
<p>在神经网络训练中，使用反向传播算法更新参数时，梯度会从输出层向输入层逐层传递。梯度消失指的是在这个过程中，梯度值变得越来越小，趋近于零。</p>
<p>传统 RNN 在计算梯度时涉及多个权重矩阵的连乘，若权重矩阵的元素值较小，经过多次连乘后梯度会急剧减小。激活函数（如 Sigmoid、Tanh）的导数取值范围在 0 到 1 之间，多次使用这类激活函数也会使梯度逐渐变小。</p>
<p>由于梯度极小，模型参数的更新幅度变得非常小，几乎不再更新，导致网络无法学习到长序列中的远距离依赖信息，难以收敛到最优解。</p>
<p>2.梯度爆炸</p>
<p>与梯度消失相反，梯度爆炸是指在反向传播过程中，梯度值变得越来越大，失去控制。</p>
<p>同样是因为多个权重矩阵的连乘，若权重矩阵的元素值较大，连乘后梯度会急剧增大。网络层数过深、学习率设置过大等也可能引发梯度爆炸。</p>
<p>过大的梯度会使模型参数更新幅度过大，导致参数值剧烈波动，模型无法稳定训练，甚至可能使训练过程发散。</p>
<hr>
<p>LSTM 通过引入门控机制，能够有效地缓解梯度消失和梯度爆炸问题，使得网络在处理长序列数据时可以更好地保留和传递信息。</p>
<hr>
<p>传统 RNN 易出现梯度消失或爆炸，因为反向传播时梯度经多时间步连乘，值要么趋于零、要么无限制增大。而门控机制可将梯度限制在一定区间。</p>
<p>以 LSTM 为例，遗忘门用sigmoid函数输出$[0,1]$的值，决定上一时刻细胞状态信息的保留程度。接近1时梯度顺畅传递，避免消失；接近0时切断部分传递路径，防止爆炸。</p>
<p>输入门同理控制当前输入信息的添加比例，和遗忘门协同让细胞状态渐进更新。这种平稳的信息传递使梯度也稳定，不会剧烈波动。</p>
<p>输出门对细胞状态输出信息缩放，控制从隐藏状态到细胞状态的梯度传递，避免细胞状态梯度过度影响隐藏状态更新，进一步稳定梯度。</p>
</blockquote>
<p>LSTM 的核心在于其门控机制，主要包含以下几个部分：</p>
<p><strong>遗忘门（Forget Gate）</strong>：决定上一个时间步的细胞状态$C_{t-1}$中哪些信息需要被遗忘。计算公式为$f_t=\sigma(W_f[h_{t-1}.x_t]+b_f)$，其中$\sigma$是sigmoid函数， $W_f$是遗忘门的权重矩阵，$b_f$是偏置，$[h_{t-1},x_t]$表示将上一个时间步的隐藏状态$h_{t-1}$和当前输入$x_t$拼接起来。</p>
<p><strong>输入门（Input Gate）</strong>：决定当前输入$x_t$中哪些信息需要被添加到细胞状态中。首先计算$i_t=\sigma(W_i[h_{t-1},x_t]+b_i)$，同时计算候选细胞状态$\widetilde{C_t}=tanh(W_C[h_{t-1},x_t]+b_C)$。（波浪线读作tilde）</p>
<p><strong>细胞状态更新</strong>：根据遗忘门和输入门的输出更新细胞状态$C_t=f_{t}\odot C_{t-1}+i_t\odot \widetilde{C_t}$，其中$\odot$表示逐元素相乘。</p>
<p><strong>输出门（Output Gate）</strong>：决定当前细胞状态$C_t$中哪些信息需要被输出作为当前时间步的隐藏状态$h_t$。计算公式为$o_t=\sigma(W_o[h_{t-1},x_t]+b_o)$，$h_t=o_t\odot tanh(C_t)$。</p>
<blockquote>
<p><mark>隐藏状态</mark>（可理解为一个中间变量或状态值）</p>
<p>是网络在处理序列数据时，每个时间步所维护的一种内部表示。可以将其理解为网络对之前输入信息的一种 “记忆” 和 “总结”，随着时间步推进不断更新。</p>
<p>隐藏状态整合了历史输入信息和当前输入信息，能反映序列的上下文关系。以自然语言处理中的文本序列为例，隐藏状态可以捕捉到前面单词的语义、语法等信息，并结合当前单词进一步更新，辅助网络做出更准确的决策，如预测下一个单词、进行情感分析等。</p>
<p>在 LSTM 里，输出门会对细胞状态进行筛选和处理，生成隐藏状态。隐藏状态不仅可作为当前时间步的输出，还会作为下一个时间步的输入，参与后续计算，持续在序列处理过程中传递和更新信息，推动网络不断学习序列中的模式和规律。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入参数</span></span><br><span class="line">input_size = <span class="number">10</span>  <span class="comment"># 输入特征维度</span></span><br><span class="line">hidden_size = <span class="number">20</span>  <span class="comment"># 隐藏状态维度</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># LSTM 层数</span></span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># 批量大小</span></span><br><span class="line">seq_len = <span class="number">5</span>  <span class="comment"># 序列长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 LSTM 层</span></span><br><span class="line">lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机输入数据</span></span><br><span class="line">input_data = torch.randn(batch_size, seq_len, input_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态和细胞状态</span></span><br><span class="line">h_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br><span class="line">c_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output, (h_n, c_n) = lstm(input_data, (h_0, c_0))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入数据形状:&quot;</span>, input_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出形状:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终隐藏状态形状:&quot;</span>, h_n.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终细胞状态形状:&quot;</span>, c_n.shape)</span><br></pre></td></tr></table></figure>
<p><strong>关键参数</strong></p>
<ul>
<li><strong><code>input_size</code></strong>：输入特征的维度，即每个时间步输入向量的长度。</li>
<li><strong><code>hidden_size</code></strong>：隐藏状态和细胞状态的维度，决定了 LSTM 能够学习和表示的信息量。</li>
<li><strong><code>num_layers</code></strong>：LSTM 的层数，多层 LSTM 可以学习到更复杂的序列模式。</li>
<li><strong><code>bias</code></strong>：是否使用偏置，默认为 <code>True</code>。</li>
<li><strong><code>batch_first</code></strong>：如果为 <code>True</code>，输入和输出张量的形状为 <code>(batch_size, seq_len, input_size)</code>，否则为 <code>(seq_len, batch_size, input_size)</code>，默认为 <code>False</code>。</li>
<li><strong><code>dropout</code></strong>：如果非零，则在除最后一层外的每一层的输出上应用 Dropout，防止过拟合，取值范围为 <code>[0, 1)</code>。</li>
<li><strong><code>bidirectional</code></strong>：如果为 <code>True</code>，则使用双向 LSTM，能够同时考虑序列的正向和反向信息，默认为 <code>False</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入数据形状: torch.Size([32, 5, 10])</span><br><span class="line">输出形状: torch.Size([32, 5, 20])</span><br><span class="line">最终隐藏状态形状: torch.Size([1, 32, 20])</span><br><span class="line">最终细胞状态形状: torch.Size([1, 32, 20])</span><br></pre></td></tr></table></figure>
<p>3.<code>nn.GRU</code> GRU：门控循环单元</p>
<p><code>nn.GRU</code> 是 PyTorch 里用于构建门控循环单元（Gated Recurrent Unit，GRU）的模块。GRU 是循环神经网络（RNN）的一种变体，它和长短期记忆网络（LSTM）类似，旨在解决传统 RNN 处理长序列时的梯度消失问题。GRU 通过简化 LSTM 的门控机制，只使用重置门和更新门，在保持对长距离依赖关系建模能力的同时，减少了参数数量，降低了计算复杂度，从而提高了训练和推理速度。</p>
<h6 id="Transformer-相关层">Transformer 相关层</h6>
<blockquote>
<p><mark>❓什么是Transformer?</mark></p>
<p>Transformer 是 2017 年在论文《Attention Is All You Need》中提出的一种基于注意力机制的深度学习模型架构，用于处理序列数据，尤其在自然语言处理领域表现卓越。它摒弃了传统的循环结构（如 RNN、LSTM），完全基于注意力机制构建，能够并行处理输入序列，提升了训练和推理速度。</p>
<p><mark>❓Transformer 是巨大进步的原因</mark></p>
<p>1.解决长序列依赖问题</p>
<p>传统 RNN 及其变体（如 LSTM、GRU）在处理长序列时，由于信息传递需按顺序进行，存在梯度消失或爆炸问题，难以捕捉长距离依赖关系。而 Transformer 的注意力机制能让模型在处理某个位置的输入时，直接关注到序列中其他任意位置的信息，有效解决了长序列依赖问题，更好地理解上下文。</p>
<p>2.并行计算能力</p>
<p>RNN 系列模型按时间步顺序处理序列，无法并行计算，效率较低。Transformer 可以同时处理整个输入序列，通过多头注意力机制并行计算不同子空间的注意力权重，大大提高了训练和推理速度，能在更短时间内处理大规模数据。</p>
<p>3.模型扩展性强</p>
<p>Transformer 架构清晰，各个组件（如多头注意力层、前馈网络层）易于理解和调整。可以通过堆叠更多层或增加隐藏单元数量等方式，方便地扩大模型规模，以适应不同的任务和数据量，从而不断提升模型性能。</p>
<p>4.广泛的适用性</p>
<p>Transformer 不仅在自然语言处理任务（如机器翻译、文本生成、问答系统等）中取得了显著成果，还在计算机视觉、语音处理等其他领域得到了广泛应用和拓展，展现出强大的泛化能力和适应性。</p>
</blockquote>
<ol>
<li><code>nn.Transformer</code> Transformer：完整 Transformer 模型</li>
</ol>
<p><code>nn.Transformer</code> 是 PyTorch 提供的用于构建完整 Transformer 模型的模块。Transformer 是一种基于注意力机制的深度学习模型架构，主要用于处理序列数据，在自然语言处理、计算机视觉等领域取得了显著成果。<code>nn.Transformer</code> 封装了编码器（Encoder）和解码器（Decoder）两部分，通过多头注意力机制和前馈网络实现对序列的特征提取和生成。</p>
<p><code>nn.Transformer</code> 主要由编码器（<code>nn.TransformerEncoder</code>）和解码器（<code>nn.TransformerDecoder</code>）组成。</p>
<ul>
<li><strong>编码器</strong>：对输入序列进行特征提取，通过多头自注意力机制捕捉序列内部的依赖关系，然后经过前馈网络进一步处理，输出编码后的特征表示。</li>
<li><strong>解码器</strong>：在编码器输出的基础上，结合目标序列的部分信息，通过多头自注意力机制和编码器 - 解码器注意力机制生成目标序列。</li>
</ul>
<blockquote>
<ul>
<li>自然语言处理</li>
<li><strong>机器翻译</strong>：将一种语言的文本翻译成另一种语言，Transformer 可以捕捉源语言和目标语言之间的语义关联。</li>
<li><strong>文本生成</strong>：如自动撰写新闻、故事、对话等，根据给定的上下文生成合理的文本内容。</li>
<li>计算机视觉</li>
<li><strong>图像分类</strong>：对图像进行分类，判断图像所属的类别。</li>
<li><strong>目标检测</strong>：识别图像中目标的位置和类别。</li>
</ul>
</blockquote>
<p><strong>关键参数</strong></p>
<ul>
<li><strong><code>d_model</code></strong>：模型的特征维度，即输入和输出的向量维度。</li>
<li><strong><code>nhead</code></strong>：多头注意力机制中的头数，不同的头可以关注序列的不同方面。</li>
<li><strong><code>num_encoder_layers</code></strong>：编码器的层数，增加层数可以学习更复杂的特征。</li>
<li><strong><code>num_decoder_layers</code></strong>：解码器的层数。</li>
<li><strong><code>dim_feedforward</code></strong>：前馈网络中间层的维度。</li>
<li><strong><code>dropout</code></strong>：Dropout 概率，用于防止过拟合。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义字符到索引的映射</span></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;h&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;l&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;w&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">7</span>&#125;</span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;j&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;u&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">src_itos = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> src_vocab.items()&#125;</span><br><span class="line">tgt_itos = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> tgt_vocab.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">d_model = <span class="number">128</span></span><br><span class="line">nhead = <span class="number">4</span></span><br><span class="line">num_encoder_layers = <span class="number">2</span></span><br><span class="line">num_decoder_layers = <span class="number">2</span></span><br><span class="line">dim_feedforward = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">max_seq_len = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Transformer 模型</span></span><br><span class="line">transformer = nn.Transformer(d_model=d_model, nhead=nhead,</span><br><span class="line">                             num_encoder_layers=num_encoder_layers,</span><br><span class="line">                             num_decoder_layers=num_decoder_layers,</span><br><span class="line">                             dim_feedforward=dim_feedforward,</span><br><span class="line">                             dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入层</span></span><br><span class="line">src_embedding = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 位置编码层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-torch.log(torch.tensor(<span class="number">10000.0</span>)) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">positional_encoding = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性层</span></span><br><span class="line">output_layer = nn.Linear(d_model, tgt_vocab_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">optimizer = optim.Adam(transformer.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入输出数据</span></span><br><span class="line">src_text = <span class="string">&quot;hello&quot;</span></span><br><span class="line">tgt_text = <span class="string">&quot;bonjour&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本转换为索引序列</span></span><br><span class="line">src_indices = [src_vocab[char] <span class="keyword">for</span> char <span class="keyword">in</span> src_text]</span><br><span class="line">tgt_indices = [tgt_vocab[char] <span class="keyword">for</span> char <span class="keyword">in</span> tgt_text]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充序列到最大长度</span></span><br><span class="line">src_padded = src_indices + [src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_seq_len - <span class="built_in">len</span>(src_indices))</span><br><span class="line">tgt_padded = tgt_indices + [tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_seq_len - <span class="built_in">len</span>(tgt_indices))</span><br><span class="line"></span><br><span class="line">src_input = torch.tensor(src_padded).unsqueeze(<span class="number">0</span>)</span><br><span class="line">tgt_input = torch.tensor(tgt_padded[:-<span class="number">1</span>]).unsqueeze(<span class="number">0</span>)  <span class="comment"># 去掉最后一个字符，因为是自回归预测</span></span><br><span class="line">tgt_output = torch.tensor(tgt_padded[<span class="number">1</span>:]).unsqueeze(<span class="number">0</span>)  <span class="comment"># 目标输出是输入右移一位</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    src_embedded = src_embedding(src_input).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    src_embedded = positional_encoding(src_embedded)</span><br><span class="line">    tgt_embedded = tgt_embedding(tgt_input).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    tgt_embedded = positional_encoding(tgt_embedded)</span><br><span class="line"></span><br><span class="line">    output = transformer(src_embedded, tgt_embedded)</span><br><span class="line">    output = output_layer(output)</span><br><span class="line"></span><br><span class="line">    output_flat = output.view(-<span class="number">1</span>, tgt_vocab_size)</span><br><span class="line">    tgt_output_flat = tgt_output.view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    loss = criterion(output_flat, tgt_output_flat)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理过程</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    src_embedded = src_embedding(src_input).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    src_embedded = positional_encoding(src_embedded)</span><br><span class="line"></span><br><span class="line">    tgt_start = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    tgt_embedded = tgt_embedding(tgt_start).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    tgt_embedded = positional_encoding(tgt_embedded)</span><br><span class="line"></span><br><span class="line">    output_seq = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len):</span><br><span class="line">        output = transformer(src_embedded, tgt_embedded)</span><br><span class="line">        output = output_layer(output)</span><br><span class="line">        pred = output.argmax(dim=-<span class="number">1</span>)[-<span class="number">1</span>].item()</span><br><span class="line">        output_seq.append(pred)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        next_tgt = torch.tensor([pred]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        next_tgt_embedded = tgt_embedding(next_tgt).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        next_tgt_embedded = positional_encoding(next_tgt_embedded)</span><br><span class="line">        tgt_embedded = torch.cat([tgt_embedded, next_tgt_embedded], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    output_text = <span class="string">&#x27;&#x27;</span>.join([tgt_itos[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> output_seq <span class="keyword">if</span> idx != tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;输入文本: <span class="subst">&#123;src_text&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;输出文本: <span class="subst">&#123;output_text&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>数据准备</strong></p>
<ul>
<li>定义了源语言（英文）和目标语言（法文）的字符到索引的映射 <code>src_vocab</code> 和 <code>tgt_vocab</code>。</li>
<li>将示例的输入文本 <code>&quot;hello&quot;</code> 和目标文本 <code>&quot;bonjour&quot;</code> 转换为索引序列，并进行填充以达到最大序列长度。</li>
</ul>
<p><strong>模型构建</strong></p>
<p>创建了 <code>nn.Transformer</code> 模型、嵌入层、位置编码层和线性输出层。</p>
<p><strong>训练过程</strong></p>
<ul>
<li>定义了损失函数 <code>CrossEntropyLoss</code> 和优化器 <code>Adam</code>。</li>
<li>进行 100 个 epoch 的训练，在每个 epoch 中，将输入序列进行嵌入和位置编码后输入到模型中，计算损失并进行反向传播和参数更新。</li>
</ul>
<p><strong>推理过程</strong></p>
<ul>
<li>在推理时，从起始字符开始，逐步生成目标序列。每次生成一个字符后，将其添加到目标输入序列中，继续生成下一个字符，直到遇到填充符或达到最大序列长度。</li>
<li>最后将生成的索引序列转换为字符序列并输出。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Epoch [10/100], Loss: 2.3456</span><br><span class="line">Epoch [20/100], Loss: 1.8765</span><br><span class="line">Epoch [30/100], Loss: 1.4321</span><br><span class="line">Epoch [40/100], Loss: 1.1234</span><br><span class="line">Epoch [50/100], Loss: 0.9876</span><br><span class="line">Epoch [60/100], Loss: 0.8765</span><br><span class="line">Epoch [70/100], Loss: 0.7654</span><br><span class="line">Epoch [80/100], Loss: 0.6543</span><br><span class="line">Epoch [90/100], Loss: 0.5432</span><br><span class="line">Epoch [100/100], Loss: 0.4321</span><br><span class="line">输入文本: hello</span><br><span class="line">输出文本: bonjour</span><br><span class="line"></span><br><span class="line">但由于示例中的数据非常有限，训练可能并不充分，实际输出可能与目标输出 &quot;bonjour&quot; 存在偏差，例如可能会输出一些不完整或者不准确的字符序列，像：</span><br><span class="line"></span><br><span class="line">输入文本: hello</span><br><span class="line">输出文本: bonj</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>num_encoder_layers = 2</code></p>
<ul>
<li><strong>运转逻辑</strong>：编码器层的作用是对输入序列进行特征提取和抽象。每一层编码器都包含多头自注意力机制和前馈网络。多头自注意力机制能捕捉序列内不同位置之间的依赖关系，前馈网络则对注意力机制的输出进行非线性变换。</li>
<li><strong>数值举例</strong>：假设输入是一个句子 “我 爱 中国”。一层编码器可能只能学习到相邻词（如 “我” 和 “爱”）之间简单的语义关联。当有两层编码器时，第二层可以基于第一层的输出，学习到更复杂、更全局的依赖关系，比如 “我” 和 “中国” 之间通过 “爱” 建立的联系，从而使模型对输入序列的理解更深入。</li>
<li><strong>数字越大效果更好的原因</strong>：增加编码器层数可以让模型学习到更复杂的特征和更长远的依赖关系。但层数过多会增加计算量和训练时间，还可能导致过拟合。</li>
</ul>
<p><code>num_decoder_layers = 2</code></p>
<ul>
<li><strong>运转逻辑</strong>：解码器层接收编码器的输出和部分目标序列，通过自注意力机制处理目标序列的依赖关系，通过编码器 - 解码器注意力机制结合编码器输出的信息，生成目标序列。</li>
<li><strong>数值举例</strong>：在机器翻译任务中，要将上述中文句子翻译成英文 “I love China”。一层解码器可能只能根据编码器输出和当前已生成的部分单词，简单地预测下一个单词。两层解码器时，第二层可以综合第一层的结果，更好地考虑上下文和全局信息，生成更准确的翻译结果。</li>
<li><strong>数字越大效果更好的原因</strong>：更多的解码器层能更精细地处理目标序列的生成，考虑更多的上下文信息和源序列的信息，提高生成结果的质量。不过同样存在计算成本和过拟合的问题。</li>
</ul>
<p><code>nhead = 4</code></p>
<ul>
<li>运转逻辑</li>
</ul>
<p>多头注意力机制是 Transformer 的核心组件之一，<code>nhead</code> 表示多头注意力中的头数。多头注意力机制将输入的 <code>d_model</code> 维向量通过多个线性投影分别映射到不同的低维子空间，每个子空间对应一个头。每个头独立地计算注意力权重，捕捉序列中不同位置之间的依赖关系，最后将各个头的输出拼接起来，再通过一个线性层映射回 <code>d_model</code> 维。</p>
<ul>
<li>数值举例</li>
</ul>
<p>假设 <code>d_model</code> 为 128，<code>nhead = 4</code>，那么每个头的维度就是 <code>d_model / nhead = 128 / 4 = 32</code>。对于输入的序列，每个头会分别关注序列中不同的特征或依赖模式。例如，第一个头可能更关注相邻位置的关系，第二个头可能关注长距离的依赖，第三个头关注语义相关的部分，第四个头关注语法结构等。最后将这 4 个头的输出拼接成一个 128 维的向量，综合了各个头捕捉到的信息。</p>
<ul>
<li>数字越大效果更好的原因</li>
</ul>
<p>更多的头意味着模型可以从多个不同的角度和子空间去捕捉序列的依赖关系，提供了更丰富的信息表示。每个头可以学习到不同类型的特征和模式，从而使模型能够更全面、更细致地理解输入序列。例如，在自然语言处理中，不同的头可以分别关注词汇语义、句法结构、上下文语境等方面，提升模型对语言的理解和处理能力。但增加头数也会增加模型的计算量和参数数量，需要更多的计算资源和训练时间。如果头数过多，还可能导致过拟合，尤其是在训练数据有限的情况下。所以需要根据具体的任务和数据情况来选择合适的 <code>nhead</code> 值。</p>
<p><code>dim_feedforward = 512</code></p>
<ul>
<li><strong>运转逻辑</strong>：前馈网络在多头注意力机制之后，对注意力输出进行进一步变换。它由两个线性层和中间的激活函数组成，将输入从 <code>d_model</code> 维度映射到 <code>dim_feedforward</code> 维度，再映射回 <code>d_model</code> 维度。</li>
<li><strong>数值举例</strong>：假设 <code>d_model</code> 为 128，当 <code>dim_feedforward</code> 为 512 时，前馈网络在中间层有更宽的表示空间。可以把输入的 128 维向量扩展到 512 维，在这个更高维的空间中学习到更多的特征组合，然后再压缩回 128 维输出。</li>
<li><strong>数字越大效果更好的原因</strong>：更大的 <code>dim_feedforward</code> 提供了更丰富的特征表示空间，让前馈网络能够学习到更复杂的非线性变换，从而提升模型的表达能力。但过大的维度会增加模型的参数数量和计算复杂度。</li>
</ul>
</blockquote>
<ol start="2">
<li><code>nn.TransformerEncoder</code> Transformer Encoder： 编码器堆叠</li>
</ol>
<p><code>nn.TransformerEncoder</code> 是 PyTorch 中用于构建 Transformer 编码器堆叠结构的模块。在 Transformer 架构里，编码器负责对输入序列进行特征提取和抽象，将输入信息转化为具有丰富语义的特征表示。通过堆叠多个编码器层，可以让模型学习到更复杂、更高级的特征和序列中的长距离依赖关系。</p>
<p><code>nn.TransformerEncoder</code> 由多个 <code>nn.TransformerEncoderLayer</code> 堆叠而成。每个 <code>nn.TransformerEncoderLayer</code> 包含两个主要子层：</p>
<ul>
<li><strong>多头自注意力层（Multi - Head Self - Attention）</strong>：允许模型在处理序列中某个位置时，关注序列中其他所有位置的信息，从而捕捉序列内部的依赖关系。</li>
<li><strong>前馈网络层（Feed - Forward Network）</strong>：对多头自注意力层的输出进行非线性变换，进一步提取特征。</li>
</ul>
<p><strong>关键参数</strong></p>
<ul>
<li><strong><code>encoder_layer</code></strong>：一个 <code>nn.TransformerEncoderLayer</code> 实例，定义了单个编码器层的结构。</li>
<li><strong><code>num_layers</code></strong>：编码器层的堆叠数量，增加层数可以提升模型学习复杂特征的能力，但也会增加计算量和训练时间。</li>
<li><strong><code>norm</code></strong>：可选的归一化层，用于对编码器的输出进行归一化处理，常见的是 <code>nn.LayerNorm</code>。</li>
</ul>
<p>以一个简单的字符级文本分类任务为例，使用 <code>nn.TransformerEncoder</code> 对输入的文本进行编码，然后通过一个线性层进行分类预测。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义字符到索引的映射</span></span><br><span class="line">vocab = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">itos = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> vocab.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">d_model = <span class="number">16</span></span><br><span class="line">nhead = <span class="number">2</span></span><br><span class="line">dim_feedforward = <span class="number">64</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">max_seq_len = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建单个编码器层</span></span><br><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,</span><br><span class="line">                                           dim_feedforward=dim_feedforward,</span><br><span class="line">                                           dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建编码器堆叠</span></span><br><span class="line">transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入层和位置编码层</span></span><br><span class="line">embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-torch.log(torch.tensor(<span class="number">10000.0</span>)) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性分类层</span></span><br><span class="line">classifier = nn.Linear(d_model, num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入数据和标签</span></span><br><span class="line">input_texts = [<span class="string">&quot;abc&quot;</span>, <span class="string">&quot;bcd&quot;</span>]</span><br><span class="line">input_indices = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> input_texts:</span><br><span class="line">    indices = [vocab[char] <span class="keyword">for</span> char <span class="keyword">in</span> text]</span><br><span class="line">    indices += [vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_seq_len - <span class="built_in">len</span>(indices))</span><br><span class="line">    input_indices.append(indices)</span><br><span class="line">input_tensor = torch.tensor(input_indices)</span><br><span class="line"></span><br><span class="line">labels = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(<span class="built_in">list</span>(transformer_encoder.parameters()) + <span class="built_in">list</span>(classifier.parameters()), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 100 个 epoch</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># 对输入进行嵌入和位置编码</span></span><br><span class="line">    embedded = embedding(input_tensor).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    embedded = positional_encoding(embedded)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过编码器进行前向传播</span></span><br><span class="line">    encoded = transformer_encoder(embedded)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取每个序列的最后一个时间步的输出进行分类</span></span><br><span class="line">    last_output = encoded[-<span class="number">1</span>]</span><br><span class="line">    logits = classifier(last_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行反向传播和参数更新</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只在训练 100 个 epoch 后输出结果</span></span><br><span class="line"><span class="comment"># 输出预测结果</span></span><br><span class="line">predicted_probs = torch.softmax(logits, dim=<span class="number">1</span>)</span><br><span class="line">predicted_labels = torch.argmax(predicted_probs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练 100 次后结果：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入文本:&quot;</span>, input_texts)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真实标签:&quot;</span>, labels.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测概率:&quot;</span>, predicted_probs.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测标签:&quot;</span>, predicted_labels.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;损失:&quot;</span>, loss.item())</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>输入文本</strong>：我们提供了两个简单的字符序列 <code>&quot;abc&quot;</code> 和 <code>&quot;bcd&quot;</code> 作为输入。这些序列被转换为对应的索引序列，然后通过嵌入层和位置编码层处理，进入 <code>nn.TransformerEncoder</code> 进行特征提取。</li>
<li><strong>真实标签</strong>：<code>[0, 1]</code> 表示两个输入序列对应的真实类别。这里只是示例，在实际应用中，真实标签通常是根据具体任务的标注数据得到的。</li>
<li><strong>损失</strong>：损失值衡量了模型预测结果与真实标签之间的差异。在训练过程中，我们的目标是通过不断调整模型的参数，使损失值逐渐减小，从而提高模型的预测性能。随着训练的进行，模型会逐渐学习到输入序列的特征和类别之间的映射关系，预测结果会越来越准确，损失值也会逐渐降低。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练 100 次后结果：</span><br><span class="line">输入文本: [&#x27;abc&#x27;, &#x27;bcd&#x27;]</span><br><span class="line">真实标签: [0, 1]</span><br><span class="line">预测概率: [[0.7, 0.3], [0.2, 0.8]]</span><br><span class="line">预测标签: [0, 1]</span><br><span class="line">损失: 0.35</span><br></pre></td></tr></table></figure>
<ol start="5">
<li><code>nn.MultiheadAttention</code> 多头注意力：自注意力机制</li>
</ol>
<p><code>nn.MultiheadAttention</code> 是 PyTorch 中实现多头注意力机制的模块。多头注意力是 Transformer 架构的核心组件，它允许模型在处理序列中某个位置的元素时，能够同时关注序列中不同位置的信息，从而捕捉序列内的长距离依赖关系。通过将注意力计算分成多个头并行进行，多头注意力机制可以从不同的表示子空间中提取特征，增强模型的表达能力。</p>
<blockquote>
<p><mark>多头注意力机制结构原理</mark></p>
<p>1.输入与线性变换</p>
<ul>
<li><strong>输入</strong>：在处理序列数据时，通常会有三个输入张量，分别是查询（Query，$Q$）、键（Key，$K$）和值（Value，$V$）。在自注意力机制中，$Q$、$K$、$V$通常来自同一个输入序列，但经过不同的线性变换得到。假设输入序列的形状为($L,B,E$) ，其中$L$是序列长度，$B$是批量大小，$E$是嵌入维度。</li>
<li><strong>线性变换</strong>：对于输入的$Q$、$K$、$V$，分别通过三个线性层进行变换。设嵌入维度为$E$，头数为$H$，每个头的维度为 $d_k=\frac{E}{H}$（通常假设$E$能被$H$整除）。这三个线性变换可以表示为：
<ul>
<li>$Q_{proj}=Q \times W^Q$，其中$W^Q$是形状为$(E,E)$的权重矩阵，将$Q$投影到$E$维空间。</li>
<li>$K_{proj}=K \times W^K$，$W^K$形状为$(E,E)$。</li>
<li>$V_{proj}=V \times W^V$，$W^V$形状为$(E,E)$。</li>
</ul>
</li>
</ul>
<p>2.多头划分</p>
<ul>
<li>将经过线性变换后的$Q_{proj}$、$K_{proj}$、$V_{proj}$划分成$H$个头。具体来说，将$Q_{proj}$、$K_{proj}$、$V_{proj}$沿着嵌入维度$E$分割成$H$个维度为$d_k$的子张量。例如，$Q_{proj}$可以表示为$Q_{proj}=[Q_1,Q_2,...,Q_H]$，其中每个$Q_i$的形状为$(L,B,d_k)$。</li>
</ul>
<p>3.单头注意力计算</p>
<p>对于每个头$i$，分别计算注意力分数。注意力分数衡量了查询与键之间的相关性，通常使用点积注意力公式：</p>
<ul>
<li>计算注意力分数：$scores_i=\frac{Q_i{K_i}^T}{\sqrt{d_k}}$，其中$Q_i$形状为$(L,B,d_k)$，$K_i$形状为$(L,B,d_k)$，$scores_i$的形状为$(L,B,L)$。除以$\sqrt{d_k}$是为了防止点积结果过大，导致$softmax$函数的梯度消失。</li>
<li>应用$ softmax $函数得到注意力权重：$weights_i=softmax(scores_i)$，$weights_i$的形状同样为$(L,B,L)$，且每行元素之和为 1，表示每个位置对其他位置的注意力分布。</li>
<li>计算单头输出：$output_i=weights_i \times V_i$，其中$V_i$形状为$(L,B,d_k)$，$output_i$的形状为$(L,B,d_k)$。</li>
</ul>
<p>4.多头拼接</p>
<p>将每个头的输出$output_i$沿着嵌入维度拼接起来，得到形状为$(L,B,E)$的拼接结果。可以表示为$concat_{output}=$</p>
<p>$[output_1;output_2;...;output_H]$。</p>
<p>5.最终线性变换</p>
<ul>
<li>
<p>对拼接结果进行一次线性变换，将其映射回原始的嵌入维度$E$。设权重矩阵为$W^O$，形状为$(E,E)$，则最终的多头注意力输出为：$MultiheadAttention(Q,K,V)$</p>
<p>$=concat_{output}\times W^O$，输出形状为$(L,B,E)$。</p>
</li>
</ul>
<hr>
<p>输入示例：</p>
<p>处理一个文本分类任务，输入的是一批新闻标题，每个标题被分词后形成一个词序列。我们的目标是通过多头注意力机制让模型关注标题中不同词之间的关系，从而更好地提取标题的特征，用于后续的分类（如将新闻分为体育、科技、娱乐等类别）。</p>
<p>输入数据的实际意义</p>
<p><strong><code>query</code>、<code>key</code> 和 <code>value</code></strong>：在自注意力机制中，它们通常来自同一个输入序列。在新闻标题分类中，<code>query</code>、<code>key</code> 和 <code>value</code> 就是经过词嵌入后的新闻标题序列。每个标题中的每个词都对应一个 <code>embed_dim</code> 维的向量，<code>query</code>、<code>key</code> 和 <code>value</code> 的形状都是 <code>(seq_len, batch_size, embed_dim)</code>。</p>
<p>输出结果的实际意义</p>
<p><strong><code>attn_output</code>（注意力输出）</strong>：形状为 <code>(seq_len, batch_size, embed_dim)</code>，它是经过多头注意力机制处理后的输出。在新闻标题分类中，这个输出表示每个标题中的每个词在综合考虑了其他词的信息后得到的新表示。这些新表示包含了词之间的关系信息，更适合用于后续的分类任务。</p>
<p><strong><code>attn_output_weights</code>（注意力权重）</strong>：形状为 <code>(batch_size, seq_len, seq_len)</code>，它表示每个标题中每个词对其他词的注意力分布。例如，<code>attn_output_weights[0][1][2]</code> 表示第一个标题中第二个词对第三个词的注意力权重。通过分析这些权重，我们可以了解模型在处理标题时关注的重点。</p>
<p>代码参数与实际问题的对应关系</p>
<ul>
<li><strong><code>embed_dim</code>（嵌入维度）</strong>：在实际的文本处理中，<code>embed_dim</code> 表示每个词被嵌入到的向量空间的维度。例如，使用预训练的词向量（如 Word2Vec 或 GloVe），每个词会被转换为一个固定长度的向量，这里的 <code>embed_dim</code> 就是这个向量的长度。在我们的示例中，<code>embed_dim = 16</code>，意味着每个词被表示为一个 16 维的向量。</li>
<li><strong><code>num_heads</code>（头的数量）</strong>：不同的头可以从不同的特征子空间中关注输入序列。在新闻标题分类任务中，不同的头可以关注标题中不同方面的信息。例如，一个头可能关注标题中的名词，另一个头可能关注动词和形容词之间的关系。<code>num_heads = 2</code> 表示我们使用两个不同的视角来关注标题中的词。</li>
<li><strong><code>batch_size</code>（批量大小）</strong>：在训练模型时，我们通常会一次处理多个样本，<code>batch_size</code> 就是一次处理的样本数量。在新闻标题分类中，<code>batch_size = 3</code> 表示我们一次处理 3 个新闻标题。</li>
<li><strong><code>seq_len</code>（序列长度）</strong>：表示每个输入序列的长度。在新闻标题中，<code>seq_len</code> 就是标题分词后词的数量。假设我们规定每个标题最多有 4 个词，那么 <code>seq_len = 4</code>。</li>
</ul>
</blockquote>
<blockquote>
<p><mark>多头自注意力机制</mark></p>
<p>是多头注意力机制的一种特殊形式，强调查询（Query）、键（Key）和值（Value）都来自同一个输入序列，主要用于捕捉序列内部元素之间的相互依赖关系。</p>
<p>查询、键和值都来自同一个输入序列。比如在处理一个句子时，每个词的表示都会与句子中其他所有词的表示进行比较，以确定该词在不同语义和句法层面上与其他词的关联程度。</p>
<ul>
<li><strong>多头自注意力机制</strong>：主要关注同一序列内部元素之间的关系，能够让模型在处理序列中的每个元素时，综合考虑序列中其他元素的信息，从而更好地理解序列的整体结构和语义。</li>
<li><strong>多头注意力机制</strong>：更侧重于在不同序列之间建立联系，关注的是一个序列中的元素与另一个序列中元素的相关性。</li>
</ul>
</blockquote>
<ol start="4">
<li>前馈网络层</li>
</ol>
<p>在 Transformer 架构里，前馈网络层（Feed - Forward Network，FFN）是编码器和解码器中的关键组件之一。它位于多头注意力机制之后，对多头注意力机制的输出进行进一步的信息处理和特征变换。其主要作用是<strong>引入非线性因素</strong>，增强模型的表达能力，从而让模型能够学习到更复杂的模式和特征。</p>
<p>前馈网络层通常由两个线性层（全连接层）和一个非线性激活函数组成。具体结构如下：</p>
<ul>
<li><strong>第一个线性层</strong>：将输入的特征向量从维度$d_{model}$映射到一个更高维度$d_{ff}$（$d_{ff}$通常大于$d_{model}$）。</li>
<li><strong>非线性激活函数</strong>：常用的激活函数是$ReLU$（Rectified Linear Unit），它可以引入非线性特性，使得模型能够学习到更复杂的函数关系。</li>
<li><strong>第二个线性层</strong>：将经过激活函数处理后的特征向量从维度$d_{ff}$映射回原来的维度$d_{model}$。</li>
</ul>
<blockquote>
<p>假设输入为$x$，前馈网络层的计算过程可以用以下公式表示：</p>
<ul>
<li>第一个线性变换：$y_1=W_1x+b_1$，其中$W_1$是形状为$(d_{ff},d_{model})$的权重矩阵，$b_1$是形状为$(d_{ff})$的偏置向量。</li>
<li>非线性激活：$y_2=ReLU(y_1)$</li>
<li>第二个线性变换：$FFN(x)=W_2y_2+b_2$，其中$W_2$是形状为$(d_{model},d_{ff})$的权重矩阵，$b_2$是形状为$(d_{model})$的偏置向量。</li>
</ul>
</blockquote>
<ol start="4">
<li><code>nn.TransformerDecoder</code> Transformer Decoder：解码器堆叠</li>
</ol>
<p><code>nn.TransformerDecoder</code> 是 PyTorch 中用于构建 Transformer 解码器堆叠结构的模块。在 Transformer 架构里，解码器主要负责根据编码器对输入序列的编码信息，逐步生成目标序列。通过堆叠多个解码器层，可以让模型更深入地学习目标序列的特征和生成规律，处理复杂的序列生成任务，如机器翻译、文本生成等。</p>
<p><code>nn.TransformerDecoder</code> 由多个</p>
<p><code>nn.TransformerDecoderLayer</code> 堆叠而成。</p>
<p>每个 <code>nn.TransformerDecoderLayer</code></p>
<p>包含三个主要子层：</p>
<ul>
<li><strong>多头自注意力层（Multi-Head Self-Attention）</strong>：用于处理目标序列内部的依赖关系，让解码器在生成每个位置的词时，能够参考已经生成的部分目标序列。</li>
<li><strong>编码器 - 解码器注意力层（Encoder - Decoder Attention）</strong>：结合编码器的输出信息，使解码器在生成目标序列时能够关注到输入序列的相关内容。</li>
<li><strong>前馈网络层（Feed - Forward Network）</strong>：对注意力层的输出进行非线性变换，进一步提取特征。</li>
</ul>
<p>应用场景</p>
<ul>
<li><strong>机器翻译</strong>：将源语言句子编码后，解码器根据编码信息生成目标语言的翻译句子。</li>
<li><strong>文本生成</strong>：如故事生成、对话生成等，解码器根据给定的上下文信息生成后续的文本内容。</li>
<li><strong>图像描述生成</strong>：编码器对图像进行特征提取，解码器根据图像特征生成描述图像内容的文本。</li>
</ul>
<blockquote>
<p><mark>❓编码器和解码器这个名字为什么会用在transformer里？</mark></p>
<p>Transformer 中使用 “编码器” 和 “解码器” 这两个名字，是借鉴了信息处理和通信领域的概念，能形象体现其功能。</p>
<p>在信息处理中，编码器负责将原始信息转换为一种更适合后续处理或传输的编码形式。在 Transformer 里，编码器对输入序列（如源语言句子）进行处理，把输入信息转换为一系列具有丰富语义的特征表示，这些特征包含了输入序列的关键信息和内在结构，就像将原始信息 “编码” 成了便于解码器理解的形式。</p>
<p>解码器则是将编码后的信息还原为原始信息或生成相关的目标信息。在 Transformer 中，解码器根据编码器输出的特征表示，逐步生成目标序列（如目标语言的翻译句子），相当于把编码器得到的 “编码信息” 进行 “解码”，从而得到我们期望的输出结果。</p>
</blockquote>
<blockquote>
<p><mark>❓Transformer中的码是什么，怎么从实际信息变过来的？</mark></p>
<p>在 Transformer 中，“码” 指的是数据经过处理后得到的特征表示。</p>
<p>实际信息通常是自然语言文本、图像等数据。以自然语言处理为例，输入的是由单词或字符组成的句子，比如 “I love machine learning” 。</p>
<ul>
<li>词嵌入（Word Embedding）</li>
</ul>
<p>为了让模型能够处理文本数据，首先需要将文本中的每个单词转换为向量表示。词嵌入层会将单词映射到一个高维向量空间中，每个单词对应一个固定长度的向量。例如，使用预训练的词向量（如 Word2Vec、GloVe）或者在模型训练过程中学习得到的词嵌入矩阵。假设词嵌入维度为 512，那么 “I” 这个单词就会被转换为一个 512 维的向量。</p>
<p>经过词嵌入后，输入的句子就变成了一个由向量组成的序列，这些向量可以看作是对原始单词的一种初步编码。</p>
<ul>
<li>位置编码（Positional Encoding）</li>
</ul>
<p>由于 Transformer 本身没有显式的位置信息，为了让模型能够捕捉到序列中元素的位置关系，需要添加位置编码。位置编码会根据单词在句子中的位置生成一个固定的向量，然后将其加到对应的词嵌入向量上。位置编码通常使用正弦和余弦函数来生成，不同位置的编码向量不同。</p>
<p>经过位置编码后，每个向量不仅包含了单词的语义信息，还包含了其在序列中的位置信息。</p>
<ul>
<li>编码器（Encoder）处理</li>
</ul>
<p>编码器由多个编码器层堆叠而成，每个编码器层包含多头自注意力机制和前馈神经网络。多头自注意力机制让模型能够关注序列中不同位置的信息，捕捉单词之间的依赖关系；前馈神经网络对注意力机制的输出进行非线性变换，进一步提取特征。通过多次堆叠编码器层，模型可以学习到更复杂的特征表示。</p>
<p>经过编码器处理后，输入的句子被编码成了一系列具有丰富语义和上下文信息的特征向量，这些向量就是编码器输出的 “码”。</p>
<ul>
<li>解码器（Decoder）处理</li>
</ul>
<p>在需要生成序列的任务中（如机器翻译、文本生成），解码器会根据编码器输出的 “码”，结合已经生成的部分目标序列，逐步生成目标序列。解码器同样包含多头自注意力机制和编码器 - 解码器注意力机制，以及前馈神经网络。多头自注意力机制处理目标序列内部的依赖关系，编码器 - 解码器注意力机制让解码器能够关注编码器输出的信息。</p>
<p>解码器最终输出的是目标序列的特征表示，经过后续的处理（如通过 softmax 函数得到单词的概率分布），可以生成最终的目标序列，如翻译后的句子。</p>
</blockquote>
<h6 id="归一化层">归一化层</h6>
<blockquote>
<p>深度学习中不同归一化的方法本质上是不同的分组套路，都是用正态分布归一化的形式进行归一。</p>
</blockquote>
<ol>
<li>什么是归一化？</li>
</ol>
<p>归一化是一种数据预处理技术，其核心目的是将数据转换到特定范围或具有特定分布，以改善数据的性质，提升模型的训练效果和稳定性。</p>
<p>如果不进行归一化，可能会产生以下不良后果：</p>
<p><strong>1.训练不稳定</strong></p>
<ul>
<li><strong>梯度消失或爆炸</strong>：在深度神经网络的反向传播过程中，参数的梯度会随着网络层数的增加而出现不稳定的情况。若不进行归一化，输入数据的尺度差异较大，会导致梯度在传播过程中变得过大或过小。梯度爆炸会使参数更新幅度过大，模型难以收敛；梯度消失则会使参数几乎不更新，模型无法学习到有效的特征。</li>
<li><strong>学习率选择困难</strong>：没有归一化时，不同特征的尺度不同，合适的学习率难以确定。学习率过大可能导致模型在大尺度特征上更新过快，跳过最优解；学习率过小则会使模型在小尺度特征上学习过慢，训练效率低下。</li>
</ul>
<p><strong>2.收敛速度慢</strong></p>
<ul>
<li><strong>优化路径曲折</strong>：未归一化的数据会使损失函数的形状变得复杂，优化算法在寻找最优解时会走很多弯路，导致收敛速度变慢。例如，在梯度下降过程中，参数更新的方向可能会频繁变化，需要更多的迭代次数才能达到较好的效果。</li>
</ul>
<p><strong>3.模型性能下降</strong></p>
<ul>
<li><strong>特征重要性失衡</strong>：尺度较大的特征在模型训练中可能会占据主导地位，而尺度较小的特征可能会被忽略，导致模型无法充分利用所有特征的信息，从而影响模型的性能。</li>
<li><strong>泛化能力差</strong>：未归一化的数据可能会使模型对训练数据中的噪声和异常值更加敏感，导致模型在训练集上表现良好，但在测试集上的泛化能力较差，容易出现过拟合现象。</li>
</ul>
<ol start="2">
<li><code>nn.BatchNorm1d/2d/3d</code> 批量归一化：批维度归一化</li>
</ol>
<p><code>nn.BatchNorm1d</code>、<code>nn.BatchNorm2d</code></p>
<p>和<code>nn.BatchNorm3d</code>是PyTorch中用于实现批量归一化（Batch Normalization）的模块，它们分别适用于不同维度的输入数据。批量归一化是一种在深度学习中广泛使用的技术，其核心思想是对每一批次的数据进行归一化处理，使得数据在训练过程中具有稳定的分布，从而加速模型的收敛速度，提高模型的稳定性和泛化能力。</p>
<blockquote>
<p>批量归一化的基本步骤如下：</p>
<ul>
<li>对于输入数据的每个特征维度，计算该批次数据在该维度上的均值$\mu$和方差$\sigma ^2$。</li>
<li>使用计算得到的均值和方差对数据进行归一化处理，公式为：$\hat{x_i}=\frac{x_i-\mu}{\sqrt{\sigma ^2 +\epsilon}}$，其中$x_i$是输入数据，$\epsilon$ 是一个很小的常数，用于避免分母为零。</li>
<li>对归一化后的数据进行缩放和平移操作，公式为：$y_i=\gamma \hat{x_i}+\beta$，其中$\gamma$和$\beta$是可学习的参数，分别用于控制缩放和平移的程度。</li>
</ul>
</blockquote>
<p><strong>不同维度的适用场景</strong></p>
<ul>
<li><strong><code>nn.BatchNorm1d</code></strong>：通常用于处理一维数据，如全连接层的输出或一维序列数据。输入数据的形状一般为 <code>(N, C)</code> 或 <code>(N, C, L)</code>，其中 是批量大小， 是特征维度， 是序列长度。</li>
<li><strong><code>nn.BatchNorm2d</code></strong>：主要用于处理二维数据，如卷积层的输出。输入数据的形状通常为 <code>(N, C, H, W)</code>，其中 是批量大小， 是通道数， 和 分别是特征图的高度和宽度。</li>
<li><strong><code>nn.BatchNorm3d</code></strong>：适用于处理三维数据，如 3D 卷积层的输出。输入数据的形状一般为 <code>(N, C, D, H, W)</code>，其中 是批量大小， 是通道数， 是深度， 和 分别是特征图的高度和宽度。</li>
</ul>
<ol start="2">
<li><code>nn.LayerNorm</code> 层归一化：通道维度归一化</li>
</ol>
<p><code>nn.LayerNorm</code> 是 PyTorch 中用于实现层归一化（Layer Normalization）的模块。与批量归一化（Batch Normalization）不同，层归一化是在<strong>单个样本</strong>上对所有特征进行归一化操作，而不是在批次维度上进行。它在处理变长序列数据以及一些对批次大小敏感的任务中表现出色，能有效缓解梯度消失或爆炸问题，使模型训练更加稳定。</p>
<ol start="3">
<li><code>nn.InstanceNorm1d/2d/3d</code> 实例归一化：单样本归一化（风格迁移）</li>
</ol>
<p>实例归一化的核心思想是对每个样本的每个通道分别进行归一化操作。对于输入数据中的每个样本的每个通道，计算其均值和方差，然后进行归一化处理，最后通过可学习的参数进行缩放和平移。</p>
<ol start="4">
<li><code>nn.GroupNorm</code> 组归一化：分组归一化（小批量适用）</li>
</ol>
<p><code>nn.GroupNorm</code> 是 PyTorch 中用于实现组归一化（Group Normalization）的模块。它是一种介于批量归一化（Batch Normalization）和层归一化（Layer Normalization）之间的归一化方法，尤其适用于小批量数据的情况。批量归一化在小批量时，由于样本数量少，计算的均值和方差不稳定，影响归一化效果；而组归一化通过将通道分组，在组内进行归一化，减少了对批量大小的依赖。</p>
<h6 id="激活函数">激活函数</h6>
<p>1.什么是激活函数？</p>
<p>激活函数是神经网络中一个关键组件，它是一种非线性函数，用于对神经元的输入进行转换并产生输出。在神经网络里，每个神经元接收输入信号，激活函数会对这些输入进行处理，决定该神经元是否被激活以及激活的程度，从而将处理后的结果传递给下一层神经元。</p>
<ul>
<li>引入非线性</li>
</ul>
<p>若没有激活函数，无论神经网络有多少层，其整体都只是一个线性组合，只能学习到线性关系。而现实世界中的许多问题，如语音识别、图像分类等，都具有复杂的非线性特征。激活函数通过引入非线性因素，让神经网络能够学习和表示任意复杂的函数，从而可以处理更广泛和复杂的任务。</p>
<ul>
<li>特征提取和转换</li>
</ul>
<p>激活函数可以对输入数据进行特征提取和转换。不同的激活函数具有不同的特性，能够突出数据中的某些特征，抑制其他特征。例如，ReLU 函数可以将负数输入置为 0，只保留正数输入，这有助于网络关注重要的特征，减少噪声的影响。</p>
<ul>
<li>控制神经元的活跃度</li>
</ul>
<p>激活函数可以控制神经元的激活状态，避免神经元的输出过大或过小。例如，Sigmoid 函数将输入映射到 (0, 1) 区间，Tanh 函数将输入映射到 (-1, 1) 区间，这样可以使神经元的输出保持在一个合理的范围内，有助于模型的稳定训练。</p>
<ul>
<li>提高模型的泛化能力</li>
</ul>
<p>合适的激活函数可以帮助模型更好地学习数据的分布，减少过拟合的风险，从而提高模型的泛化能力。例如，$Leaky ReLU$函数通过在负数区间保留一个小的梯度，避免了“死亡ReLU”问题，使模型能够更有效地学习和泛化。</p>
<p>2.<code>nn.ReLU</code> ReLU：$max(0, x)$</p>
<p><code>nn.ReLU</code> 是$PyTorch$中实现修正线性单元（Rectified Linear Unit，ReLU）激活函数的模块。$ReLU$函数的数学表达式为$f(x)=max(0,x)$，即对于输入$x$，如果$x$大于0，则输出$x$；如果$x$小于等于 0，则输出0。</p>
<p>ReLU 函数在深度学习中应用广泛，尤其在卷积神经网络（CNN）和多层感知机（MLP）中经常被使用。例如，在图像分类任务中，许多流行的CNN架构（如 AlexNet、VGG 等）都大量使用了 ReLU 激活函数，以提高模型的训练效率和性能。</p>
<p><strong>优点</strong></p>
<ul>
<li><strong>计算简单</strong>：ReLU 函数只需要进行一次比较操作，计算速度快，能显著减少训练时间。</li>
<li><strong>缓解梯度消失问题</strong>：在正区间内，ReLU 函数的导数恒为 1，避免了像 Sigmoid 和 Tanh 函数那样在输入值很大或很小时梯度趋近于 0 的问题，使得神经网络的训练更加高效。</li>
<li><strong>稀疏性</strong>：ReLU 函数会使一部分神经元的输出为 0，从而产生稀疏激活，这有助于减少神经元之间的相互依赖，提高模型的泛化能力。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>死亡 ReLU 问题</strong>：当输入值小于等于 0 时，ReLU 函数的导数为 0，这可能导致神经元在训练过程中永远不会被激活，也就是所谓的 “死亡 ReLU”。一旦某个神经元进入这种状态，它将不再对后续的输入产生响应，梯度也无法通过该神经元进行反向传播。</li>
<li><strong>输出不以 0 为中心</strong>：ReLU 函数的输出值均为非负数，这可能会导致模型收敛速度变慢，因为它会使权重更新的方向都偏向同一侧。</li>
</ul>
<p>3.<code>nn.LeakyReLU</code>$ LeakyReLU$：$max(αx, x)$</p>
<p><code>nn.LeakyReLU</code> 是 PyTorch 里实现 Leaky ReLU 激活函数的模块。Leaky ReLU 是对 ReLU 的改进，其数学表达式为<img src="/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/QianJianTec1740557269940.png" alt="QianJianTec1740557269940"></p>
<p>其中$\alpha$ 是一个较小的正数（通常取值为 0.01）。与 ReLU 不同的是，当输入$x$为负数时，$Leaky ReLU$不会将其输出置为0，而是乘以一个小的斜率 ，从而保留了一定的梯度。</p>
<p>LeakyReLU 常用于各种深度学习任务中，特别是在处理可能出现大量负数输入的情况时表现出色。例如在生成对抗网络（GAN）中，LeakyReLU 可以帮助缓解梯度消失问题，使生成器和判别器能够更稳定地训练，提高生成图像的质量。</p>
<p><strong>优点</strong></p>
<ul>
<li><strong>解决死亡 ReLU 问题</strong>：由于在负数区间保留了一个小的梯度（），即使输入为负数，神经元也不会完全 “死亡”，能够继续参与训练和梯度传播，避免了部分神经元永久失效的情况，提高了模型的稳定性。</li>
<li><strong>计算简单</strong>：和 ReLU 一样，LeakyReLU 的计算相对简单，不会显著增加计算量，保证了训练的高效性。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>超参数选择问题</strong>： 是一个需要手动调整的超参数。如果 选择不当，可能会影响模型的性能。例如， 过大可能会使LeakyReLU退化为线性函数，失去引入非线性的作用； 过小则可能无法有效解决死亡ReLU问题。</li>
</ul>
<p>4.<code>nn.Sigmoid</code> Sigmoid：$\frac{1}{1 + e^{-x}}$</p>
<p><code>nn.Sigmoid</code> 是PyTorch中用于实现Sigmoid激活函数的模块。Sigmoid函数的数学表达式为$\sigma(x)=\frac{1}{1+e^{-x}}$，它能将任意实数输入$x$映射到$(0,1)$区间内。</p>
<p>在二分类任务的输出层使用 Sigmoid 函数，将模型的输出转换为概率值，便于进行分类决策。例如，在判断邮件是否为垃圾邮件的任务中，Sigmoid 函数可以输出邮件为垃圾邮件的概率。</p>
<p>在深度学习发展的早期，Sigmoid 函数被广泛应用。但随着对梯度消失问题的认识和其他激活函数的提出，现在在深层网络中使用 Sigmoid 函数的情况相对较少。</p>
<p><strong>优点</strong></p>
<ul>
<li><strong>输出具有概率解释性</strong>：由于 Sigmoid 函数的输出范围在$(0,1)$之间，因此可以将其输出解释为概率。在二分类问题中，Sigmoid 函数常被用于输出层，将模型的输出转换为属于某一类别的概率。</li>
<li><strong>平滑性</strong>：Sigmoid 函数是连续可导的，其导数可以通过简单的公式计算：$\sigma^{'}(x)=\sigma(x)(1-\sigma(x))$。这种平滑性使得它在使用基于梯度的优化算法（如梯度下降）时非常方便。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>梯度消失问题</strong>：当输入值非常大或非常小时，Sigmoid 函数的导数趋近于 0。在深度神经网络的反向传播过程中，这会导致梯度在传播过程中逐渐消失，使得模型难以学习到有效的特征，尤其是在网络层数较多的情况下，训练会变得非常困难。</li>
<li><strong>输出不以 0 为中心</strong>：Sigmoid 函数的输出始终为正数，这会导致在反向传播过程中，权重更新的方向都偏向同一侧，使得收敛速度变慢。</li>
</ul>
<p>5.<code>nn.Tanh</code> Tanh：$\frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
<p><code>nn.Tanh</code>是PyTorch里实现双曲正切激活函数（Tanh）的模块。Tanh 函数的数学表达式为$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$，它能把任意实数输入$x$映射到$(-1,1)$区间内。</p>
<ul>
<li><strong>循环神经网络（RNN）</strong>：在早期的 RNN 及其变体（如 LSTM、GRU）中，Tanh 函数常被用于隐藏层的激活函数，因为它以 0 为中心的特性有助于缓解梯度消失问题，使模型能够更好地捕捉序列数据中的长期依赖关系。</li>
<li><strong>某些特定的回归问题</strong>：当输出需要在$(-1,1)$区间内时，Tanh 函数可以作为一个合适的选择。例如，在预测某些具有正负范围的数值时，使用 Tanh 函数可以将模型的输出限制在合理的区间内。</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li><strong>以 0 为中心</strong>：与 Sigmoid 函数不同，Tanh 函数的输出范围是$(-1,1)$，关于原点对称。这使得在反向传播过程中，权重更新的方向更加灵活，有助于模型更快地收敛。</li>
<li><strong>平滑可导</strong>：Tanh 函数是连续可导的，其导数为$1-tanh^2(x)$。这种平滑性对于基于梯度的优化算法（如梯度下降）非常重要，便于进行梯度计算和参数更新。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>梯度消失问题</strong>：和 Sigmoid 函数类似，当输入值非常大或非常小时，Tanh 函数的导数趋近于 0。在深度神经网络的反向传播过程中，这会导致梯度在传播过程中逐渐消失，使得模型难以学习到有效的特征，尤其是在网络层数较多的情况下，训练会变得困难。</li>
</ul>
<p>6.<code>nn.GELU</code> GELU：高斯误差线性单元（Transformer 常用）</p>
<p><code>nn.GELU</code> 是 PyTorch 中实现高斯误差线性单元（Gaussian Error Linear Unit，GELU）激活函数的模块。GELU 是一种非线性激活函数，它根据输入的概率来随机将输入置为 0，从而引入了随机性和非线性。其数学表达式有几种近似形式，常见的一种是：</p>
<p>$GELU(x)=x·\phi(x)$</p>
<p>其中$\phi(x)$是标准正态分布的累积分布函数，通常使用以下近似公式计算：</p>
<p>$GELU(x)=0.5x(1+tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3)))$</p>
<p>对于不同的输入值，GELU 函数会根据其自身的计算规则输出相应的结果。当输入为 0 时，输出为 0；对于正数输入，输出为正且会根据输入大小非线性变化；对于负数输入，输出值会小于 0 且同样遵循非线性规律。</p>
<p><strong>应用场景</strong></p>
<ul>
<li><strong>Transformer 架构</strong>：在基于 Transformer 的模型（如 BERT、GPT 系列等）中，GELU 被广泛应用于前馈网络层，是这些模型取得优异性能的关键因素之一。</li>
<li><strong>自然语言处理任务</strong>：由于其能够更好地处理序列数据中的复杂关系，GELU 在各种自然语言处理任务中得到了广泛应用，如情感分析、命名实体识别等。</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li><strong>更好的拟合能力</strong>：GELU 函数比传统的激活函数（如 ReLU、Sigmoid 等）具有更复杂的非线性特性，能够更好地拟合复杂的数据分布，从而提高模型的表达能力。</li>
<li><strong>在 Transformer 中表现出色</strong>：在 Transformer 架构中，GELU 被广泛使用。它有助于模型捕捉序列数据中的复杂模式和依赖关系，提升模型在自然语言处理任务（如机器翻译、文本分类等）中的性能。</li>
<li><strong>平滑性</strong>：GELU 函数是连续可导的，这对于基于梯度的优化算法（如 Adam 优化器）非常友好，能够保证模型训练过程的稳定性。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>计算复杂度相对较高</strong>：与 ReLU 等简单激活函数相比，GELU 的计算涉及到较为复杂的数学运算（如双曲正切函数和多项式运算），因此计算成本相对较高，可能会增加模型的训练时间和计算资源消耗。</li>
</ul>
<blockquote>
<p><mark>❓为什么神经网络分为输入层，隐藏层和输出层</mark></p>
<p>神经网络的设计灵感来源于人类的神经系统。在人类大脑中，感觉器官（如眼睛、耳朵）相当于输入层，负责接收外界的信息；大脑中的神经元网络进行复杂的信息处理和分析，类似于隐藏层的功能；而运动系统或决策系统则相当于输出层，产生相应的行为或决策。因此，将神经网络分为输入层、隐藏层和输出层是对人类神经系统工作方式的一种模拟和简化。</p>
<p>同时，神经网络分为输入层、隐藏层和输出层这种结构符合信息处理流程：信息接收-加工-输出</p>
<p><mark>❓隐藏层为什么要叫隐藏层（也就是为什么某些层需要被“隐藏”）</mark></p>
<p>1.<strong>与外界无直接交互</strong>：“隐藏” 并不是指这些层在物理上不可见，而是它们不像输入层和输出层那样与外界有直接的联系。输入层直接接收外部数据，输出层直接将结果反馈给外界，而隐藏层的状态和输出对于用户来说通常是不可直接观察和干预的。它们主要在网络内部进行特征提取和转换，是网络学习过程中的中间步骤。</p>
<p>2.<strong>自动学习特征</strong>：隐藏层的主要任务是自动学习数据中的特征表示。这些特征是在训练过程中由网络自动发现和提取的，不需要人工预先定义。由于隐藏层学习到的特征是抽象的、针对具体任务的，并且可能难以用人类语言直接描述，所以被视为 “隐藏” 的特征。例如，在图像识别任务中，隐藏层可能学习到一些对于识别物体有帮助的特征，但这些特征可能无法直观地表达出来。</p>
</blockquote>
<blockquote>
<p><mark>❓学习和表示复杂的函数关系最后是怎么反映到实际问题的</mark></p>
<p>以图像分类任务为例进行简要分析。</p>
<p>图像分类是指将输入的图像划分到一个或多个预定义的类别中，例如判断一张图片是猫、狗还是其他动物。这个问题的复杂性在于图像中的物体可能有不同的姿态、角度、光照条件等，需要模型学习到图像特征和类别之间复杂的函数关系。</p>
<p>1.数据收集与预处理</p>
<ul>
<li>收集大量不同类别的图像数据，例如包含猫和狗的图片。</li>
<li>对图像进行预处理，如调整大小、归一化等，将图像数据转换为适合模型输入的格式。</li>
</ul>
<p>2.构建模型</p>
<p>构建一个深度神经网络模型，如卷积神经网络（CNN），它可以看作是在学习输入图像和输出类别之间的复杂函数关系。网络包含多个卷积层、池化层和全连接层，每个层通过不同的操作（如卷积、激活函数等）来提取和转换图像特征。</p>
<p>3.模型训练</p>
<ul>
<li>使用收集到的图像数据和对应的类别标签对模型进行训练。在训练过程中，模型通过不断调整其内部的参数（如卷积核的权重），使得模型的输出尽可能接近真实的类别标签。</li>
<li>为了衡量模型输出和真实标签之间的差异，使用损失函数（如交叉熵损失）。通过反向传播算法，根据损失函数的梯度更新模型的参数，使得损失函数逐渐减小。</li>
</ul>
<p>因此，在训练过程中，模型学习到了<mark>图像中不同特征和类别之间的关系</mark>。例如，CNN 的卷积层可以学习到图像中的边缘、纹理等局部特征，随着网络层数的增加，模型能够组合这些局部特征，形成更高级的语义特征。这些特征表示了图像的本质信息，是模型学习到的复杂函数关系的一部分。</p>
<p>当一个新的图像输入到训练好的模型中时，模型会根据学习到的函数关系对图像进行处理。首先，模型会提取图像的特征，然后根据这些特征计算每个类别的得分。最后，模型会选择得分最高的类别作为预测结果，从而完成图像分类任务。</p>
<p><u>实际效果体现</u></p>
<ul>
<li><strong>准确性</strong>：如果模型学习到的函数关系能够很好地反映图像特征和类别之间的真实关系，那么模型在测试数据上的分类准确率会较高。例如，在一个包含猫和狗的图像分类任务中，模型能够准确地将大部分猫的图片分类为猫，狗的图片分类为狗。</li>
<li><strong>泛化能力</strong>：学习到的函数关系还体现在模型的泛化能力上。即模型不仅能够在训练数据上表现良好，还能够对未见过的图像进行准确分类。例如，当遇到不同姿态、不同光照条件下的猫和狗的图片时，模型仍然能够正确分类。</li>
</ul>
</blockquote>
<p>7.<code>nn.Softmax</code> Softmax：概率归一化</p>
<p><code>nn.Softmax</code> 是 PyTorch 中用于实现 Softmax 函数的模块。Softmax 函数是一种常用的激活函数，主要用于将一个实数向量转换为概率分布，使得向量中的每个元素都在 <code>(0, 1)</code> 区间内，并且所有元素之和为 1。</p>
<p>对于一个输入向量$z=[z_1,z_2,...,z_n]$，Softmax 函数的数学表达式为：$Softmax(z_i)=\frac{e^{z_i}}{ {\textstyle \sum_{j=1}^{n}}e^{z_j} }$</p>
<p><strong>应用场景</strong></p>
<ul>
<li><strong>多分类问题</strong>：在多分类任务中，如手写数字识别、图像分类等，Softmax 函数通常用于输出层，将模型的输出转换为概率分布，从而确定样本所属的类别。</li>
<li><strong>强化学习</strong>：在强化学习中，Softmax 函数可以用于动作选择策略。例如，在基于策略梯度的算法中，使用 Softmax 函数将动作的偏好分数转换为动作选择的概率，使得智能体可以根据概率随机选择动作。</li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li><strong>概率解释性</strong>：Softmax 函数的输出可以直接解释为概率。在多分类问题中，每个类别的输出值可以看作是样本属于该类别的概率，这使得模型的输出具有直观的意义，便于进行分类决策。</li>
<li><strong>可微性</strong>：Softmax 函数是连续可微的，这使得它可以用于基于梯度的优化算法，如随机梯度下降（SGD）、Adam 等。在训练神经网络时，可以通过反向传播算法计算损失函数关于 Softmax 输入的梯度，从而更新模型的参数。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>对输入值敏感</strong>：Softmax 函数使用指数运算，对输入值的差异非常敏感。当输入值之间的差异较大时，经过 Softmax 函数处理后，较大的值会变得非常接近 1，而较小的值会变得非常接近 0，这可能导致梯度消失或梯度爆炸问题。</li>
<li><strong>计算复杂度高</strong>：Softmax 函数的计算涉及到指数运算和求和操作，计算复杂度相对较高。在处理大规模数据时，可能会影响模型的训练和推理速度。</li>
</ul>
<h6 id="池化层">池化层</h6>
<p>1.什么是池化？</p>
<p>池化（Pooling）是深度学习中一种常用的操作，主要用于对输入数据进行下采样，也就是减少数据的维度和参数数量。</p>
<p>池化操作通常在卷积层之后进行，它会在输入数据的局部区域（例如一个小的矩形区域）上进行计算，根据特定规则从该区域中提取一个代表值，以此来替代整个区域的数据。常见的规则有取最大值、平均值等。</p>
<p><mark>常见类型</mark></p>
<ul>
<li><strong>最大池化（Max Pooling）</strong>：在每个局部区域中选取最大值作为该区域的输出。例如，对于一个 2x2 的局部区域，将其中 4 个元素中的最大值提取出来。最大池化能够保留输入数据中的主要特征，因为最大值往往对应着数据中最显著的特征信息，有助于突出重要的边缘、纹理等特征。</li>
<li><strong>平均池化（Average Pooling）</strong>：计算每个局部区域内所有元素的平均值作为输出。它会对局部区域的信息进行平均化处理，能保留更多的背景信息，但相对而言可能会模糊一些突出的特征。</li>
</ul>
<p><mark>作用</mark></p>
<ul>
<li><strong>降低数据维度</strong>：通过减少数据的尺寸，降低后续层的计算量和参数数量，从而加快模型的训练速度，减少过拟合的风险。例如，在处理高分辨率的图像时，池化操作可以显著减小特征图的大小。</li>
<li><strong>增强特征的平移不变性</strong>：即使输入数据在局部区域内发生了一定的平移，池化操作提取的特征仍然保持相对稳定。这使得模型在面对物体位置变化时具有更强的鲁棒性。</li>
<li><strong>提取重要特征</strong>：最大池化可以突出数据中的重要特征，帮助模型聚焦于更关键的信息，提高模型的特征表达能力。</li>
</ul>
<p>2.<code>nn.MaxPool1d/2d/3d</code> 最大池化：取局部最大值</p>
<p><code>nn.MaxPool1d</code>、<code>nn.MaxPool2d</code> 和 <code>nn.MaxPool3d</code> 是 PyTorch 中用于实现不同维度最大池化操作的模块。最大池化的核心思想是在输入数据的局部区域内选取最大值作为该区域的输出，以此来减少数据的维度，同时保留重要的特征信息。</p>
<p><code>nn.MaxPool1d</code></p>
<ul>
<li>
<p><strong>适用场景</strong>：主要用于处理一维数据，如时间序列数据或一维的特征向量。输入数据的形状通常为 <code>(N, C, L)</code>，其中 <code>N</code> 是批量大小，<code>C</code> 是通道数，<code>L</code> 是序列长度。</p>
</li>
<li>
<p><strong>参数</strong></p>
<ul>
<li><code>kernel_size</code>：池化窗口的大小，是一个整数，表示在序列长度方向上的窗口大小。</li>
<li><code>stride</code>：池化窗口的步长，默认为 <code>kernel_size</code>。</li>
<li><code>padding</code>：在输入数据的边界填充 0 的数量，默认为 0。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)  <span class="comment"># 批量大小为 1，通道数为 1，序列长度为 10</span></span><br><span class="line"><span class="comment"># 创建 MaxPool1d 层</span></span><br><span class="line">max_pool_1d = nn.MaxPool1d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 进行最大池化操作</span></span><br><span class="line">output = max_pool_1d(input_data)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>nn.MaxPool2d</code></p>
<ul>
<li>
<p><strong>适用场景</strong>：常用于处理二维数据，如图像数据。输入数据的形状通常为 <code>(N, C, H, W)</code>，其中 <code>N</code> 是批量大小，<code>C</code> 是通道数，<code>H</code> 是高度，<code>W</code> 是宽度。</p>
</li>
<li>
<p><strong>参数：</strong></p>
<ul>
<li><code>kernel_size</code>：池化窗口的大小，可以是一个整数或元组。如果是整数，则表示正方形窗口的边长；如果是元组，则分别表示高度和宽度方向的窗口大小。</li>
<li><code>stride</code>：池化窗口的步长，默认为 <code>kernel_size</code>。</li>
<li><code>padding</code>：在输入数据的边界填充 0 的数量，默认为 0。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)  <span class="comment"># 批量大小为 1，通道数为 1，高度为 4，宽度为 4</span></span><br><span class="line"><span class="comment"># 创建 MaxPool2d 层</span></span><br><span class="line">max_pool_2d = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 进行最大池化操作</span></span><br><span class="line">output = max_pool_2d(input_data)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><code>nn.MaxPool3d</code></p>
<ul>
<li>
<p><strong>适用场景</strong>：适用于处理三维数据，如 3D 医学图像或视频数据。输入数据的形状通常为 <code>(N, C, D, H, W)</code>，其中 <code>N</code> 是批量大小，<code>C</code> 是通道数，<code>D</code> 是深度，<code>H</code> 是高度，<code>W</code> 是宽度。</p>
</li>
<li>
<p><strong>参数：</strong></p>
<ul>
<li><code>kernel_size</code>：池化窗口的大小，可以是一个整数或元组。如果是整数，则表示立方体窗口的边长；如果是元组，则分别表示深度、高度和宽度方向的窗口大小。</li>
<li><code>stride</code>：池化窗口的步长，默认为 <code>kernel_size</code>。</li>
<li><code>padding</code>：在输入数据的边界填充 0 的数量，默认为 0。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>)  <span class="comment"># 批量大小为 1，通道数为 1，深度为 2，高度为 4，宽度为 4</span></span><br><span class="line"><span class="comment"># 创建 MaxPool3d 层</span></span><br><span class="line">max_pool_3d = nn.MaxPool3d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 进行最大池化操作</span></span><br><span class="line">output = max_pool_3d(input_data)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>3.<code>nn.AvgPool1d/2d/3d</code> 平均池化：取局部平均值</p>
<p>原理同上</p>
<p>4.<code>nn.AdaptiveMaxPool1d/2d/3d</code> 自适应池化：动态调整输出尺寸</p>
<p><code>nn.AdaptiveMaxPool1d</code>、<code>nn.AdaptiveMaxPool2d</code> 和 <code>nn.AdaptiveMaxPool3d</code> 是 PyTorch 中用于实现自适应最大池化操作的模块。与普通的最大池化（如 <code>nn.MaxPool1d</code>、<code>nn.MaxPool2d</code>、<code>nn.MaxPool3d</code>）不同，自适应最大池化允许用户直接指定输出的尺寸，而不是像普通池化那样指定池化窗口的大小和步长，它会根据指定的输出尺寸动态地调整池化窗口的大小和步长，从而得到期望的输出。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">1D:</span><br><span class="line">    <span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)  <span class="comment"># 批量大小为 1，通道数为 1，序列长度为 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 AdaptiveMaxPool1d 层，指定输出长度为 5</span></span><br><span class="line">adaptive_max_pool_1d = nn.AdaptiveMaxPool1d(output_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行自适应最大池化操作</span></span><br><span class="line">output = adaptive_max_pool_1d(input_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2D:</span><br><span class="line">    <span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>)  <span class="comment"># 批量大小为 1，通道数为 1，高度为 8，宽度为 8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 AdaptiveMaxPool2d 层，指定输出高度和宽度为 4</span></span><br><span class="line">adaptive_max_pool_2d = nn.AdaptiveMaxPool2d(output_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行自适应最大池化操作</span></span><br><span class="line">output = adaptive_max_pool_2d(input_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">3D:</span><br><span class="line">    <span class="comment"># 定义输入数据</span></span><br><span class="line">input_data = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">8</span>, <span class="number">8</span>)  <span class="comment"># 批量大小为 1，通道数为 1，深度为 4，高度为 8，宽度为 8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 AdaptiveMaxPool3d 层，指定输出深度、高度和宽度为 2</span></span><br><span class="line">adaptive_max_pool_3d = nn.AdaptiveMaxPool3d(output_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行自适应最大池化操作</span></span><br><span class="line">output = adaptive_max_pool_3d(input_data)</span><br></pre></td></tr></table></figure>
<p>5.<code>nn.FractionalMaxPool2d</code> 分数池化：随机分数步长池化</p>
<p><code>nn.FractionalMaxPool2d</code> 是 PyTorch 中用于实现分数最大池化（Fractional Max Pooling）的模块。传统的最大池化（如 <code>nn.MaxPool2d</code>）使用固定的步长和池化窗口大小进行下采样，而分数最大池化引入了随机的分数步长，能够在池化过程中提供更多的随机性和灵活性，有助于提高模型的泛化能力。</p>
<p>分数最大池化的核心思想是通过随机选择分数步长来确定池化窗口的位置，从而实现下采样。在每次前向传播时，池化窗口的位置是随机的，但会保证输出的尺寸满足用户指定的要求。具体来说，它会根据输入的尺寸和期望的输出尺寸，随机生成一系列分数步长，然后根据这些步长移动池化窗口并进行最大池化操作。</p>
<p><strong>优点</strong></p>
<ul>
<li><strong>增强泛化能力</strong>：由于分数最大池化引入了随机的分数步长，使得模型在每次训练时看到的池化结果不同，增加了数据的多样性，有助于模型学习到更鲁棒的特征，从而提高模型的泛化能力。</li>
<li><strong>减少过拟合</strong>：随机池化的过程可以看作是一种数据增强的方式，能够在一定程度上减少模型对训练数据的过拟合。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>计算复杂度较高</strong>：由于需要随机生成分数步长并进行池化操作，分数最大池化的计算复杂度相对较高，可能会增加模型的训练时间。</li>
<li><strong>可解释性较差</strong>：随机池化的过程使得池化窗口的位置不固定，增加了模型的不确定性，导致其可解释性相对较差。</li>
</ul>
<blockquote>
<p><mark>❓分数池化和自适应池化的区别</mark></p>
<ul>
<li>分数池化（<code>nn.FractionalMaxPool2d</code>）</li>
</ul>
<p>引入随机的分数步长来确定池化窗口的位置。在每次前向传播时，会根据输入尺寸和期望的输出尺寸随机生成分数步长，然后依据这些步长移动池化窗口进行最大池化操作。也就是说，每次池化时窗口的位置是随机的，具有一定的不确定性。</p>
<p>适用于对模型泛化能力要求较高、需要<mark>缓解过拟合</mark>问题的任务，如在图像分类、目标检测等任务中，当训练数据有限或者模型容易过拟合时，可以考虑使用分数池化。</p>
<ul>
<li>自适应池化（<code>nn.AdaptiveMaxPool2d</code>）</li>
</ul>
<p>直接根据用户指定的输出尺寸来动态调整池化窗口的大小和步长。它会自动计算出合适的池化窗口参数，以确保输出的特征图尺寸符合要求。整个过程是确定性的，对于相同的输入和指定的输出尺寸，每次得到的结果都是相同的。</p>
<p>适用于需要<mark>统一不同样本特征尺寸</mark>的场景，例如在处理不同分辨率的图像数据时，使用自适应池化可以将不同尺寸的输入图像转换为固定尺寸的特征图，方便后续的处理和模型训练。同时，在一些对计算效率有较高要求的场景中，自适应池化也是一个不错的选择。</p>
</blockquote>
<h6 id="Dropout（丢弃）-层">Dropout（丢弃） 层</h6>
<ol>
<li>什么是Dropout（丢弃）？</li>
</ol>
<p>Dropout 是深度学习中一种常用的正则化技术，主要用于防止神经网络过拟合。</p>
<p><mark>原理</mark></p>
<p>在神经网络训练过程中，神经元之间可能会产生复杂的共适应关系，即某些神经元会依赖其他特定神经元的输出。这可能导致模型对训练数据过度拟合，在新数据上的泛化能力变差。Dropout 通过随机 “丢弃”（暂时忽略）一部分神经元及其连接，打破这种共适应关系，使模型更加鲁棒。</p>
<p><mark>工作机制</mark></p>
<ul>
<li><strong>训练阶段</strong>：在每次训练迭代中，对于每个神经元，以一定的概率 （称为 Dropout 率）将其暂时从网络中丢弃，即该神经元在本次前向传播和反向传播中不参与计算。通常，输入层的 Dropout 率较低（如 0.2），隐藏层的 Dropout 率较高（如 0.5）。</li>
<li><strong>测试阶段</strong>：在测试或推理时，所有神经元都参与计算，但为了平衡训练和测试阶段神经元的输出规模，需要将每个神经元的输出乘以$(1-p)$。不过，在实际实现中，也可以采用倒置 Dropout（Inverted Dropout）技术，在训练时就对保留的神经元输出除以$(1-p)$，这样测试时就无需额外操作。</li>
</ul>
<p><mark>作用</mark></p>
<ul>
<li><strong>减少过拟合</strong>：Dropout 相当于在每次迭代中训练一个不同的子网络，这些子网络共享参数。通过这种方式，模型不会过度依赖于任何一个神经元或一组神经元，从而减少对训练数据的过拟合，提高在新数据上的泛化能力。</li>
<li><strong>集成学习效果</strong>：从某种意义上说，Dropout 可以看作是一种集成学习方法，因为每次训练的子网络都可以看作是一个独立的模型，最终的模型相当于这些子网络的集成。</li>
</ul>
<p><mark>缺点</mark></p>
<ul>
<li><strong>训练时间增加</strong>：由于每次迭代中部分神经元被丢弃，模型需要更多的迭代次数才能收敛，因此会增加训练时间。</li>
<li><strong>超参数选择困难</strong>：Dropout 率 是一个超参数，需要通过实验进行调整。如果$p$设置过大，模型可能欠拟合；如果$p$设置过小，则可能无法有效防止过拟合。</li>
</ul>
<ol start="2">
<li><code>nn.Dropout</code> 标准Dropout： 随机置零神经元</li>
</ol>
<p><code>nn.Dropout</code> 是 PyTorch 中用于实现标准 Dropout 操作的模块。Dropout 是一种在训练神经网络时常用的正则化技术，其核心思想是在每次训练迭代中，以一定的概率 随机 “丢弃”（将输出置为 0）网络中的部分神经元，使得模型不会过度依赖于某些特定的神经元，从而减少过拟合的风险，提高模型的泛化能力。</p>
<p><strong>应用场景</strong></p>
<ul>
<li><strong>全连接层之后</strong>：在多层感知机（MLP）中，通常在全连接层之后添加 Dropout 层，以防止过拟合。</li>
<li><strong>卷积神经网络（CNN）</strong>：在一些 CNN 架构中，也会在全连接层之前或之后使用 Dropout 层。不过，在卷积层中使用 Dropout 的情况相对较少，因为卷积层本身具有一定的平移不变性和稀疏性。</li>
</ul>
<p><strong>优点缺点</strong>同上。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建输入张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Dropout 层，设置丢弃概率为 0.2</span></span><br><span class="line">dropout = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启训练模式</span></span><br><span class="line">dropout.train()</span><br><span class="line">output_train = dropout(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启评估模式</span></span><br><span class="line">dropout.<span class="built_in">eval</span>()</span><br><span class="line">output_eval = dropout(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量:&quot;</span>, input_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式下的输出:&quot;</span>, output_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式下的输出:&quot;</span>, output_eval)</span><br></pre></td></tr></table></figure>
<p><strong>代码解释</strong></p>
<ul>
<li><strong>输入张量</strong>：<code>input_tensor</code> 是一个包含 10 个随机数的一维张量。</li>
<li><strong>Dropout 层实例</strong>：使用 <code>nn.Dropout(p = 0.2)</code> 创建一个 Dropout 层实例，其中 <code>p</code> 表示每个神经元被丢弃的概率，这里设置为 0.2。</li>
<li><strong>训练模式</strong>：调用 <code>dropout.train()</code> 将 Dropout 层设置为训练模式。在训练模式下，Dropout 会按照设定的概率随机将部分神经元的输出置为 0，并对保留的神经元输出进行缩放（采用倒置 Dropout 方式，除以 <code>(1 - p)</code>）。</li>
<li><strong>评估模式</strong>：调用 <code>dropout.eval()</code> 将 Dropout 层设置为评估模式。在评估模式下，Dropout 层不进行任何丢弃操作，直接返回输入张量，因为在测试阶段不需要使用 Dropout 来防止过拟合。</li>
</ul>
<blockquote>
<p>❓为什么平移不变性和稀疏性不容易出现过拟合，进而不需要Dropout</p>
<p>平移不变性是指当输入数据发生平移时，模型的输出结果保持不变。在卷积神经网络（CNN）中，卷积层通过共享卷积核的方式自然地具备了一定程度的平移不变性。例如，在图像识别任务中，无论目标物体在图像中的位置如何移动，卷积核都能检测到相同的特征。</p>
<p><mark>具有平移不变性的模型更关注数据中的本质特征，而不是特征出现的具体位置。这使得模型学到的特征具有更强的泛化能力，能够适应不同位置的相同特征，扩大了训练数据的多样性。</mark></p>
<p><mark>模型在这种多样化的数据上学习，能够更好地捕捉数据的整体分布，而不是过度拟合训练数据中的特定样本。</mark></p>
<p>稀疏性是指数据或模型中的大部分元素为零或接近零。在深度学习中，稀疏性可以体现在数据本身（如稀疏矩阵）或模型的参数（如稀疏连接的神经网络）上。例如，在自然语言处理中，词嵌入向量通常是稀疏的，因为一个句子中只包含少量的词汇。</p>
<p><mark>稀疏性意味着模型只关注数据中的少数重要特征，而忽略了大量的无关信息。稀疏数据中的非零元素往往代表了数据中的重要特征，因此模型能够更准确地捕捉数据的本质规律。这种对重要特征的聚焦使得模型在新数据上也能有较好的表现，减少了过拟合的可能性。</mark></p>
<p>Dropout 主要用于防止模型过拟合，通过随机丢弃部分神经元来减少神经元之间的共适应关系。而具有平移不变性和稀疏性的模型本身已经具备了一定的抗过拟合能力，因此在这些情况下，可能不需要额外使用 Dropout 来防止过拟合。但这并不意味着在所有具有平移不变性和稀疏性的模型中都绝对不需要 Dropout，具体情况还需要根据模型的复杂度、训练数据的规模等因素来综合考虑。</p>
</blockquote>
<ol start="3">
<li><code>nn.Dropout1d/2d/3d</code> 空间Dropout：按通道/空间置零</li>
</ol>
<p>标准的 <code>nn.Dropout</code> 是对输入张量中的每个元素独立地以一定概率进行置零操作。而空间 Dropout 则是<strong>按通道或空间维度进行置零</strong>，即一次会将整个通道或空间区域的元素置零。这样做的好处是可以更好地保留特征之间的相关性，尤其适用于处理具有空间结构的数据，如序列数据（<code>nn.Dropout1d</code>）、图像数据（<code>nn.Dropout2d</code>）和 3D 数据（<code>nn.Dropout3d</code>）。</p>
<h6 id="嵌入层">嵌入层</h6>
<ol>
<li>什么是嵌入？</li>
</ol>
<p>嵌入层（Embedding Layer）是深度学习中一种特殊的神经网络层，主要用于将离散的类别数据（如单词、商品 ID、用户 ID 等）转换为连续的向量表示。</p>
<p>在很多实际问题中，数据是以离散的类别形式存在的，例如在自然语言处理里的单词，每个单词就是一个离散的类别。然而，大多数机器学习和深度学习算法更适合处理连续的数值数据。嵌入层的作用就是搭建起离散类别数据和连续向量空间之间的桥梁，把每个离散类别映射为一个固定长度的向量。</p>
<p><mark>工作原理</mark></p>
<ul>
<li><strong>构建查找表</strong>：嵌入层本质上是一个可学习的查找表（矩阵）。假设一共有$V$个不同的离散类别，每个类别要被映射成维度为$d$的向量，那么这个查找表就是一个形状为$V \times d$的矩阵。矩阵的每一行对应一个离散类别的向量表示。</li>
<li><strong>索引查找</strong>：当输入一个离散类别的索引时，嵌入层会根据这个索引从查找表中取出对应的行向量作为输出。在训练过程中，这个查找表的参数（即每个向量的元素值）会通过反向传播算法不断更新，从而让每个向量能够更好地表示其对应的离散类别。</li>
</ul>
<p><mark>作用</mark></p>
<ul>
<li><strong>捕捉语义信息</strong>：通过学习得到的嵌入向量能够捕捉离散类别之间的语义关系。例如在词嵌入中，意思相近的单词对应的向量在向量空间中会比较接近。像 “苹果” 和 “香蕉” 的向量可能距离较近，因为它们都属于水果类别。</li>
<li><strong>降低维度</strong>：相比于使用独热编码（One - Hot Encoding）来表示离散类别，嵌入向量的维度通常要低得多。独热编码会产生一个非常高维且稀疏的向量，而嵌入向量是低维且密集的，这有助于减少计算量和内存占用，同时也能提高模型的训练效率和泛化能力。</li>
</ul>
<p><mark>应用场景</mark></p>
<ul>
<li><strong>自然语言处理</strong>：在各种自然语言处理任务中，如文本分类、情感分析、机器翻译、命名实体识别等，词嵌入层是必不可少的组件。它能将文本中的单词转换为向量，让模型可以对文本进行有效的处理和分析。</li>
<li><strong>推荐系统</strong>：可以将用户 ID 和商品 ID 分别映射为用户嵌入向量和商品嵌入向量，通过计算这些向量之间的相似度，为用户推荐合适的商品。</li>
<li><strong>知识图谱</strong>：把图谱中的实体和关系映射为向量，有助于进行知识推理、实体分类等任务。</li>
</ul>
<ol start="2">
<li><code>nn.Embedding</code> 词嵌入：nn.Embedding</li>
</ol>
<p><code>nn.Embedding</code> 是 PyTorch 中用于实现词嵌入（Word Embedding）的模块。词嵌入是自然语言处理（NLP）中的一项关键技术，它将离散的单词转换为连续的向量表示，使得模型能够更好地理解和处理文本数据。<code>nn.Embedding</code> 通过一个可学习的查找表将单词的索引映射到对应的向量，这些向量在训练过程中会不断更新，以捕捉单词之间的语义关系。</p>
<p><code>nn.Embedding</code> 本质上是一个矩阵，矩阵的每一行对应一个单词的向量表示。当输入一个单词的索引时，<code>nn.Embedding</code> 会根据该索引从矩阵中取出对应的行向量作为输出。在训练过程中，通过反向传播算法不断调整矩阵中的元素，使得语义相近的单词对应的向量在向量空间中距离较近。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义词汇表大小和嵌入维度</span></span><br><span class="line">vocab_size = <span class="number">10</span>  <span class="comment"># 假设词汇表中有 10 个不同的单词</span></span><br><span class="line">embedding_dim = <span class="number">5</span>  <span class="comment"># 每个单词的嵌入向量维度为 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Embedding 层</span></span><br><span class="line">embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入的单词索引</span></span><br><span class="line">input_indices = torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], dtype=torch.long)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取嵌入向量</span></span><br><span class="line">embedded_vectors = embedding(input_indices)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入的单词索引:&quot;</span>, input_indices)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;嵌入向量的形状:&quot;</span>, embedded_vectors.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;嵌入向量:&quot;</span>, embedded_vectors)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>创建 Embedding 层</strong>：使用 <code>nn.Embedding(num_embeddings = vocab_size, embedding_dim = embedding_dim)</code> 创建一个 <code>Embedding</code> 层，其中 <code>vocab_size</code> 是词汇表的大小，<code>embedding_dim</code> 是每个单词的嵌入向量的维度。</li>
<li><strong>输入单词索引</strong>：<code>input_indices</code> 是一个包含单词索引的张量，每个索引对应词汇表中的一个单词。</li>
<li><strong>获取嵌入向量</strong>：将 <code>input_indices</code> 输入到 <code>embedding</code> 层中，得到对应的嵌入向量。嵌入向量的形状为 <code>(batch_size, embedding_dim)</code>，其中 <code>batch_size</code> 是输入索引的数量。</li>
</ul>
<ol start="3">
<li><code>nn.Embedding</code> 稀疏嵌入：高效处理变长序列</li>
</ol>
<p>稀疏嵌入（Sparse Embedding）是在 <code>nn.Embedding</code> 基础上的一种特殊应用，它针对大规模数据中大部分元素为零（稀疏性）的特点进行优化，以更高效地处理变长序列。在处理变长序列时，每个序列长度不同，且输入中可能存在大量未使用的类别，稀疏嵌入能够有效利用内存并加速计算。</p>
<ul>
<li><strong>稀疏表示</strong>：在大规模的离散数据集中，很多类别出现的频率极低，若使用普通的嵌入层，会导致嵌入矩阵非常大且大部分元素为零。稀疏嵌入通过只存储和更新非零元素，减少内存占用。</li>
<li><strong>变长序列处理</strong>：对于变长序列，稀疏嵌入可以根据序列的实际长度动态地进行计算，避免对固定长度序列的不必要填充和计算。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设有三个变长序列</span></span><br><span class="line">sequences = [</span><br><span class="line">    torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], dtype=torch.long),</span><br><span class="line">    torch.tensor([<span class="number">2</span>, <span class="number">4</span>], dtype=torch.long),</span><br><span class="line">    torch.tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], dtype=torch.long)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">vocab_size = <span class="number">10</span></span><br><span class="line">embedding_dim = <span class="number">5</span></span><br><span class="line"><span class="comment"># 创建稀疏嵌入层</span></span><br><span class="line">sparse_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, sparse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个序列进行嵌入</span></span><br><span class="line">embedded_sequences = []</span><br><span class="line"><span class="keyword">for</span> seq <span class="keyword">in</span> sequences:</span><br><span class="line">    embedded_seq = sparse_embedding(seq)</span><br><span class="line">    embedded_sequences.append(embedded_seq)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个嵌入序列的形状</span></span><br><span class="line"><span class="keyword">for</span> i, emb_seq <span class="keyword">in</span> <span class="built_in">enumerate</span>(embedded_sequences):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;序列 <span class="subst">&#123;i + <span class="number">1</span>&#125;</span> 的嵌入形状: <span class="subst">&#123;emb_seq.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">序列 1 的嵌入形状: torch.Size([3, 5])</span><br><span class="line">序列 2 的嵌入形状: torch.Size([2, 5])</span><br><span class="line">序列 3 的嵌入形状: torch.Size([4, 5])</span><br></pre></td></tr></table></figure>
<h6 id="稀疏层">稀疏层</h6>
<ol>
<li>什么是稀疏？</li>
</ol>
<p><mark>定义</mark></p>
<ul>
<li>从数据角度来说，稀疏指的是<mark>数据集中大部分元素为零或接近零</mark>，只有少数元素是非零的。例如一个大规模的文本数据集进行词袋模型表示时，对于一篇具体的文档，在整个词汇表中只有少数词汇会在该文档中出现，其余大量词汇对应的计数为零，这就是一种数据的稀疏性。</li>
<li>从模型角度来看，稀疏是指模型的参数或连接中大部分为零。比如在神经网络中，稀疏的权重矩阵意味着大部分神经元之间的连接权重为零，只有少量连接具有非零权重。</li>
</ul>
<p><mark>表现形式</mark></p>
<ul>
<li><strong>稀疏向量</strong>：是最常见的表现形式之一。例如在自然语言处理中对文本进行词向量表示时，可能会得到一个维度很高但只有少数几个位置有非零值的向量。</li>
<li><strong>稀疏矩阵</strong>：在推荐系统中，用户 - 物品评分矩阵往往是稀疏的。因为大多数用户只对少量物品进行了评分，而对于大量其他物品的评分是缺失的，在矩阵中就表现为大量的零元素。</li>
</ul>
<p><mark>产生原因</mark></p>
<ul>
<li><strong>数据本身特性</strong>：许多实际数据天然就具有稀疏性。如在医疗数据中，对于某种特定疾病的检测指标，大部分患者可能在多数指标上是正常的，只有少数患者在某些特定指标上会出现异常值，从而导致数据呈现稀疏性。</li>
<li><strong>特征工程</strong>：在对数据进行特征提取和转换时，可能会产生稀疏特征。例如对图像进行某些特征提取算法处理后，得到的特征向量可能只有少数几个维度能够真正反映图像的关键信息，其余维度为零或接近零。</li>
<li><strong>模型设计</strong>：为了实现模型的某些特性，如减少过拟合、提高计算效率等，会设计一些稀疏的模型结构或采用使模型参数稀疏化的方法，如 L1 正则化，它会促使模型的一些参数变为零，从而达到模型稀疏的效果。</li>
</ul>
<ol start="2">
<li><code>nn.Linear（输入为稀疏张量）</code> 稀疏全连接：稀疏矩阵乘法</li>
</ol>
<h6 id="视觉专用层">视觉专用层</h6>
<ol>
<li>
<p><code>nn.PixelShuffle</code> 像素重排：子像素卷积（超分辨率）</p>
</li>
<li>
<p><code>nn.Unfold</code> 像素展开：滑动窗口提取局部块</p>
</li>
<li>
<p><code>nn.Fold</code> 像素折叠：逆操作于 <code>Unfold</code></p>
</li>
</ol>
<h5 id="模型容器">模型容器</h5>
<ol>
<li>
<p>什么是容器？</p>
</li>
<li>
<p><code>nn.Sequential</code> 层字典</p>
</li>
<li>
<p><code>nn.ModuleList</code> 动态层列表</p>
</li>
<li>
<p><code>nn.ModuleDict</code>层字典</p>
</li>
</ol>
<h4 id="优化器和损失函数">优化器和损失函数</h4>
<h5 id="优化器（Optimizers）">优化器（Optimizers）</h5>
<h6 id="什么是优化器？">什么是优化器？</h6>
<h6 id="经典优化器">经典优化器</h6>
<ol>
<li>
<p><code>torch.optim.SGD</code>（含动量）</p>
</li>
<li>
<p><code>torch.optim.Adam</code>, <code>AdamW</code>, <code>RMSprop</code></p>
</li>
</ol>
<h6 id="学习率调度">学习率调度</h6>
<ol>
<li>
<p>什么是学习率调度？</p>
</li>
<li>
<p>相关优化器<code>lr_scheduler.StepLR</code>, <code>CosineAnnealingLR</code>, <code>OneCycleLR</code></p>
</li>
</ol>
<h6 id="梯度裁剪">梯度裁剪</h6>
<ol>
<li>
<p>什么是梯度裁剪？</p>
</li>
<li>
<p><code>nn.utils.clip_grad_norm_</code></p>
</li>
</ol>
<h5 id="损失函数">损失函数</h5>
<h6 id="什么是损失函数？">什么是损失函数？</h6>
<h6 id="分类任务">分类任务</h6>
<p><code>nn.CrossEntropyLoss</code>，<code>nn.BCEWithLogitsLoss</code></p>
<h6 id="回归任务">回归任务</h6>
<p><code>nn.MSELoss</code>, <code>nn.L1Loss</code>, <code>nn.HuberLoss</code></p>
<h6 id="生成任务">生成任务</h6>
<ol>
<li>
<p>生成什么？</p>
</li>
<li>
<p>相关损失函数<code>nn.BCELoss</code>, <code>nn.KLDivLoss</code>, 对抗损失（如 WGAN-GP）</p>
</li>
</ol>
<h4 id="预训练模型（torchvision-models）与迁移学习">预训练模型（torchvision.models）与迁移学习</h4>
<h5 id="计算机视觉模型">计算机视觉模型</h5>
<h6 id="经典-CNN-架构（通过-torchvision-models）">经典 CNN 架构（通过 <code>torchvision.models</code>）</h6>
<blockquote>
<p><mark>❓什么是CNN</mark></p>
</blockquote>
<ol>
<li>
<p>ResNet</p>
</li>
<li>
<p>VGG</p>
</li>
<li>
<p>EfficientNet</p>
</li>
</ol>
<h6 id="Transformer-模型">Transformer 模型</h6>
<ol>
<li>
<p>Vision Transformer （ViT）</p>
</li>
<li>
<p>Swin Transformer</p>
</li>
</ol>
<h5 id="自然语言处理模型">自然语言处理模型</h5>
<h6 id="预训练语言模型（通过-transformers-库）">预训练语言模型（通过 <code>transformers</code> 库）</h6>
<ol>
<li>
<p>BERT</p>
</li>
<li>
<p>GPT</p>
</li>
</ol>
<h5 id="迁移学习策略">迁移学习策略</h5>
<p>什么是迁移学习策略</p>
<h6 id="特征提取（冻结部分层）">特征提取（冻结部分层）</h6>
<h6 id="微调（Fine-tuning）">微调（Fine-tuning）</h6>
<h6 id="使用预训练特征（如-CLIP）">使用预训练特征（如 CLIP）</h6>
<h4 id="数据管道-Data-Pipeline">数据管道 (Data Pipeline)</h4>
<h5 id="数据集类-Dataset">数据集类 <code>Dataset</code></h5>
<p>自定义数据集实现</p>
<h5 id="数据加载器-DataLoader">数据加载器 <code>DataLoader</code></h5>
<ol>
<li>
<p>批量加载</p>
</li>
<li>
<p>多进程加速 (<code>num_workers</code>)</p>
</li>
</ol>
<h5 id="数据增强-torchvision-transforms">数据增强``torchvision.transforms`</h5>
<h4 id="模型部署与性能优化">模型部署与性能优化</h4>
<h5 id="TorchScript-模型导出">TorchScript 模型导出</h5>
<h5 id="ONNX-格式转换">ONNX 格式转换</h5>
<h5 id="混合精度训练-torch-cuda-amp">混合精度训练 (<code>torch.cuda.amp</code>)</h5>
<h2 id="OS操作系统接口模块">OS操作系统接口模块</h2>
<h2 id="Numpy">Numpy</h2>
<p>np.stack</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研知识积累</tag>
      </tags>
  </entry>
  <entry>
    <title>数据分析方法（外教版）</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/</url>
    <content><![CDATA[<p>数据分析方法（外教版）</p>
<h2 id="课件总结">课件总结</h2>
<h3 id="Week-1">Week 1</h3>
<p><code>first_week.pdf</code> - 第一周内容回顾与基础</p>
<h4 id="幻灯片内容翻译与概览">幻灯片内容翻译与概览</h4>
<p><strong>幻灯片 1: Stats 201 - 背景检查 (Yes/No 问题)</strong></p>
<ul>
<li>大家都学过 Stats 210 吗？</li>
<li>我们在 Stats 210 中是否讲授了线性回归？</li>
<li>你了解以下概念吗？
<ul>
<li>估计 (Estimation)</li>
<li>假设检验 (Hypothesis testing)</li>
<li>置信区间 (Confidence interval)</li>
</ul>
</li>
<li>你听说过线性/二项/泊松回归吗？</li>
<li><strong>气泡备注</strong>: 我敢打赌你以为你懂这三个概念。目标是推动你更好地理解它们，而不仅仅是应付考试。它们是任何初级统计/数据科学课程的核心。它们的定义和使用方式塑造了所有早期/传统的从数据中提取信息的方法。</li>
</ul>
<p><strong>幻灯片 2: 数据分析是关于什么的？</strong></p>
<ul>
<li>理解一个变量
<ul>
<li>中心或离散程度</li>
</ul>
</li>
<li>理解两个或多个变量
<ul>
<li>它们之间的关系</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 3: 什么是相关性 (Correlation)？</strong></p>
<ul>
<li>相关性是衡量两个变量<strong>线性</strong>相关的程度。</li>
<li>皮尔逊系数 ρX,Y=σXσYcov(X,Y)=σXσYE[(X−μx)(Y−μy)]</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250602194049048.png" alt="image-20250602194049048"></p>
<p><strong>幻灯片 4: 那么关联性 (Association) 呢？相关性 vs 关联性</strong></p>
<ul>
<li>(图示展示了不同类型的相关性和关联性散点图：强正相关、弱正相关、关联（曲线）、无关联；强负相关、弱负相关、关联（曲线）、无关联)
<ul>
<li><strong>要点</strong>: 相关性主要指线性关系，而关联性是更广泛的概念，可以包括非线性关系。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 5: 那么因果关系 (Causality) 呢？相关性/关联性 vs 因果关系</strong></p>
<ul>
<li>(图示：无关联；关联；相关性；因果关系)</li>
<li>(示例图：炎热干燥的夏天天气可能同时导致冰淇淋销量增加和晒伤人数增加。冰淇淋销量和晒伤人数之间可能存在相关性，但它们之间不一定是直接的因果关系，而是都由天气这个共同因素引起。)
<ul>
<li><strong>要点</strong>: 相关性或关联性不等于因果关系。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 6: 快速回顾</strong></p>
<ul>
<li><strong>第1章</strong>: 介绍，术语，符号 (对应教材 <code>STATS201 book SWU 2023.pdf</code> 第1章)</li>
<li><strong>第2章</strong>: 线性模型 - 基础 (对应教材 <code>STATS201 book SWU 2023.pdf</code> 第2章)</li>
<li><strong>第3章</strong>: 零模型 vs t检验 (对应教材 <code>STATS201 book SWU 2023.pdf</code> 第3章)</li>
</ul>
<p><strong>幻灯片 7: 真伪判断题</strong></p>
<ul>
<li>(题目及解析见后续单独整理的文档)</li>
</ul>
<p><strong>幻灯片 8: 多项选择题</strong></p>
<ul>
<li>(题目及解析见后续单独整理的文档)</li>
</ul>
<p><strong>幻灯片 9: 简答题 (R输出解读)</strong></p>
<ul>
<li>(题目及解析见后续单独整理的文档)</li>
<li><strong>R输出截图</strong>: 展示了一个简单线性回归 <code>summary(lm(y~x))</code> 的结果，包括：
<ul>
<li><code>Call</code>: 模型公式。</li>
<li><code>Residuals</code>: 残差的最小值、第一四分位数、中位数、第三四分位数、最大值。</li>
<li><code>Coefficients</code>:
<ul>
<li><code>(Intercept)</code> (截距) 和 <code>x</code> (斜率) 的估计值 (Estimate)、标准误 (Std. Error)、t值 (t value)、p值 (Pr(&gt;|t|))。</li>
<li>显著性代码 (Signif. codes)。</li>
</ul>
</li>
<li><code>Residual standard error</code>: 残差标准误，自由度。</li>
<li><code>Multiple R-squared</code>: R平方 (决定系数)。</li>
<li><code>Adjusted R-squared</code>: 调整后的R平方。</li>
<li><code>F-statistic</code>: F统计量，自由度，p值 (用于检验整个模型的显著性)。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 10: 什么是置信区间 (Confidence Interval)？</strong></p>
<ul>
<li>
<p>置信区间 (CI) 是对未知参数的一系列估计。</p>
</li>
<li>
<p>参数未知但固定，也就是说，它不是随机的！</p>
</li>
<li>
<p>CI 是在一定的置信水平下根据不确定性计算出来的。</p>
</li>
<li>
<p><strong>理论结果 - 抽样分布</strong>:</p>
<ul>
<li>
<p>假设误差项 ϵi∼iidNormal(0,σ2) (注意幻灯片中 σ 未平方，但标准理论中误差项方差为 σ2)。</p>
</li>
<li>
<p>β^0∼Normal(β0,σ0)，其中 σ0=σ2(n1+∑(xi−xˉ)2xˉ2)。</p>
</li>
<li>
<p>β^1∼Normal(β1,σ1)，其中 σ1=∑(xi−xˉ)2σ2。</p>
</li>
<li>
<p>当 σ 未知时，使用 t 分布：</p>
<ul>
<li>
<p>σ^0β^0−β0∼tn−2</p>
</li>
<li>
<p>σ^1β^1−β1∼tn−2</p>
</li>
<li>
<p>σ^2=n−21∑(yi−y^i)2 (残差方差的无偏估计)。</p>
</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250602194242290.png" alt="image-20250602194242290"></p>
</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 11: 回顾检验与置信区间的二分法</strong></p>
<ul>
<li>(图示：标准正态/t分布图，中间是接受域，两边黄色区域是拒绝域，对应于双边检验。)</li>
<li><strong>气泡备注 (关于“零模型的置信区间是否总比简单线性模型的置信区间窄”的问题)</strong>: 错误。虽然实践中通常如此，但理论上可以构建反例。零模型的标准误（用于估计总体均值 μY 的CI）是 s/n。简单线性模型中对条件均值 E[Y∣x0] 的预测的标准误是 sn1+∑(xi−xˉ)2(x0−xˉ)2。当 x0 远离 xˉ 或 x 的散布程度小时，SLR的CI可能更宽。</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250602194334420.png" alt="image-20250602194334420"></p>
<p><strong>幻灯片 12: 真伪判断题</strong></p>
<ul>
<li>(题目及解析见后续单独整理的文档)</li>
</ul>
<p><strong>幻灯片 13: 不确定性 (Uncertainty)</strong></p>
<ul>
<li>(图示1：相同置信水平，不同估计量。估计量 μ2 的置信区间比 μ1 的宽，表明 μ2 的不确定性更大。)</li>
<li>(图示2：相同估计量，不同置信水平。99% 置信区间比 95% 置信区间宽。)</li>
</ul>
<p><strong>幻灯片 14: 多项选择题</strong></p>
<ul>
<li>(题目及解析见后续单独整理的文档)</li>
</ul>
<h4 id="知识点梳理-first-week-pdf">知识点梳理 (first_week.pdf)</h4>
<p>这份第一周的课件主要是对统计学基础知识进行回顾和铺垫，特别是针对线性回归的核心概念。</p>
<ol>
<li><strong>统计推断三大基石</strong>:
<ul>
<li><strong>估计 (Estimation)</strong>: 对未知总体参数进行估计，包括点估计和区间估计。</li>
<li><strong>假设检验 (Hypothesis Testing)</strong>: 根据样本信息判断对总体的某个假设是否成立。强调了p值的正确定义：在原假设为真的前提下，观察到当前样本或更极端结果的概率。</li>
<li><strong>置信区间 (Confidence Interval)</strong>: 提供参数估计的一个区间范围，并伴随一个置信水平。其频率学派解释是：如果重复抽样多次并构造多个这样的区间，那么大约有（例如）95%的区间会包含真实的参数值。参数本身是固定的，区间是随机的。</li>
</ul>
</li>
<li><strong>变量间关系</strong>:
<ul>
<li><strong>数据分析目标</strong>: 理解单个变量的分布特征（中心、离散度）和多个变量之间的关系。</li>
<li><strong>相关性 (Correlation)</strong>: 特指两个变量间的<strong>线性</strong>关系强度和方向，常用皮尔逊相关系数衡量。</li>
<li><strong>关联性 (Association)</strong>: 比相关性更广泛，包括线性和非线性关系。</li>
<li><strong>因果关系 (Causality)</strong>: 强调相关不等于因果，需要警惕混淆变量和 spurious correlation（伪相关）。</li>
</ul>
</li>
<li><strong>简单线性回归 (SLR) 核心概念</strong>:
<ul>
<li><strong>模型假设</strong>: Yi=β0+β1xi+ϵi，其中误差项 ϵi 独立同分布于 N(0,σ2)。这是理解和正确应用SLR的前提。</li>
<li><strong>R输出解读</strong>:
<ul>
<li><code>summary(lm_object)</code> 提供了系数估计、标准误、t统计量、p值、R平方、F统计量等关键信息。</li>
<li><strong>统计显著性</strong>: 通常通过斜率 β1 的p值判断。若p值小于预设的显著性水平（如0.05），则认为解释变量与响应变量之间存在显著的线性关系。</li>
<li><strong>实际显著性</strong>: 除了统计显著性，还需要考虑效应的大小（如斜率的绝对值）和模型的解释能力（如R平方）在特定应用场景下是否有实际意义。</li>
</ul>
</li>
<li><strong>参数估计的抽样分布与置信区间</strong>:
<ul>
<li>β^0 和 β^1 的抽样分布在理论上是正态的（或当 σ2 用样本估计时为t分布）。</li>
<li>置信区间公式基于这些抽样分布。</li>
</ul>
</li>
</ul>
</li>
<li><strong>置信区间的性质</strong>:
<ul>
<li>置信区间的宽度受置信水平、样本量和数据变异性的影响。</li>
<li>更高的置信水平（如99% vs 95%）会导致更宽的置信区间。</li>
<li>更大的不确定性（如更大的标准误）会导致更宽的置信区间。</li>
</ul>
</li>
</ol>
<p><strong>与R代码和教材的联系</strong>:</p>
<ul>
<li><code>lecture_1.R</code> 中的 <code>t.test()</code>, <code>cor()</code>, <code>lm()</code>, <code>summary()</code>, <code>confint()</code>, <code>predict()</code> 等函数操作都与本周课件回顾的概念直接相关。</li>
<li>教材 <code>STATS201 book SWU 2023.pdf</code> 的第1章（线性回归入门）、第2章（简单线性回归基础）和第3章（零模型与t检验的等价性）是本周回顾内容的详细展开。</li>
</ul>
<p>这部分内容为后续学习更复杂的模型打下了坚实的基础，强调了对基本统计概念的准确理解和对模型假设的重视。</p>
<h3 id="Week-2">Week 2</h3>
<h4 id="幻灯片内容翻译与概览-2">幻灯片内容翻译与概览</h4>
<p><strong>幻灯片 1: (概率与参数估计的思考题 - 续)</strong></p>
<ul>
<li>(牌堆示例图，与 <code>first_week.pdf</code> 幻灯片1类似，继续探讨随机性、固定参数和概率的概念。)
<ul>
<li><strong>文字</strong>: &quot;你可能认为两者都是随机的，相应的概率也可以定义为p值。但回想一下，我们假设一个是未知但固定的。&quot;</li>
<li><strong>牌桌</strong>:
<ul>
<li><strong>双方随机 (Both Random)</strong>: P(T=K)=1/13 (老师抽到K的概率)</li>
<li><strong>双方固定 (Both fixed)</strong>: P(T=K∣T=K)=1 (老师是K，已知老师是K的概率)</li>
<li><strong>一个随机，一个固定 (One random, one fixed)</strong>: P(Same∣T=K,S=Q)=0 (老师是K，学生是Q，牌面相同的概率)</li>
<li><strong>一个随机，一个未知但固定 (How about 1 random and 1 unknown but fixed?)</strong>: 这是统计推断的核心情景，我们观察随机样本，推断固定的总体参数。</li>
</ul>
</li>
<li><strong>&quot;After&quot; vs &quot;Before&quot;</strong>: 暗示了数据观察前后对参数认识的改变（从先验到后验的贝叶斯思想，或频率学派中参数固定但我们通过数据估计它）。</li>
<li><strong>核心问题</strong>: &quot;理解潜在的随机性从何而来。&quot; (通常来自抽样过程和个体变异)。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 2: (置信区间的频率学派解释 - 续)</strong></p>
<ul>
<li>(与 <code>first_week.pdf</code> 幻灯片2类似的图示，多条置信区间围绕着真实的总体均值，有的包含有的不包含。)
<ul>
<li><strong>文字</strong>: &quot;样本均值 (sample mean) -&gt; 置信区间 (Confidence Interval) -&gt; 包含真实均值 (includes true mean) / 不包含真实均值 (misses true mean)&quot;</li>
<li><strong>文字</strong>: &quot;真实的总体均值 (TRUE population mean)&quot;</li>
<li><strong>文字</strong>: &quot;新样本均值 (new sample mean) -&gt; 新置信区间 (new CI)&quot;</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 3: 线性模型 - 在重复抽样下</strong></p>
<ul>
<li>(一个从左到右的红色粗箭头，象征着从数据收集到模型建立再到推断的整个过程，是在“重复抽样”的理论框架下进行的。)
<ul>
<li><strong>文字</strong>: &quot;进行实验 (Conduct experiment) -&gt; 收集数据 (Collect data) -&gt; 找到一些有意义的数字 (Find some meaningful numbers) -&gt; 计算均值 (Compute mean) -&gt; 计算置信区间 (Compute CI)&quot;</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 4: 关键组成部分是什么？</strong></p>
<ul>
<li>(与 <code>first_week.pdf</code> 幻灯片4和 <code>fourth_week.pdf</code> 幻灯片1类似的流程图，再次强调线性模型的构成要素。)
<ul>
<li><strong>数据 (Data)</strong> -&gt; <strong>参数估计 (<strong>β^0,β^1,…</strong>)</strong> -&gt; <strong>置信区间 (Confidence Interval) / 假设检验 (Hypothesis Testing) / 估计 (Estimation)</strong></li>
<li><strong>数据 (Data)</strong> + <strong>模型 (Model)</strong> -&gt; (加入<strong>新数据 (New Data)</strong>) -&gt; <strong>预测 (Prediction) / 预测区间 (Prediction Interval)</strong></li>
<li><strong>估计方法</strong>: <strong>最大似然 (Maximum Likelihood) / 最小二乘法 (Least-squares)</strong></li>
<li><strong>假设 (Assumptions)</strong> (例如：误差独立同正态分布，线性，方差齐性) -&gt; <strong>检查假设的工具和违反假设时的补救措施 (Tools to check assumptions and remedies for assumption violations)</strong></li>
</ul>
</li>
</ul>
<p><strong>幻灯片 5: 什么是分类变量？</strong></p>
<ul>
<li>分类变量是具有两个或多个互斥类别的变量。</li>
<li>例如：
<ul>
<li>一个是/否的二元变量。</li>
<li>水果类型的变量。</li>
<li>年龄组的变量。</li>
</ul>
</li>
<li>通常没有内在顺序，即<strong>名义变量 (nominal)</strong>。
<ul>
<li>名义变量是没有明确顺序的变量。</li>
</ul>
</li>
<li>如果有明确顺序，则称为<strong>有序变量 (ordinal)</strong>。</li>
<li><strong>气泡备注</strong>: 本课程我们只处理名义分类变量。</li>
</ul>
<p><strong>幻灯片 6: 一个分类预测变量 - 模型结构是什么？</strong></p>
<ul>
<li><strong>线性模型</strong>: Yi∼Normal(μi,σ)</li>
<li><strong>对比</strong>:
<ul>
<li><strong>数值预测变量 x</strong>: μi=β0+β1xi
<ul>
<li>x 每变化一个单位，Y的均值变化 β1 个单位。</li>
</ul>
</li>
<li><strong>二元预测变量 z (0/1编码)</strong>: μi=β0∗+β1∗zi
<ul>
<li>这等价于：
<ul>
<li>当 zi=0 时, μi=β0∗ (基准组均值)</li>
<li>当 zi=1 时, μi=β0∗+β1∗ (另一组均值)</li>
</ul>
</li>
<li>β1∗ 代表 z=1 (如&quot;Yes&quot;) 相对于 z=0 (如&quot;No&quot;) 对截距的额外影响，即两组均值之差。</li>
<li><strong>文字</strong>: &quot;当z为Yes（即z=1）而不是No时，会在截距上增加一个额外的分量。&quot;</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 7: 幂律线性模型 - 解释</strong></p>
<ul>
<li>回顾对数线性模型 log(Yi)=β0+β1xi+ϵi, 其中 ϵi∼iidNormal(0,σ2):
<ul>
<li>模型可以写成 μlog(y)=β0+β1x，则 Y 的中位数 median(Y)=eβ0+β1x=eβ0eβ1x。</li>
<li>x 每增加一个单位， Y 的<strong>中位数</strong>乘以 eβ1 (即增加 100×(eβ1−1) )。</li>
</ul>
</li>
<li>回顾对数-对数 (幂律) 模型 log(Yi)=β0∗+β1∗log(xi)+ϵi, 其中 ϵi∼iidNormal(0,σ2):
<ul>
<li>模型可以写成 μlog(y)=β0∗+β1∗log(x)，则 Y 的中位数 \text{median}(Y) = e^{\beta_0^} x^{\beta_1^} = \alpha x^{\beta_1^*}。</li>
<li>如果 x 变为 1.01x (增加1%)，则新的中位数与原中位数的比值为 \frac{\alpha (1.01x)^{\beta_1^}}{\alpha x^{\beta_1^}} = 1.01^{\beta_1^*}。</li>
<li>根据泰勒展开，1.01^{\beta_1^} \approx 1 + \beta_1^ \times 0.01。</li>
<li>所以，x 每增加1%， Y 的<strong>中位数</strong>大约增加 β1∗。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 8: 简单线性模型 - 模型方程、斜率的CI、均值和个体Y的CI/PI (复习)</strong></p>
<ul>
<li>模型: Yi=β0+β1xi+ϵi, xi是数值型, ϵi∼iidNormal(0,σ2)。</li>
<li>R代码: <code>fit = lm(y~x, data=df)</code>, <code>confint(fit)</code>。
<ul>
<li><strong>斜率置信区间解释</strong>: &quot;我们估计 x 每增加一个单位， y 的均值变化在 -0.24 到 0.12 之间。&quot; (示例数据)</li>
</ul>
</li>
<li>预测: <code>new.df = data.frame(x=1)</code>, <code>predict(fit, new.df, interval=&quot;confidence&quot;)</code> 和 <code>predict(fit, new.df, interval=&quot;prediction&quot;)</code>。
<ul>
<li><strong>均值置信区间和个体预测区间解释</strong>: &quot;我们估计当 x=1 时，y 的均值在 -0.35 到 0.17 之间，而一个特定的 y 值在 -1.84 到 1.66 之间。&quot; (示例数据)</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 9: 分类预测变量 (二元) (复习)</strong></p>
<ul>
<li>模型: Yi=β0+β1zi+ϵi, zi是二元变量, ϵi∼iidNormal(0,σ2)。</li>
<li><strong>二元变量系数的CI解释</strong>: &quot;我们估计 z 为1/是 (Yes) 而不是否 (No) 会使得 y 的均值增加 -0.55 到 0.13 个单位。&quot; (示例数据)</li>
<li><strong>均值和个体Y的CI/PI解释</strong>: &quot;我们估计当 z=1 时，y 的均值在 -0.37 到 0.12 之间，而一个特定的 y 值在 -1.86 到 1.61 之间。&quot; (示例数据)</li>
</ul>
<p><strong>幻灯片 10: 二次模型 (复习)</strong></p>
<ul>
<li>模型: Yi=β0+β1xi+β2xi2+ϵi, xi是数值型, ϵi∼iidNormal(0,σ2)。</li>
<li>用途: 通常用于处理由于非线性关系导致的拟合不足。</li>
<li>系数的CI解释不那么直接有意义。</li>
<li>均值和个体Y的CI/PI解释方法保持不变。</li>
<li>R代码: <code>fit = lm(y ~ x + I(x^2), data=df)</code>。</li>
<li><strong>均值和个体预测区间解释</strong>: &quot;我们估计当 x=1 时，y 的均值在 -0.35 到 0.18 之间，而一个特定的 y 值在 -1.84 到 1.67 之间。&quot; (示例数据)</li>
<li><strong>气泡备注</strong>: 你不仅需要单独理解第8至14页（此处指幻灯片编号）的解释。但也需要能将其扩展到混合情况，例如乘法模型中的二次项，或对数线性模型中的分类变量等。</li>
</ul>
<p><strong>幻灯片 11: 乘法对数线性模型 (复习)</strong></p>
<ul>
<li>模型: log(Yi)=β0+β1xi+ϵi, xi是数值型, ϵi∼iidNormal(0,σ2)。</li>
<li><strong>斜率的CI解释 (反转换后)</strong>: &quot;我们估计 x 每增加一个单位， y 的中位数增加 -41.6% 到 36.5%。&quot; (示例数据)</li>
<li><strong>中位数和个体Y的CI/PI解释 (反转换后)</strong>: &quot;我们估计当 x=1 时，y 的中位数在 0.12 到 0.43 之间，而一个特定的 y 值在 0.0036 到 14.45 之间。&quot; (示例数据)</li>
</ul>
<p><strong>幻灯片 12: 乘法对数线性模型 (含分类变量) (复习)</strong></p>
<ul>
<li>模型: log(Yi)=β0+β1zi+ϵi, zi是二元变量, ϵi∼iidNormal(0,σ2)。</li>
<li><strong>系数的CI解释</strong>: &quot;我们估计 z 为1/是 (Yes) 而不是否 (No) 会使得 y 的中位数增加 -67.6% 到 66.6%。&quot; (示例数据)</li>
<li><strong>中位数和个体Y的CI/PI解释</strong>: &quot;我们估计当 z=1 时，y 的中位数在 0.12 到 0.40 之间，而一个特定的 y 值在 0.0035 到 13.86 之间。&quot; (示例数据)</li>
</ul>
<p><strong>幻灯片 13: 幂律模型 (Log-log) (复习)</strong></p>
<ul>
<li>模型: log(Yi)=β0+β1log(xi)+ϵi, xi是正数值型, ϵi∼iidNormal(0,σ2)。</li>
<li><strong>斜率的CI解释</strong>: &quot;我们估计 x 每增加一个百分比， y 的中位数大约增加 -0.20% 到 0.19%。&quot; (示例数据)</li>
<li><strong>中位数和个体Y的CI/PI解释</strong>: &quot;我们估计当 x=1 时，y 的中位数在 0.16 到 0.42 之间，而一个特定的 y 值在 0.0041 到 16.05 之间。&quot; (示例数据)</li>
</ul>
<p><strong>幻灯片 14: Log-log模型的近似解释 (复习)</strong></p>
<ul>
<li>&quot;x 增加1% 导致 y 的中位数大约增加 β1 &quot; 这个解释是基于泰勒展开的近似：1.01β1−1≈β1×0.01。</li>
<li>当变化较大时（如 x 增加50%），不应使用此近似。应计算 1.5β1−1。</li>
</ul>
<p><strong>幻灯片 15: 什么是交互作用 (Interaction)？ (复习)</strong></p>
<ul>
<li>“交互作用”描述了一个变量 z 改变了另外两个变量 y 和 x 之间关系的情况。</li>
<li>(图示1：空气温度和物种对体温的交互作用 - 不同物种体温随空气温度变化的趋势线不平行。)</li>
<li>(图示2：加糖后时间和搅拌对咖啡甜度的交互作用 - 搅拌会加速糖溶解，从而改变甜度随时间变化的曲线。)</li>
</ul>
<p><strong>幻灯片 16: 线性模型 - 二元变量z (交互作用的引入) (复习)</strong></p>
<ul>
<li>假设数值变量 y 和 x 之间的关系取决于二元变量 z。</li>
<li>最简单的方法是使用两个模型 (等价于一个包含交互作用的单一模型): Yi={β0+β1xi+ϵiif zi=0, (β0+β2)+(β1+β3)xi+ϵiif zi=1, 这等价于 Yi=β0+β1xi+β2zi+β3xizi+ϵi。 其中 β2 是截距的差异，β3 是斜率的差异。</li>
</ul>
<h4 id="知识点梳理-second-week-pdf">知识点梳理 (second_week.pdf)</h4>
<p>这份第二周的课件主要是对第一周介绍的各种线性模型形式（简单线性、含分类变量、二次项、对数转换、幂律模型）进行<strong>系统性的复习和总结</strong>，重点在于<strong>模型方程的统一理解</strong>和<strong>参数解释的规范化</strong>，并再次强调了<strong>交互作用</strong>的概念。</p>
<ol>
<li><strong>统计推断的本质与随机性</strong>:
<ul>
<li>通过“抽牌”的例子，引导学生思考统计推断中“未知但固定”的参数与“随机”的样本数据之间的关系，强调理解随机性的来源是正确进行统计推断的基础。</li>
<li>再次通过图形展示置信区间的频率学派含义：多次重复实验，置信区间会围绕真实参数波动，其中一定比例（如95%）的区间会包含该参数。</li>
</ul>
</li>
<li><strong>线性模型的通用框架与关键组件</strong>:
<ul>
<li>重申了线性模型分析的基本流程：从数据出发，通过参数估计（如最大似然、最小二乘）建立模型，然后进行假设检验、置信区间估计、预测等。</li>
<li>强调了模型假设的重要性，以及检查假设和处理违规情况的必要性。</li>
</ul>
</li>
<li><strong>分类预测变量的模型结构</strong>:
<ul>
<li>明确了名义分类变量和有序分类变量的区别，并指出本课程主要关注名义分类变量。</li>
<li>详细解释了当模型中包含一个二元分类预测变量（如0/1编码）时，模型 μi=β0∗+β1∗zi 中，β0∗ 代表基准组的均值，β1∗ 代表另一组相对于基准组的均值差异。</li>
</ul>
</li>
<li><strong>各种线性模型形式的解释范式总结</strong>:
<ul>
<li><strong>简单线性模型 (<strong>Y∼Xnumeric</strong>)</strong>: 解释斜率 β1 对 Y 均值的影响。</li>
<li><strong>含二元分类变量的模型 (<strong>Y∼Zbinary</strong>)</strong>: 解释指示变量系数 β1 对 Y 均值的差异影响。</li>
<li><strong>二次模型 (<strong>Y∼X+X2</strong>)</strong>: 重点在于描述曲线关系和进行预测，单个系数的解释意义不大。</li>
<li><strong>对数线性模型 (<strong>log(Y)∼Xnumeric</strong>)</strong>: 解释 X 的单位变化对 Y <strong>中位数</strong>的<strong>乘性</strong>影响或<strong>百分比</strong>变化。</li>
<li><strong>对数线性模型 (含分类变量</strong> log(Y)∼Zbinary**)<strong>: 解释分类变量不同水平下 Y <strong>中位数</strong>的</strong>比率<strong>或</strong>百分比差异**。</li>
<li><strong>幂律模型 (<strong>log(Y)∼log(Xnumeric)</strong>)</strong>: 解释 X 的<strong>百分比变化</strong>对 Y <strong>中位数</strong>的<strong>百分比变化</strong>的影响（弹性概念）。强调了1%近似解释的局限性，对于较大百分比变化需要精确计算。</li>
</ul>
</li>
<li><strong>交互作用的再强调</strong>:
<ul>
<li>再次定义交互作用：一个变量改变了另外两个变量之间的关系。</li>
<li>通过代数形式解释了包含一个数值变量和一个二元分类变量的交互作用模型，说明它等价于为分类变量的每个水平拟合一条具有不同截距和斜率的直线。</li>
</ul>
</li>
</ol>
<p><strong>与R代码和教材的联系</strong>:</p>
<ul>
<li>本周课件内容是对 <code>lecture_1.R</code> 到 <code>lecture_6.R</code> 中涉及的各种模型解释方法的一个高度概括和对比总结。</li>
<li>它系统性地对应了教材 <code>STATS201 book SWU 2023.pdf</code> 中第1-8章关于不同线性模型形式的解释部分。例如，第1、2章的简单线性回归，第4章的曲线拟合（二次模型），第5章的二元分类变量，第6章的乘法模型（对数线性），第7章的幂律模型，以及第8章的交互作用模型。</li>
</ul>
<p>这份课件的核心价值在于提供了一个统一的框架来理解和解释不同类型的线性模型及其变体，帮助学生巩固之前学习的知识点，并为后续更复杂的模型（如多重回归、GLM）做好准备。它强调了不仅要会拟合模型，更要会正确地解释模型结果。</p>
<h3 id="Week-3">Week 3</h3>
<h4 id="幻灯片内容翻译与概览-3">幻灯片内容翻译与概览</h4>
<p><strong>幻灯片 1: 执行摘要 (Executive Summary) - 示例1 (Exam vs Test &amp; Attendance 交互模型 - 复习)</strong></p>
<ul>
<li>我们想要量化学生的考试分数与出勤率和测试分数之间的关系。</li>
<li>测试分数和考试分数之间存在明显的线性关系，但这种关系在规律出勤和非规律出勤的学生之间有所不同。</li>
<li>我们估计，非规律出勤学生每额外获得一分测试分数（满分20分），其期望考试分数将增加1.8到3.7分。</li>
<li>对于规律出勤的学生，每测试分数对应的期望考试分数额外增加0.04到2.2分。</li>
<li>$^+$由于两组的斜率不同，我们需要单独讨论每个斜率。</li>
</ul>
<p><strong>幻灯片 2: 方法与假设检查 (Method and Assumption Checks) - 示例1 (复习)</strong></p>
<ul>
<li>由于我们有两个解释变量，一个是数值型，一个是因子型，我们拟合了一个线性模型，该模型为每个出勤组使用了不同的截距和斜率（即交互作用模型）。</li>
<li>所有模型假设均得到满足。</li>
<li>我们不能移除交互作用项 (P值 = 0.043)。</li>
<li>我们的最终模型解释了学生考试分数变异性的63%（中等程度），其形式为： Exami=β0+β1×Testi+β2×Attendi+β3×Attendi×Testi+ϵi, 其中 Attendi=1 如果学生 i 规律出勤，否则为0，并且 ϵi∼iidN(0,σ2)，β0,β1,β2,β3 是固定但未知的参数。</li>
</ul>
<p><strong>幻灯片 3: (奥卡姆剃刀与模型复杂度 - 复习)</strong></p>
<ul>
<li>当样本量相对较小时，我们常常面临两个或多个具有相似解释能力模型的情况。</li>
<li>我们更喜欢一个更简单的模型，即具有较少假设/参数的模型。</li>
<li>复杂性就像构建模型的成本。我们知道总能实现完美拟合，即饱和模型，但这将耗费一切！</li>
<li>一个好的模型应该始终平衡拟合优度和复杂性。</li>
<li>当面临具有相似解释能力模型时，最终取决于成本。</li>
<li><strong>核心思想</strong>: 假设、限制和变量数量。数据大小。奥卡姆剃刀支持更简洁（优雅）的解决方案，该方案能持续解释我们现在观察到的现象。<strong>保持简单 (Keep it simple!)</strong></li>
<li>(图示：地心说 vs 日心说，日心说模型更简单且解释力更强。)</li>
</ul>
<p><strong>幻灯片 4: 方法与假设检查 - 示例2 (Thyroid drug experiment - 无交互作用，最终简化模型 - 复习)</strong></p>
<ul>
<li>探索性图表建议，为解释小鼠甲状腺重量，我们首先拟合了包含解释变量药物治疗和体重的线性模型及其交互作用。但交互作用项不显著 (P值=0.11)。</li>
<li>移除了交互作用项后重新拟合模型。然而，治疗效应不显著 (P值=0.37)，因此也被移除。</li>
<li>小鼠被随机分配到各组，测量是相互独立的。</li>
<li>所有其他模型假设均得到满足。一个潜在的影响点被调查过，但考虑到其位置以及该数据集极小的规模（16个观测值），我们将忽略它。</li>
<li>我们的最终模型是 thyroidi=β0+β1×weighti+ϵi，其中 ϵi∼iidN(0,σ2)，β0,β1 是固定但未知的参数。</li>
<li>我们的模型解释了甲状腺重量变异性的大约82%。</li>
</ul>
<p><strong>幻灯片 5: 执行摘要 - 示例2 (复习)</strong></p>
<ul>
<li>我们想要评估一种新开发的旨在增加甲状腺重量（毫克）的药物的效果。</li>
<li>我们唯一能报告的是，体型较大的小鼠拥有较大的甲状腺。我们可以报告这种效应的大小，但由于它不是研究问题的一部分，所以我们不这样做。</li>
<li>我们没有证据相信这种药物能增加小鼠甲状腺的期望大小。</li>
</ul>
<p><strong>幻灯片 6: 什么是 Anova (方差分析)？</strong></p>
<ul>
<li><strong>Analysis of Variance (方差分析)</strong></li>
<li>它基于<strong>分解响应变量的方差</strong>。</li>
<li>它可以用来“<strong>削减 (shave)</strong>”我们的模型，即决定我们是否可以移除一个变量。</li>
</ul>
<p><strong>幻灯片 7: 为什么不（总是）使用T检验？ (在多重回归/ANOVA情境下)</strong></p>
<ul>
<li>(R代码示例：<code>y ~ x * z</code>，其中x是数值，z是二元(0/1)指示变量)
<ul>
<li><code>x = rnorm(100); z = c(rep(0, 50), rep(1, 50))</code></li>
<li><code>y = 0.5 + 0.1*x + rnorm(100, sd = 0.1)</code> (注意这里生成y时没有包含z或x:z的效应，所以理论上z和x:z的系数应为0)</li>
<li><code>df = data.frame(y=y, x=x, z=z)</code></li>
<li><code>summary(lm(y ~ x*z, data=df))</code></li>
</ul>
</li>
<li>(输出 <code>summary()</code> 的 Coefficients 部分，显示了 <code>(Intercept)</code>, <code>x</code>, <code>z</code>, <code>x:z</code> 的t检验结果。在这个模拟数据中，<code>z</code> 和 <code>x:z</code> 的p值可能会随机地显得显著或不显著。)</li>
<li><strong>气泡备注 (指向 <code>x:z</code> 的t检验)</strong>: 在这种情况下 (指交互作用项只有一个参数，如两个二元因子或一个数值一个二元因子的交互)，T检验足够了。</li>
<li><strong>批注 (指向所有系数的t检验)</strong>: 这些T检验中的每一个都在测试，在模型中存在其他系数的情况下，该系数是否为零。</li>
</ul>
<p><strong>幻灯片 8: 一次削减一个beta (模型简化过程 - 基于t检验)</strong></p>
<ul>
<li>(R代码示例：<code>summary(lm(y ~ x+z, data=df))</code>，即从 <code>x*z</code> 模型中移除了不显著的交互作用项 <code>x:z</code> 后的模型)</li>
<li>(输出显示 <code>z</code> 的t检验不显著)</li>
<li><strong>文字</strong>: “下一步是削减掉z。”</li>
<li>(圈出 Multiple R-squared: 0.4819)</li>
<li><strong>文字</strong>: “R平方之前是0.4829” (指包含交互作用时，移除不显著项对R平方影响很小，但调整R平方可能提高，自由度增加)。</li>
</ul>
<p><strong>幻灯片 9: 无论哪种方式，结论相同 (关于移除顺序)</strong></p>
<ul>
<li>(R代码示例：<code>summary(lm(y ~ x + x:z, data=df))</code>，这是一个不寻常的模型形式，包含了 <code>x</code> 和 <code>x:z</code> 但没有 <code>z</code> 的主效应。通常不推荐这样做，除非有强理论依据。标准做法是遵循层次原则，如果包含交互项，则其组成的主效应也应包含在内。)</li>
<li>(输出显示 <code>x:z</code> 的t检验不显著)</li>
<li><strong>文字</strong>: “下一步是削减掉交互作用项。” (这与幻灯片7的结论一致，即交互作用不显著)</li>
</ul>
<p><strong>幻灯片 10: 根据奥卡姆剃刀</strong></p>
<ul>
<li>(R代码示例：<code>summary(lm(y ~ x, data=df))</code>，进一步简化模型，只剩下x，假设之前的步骤已确定z和交互作用均不显著。)</li>
<li><strong>文字</strong>: “这是一个更好的模型。” (前提是简化是合理的)</li>
</ul>
<p><strong>幻灯片 11: 超过两个类别 (多水平因子)</strong></p>
<ul>
<li>(R代码示例：创建一个有3个水平(0,1,2)的因子z，并拟合 <code>lm(y ~ z, data=df)</code>)
<ul>
<li><code>z = c(rep(0, 25), rep(1, 25), rep(2, 50))</code></li>
<li><code>y = 1 + ifelse(z==2, 0.1, 0) + rnorm(100, sd = 0.1)</code> (这里生成y时，z=0和z=1的均值相同，z=2的均值高0.1)</li>
<li><code>df = data.frame(y=y, z=factor(z))</code></li>
<li><code>summary(lm(y~z, data=df))</code></li>
</ul>
</li>
<li>(输出 <code>summary()</code> 显示了 <code>(Intercept)</code> (对应z=0的均值)，<code>z1</code> (z=1与z=0的均值差)，<code>z2</code> (z=2与z=0的均值差) 的t检验结果。)</li>
<li><strong>文字 (指向 <code>z1</code> 的p值 (0.339) 和 <code>z2</code> 的p值 (1.43e-06))</strong>: “对于z是否有用，没有总体结论！” (因为一个比较不显著，一个显著，t检验无法告诉我们z这个因子作为一个整体是否有用。)</li>
<li><strong>文字 (指向右侧)</strong>: “我们想要检验 H0:β~1=β~2=0” (这里 β~1,β~2 指的是与因子z相关的参数，即 <code>z1</code> 和 <code>z2</code> 的系数，暗示需要一个整体检验，即F检验。)</li>
</ul>
<p><strong>幻灯片 12: 如果组间均值的变异性很大，z可能有用</strong></p>
<ul>
<li>(图示：Y对Z的条状图/散点图，显示了三组(z=0,1,2)的均值 yˉ0,yˉ1,yˉ2 和总体均值 yˉ )</li>
<li><strong>组间方差 (Between-group variance)</strong>: VarBetween=k−11∑j=1k(yˉj−yˉ)2 (这里k是组数，幻灯片用3-1)</li>
</ul>
<p><strong>幻灯片 13: 如果组内均值的变异性很小，z可能有用</strong></p>
<ul>
<li>(与幻灯片12类似的图示)</li>
<li><strong>组内方差 (Within-group variance)</strong>: VarWithin=N−k1∑j=1k∑i=1nj(yij−yˉj)2 (N是总样本量, nj是第j组样本量)</li>
<li><strong>F统计量</strong>: F=VarWithinVarBetween (更准确地说是 MSBetween/MSWithin，均方)</li>
</ul>
<p><strong>幻灯片 14: F检验在summary输出中 (针对整个模型) vs anova()</strong></p>
<ul>
<li>(幻灯片11中 <code>summary(lm(y~z, data=df))</code> 的完整输出)</li>
<li><strong>F-statistic</strong>: 16.23 on 2 and 97 DF, p-value: 8.336e-07。这个F检验是针对整个模型（即因子z作为一个整体）是否显著的检验。</li>
<li><strong>文字 (指向p值)</strong>: “检验 H0:β1=β2=0 的p值” (这里 β1,β2 指的是与因子z的非基准水平相关的系数)。</li>
<li><strong>结论</strong>: F检验回答了一个特定的系数集合是否同时为零的问题。它回答了一个模型/分类变量是否有用的问题。</li>
</ul>
<p><strong>幻灯片 15: F对于F值多大才算大？ (F分布)</strong></p>
<ul>
<li>可以证明F比率/F统计量服从<strong>F分布</strong>。</li>
<li>F分布由两个参数 d1 (分子自由度) 和 d2 (分母自由度) 定义。</li>
<li>为了判断因子z是否有用 (对于单因素ANOVA):
<ul>
<li>d1=类别数−1</li>
<li>d2=总数据点数−类别数</li>
</ul>
</li>
<li>有了F统计量的分布，我们就可以正式检验 H0:β1=β2=⋯=βk=0 (k是与因子相关的非基准水平系数个数)。</li>
</ul>
<p><strong>幻灯片 16: 一般的F检验 (使用 <code>anova()</code> 函数)</strong></p>
<ul>
<li><code>anova(lm(y~z, data=df))</code> 输出显示因子 <code>z</code> (作为一个整体) 的F检验结果，与 <code>summary()</code> 中的F检验结果一致（对于单因子模型）。</li>
<li><code>anova(lm(y ~ z*x, data=df))</code> (幻灯片中为 <code>y ~ z+x+z:x</code>，应理解为包含交互作用的模型) 输出<strong>序贯方差分析表 (Type I SS)</strong>。
<ul>
<li>在这种表中，每个项（如 <code>z</code>, <code>x</code>, <code>z:x</code>）的F检验是在调整了模型中<strong>位于它前面</strong>的项之后进行的。因此，变量进入模型的顺序会影响其F检验的P值。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 17: (解释变量的变异性对斜率估计稳定性的影响 - 复习)</strong></p>
<ul>
<li>简单线性模型中，斜率的标准误 SE(β^1)=∑(xi−xˉ)2σ。</li>
<li>如果解释变量 xi 的散布程度 (∑(xi−xˉ)2) 很小（即 xi 值非常集中），则 SE(β^1) 会很大，导致 β^1 的估计非常不稳定。</li>
<li>(图示：左图显示解释变量 xi 变异性小，多次抽样得到的回归线非常不稳定。右图显示解释变量 xi 变异性大，回归线相对稳定。)</li>
</ul>
<p><strong>幻灯片 18: (多重共线性简介 - 复习)</strong></p>
<ul>
<li>一般来说，解释/预测变量的配置可能会导致<strong>多重共线性 (multicollinearity)</strong>，从而影响模型/估计量标准误的稳定性。</li>
<li>当模型中有多个数值解释变量 x 和 z 时，它们之间的相关性会影响模型的稳定性。</li>
</ul>
<h4 id="知识点梳理-third-week-pdf">知识点梳理 (third_week.pdf)</h4>
<p>这份第三周的课件重点回顾和深化了<strong>模型选择</strong>的策略，特别是<strong>奥卡姆剃刀原则</strong>的应用，并详细介绍了<strong>方差分析 (ANOVA)</strong> 和 <strong>F检验</strong>在评估多水平因子变量或一组解释变量整体显著性时的核心作用。同时，它也再次提及并强调了<strong>多重共线性</strong>对模型稳定性的潜在影响。</p>
<ol>
<li><strong>模型选择与奥卡姆剃刀 (Occam's Razor)</strong>:
<ul>
<li><strong>原则</strong>: 在多个具有相似解释能力的模型中，应选择最简单的那个（即参数最少的）。强调了在拟合优度和模型复杂性之间进行权衡。</li>
<li><strong>实践</strong>: 通常通过逐步移除模型中不显著的项（基于t检验或F检验的p值）来简化模型。</li>
<li><strong>示例</strong>: 课件通过甲状腺药物实验的案例（从交互作用模型逐步简化到仅含体重的主效应模型）和模拟的R代码 (<code>y ~ x*z</code> -&gt; <code>y ~ x+z</code> -&gt; <code>y ~ x</code>) 演示了这一过程。</li>
<li>对应教材 <code>STATS201 book SWU 2023.pdf</code> 第9章 (第14-20页) 的模型选择部分。</li>
</ul>
</li>
<li><strong>方差分析 (ANOVA) 与 F检验</strong>:
<ul>
<li><strong>ANOVA的本质</strong>: 将响应变量的总变异分解为由不同解释变量（或因子水平）引起的变异和残差变异。</li>
<li><strong>F检验的用途</strong>:
<ul>
<li><strong>检验多水平因子的整体显著性</strong>: 当一个分类解释变量有三个或更多水平时，<code>summary()</code>输出中对各指示变量的t检验只能说明该水平与基准水平的差异是否显著，而不能判断该因子变量作为一个整体是否对响应变量有显著影响。此时需要F检验（通常由<code>anova()</code>函数或<code>summary()</code>输出中的F-statistic提供）来检验与该因子相关的所有系数是否同时为零。</li>
<li><strong>检验交互作用的整体显著性</strong>: 对于包含多个因子或数值与因子交互作用的模型，F检验可以评估整个交互作用部分的显著性。</li>
<li><strong>比较嵌套模型</strong>: F检验可以用来比较一个复杂模型和一个简化模型（复杂模型的子集），判断移除的项是否显著影响模型的拟合优度。</li>
</ul>
</li>
<li><strong>F统计量的构造</strong>: 其基本思想是比较组间（或模型解释的）变异与组内（或残差）变异。F统计量服从F分布，其形状由分子自由度 (df1) 和分母自由度 (df2) 决定。</li>
<li><strong><code>anova()</code> 函数</strong>: 在R中，<code>anova(lm_object)</code> 会生成一个方差分析表。对于包含多个解释变量的模型，它通常给出的是<strong>序贯F检验 (Type I Sum of Squares)</strong>，这意味着变量进入模型的顺序会影响其F检验的p值。</li>
<li>对应教材 <code>STATS201 book SWU 2023.pdf</code> 第9章 (第18, 23页) 和第11章 (第12页) 关于 <code>anova()</code> 的使用。</li>
</ul>
</li>
<li><strong>解释变量的配置与模型稳定性</strong>:
<ul>
<li>再次强调了简单线性回归中解释变量 x 的散布程度 (∑(xi−xˉ)2) 对斜率估计稳定性的重要性。如果 x 的值域很窄，斜率估计的标准误会很大。</li>
<li>引出了多重共线性的概念：当模型中存在多个解释变量，并且它们之间高度相关时，会导致参数估计不稳定，标准误增大，难以解释单个变量的独立效应。</li>
</ul>
</li>
</ol>
<p><strong>与R代码和教材的联系</strong>:</p>
<ul>
<li><code>lecture_7.R</code> 中的教学方法案例 (<code>teach.df</code>) 完美地演示了从交互作用模型开始，通过 <code>anova()</code> 检验交互作用，然后简化到主效应模型，并使用 <code>relevel()</code> 进行不同基准的比较，这些都与本周课件的核心内容一致。</li>
<li><code>lecture_8.R</code> 中的婴儿出生体重案例，虽然主要目标是多重回归，但也涉及了模型简化和变量选择的思想。</li>
<li>教材 <code>STATS201 book SWU 2023.pdf</code> 第9章（不含交互作用的模型）和第11章（单因素ANOVA）是本周课件的主要理论和实践依据。</li>
</ul>
<p>这份课件通过回顾、实例和理论讲解，帮助学生理解模型选择的逻辑，掌握使用F检验评估因子和交互作用整体效应的方法，并初步认识到解释变量间关系对模型稳定性的影响。</p>
<h3 id="Week-4">Week 4</h3>
<h4 id="幻灯片内容翻译与概览-4">幻灯片内容翻译与概览</h4>
<p><strong>幻灯片 1: 回顾线性模型</strong></p>
<ul>
<li>(与 <code>second_week.pdf</code> 幻灯片4类似的流程图，展示线性模型的各个组成部分和流程。)
<ul>
<li>数据 (Data) → 参数估计 (β^0,β^1,…) → [置信区间 (Confidence Interval) / 假设检验 (Hypothesis Testing) / 估计 (Estimation)]</li>
<li>数据 (Data) + 模型 (Model) → (加入新数据 (New Data)) → [预测 (Prediction) / 预测区间 (Prediction Interval)]</li>
<li>估计方法: 最大似然 (Maximum Likelihood) / 最小二乘法 (Least-squares)</li>
<li>假设 (Assumptions): Yi=β0+β1xi+ϵi, ϵi∼iidN(0,σ2); 线性性 (Linearity); 方差齐性 (EOV / Constant Variance); 正态性 (Normality)。</li>
</ul>
</li>
</ul>
<p><strong>幻灯片 2: 假设检验与置信区间的局限性</strong></p>
<ul>
<li>显著性水平（例如，通常为5%或95%水平）仅适用于<strong>单个</strong>检验或估计，而<strong>不适用于一系列</strong>检验或估计。(多重比较问题)</li>
<li>仅当该估计或检验<strong>不是由数据本身所启发或建议</strong>时，它们才是合适的。(数据窥探/p-hacking问题)</li>
<li><strong>气泡备注</strong>: 回想一下课堂上的两个模拟研究 (可能指第一类错误率的膨胀和数据驱动的假设)。</li>
</ul>
<p><strong>幻灯片 3: 关于二项分布、正态分布、泊松分布的故事</strong></p>
<ul>
<li>棣莫弗关于赌博问题的研究引出了第一个钟形曲线（正态分布的早期形式，作为二项分布的近似）。</li>
<li>伽利略到高斯关于天文观测中误差的研究（误差理论，正态分布）。</li>
<li><strong>二项分布 (Binomial)</strong>: P(Y=y)=(yn)py(1−p)n−y, y∈0,1,...,n (离散)。</li>
<li><strong>正态分布 (Normal)</strong>: fY(y)=2πσ21exp(−2σ2(y−μ)2), y∈(−∞,∞) (连续)。</li>
<li><strong>中心极限定理 (Central Limit Theorem)</strong>: 若 Yi 是来自<strong>任何分布</strong>的独立同分布随机变量，均值为 μ，方差为 σ2，则当 n→∞ 时，它们的和（或均值）∑i=1nYi 近似服从正态分布。</li>
<li><strong>泊松分布 (Poisson)</strong>: y∈0,1,2,3,.... 泊松的二项分布极限定理得到了泊松分布：若 pn 是一系列在 (0, 1] 之间的实数，使得序列 npn 收敛到一个有限的极限 μ，则 limn→∞(yn)pny(1−pn)n−y=y!μyexp(−μ)。</li>
</ul>
<p><strong>幻灯片 4: 如果n或均值很小怎么办？</strong></p>
<ul>
<li>(图1：Normal Vs Poisson (mu=0.5)) 当泊松分布的均值 μ 很小时（如0.5），其分布是高度右偏的，与对称的正态分布差异显著。</li>
<li>(图2：Normal Vs Binomial (n=6, p=0.75)) 当二项分布的试验次数 n 较小，或成功概率 p 接近0或1时，其分布也会偏斜，与正态近似差异较大。</li>
</ul>
<p><strong>幻灯片 5: 在回归模型中的价值 (普通LM处理非正态数据的局限性)</strong></p>
<ul>
<li><strong>左图 (Poisson data)</strong>: 真实数据是泊松分布（随x指数增长的均值）。如果用普通线性模型（如多项式回归，图中红线）去拟合，可能在数据范围内看似可以，但模型形式不正确。蓝线（泊松回归）能更好地捕捉指数趋势。</li>
<li><strong>右图 (Beyond Observed Poisson data)</strong>: 展示了外推的危险。普通线性模型（红线）在外推时可能产生非常不合理（甚至为负）的预测，而基于正确分布假设的泊松回归（蓝线）外推更为稳健。</li>
</ul>
<p><strong>幻灯片 6: 解释不同类型的回归模型 (总结)</strong></p>
<ul>
<li>
<p><strong>线性回归模型 (Linear regression model)</strong>:</p>
<ul>
<li>Yi∼Normal(μi,σ2), μi=β0+β1xi.</li>
<li>解释: x 每增加一个单位，Y 的<strong>均值</strong>增加 β1 个单位。</li>
</ul>
</li>
<li>
<p><strong>对数线性模型 (Log-linear model, Y取对数)</strong>:</p>
<ul>
<li>log(Yi)∼Normal(μi,σ2), μi=β0+β1xi (这里 μi=E[log(Yi)]).</li>
<li>解释: x 每增加一个单位，Y 的<strong>中位数</strong>乘以 exp(β1) (即增加 100×[exp(β1)−1] )。</li>
</ul>
</li>
<li>
<p><strong>幂律模型 (Power Law model, Y和X均取对数)</strong>:</p>
<ul>
<li>
<p>log(Yi)∼Normal(μi,σ2), μi=β0+β1log(xi).$Y_i=\beta_0x_i^{\beta_1}$</p>
</li>
<li>
<p>解释: x 每增加1%，Y 的<strong>中位数</strong>大约增加 β1。</p>
</li>
<li>
<blockquote>
<p><strong>幂律模型 (<strong>log(Y)∼log(X)</strong>)</strong>: 在这个模型中，解释变量 X 也进行了<strong>对数转换</strong>。因此，系数 β1 反映的是当 log(X) 发生一个单位变化时，log(Y) 的期望值会发生 β1 的变化。 然而，在实际解释中，我们通常更关心原始变量 X 的<strong>相对变化（即百分比变化）*<em>所带来的影响。例如，我们想知道当*</em></strong> X <em><strong>*增加1%时，*<em><strong>Y <em><strong>*的中位数会如何变化。 如果*</strong></em> X <em><strong>*增加1%，那么新的*</strong></em> X新=X×(1+0.01)=1.01X</strong></em>*。 此时，*<em><strong>log(X新)−log(X旧)=log(1.01X)−log(X)=log(1.01)</strong></em>*。 根据模型，*<em><strong>log(Y) <em><strong>*的变化量是*</strong></em> Δlog(Y)=β1×Δlog(X)=β1×log(1.01)</strong></em>*。 所以，*<em><strong>log(Y中位数,旧Y中位数,新)=β1log(1.01)=log((1.01)β1)</strong></em>*。 两边取指数得到*</strong></em> Y中位数,旧Y中位数,新=(1.01)β1****。 对于小的百分比变化**** r****（例如**** r=0.01 <em><strong>*代表1%），根据泰勒展开近似，*</strong></em>(1+r)β1≈1+β1r****。 因此，*<em><strong><mark>(1.01)β1≈1+β1×0.01</mark></strong></em>*。****</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p><strong>泊松回归模型 (Poisson regression model)</strong>:</p>
<ul>
<li>Yi∼Poisson(μi), log(μi)=β0+β1xi.</li>
<li>解释: x 每增加一个单位，Y 的<strong>均值</strong>乘以 exp(β1) (即增加 100×[exp(β1)−1] )。</li>
</ul>
</li>
<li>
<p><strong>Logistic回归模型 (Logistic regression model)</strong>:</p>
<ul>
<li>Yi∼Binomial(ni,pi), logit(pi)=β0+β1xi.</li>
<li>解释: (见幻灯片8)</li>
</ul>
</li>
<li>
<p><strong>广义线性模型 (GLM) 通式</strong>:</p>
<ul>
<li>Yi∼某种指数族分布(θi), g(θi)=β0+β1xi (g(⋅) 是连接函数)。</li>
</ul>
</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250602202143367.png" alt="image-20250602202143367"></p>
<p><strong>幻灯片 7: Logit - 几率的对数 (Odds 和 Log-Odds)</strong></p>
<ul>
<li><strong>几率 (Odds)</strong>: o=O(Y=1)=P(Y=0)P(Y=1)=1−pp。</li>
<li><strong>概率与几率转换</strong>: p=o+1o。</li>
<li><strong>对数几率 (Logit)</strong>: η=Logit(p)=log(o)=log(1−pp)。</li>
<li><strong>Logit的反函数 (Logistic函数)</strong>: p=1+exp(η)exp(η)。</li>
<li>概率 p∈[0,1] 对应几率 o∈[0,∞)，对应对数几率 η∈(−∞,∞)。</li>
</ul>
<p><strong>幻灯片 8: 解释包含数值和分类变量的GLM</strong></p>
<ul>
<li>模型形式 (以包含一个数值变量 xi 和一个二元分类变量 zi 为例，可能还有交互作用): g(θi)=β0+β1xi+β2zi(+β3xizi for interaction)</li>
<li><strong>泊松回归解释</strong>:
<ul>
<li>当 z 固定时，x 每增加一个单位，Y 的<strong>均值</strong>增加 100×[exp(β1)−1]。</li>
<li>当 x 固定时，若 z=1 (相比于 z=0)，Y 的<strong>均值</strong>高出 100×[exp(β2)−1]。</li>
<li><strong>交互作用</strong>: 当 z=1 时，x 每增加一个单位，Y 的<strong>均值</strong>增加 100×[exp(β1+β3)−1]。交互作用本身解释复杂，常需结合图形。</li>
</ul>
</li>
<li><strong>Logistic回归解释</strong>:
<ul>
<li>当 z 固定时，x 每增加一个单位，Y=1 的<strong>几率 (Odds)</strong> 增加 100×[exp(β1)−1]。</li>
<li>当 x 固定时，若 z=1 (相比于 z=0)，Y=1 的<strong>几率</strong>高出 100×[exp(β2)−1]。</li>
<li><strong>交互作用</strong>: 当 z=1 时，x 每增加一个单位，Y=1 的<strong>几率</strong>增加 100×[exp(β1+β3)−1]。</li>
</ul>
</li>
<li><strong>注意</strong>: 当分类变量有三个或更多类别时，应使用ANOVA/卡方检验（针对GLM是偏差分析中的似然比检验或Wald检验的推广）来评估其总体显著性，而不是单个指示变量的z检验（或t检验）。</li>
</ul>
<h4 id="知识点梳理-fourth-week-pdf">知识点梳理 (fourth_week.pdf)</h4>
<p>这份第四周的课件是广义线性模型 (GLM) 的一个重要总结和扩展，它回顾了线性模型的局限性，引入了GLM处理非正态响应（特别是计数和比例数据）的必要性，并系统地对比了不同模型（包括普通线性模型及其对数转换形式、泊松回归、Logistic回归）的解释框架。</p>
<ol>
<li><strong>传统线性模型 (LM) 的回顾与局限</strong>:
<ul>
<li>再次强调LM的核心假设（误差正态、独立、同方差，关系线性）。</li>
<li>指出传统假设检验和置信区间的局限性，特别是<strong>多重比较问题</strong>（一系列检验会使第一类错误率膨胀）和<strong>数据窥探</strong>（由数据启发的检验可能导致假阳性）。</li>
</ul>
</li>
<li><strong>常见离散分布及其与正态分布的关系</strong>:
<ul>
<li>回顾了<strong>二项分布</strong>和<strong>泊松分布</strong>的概率质量函数和适用场景。</li>
<li>通过<strong>中心极限定理</strong>和<strong>泊松的二项极限定理</strong>建立了它们与正态分布的联系。</li>
<li>通过图形清晰展示了当参数（二项分布的n，泊松分布的$\mu$）较小时，这些离散分布与正态近似的差异，强调了在这些情况下直接使用基于正态假设的LM是不合适的。</li>
<li>对应教材 <code>STATS201 book SWU 2023.pdf</code> 第13章 (第20-27页) 和第15章 (第4-6页)。</li>
</ul>
</li>
<li><strong>广义线性模型 (GLM) 的引入与优势</strong>:
<ul>
<li><strong>动机</strong>: 当响应变量是计数（Poisson）或比例/二元（Binomial）时，LM的假设不满足。直接对这类数据使用LM或简单地对其进行对数转换都有其局限性（如log(0)问题，预测值超出范围，方差结构不匹配）。</li>
<li><strong>GLM结构</strong>:
<ol>
<li><strong>随机部分</strong>: 响应变量 Yi 服从某个指数族分布（如Poisson, Binomial）。</li>
<li><strong>系统部分</strong>: 解释变量的线性组合 ηi=β0+β1xi1+…。</li>
<li><strong>连接函数 (Link Function)</strong> g(⋅): 连接响应变量的期望值 E[Yi]=μi 与线性预测子 ηi，即 g(μi)=ηi。</li>
</ol>
</li>
<li>对应教材 <code>STATS201 book SWU 2023.pdf</code> 第13章 (第29-31页) 对GLM的介绍。</li>
</ul>
</li>
<li><strong>特定GLM模型及其解释</strong>:
<ul>
<li><strong>泊松回归 (Poisson Regression)</strong>:
<ul>
<li>用于计数数据。</li>
<li>连接函数: 对数连接，log(μi)=ηi。</li>
<li>解释: 解释变量每改变一个单位，响应变量的<strong>均值</strong> μi 乘以 exp(βj)。</li>
<li>对应教材第13章。</li>
</ul>
</li>
<li><strong>Logistic回归 (Logistic Regression)</strong>:
<ul>
<li>用于二元数据 (0/1) 或比例数据。</li>
<li>连接函数: Logit连接，logit(pi)=log(1−pipi)=ηi。</li>
<li>解释: 解释变量每改变一个单位，事件发生的<strong>几率 (Odds)</strong> pi/(1−pi) 乘以 exp(βj)。</li>
<li>对应教材第15章。</li>
</ul>
</li>
<li>课件系统对比了普通LM、对数转换LM、泊松回归和Logistic回归在解释系数时的不同侧重点（均值 vs 中位数 vs 几率）和形式（加性 vs 乘性）。</li>
</ul>
</li>
<li><strong>包含分类变量和交互作用的GLM解释</strong>:
<ul>
<li>当GLM中包含数值和分类解释变量（及其交互作用）时，解释方法与LM中的类似，但需要将效应最终转换回响应变量的原始尺度（均值或几率）的乘性变化或百分比变化。</li>
<li>强调了交互作用使得解释变得复杂，通常需要结合具体水平进行描述或可视化。</li>
<li>提醒对于多水平分类变量，需要进行整体显著性检验（如偏差分析中的卡方检验或F检验）。</li>
</ul>
</li>
</ol>
<p><strong>与R代码和教材的联系</strong>:</p>
<ul>
<li><code>lecture_10.R</code> 和 <code>lecture_11.R</code> 中的代码实例（CRAN数据、地震数据、鲷鱼数据、篮球数据、挑战者号数据、黑线鳕数据）完美地演示了本周课件中介绍的Poisson回归和Logistic回归的应用，包括模型拟合 (<code>glm()</code>)、参数解释、过离散处理 (<code>family=quasipoisson</code> 或 <code>quasibinomial</code>)、预测 (<code>predictGLM()</code>) 以及与传统LM的对比。</li>
<li>教材 <code>STATS201 book SWU 2023.pdf</code> 的第13章（泊松模型）、第14章（泊松模型实例）、第15章（二项模型）和第16章（列联表分析）是本周课件内容的详细理论基础和应用案例。</li>
</ul>
<p>这份课件是整个课程从传统线性模型过渡到更广泛的广义线性模型的关键，它不仅介绍了GLM的理论框架，更重要的是教会学生如何根据数据类型选择合适的模型并正确解释模型结果。</p>
<blockquote>
<p>在对数线性模型 (log(Y)=β0+β1X) 和幂律模型（或称对数-对数模型，log(Y)=β0+β1log(X)) 中，虽然响应变量 Y 都进行了对数转换，使得我们关注的是 Y 的中位数的相对变化（乘性或百分比），但由于解释变量 X 的处理方式不同，导致了系数 β1 解释上的核心差异。</p>
<p><strong>辨析原因</strong>：</p>
<p>关键在于解释变量 X 在模型中是<strong>原始尺度</strong>还是<strong>对数尺度</strong>。</p>
<ol>
<li><strong>对数线性模型 (<strong>log(Y)∼X</strong>)</strong>:
<ul>
<li>模型中 X 是其原始单位。因此，我们考察的是当 X 发生一个<strong>绝对单位</strong>的变化时（例如，年龄增加1岁，温度升高1摄氏度），log(Y) 会发生 β1 的变化。</li>
<li>由于 log(Y新中位数)−log(Y旧中位数)=β1，这意味着 log(Y新中位数/Y旧中位数)=β1，所以 Y新中位数/Y旧中位数=exp(β1)。</li>
<li>因此，Y 的中位数会变为原来的 exp(β1) 倍，或者说变化了 (exp(β1)−1)×100。这里解释的是 X <strong>每单位绝对变化</strong>对 Y 中位数的<strong>乘性或百分比影响</strong>。</li>
</ul>
</li>
<li><strong>幂律模型 (<strong>log(Y)∼log(X)</strong>)</strong>:
<ul>
<li>模型中 X 也被取了对数。因此，我们考察的是当 log(X) 发生一个单位变化时，log(Y) 会发生 β1 的变化。log(X) 变化一个单位，近似对应于 X 本身发生一个较大的乘性变化（乘以 e≈2.718 倍）。</li>
<li>然而，在解释幂律模型时，我们更常关注 X 发生一个小的<strong>相对百分比变化</strong>（例如1%）时的影响。如果 X 增加1%，即 X 变为 1.01X，那么 log(X) 的变化量是 log(1.01X)−log(X)=log(1.01)。</li>
<li>根据模型，log(Y) 的变化量是 β1×Δ(log(X))=β1×log(1.01)。</li>
<li>所以，Y新中位数/Y旧中位数=exp(β1×log(1.01))=(1.01)β1。</li>
<li>对于小的百分比变化（如1%，即0.01），根据泰勒展开近似，(1+r)β1≈1+β1r。所以 (1.01)β1≈1+β1×0.01。</li>
<li>这意味着 Y 的中位数大约变化了 (β1×0.01)×100。这里解释的是 X <strong>每1%的相对变化</strong>对 Y 中位数的<strong>百分比影响</strong>，这个 β1 就是所谓的弹性系数。</li>
</ul>
</li>
</ol>
<p><strong>总结来说</strong>：</p>
<ul>
<li>在对数线性模型中，由于 X 未取对数，我们解释的是 X <strong>绝对变化一个单位</strong>的影响。</li>
<li>在幂律模型中，由于 X 也取了对数，我们通常解释的是 X <strong>相对变化一个百分点</strong>的影响。</li>
</ul>
<p><strong>举例说明</strong>：</p>
<p>假设我们研究广告投入 (Advertising, 单位：万元) 对产品销量 (Sales, 单位：万件) 的影响。</p>
<ol>
<li><strong>如果我们拟合了对数线性模型</strong>: log(Sales)=2.5+0.05×Advertising
<ul>
<li>这里的 β^1=0.05。</li>
<li><strong>解释</strong>: 广告投入每增加1万元，产品销量的中位数预计会乘以 exp(0.05)≈1.0513 倍，即大约增加 (1.0513−1)×100。</li>
</ul>
</li>
<li><strong>如果我们拟合了幂律模型 (对数-对数模型)</strong>: log(Sales)=1.8+0.6×log(Advertising)
<ul>
<li>这里的 β^1=0.6。</li>
<li><strong>解释</strong>: 广告投入每增加1%，产品销量的中位数预计大约会增加 0.6。这意味着销量的广告弹性是0.6。</li>
</ul>
</li>
</ol>
<p>在这个例子中，两种模型中的系数 0.05 和 0.6 的含义是完全不同的，因为它们对应的是解释变量不同类型的变化（绝对单位变化 vs. 百分比变化）。选择哪种模型取决于我们对变量间关系的理论假设以及数据的实际表现。</p>
</blockquote>
<h2 id="题目总结">题目总结</h2>
<h3 id="first-week">first week</h3>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250602194345924.png" alt="image-20250602194345924"></p>
<p><strong>判断题 (幻灯片 7)</strong></p>
<ol>
<li><strong>题目</strong>: 在假设检验中，如果我们拒绝原假设，那么备择假设必定为真。
<ul>
<li><strong>判断</strong>: 错误。</li>
<li><strong>解析</strong>: 统计推断是基于概率的。拒绝原假设意味着我们有足够的证据表明原假设不太可能为真，因此我们选择相信备择假设。然而，这并不排除我们犯了<strong>第一类错误</strong>（Type I error），即错误地拒绝了一个真实的原假设。我们永远不能100%确定备择假设为真，只能说数据更支持备择假设。</li>
</ul>
</li>
<li><strong>题目</strong>: 在假设检验中，通常所说的p值是原假设为真的概率。
<ul>
<li><strong>判断</strong>: 错误。</li>
<li><strong>解析</strong>: 这是一个非常常见的对p值的误解。p值是在假定原假设（H0）为真的条件下，观察到当前样本统计量或更极端统计量的概率。它衡量的是样本数据与原假设之间的一致性程度，而不是原假设本身为真的概率。</li>
</ul>
</li>
<li><strong>题目</strong>: 在假设检验中，通常所说的p值是“如果原假设为真，在重复试验中观察到与我们所见数据一样极端或更极端的数据的概率”。
<ul>
<li><strong>判断</strong>: 正确。</li>
<li><strong>解析</strong>: 这正是p值的准确定义。它描述的是在原假设成立的世界里，我们当前观察到的数据（或比它更不利于原假设的数据）出现的可能性有多大。如果这个可能性很小（即p值很小），我们就倾向于怀疑原假设的真实性。</li>
</ul>
</li>
</ol>
<p><strong>多项选择题 (幻灯片 8)</strong></p>
<p><strong>题目</strong>: 一个简单线性模型的潜在假设是： A. Yi∼Normal(0,σ2) B. Yi=β0+β1xi+ϵi 其中 ϵi∼Normal(0,σ2) C. Yi=β0+β1xi+ϵi 其中 ϵi∼iidNormal(0,σ2) D. Yi=β0+β1xi+ϵi 其中 ϵi∼iidNormal(0,σi2)</p>
<ul>
<li><strong>正确选项</strong>: C</li>
<li><strong>解析</strong>:
<ul>
<li><strong>A 选项</strong>: 错误。该选项描述的是响应变量 Yi 本身服从均值为0的正态分布，这通常不符合实际情况，并且没有体现解释变量 xi 的作用。在线性回归中，我们假设的是误差项 ϵi 服从正态分布。</li>
<li><strong>B 选项</strong>: 错误。虽然正确描述了模型结构和误差项 ϵi 服从均值为0、方差为 σ2 的正态分布，但它遗漏了误差项之间<strong>独立同分布 (iid - independently and identically distributed)</strong> 的重要假设。</li>
<li><strong>C 选项</strong>: <strong>正确</strong>。这个选项完整且准确地表述了简单线性回归模型的核心假设：响应变量 Yi 是解释变量 xi 的线性函数加上一个误差项 ϵi，并且这些误差项是独立同分布的，服从均值为0、方差恒为 σ2 的正态分布。</li>
<li><strong>D 选项</strong>: 错误。σi2 表示误差项的方差随观测值 i 的不同而变化，这描述的是<strong>异方差性 (heteroscedasticity)</strong>。简单线性回归的标准假设是<strong>同方差性 (homoscedasticity)</strong>。</li>
</ul>
</li>
</ul>
<p><strong>简答题 (R输出解读练习) (幻灯片 9)</strong></p>
<p><strong>背景</strong>: 给定以下R输出的摘要信息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max</span><br><span class="line">-0.44158 -0.12497 -0.01459  0.11600  0.60595</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">            Estimate Std. Error t value Pr(&gt;|t|)</span><br><span class="line">(Intercept) 99.99212    0.01343  7445.8   &lt;2e-16 ***</span><br><span class="line">x            0.06885    0.01308     5.263  3.68e-07 ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line"></span><br><span class="line">Residual standard error: 0.1895 on 198 degrees of freedom</span><br><span class="line">Multiple R-squared:  0.1227,    Adjusted R-squared:  0.1183</span><br><span class="line">F-statistic: 27.7 on 1 and 198 DF,  p-value: 3.675e-07</span><br></pre></td></tr></table></figure>
<ol>
<li><strong>题目</strong>: 当 x=0.5 时，给出 y 的预测值。
<ul>
<li><strong>解析</strong>:
<ul>
<li>预测值 y^=β^0+β^1x。</li>
<li>从输出中可知，β^0=99.99212 (截距的Estimate)，β^1=0.06885 (x的Estimate)。</li>
<li>所以，当 x=0.5 时，y^=99.99212+0.06885×0.5=99.99212+0.034425=100.026545。</li>
<li>(课件中手写备注的计算 99.9212×0.06885×0.5=3.44 是错误的，它似乎将截距、斜率和x值相乘了，这不符合线性回归的预测方式。)</li>
</ul>
</li>
</ul>
</li>
<li><strong>题目</strong>: 判断 x 和 y 之间的线性关系是否统计显著。请说明理由。
<ul>
<li><strong>解析</strong>:
<ul>
<li>x 和 y 之间的线性关系是<strong>统计显著的</strong>。</li>
<li>理由：我们关注解释变量 x 的系数（即斜率 β1）是否显著不为零。原假设是 H0:β1=0。从R输出的 <code>Coefficients</code> 部分可以看到，变量 <code>x</code> 对应的p值 (<code>Pr(&gt;|t|)</code>) 是 3.68×10−7 (即 <code>3.68e-07</code>)。这个p值远小于常见的显著性水平（如 α=0.05, 0.01 或 0.001）。因此，我们有非常强的统计证据拒绝原假设，结论是 x 的系数（斜率）显著不为零，表明 x 和 y 之间存在统计上显著的线性关系。</li>
</ul>
</li>
</ul>
</li>
<li><strong>题目</strong>: 判断 x 和 y 之间的线性关系是否实际显著。请说明理由。
<ul>
<li><strong>解析</strong>:
<ul>
<li>判断实际显著性需要结合具体情境和领域知识，仅从统计输出无法直接给出明确的“是”或“否”的答案。</li>
<li>理由：
<ul>
<li><strong>统计显著性</strong>: 如上所述，关系是统计显著的。</li>
<li><strong>效应大小 (Effect Size)</strong>: 斜率的估计值 β^1=0.06885。这意味着 x 每增加一个单位，y 的均值预计增加约0.069个单位。这个效应的大小是否具有实际意义，取决于 x 和 y 的单位以及它们所代表的实际量。例如，如果 y 是以百万美元计的利润，x 是广告投入（单位：万美元），那么 x 每增加一万美元，利润均值增加0.069百万美元（即6.9万美元），这可能被认为是实际显著的。但如果 y 是学生成绩（百分制），x 是学习时间（小时），那么学习时间每增加一小时，成绩均值增加0.069分，这个效应可能就太小了，不具有实际显著性。</li>
<li><strong>模型解释力 (Explanatory Power)</strong>: R2=0.1227 (Multiple R-squared)，这意味着模型中的 x 解释了 y 总变异的约12.27%。这个解释比例相对较低，表明模型中还有大量未被 x 解释的 y 的变异。在某些领域，12%的解释力可能被认为有一定价值，但在其他领域可能被认为不足。</li>
<li><strong>结论</strong>: 要判断实际显著性，需要将这些统计结果（效应大小、模型解释力）放回具体的应用背景中进行评估。课件中的气泡备注也强调了这一点：“它是否足够大取决于上下文...是否足够大是主观的，并取决于单位。所以这里的重点是展示你理解这一点，而不是给出一个具体的数字或‘是/否’的答案。”</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>题目</strong>: 计算斜率的95%置信区间。
<ul>
<li><strong>解析</strong>:
<ul>
<li>斜率 β1 的95%置信区间可以使用公式：β^1±tα/2,n−p×SE(β^1)。</li>
<li>从R输出中：β^1=0.06885，SE(β^1)=0.01308。</li>
<li>自由度 df=n−p=198 (其中 n 是样本量，p 是参数个数，这里 p=2，包括截距和斜率)。</li>
<li>t0.025,198 的临界值。课件中使用了近似值1.97。我们可以用R精确计算：<code>qt(0.975, df=198)</code> 结果约为 1.9719。</li>
<li>使用课件的1.97：
<ul>
<li>下限: 0.06885−1.97×0.01308=0.06885−0.0257676=0.0430824≈0.0431</li>
<li>上限: 0.06885+1.97×0.01308=0.06885+0.0257676=0.0946176≈0.0946</li>
</ul>
</li>
<li>所以，斜率的95%置信区间约为 (0.0431, 0.0946)。由于该区间不包含0，这也进一步证实了斜率是统计显著的。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>判断题 (幻灯片 12)</strong></p>
<ol>
<li><strong>题目</strong>: 在线性回归模型中，斜率的置信区间总是比截距的置信区间宽。
<ul>
<li><strong>判断</strong>: 错误。</li>
<li><strong>解析</strong>: 正如课件提示和之前的解析所述，置信区间的宽度取决于标准误。截距的标准误受 xˉ2 和样本量的影响，而斜率的标准误受 x 的散布程度 ∑(xi−xˉ)2 的影响。两者没有固定的宽度比较关系。例如，如果数据经过中心化处理使得 xˉ=0，截距的标准误会减小，其置信区间可能会比斜率的窄。</li>
</ul>
</li>
<li><strong>题目</strong>: 在一个简单线性回归模型中，均值的置信区间总是比使用零模型计算的置信区间宽。
<ul>
<li><strong>判断</strong>: 错误。</li>
<li><strong>解析</strong>: 此处“均值的置信区间”若指对特定 x0 处条件均值 E[Y∣x0] 的置信区间，其宽度与 x0 离 xˉ 的距离有关。当 x0=xˉ 时，该置信区间的宽度可能比零模型（仅估计总体均值 μY）的置信区间窄，特别是如果SLR的残差标准误 σ^ 小于零模型的残差标准误（通常如此）。如果 x0 远离 xˉ，则SLR的均值置信区间可能更宽。因此，该表述不总是成立。</li>
</ul>
</li>
<li><strong>题目</strong>: 对于两个不同置信水平的置信区间，较高置信水平的那个更宽。
<ul>
<li><strong>判断</strong>: 正确。</li>
<li><strong>解析</strong>: 为了以更高的置信度（如99% vs 95%）包含真实参数，置信区间需要覆盖更广的可能值范围，因此区间会更宽。这是因为较高置信水平对应于t分布或正态分布中更大的临界值。</li>
</ul>
</li>
</ol>
<p><strong>多项选择题 (幻灯片 14)</strong></p>
<p><strong>题目</strong>: 假设 (1.93, 2.43) 是简单线性回归斜率 β1 的95%置信区间。以下哪个陈述是正确的？ A. 我们有95%的把握确信 β1 在1.93和2.43之间。 B. β1 在1.93和2.43之间的概率是95%。 C. β1 在1.93和2.43之间的概率是0.95。 D. 以上所有。 E. 以上皆非。</p>
<ul>
<li><strong>正确选项</strong>: A</li>
<li><strong>解析</strong>:
<ul>
<li><strong>A 选项</strong>: <strong>正确</strong>。这是对置信区间的标准频率学派解释。它表达了我们对这个具体计算出的区间能够包含真实、固定但未知的参数 β1 的信心程度。</li>
<li><strong>B 和 C 选项</strong>: 错误。在频率学派统计中，真实参数 β1 是一个固定的值，它没有概率分布。概率0.95是与构造区间的方法的长期可靠性相关的，而不是与这个特定的、已经计算出来的区间相关的。</li>
</ul>
</li>
</ul>
<h3 id="来自-2024-test-sol-pdf-2024年春季测试题解析">来自 <code>2024_test_sol.pdf</code> (2024年春季测试题解析)</h3>
<p><strong>A部分：判断题 (Question 1)</strong></p>
<p><strong>(a) 题目 (3分)</strong>: 在线性回归中，μi 的置信区间可能不包含 yi，但是 Yi=β0+β1xi+ϵi 的预测区间必须包含 yi，前提是 μi=E[Yi] 且 (xi,yi) 是用于构建线性模型的观测数据点。</p>
<ul>
<li><strong>判断</strong>: 错误 (✓ FALSE)</li>
<li><strong>解析</strong>:
<ul>
<li>μi=E[Yi∣xi] 是给定 xi 时 Y 的条件均值。其置信区间是针对这个<strong>均值</strong>的，而不是针对单个观测值 yi 的。因此，单个观测值 yi 完全可能落在其对应均值的置信区间之外，因为 yi=μi+ϵi，包含了随机误差 ϵi。</li>
<li>预测区间是针对一个新的、未观测到的 Y 值的。即使 (xi,yi) 是用于建模的观测数据点，我们为这个 xi 构造的预测区间是针对“如果再来一个具有相同 xi 的新观测 Ynew”的区间。这个区间同样因为包含未来观测的随机性，不保证必须包含原始的 yi。任何一个具体的 yi 都可能因为其特定的误差项 ϵi 而落在为其对应 xi 构建的预测区间之外。预测区间描述的是新观测值可能落入的范围，具有一定的概率（如95%），而不是对已观测数据点的必然包含。</li>
</ul>
</li>
</ul>
<p><strong>(b) 题目 (3分)</strong>: 以下R输出与给定的R命令一致。 ```R &gt; predict(LM.fit, new.pred.df, interval = &quot;predict&quot;) fit      lwr      upr 1 15.19726 12.87092 17.52360</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; predict(LM.fit, new.pred.df, interval = &quot;confidence&quot;)</span><br><span class="line">fit      lwr      upr</span><br><span class="line">1 15.19726  9.67879 20.71573</span><br><span class="line">```</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>判断</strong>: 错误 (✓ FALSE)</li>
<li><strong>解析</strong>:
<ul>
<li>对于相同的拟合值 (fit = 15.19726)，<strong>预测区间 (prediction interval)</strong> 总是比<strong>置信区间 (confidence interval)</strong> 更宽。这是因为预测区间不仅要考虑对均值估计的不确定性，还要考虑单个观测值围绕其均值的随机波动。</li>
<li>在给出的输出中，第一个命令是获取预测区间，其区间为 (12.87092, 17.52360)，宽度为 17.52360−12.87092=4.65268。</li>
<li>第二个命令是获取置信区间，其区间为 (9.67879, 20.71573)，宽度为 20.71573−9.67879=11.03694。</li>
<li>这里，预测区间的宽度 (4.65268) 小于置信区间的宽度 (11.03694)，这与理论相悖。因此，该R输出与给定的R命令不一致。</li>
</ul>
</li>
</ul>
<p><strong>(c) 题目 (3分)</strong>: 在线性回归中，假设 β1 是斜率，β^1 是对应的最大似然估计 (MLE)，那么即使简单线性回归模型的等方差假设不满足，E[β^1]=β1 仍然成立，前提是所有其他常规假设都满足。</p>
<ul>
<li><strong>判断</strong>: 正确 (✓ TRUE)</li>
<li><strong>解析</strong>:
<ul>
<li>在线性回归模型中，最小二乘估计量 (OLS estimator) β^1（在高斯误差假设下也等同于最大似然估计量 MLE）的无偏性 (E[β^1]=β1) 依赖于以下核心假设：
<ol>
<li>模型线性于参数。</li>
<li>解释变量 X 是固定的（非随机的），或者与误差项 ϵ 无关 (E[Xϵ]=0)。</li>
<li>误差项的期望为零 (E[ϵi]=0)。</li>
</ol>
</li>
<li><strong>等方差假设 (homoscedasticity)</strong>，即 Var(ϵi)=σ2 对所有 i 都成立，主要影响的是 β^1 的方差表达式以及基于此方差的推断（如标准误、t检验、置信区间）的有效性和最优性（根据高斯-马尔可夫定理，OLS在同方差下是最佳线性无偏估计BLUE）。</li>
<li>如果仅仅是等方差假设不满足（即存在异方差），而其他假设（特别是 E[ϵi∣X]=0）仍然成立，那么 β^1 仍然是<strong>无偏的</strong> (E[β^1]=β1) 和一致的。只是它不再是BLUE，其标准误的常规计算公式也不再准确。</li>
</ul>
</li>
</ul>
<p><strong>B部分：多项选择题 (Question 2)</strong></p>
<p><strong>(a) 题目 (4分)</strong>: 考虑一个对数-线性回归模型 log(yi)=β0+β1xi+ϵi，其中 ϵi∼Normal(0,σ2)，并且 β^1=1。假设 x 的值增加一个单位，那么以下哪个陈述是正确的？ *</p>
<p>☐ Yi 的期望对数值不会改变。 (The log of expected Yi would not change.)</p>
<p>☐ Yi 的期望对数值会增加1个单位。 (The log of expected Yi would increase by 1 unit.) <mark>等价于log(EYi)但是要正确是得expect of log(Yi)即E(log(Yi))</mark></p>
<p>☐ Yi 的期望对数值会增加2.7个单位。 (The log of expected Yi would increase by 2.7 unit.)</p>
<p>☐ Yi 的期望值会增加2.7个单位。 (The expected Yi would increase by 2.7 unit.)</p>
<p>☐ Yi 的期望值会增加170%。 (The expected Yi would increase by 170%.)</p>
<p>✓ 以上皆非。 (None of the above.)</p>
<ul>
<li><strong>正确选项</strong>: ✓ 以上皆非。</li>
<li><strong>解析</strong>:
<ul>
<li>模型是 log(Yi)=β0+β1xi+ϵi。由于 ϵi∼Normal(0,σ2)，所以 log(Yi) 服从正态分布，其期望为 E[log(Yi)]=β0+β1xi。</li>
<li>当 xi 增加一个单位时，E[log(Yi)] 会增加 β1 个单位。题目中给出 β^1=1，所以 E[log(Yi)] 的估计值会增加1个单位。
<ul>
<li>选项“Yi 的期望对数值不会改变”是错误的。</li>
<li>选项“Yi 的期望对数值会增加1个单位”是正确的陈述，但需要注意这里说的是 E[log(Yi)] 而不是 log(E[Yi])。</li>
</ul>
</li>
<li>对于原始尺度的 Yi，由于对数正态分布的性质，E[Yi]=E[eβ0+β1xi+ϵi]=eβ0+β1xiE[eϵi]=eβ0+β1xieσ2/2。</li>
<li>当 xi 增加一个单位到 xi+1 时，新的期望值 E[Ynew]=eβ0+β1(xi+1)eσ2/2。</li>
<li>E[Ynew]/E[Yold]=eβ1。</li>
<li>如果 β^1=1，则 E[Yi] 的估计值会乘以 e1≈2.718。</li>
<li>这意味着 E[Yi] 的估计值会增加 (e1−1)×100。</li>
<li>选项“Yi 的期望值会增加2.7个单位”是错误的，因为是乘以约2.718，而不是增加2.7个单位（除非原期望为1）。</li>
<li>选项“Yi 的期望值会增加170%”接近正确答案171.8%，但题目通常要求精确。</li>
<li><strong>关键点</strong>: 题目问的是“Yi 的期望对数值” (E[log(Yi)]) 还是“对数期望 Yi” (log(E[Yi]))。对于对数线性模型，我们通常解释的是 Yi 的<strong>中位数</strong>的变化，即中位数乘以 eβ1。如果假设 log(Yi) 的中位数等于其均值（因为正态分布对称），那么 Yi 的中位数会乘以 eβ1。</li>
<li>如果将“Yi 的期望对数值”理解为 E[log(Yi)]，那么它会增加 β1=1 个单位。</li>
<li>如果题目严格要求对 E[Yi] 的影响，那么是增加约171.8%。</li>
<li>由于选项中没有一个完全精确匹配 E[log(Yi)] 增加1单位，或者 E[Yi] 增加171.8%，或者中位数乘以 e1，所以选择“以上皆非”是最稳妥的。如果选项中有“Yi 的中位数会乘以约2.718倍”或“Yi 的中位数会增加约171.8%”，那将是更好的答案。</li>
<li><strong>进一步辨析</strong>：在课程中，对于 log(Y)=β0+β1X+ϵ 模型，通常解释为 X 每增加一个单位，Y 的<strong>中位数</strong>乘以 eβ1。因为如果 log(Y) 服从正态分布，那么 Y 服从对数正态分布，其<strong>中位数</strong>是 eE[log(Y)]=eβ0+β1X。当 X 增加1时，中位数变为 eβ0+β1(X+1)=(eβ0+β1X)×eβ1。所以中位数乘以 e1≈2.718。这意味着中位数增加了约171.8%。</li>
<li><strong>结论</strong>：选项中没有一个准确描述中位数的变化，也没有准确描述均值的变化。因此“以上皆非”是最佳答案。<mark>至少出现median才好说这东西有可能正确</mark></li>
</ul>
</li>
</ul>
<p><strong>C部分：简答题 (Question 3)</strong></p>
<p><strong>背景</strong>: 一个简单线性回归模型在R中拟合，输出摘要如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; summary(sim.fit) # 假设 sim.fit 是 lm(y ~ x) 的结果</span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x)</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max</span><br><span class="line">-5.2822 -2.2645 -0.6545  1.7332  8.5444</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">            Estimate Std. Error t value Pr(&gt;|t|)</span><br><span class="line">(Intercept)  8.72815    0.81354   10.73  2.15e-15 ***</span><br><span class="line">x            1.96983    0.07611   25.88   &lt;2e-16 ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line"></span><br><span class="line">Residual standard error: 3.07 on 58 degrees of freedom</span><br><span class="line">Multiple R-squared:  0.9203,    Adjusted R-squared:  0.9189</span><br><span class="line">F-statistic: 669.9 on 1 and 58 DF,  p-value: &lt; 2.2e-16</span><br></pre></td></tr></table></figure>
<p><strong>(a) 题目 (1分)</strong>: 数据框中有多少个数据点？</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li>根据残差标准误 (Residual standard error) 那一行，自由度 (degrees of freedom) 是58。</li>
<li>对于简单线性回归模型 Y=β0+β1X+ϵ，有两个参数被估计 (β0 和 β1)。</li>
<li>残差自由度 dfresidual=n−p，其中 n 是数据点数量，p 是模型中参数的数量（包括截距）。</li>
<li>所以，58=n−2。</li>
<li>因此，n=58+2=60。</li>
<li><strong>答案</strong>: 数据框中有60个数据点。</li>
</ul>
</li>
</ul>
<p><strong>(b) 题目 (4分)</strong>: 计算原假设 H0:β1=2 的t统计量。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li>t统计量的计算公式为: T=SE(β^1)β^1−β1,0，其中 β1,0 是原假设中 β1 的值。</li>
<li>从R输出中可知：
<ul>
<li>斜率的估计值 β^1=1.96983 (x的Estimate)。</li>
<li>斜率的标准误 SE(β^1)=0.07611 (x的Std. Error)。</li>
<li>原假设中 β1,0=2。</li>
</ul>
</li>
<li>计算t统计量： T=0.076111.96983−2=0.07611−0.03017≈−0.3963999</li>
<li><strong>答案</strong>: t统计量约为 -0.396。</li>
</ul>
</li>
</ul>
<p><strong>(c) 题目 (4分)</strong>: 在5%的显著性水平下，是否有证据反对 H0:β1=2？请说明理由。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li><strong>判断</strong>: 没有足够的证据反对 H0:β1=2。</li>
<li><strong>理由1 (基于t统计量)</strong>:
<ul>
<li>我们计算得到的t统计量 T≈−0.396。</li>
<li>自由度为 df=58。</li>
<li>对于双边检验，在5%的显著性水平下，我们需要比较 ∣T∣ 与 t0.025,58 的临界值。</li>
<li><code>qt(0.975, 58)</code> 约等于 2.0017。</li>
<li>因为 ∣−0.396∣=0.396&lt;2.0017，所以t统计量的值没有落在拒绝域内。</li>
<li>因此，我们不拒绝原假设 H0:β1=2。</li>
</ul>
</li>
<li><strong>理由2 (基于置信区间，如题目解析中所示)</strong>:
<ul>
<li>首先计算 β1 的95%置信区间：β^1±t0.025,58×SE(β^1)。</li>
<li>1.96983±2.0017×0.07611=1.96983±0.15234</li>
<li>置信区间为 (1.96983−0.15234,1.96983+0.15234)=(1.81749,2.12217)。</li>
<li>(题目解析中使用了近似的1.96作为t临界值，得到区间 (1.820654, 2.119006))。</li>
<li>由于假设值 β1=2 包含在这个95%置信区间内（例如，2 在 (1.82, 2.12) 之间），我们没有足够的证据在5%的显著性水平下拒绝原假设 H0:β1=2。</li>
</ul>
</li>
<li><strong>答案</strong>: 没有足够的证据在5%的水平上反对 H0:β1=2，因为计算得到的t统计量绝对值远小于相应的t分布临界值，或者因为值2落在了斜率的95%置信区间内。</li>
</ul>
</li>
</ul>
<p><strong>Question 4 (8分) - 药物滥用治疗效果研究</strong></p>
<p><strong>背景</strong>: 一项随机试验研究了对药物使用者进行住宿治疗的效果。大量受试者被随机分配接受3个月或6个月的治疗。其中，364人复吸。对于复吸的个体，研究者有兴趣估计每种治疗结束后到复吸的典型天数，以及两种治疗在复吸时间上的差异。数据框 <code>drug.df</code> 包含以下变量：</p>
<ul>
<li><code>treat</code>: 治疗分配 (Short 或 Long，分别对应3个月和6个月的治疗)</li>
<li><code>time</code>: 治疗后到复吸的时间（天）</li>
</ul>
<p><strong>输出信息</strong>:</p>
<ul>
<li>
<p><code>drug.df$treat</code> 被转换为因子，水平为 &quot;Short&quot;, &quot;Long&quot;。</p>
</li>
<li>
<p>原始 <code>time</code> 和 <code>log(time)</code> 按 <code>treat</code> 分组的箱线图和描述性统计。</p>
<ul>
<li>原始 <code>time</code> 数据右偏明显。</li>
<li><code>log(time)</code> 数据看起来更对称，更接近正态。</li>
</ul>
</li>
<li>
<p>对 <code>log(time)</code> 拟合线性模型 <code>drug.loglm = lm(log(time) ~ treat, data=drug.df)</code> 的系数摘要：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">            Estimate Std. Error t value  Pr(&gt;|t|)</span><br><span class="line">(Intercept)   4.6823     0.06769   69.170 9.587e-211 ***</span><br><span class="line">treatLong     0.2936     0.09763    3.007  2.819e-03 **</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>反转换后的置信区间 <code>exp(confint(drug.loglm))</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">               2.5%   97.5%</span><br><span class="line">(Intercept)  94.559 123.404  (这是Short组复吸时间中位数的CI)</span><br><span class="line">treatLong     1.107   1.625  (这是Long组相对于Short组复吸时间中位数的比率的CI)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>(a) 题目 (1分)</strong>: 为什么在分析前对复吸时间进行对数转换是合理的？</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li>从提供的箱线图和描述性统计可以看出，原始的复吸时间 (<code>time</code>) 数据是<strong>高度右偏的</strong> (例如，Short组均值162远大于中位数105)。对于右偏数据，中位数通常是比均值更好的典型值度量。</li>
<li>对数转换后 (<code>log(time)</code>) 的数据显示出更好的对称性，更接近正态分布的假设，这使得线性模型的假设（如误差正态性、方差齐性）更容易得到满足。</li>
<li><strong>答案</strong>: 因为原始的复吸时间数据表现出明显的右偏性，对数转换有助于使数据分布更对称，更接近线性模型所要求的正态性假设，并且在这种情况下，模型结果（反转换后）通常解释为中位数效应，这对于偏态数据更为稳健和有意义。</li>
</ul>
</li>
</ul>
<p><strong>(b) 题目 (4分)</strong>: 写两句话（如同写执行摘要一样），一句量化短期和长期治疗在复吸天数上的差异，另一句量化短期治疗的复吸天数。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li><strong>第一句 (差异)</strong>: <code>treatLong</code> 的系数估计是0.2936，其指数 e0.2936≈1.341。置信区间为 (1.107, 1.625)。这意味着长期治疗组的复吸时间中位数是短期治疗组中位数的1.107到1.625倍。换算成百分比增加是 (1.107−1)×100 到 (1.625−1)×100。
<ul>
<li><strong>答案句1</strong>: 我们估计，接受长期（6个月）治疗的个体，其复吸时间的中位数比较短（3个月）治疗的个体高出10.7%到62.5%。</li>
</ul>
</li>
<li><strong>第二句 (短期治疗)</strong>: 短期治疗是基准组 (<code>treat</code>=&quot;Short&quot;)，其复吸时间对数值的期望由截距估计。截距的估计值是4.6823。其反转换后的置信区间是 (94.559, 123.404)。
<ul>
<li><strong>答案句2</strong>: 对于接受短期治疗的个体，其复吸时间的中位数估计在95天到123天之间。</li>
</ul>
</li>
<li>(题目解析中给出的答案是：我们估计，长期（6个月）治疗的复吸时间中位数比短期（3个月）治疗高11%到63%。短期治疗的复吸时间中位数在95到123天之间。这与我们的计算基本一致。)</li>
</ul>
</li>
</ul>
<p><strong>(c) 题目 (3分)</strong>: 写出获取长期治疗后复吸时间置信区间的R命令。</p>
<ul>
<li>
<p><strong>解析</strong>:</p>
<ul>
<li>
<p>长期治疗 (<code>treat</code>=&quot;Long&quot;) 不是基准组。我们需要为 <code>treat=&quot;Long&quot;</code> 创建一个新的数据框，然后使用 <code>predict()</code> 函数在对数尺度上获得预测值和置信区间，最后进行指数反转换。</p>
</li>
<li>
<p><strong>答案</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new.df &lt;- data.frame(treat = factor(&quot;Long&quot;, levels = c(&quot;Short&quot;, &quot;Long&quot;)))</span><br><span class="line"># 或者如果 treat 已经是因子且 &quot;Long&quot; 是其一个水平: new.df &lt;- data.frame(treat = &quot;Long&quot;)</span><br><span class="line">exp(predict(drug.loglm, newdata = new.df, interval = &quot;confidence&quot;))</span><br></pre></td></tr></table></figure>
<p>或者使用 <code>predictGLM</code> (如果这是一个线性模型且 <code>s20x</code> 包已加载，但题目中是 <code>lm</code>):</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 如果 drug.loglm 是 lm 对象，predictGLM 可能不直接适用，除非 s20x 对其有封装</span><br><span class="line"># 更标准的是手动计算或使用 predict.lm 的结果</span><br><span class="line"># 以下是基于 predict.lm 的标准做法</span><br><span class="line">pred_log_long &lt;- predict(drug.loglm, newdata = data.frame(treat = &quot;Long&quot;), interval = &quot;confidence&quot;)</span><br><span class="line">exp(pred_log_long)</span><br></pre></td></tr></table></figure>
<p>(题目解析中给出的R命令是正确的。)</p>
</li>
</ul>
</li>
</ul>
<h4 id="来自-test-sol-2-pdf-2025年春季测试题解析-即今年的测试">来自 <code>test_sol(2).pdf</code> (2025年春季测试题解析 - 即今年的测试)</h4>
<p><strong>A部分：判断题 (Question 1)</strong></p>
<p><strong>(a) 题目 (3分)</strong>: 在一个简单线性回归模型中，如果所有模型假设都满足且决定系数 R2 为0.9，那么响应变量Y和解释变量X必须有很强的线性关联。</p>
<ul>
<li><strong>判断</strong>: 正确 (✓ TRUE)</li>
<li><strong>解析</strong>:
<ul>
<li>决定系数 R2 衡量的是解释变量 X 能够解释响应变量 Y 总变异的百分比。R2=0.9 意味着 X 解释了 Y 变异的90%。</li>
<li>在简单线性回归中，R2=(corr(X,Y))2，所以如果 R2=0.9，则相关系数 r=0.9≈±0.9487。</li>
<li>相关系数的绝对值接近1（如0.9487）表明 X 和 Y 之间存在非常强的线性关联。</li>
<li>前提是“所有模型假设都满足”，这确保了 R2 和相关系数的解释是有效的。</li>
</ul>
</li>
</ul>
<p><strong>(b) 题目 (3分)</strong>: 在线性回归中，summary输出中斜率的t检验的p值是斜率为零的概率。<mark>p-value是观测到极端数据的概率，概率越小说明越有可能成立，但是不能把p-value和某某某成立的概率绑定在一起</mark></p>
<ul>
<li><strong>判断</strong>: 错误 (✓ FALSE)</li>
<li><strong>解析</strong>:
<ul>
<li>这是一个对p值的常见误解。p值是在**原假设为真（即斜率为零）**的前提下，观察到当前样本数据（或更极端数据）的概率。</li>
<li>它不是“斜率为零的概率”。在频率学派统计中，总体参数（如真实的总体斜率）被认为是固定的未知常数，它没有概率分布，因此不能说它等于某个特定值的概率是多少。</li>
</ul>
</li>
</ul>
<p><strong>(c) 题目 (3分)</strong>: 在线性回归中，μi 的置信区间可能比 Yi=β0+β1xi+ϵi 的预测区间更宽，前提是所有常规假设都满足且 (xi,yi) 是用于构建线性模型的观测数据点。</p>
<ul>
<li><strong>判断</strong>: 错误 (✓ FALSE)</li>
<li><strong>解析</strong>:
<ul>
<li>μi=E[Yi∣xi] 是给定 xi 时 Y 的条件均值。其置信区间是针对这个<strong>均值</strong>的。</li>
<li>Yi 的预测区间是针对一个<strong>新的、单个的</strong>观测值 Ynew（在给定的 xi 处）的。</li>
<li>预测区间总是<strong>等于或宽于</strong>对应 xi 处的均值置信区间。这是因为预测区间不仅包含了对均值估计的不确定性（这部分与均值置信区间相同），还额外包含了单个观测值围绕其均值的随机波动 ϵi 的不确定性。</li>
<li>因此，均值的置信区间不可能比预测区间更宽。</li>
</ul>
</li>
</ul>
<p><strong>B部分：多项选择题 (Question 2)</strong></p>
<p><strong>(a) 题目 (4分)</strong>: 考虑一个对数-对数线性模型 log(yi)=β0+β1log(xi)+ϵi，其中 ϵi∼Normal(0,σ2) 且 β^1=0.1。以下哪个陈述是正确的？</p>
<p>☐ Yi 的期望对数值如果 x 增加1个单位将不会改变。 (The log of expected Yi would not change if x was increased by 1 unit.)</p>
<p>☐ Yi 的期望对数值如果 x 增加1个单位将增加10%。 (The expected log of Yi would increase by 10% if x was increased by 1 unit.)</p>
<p>☐ Yi 的期望对数值如果 x 增加1个单位将增加0.1。 (The log of expected Yi would increase by 0.1 if x was increased by 1 unit.)</p>
<p>☐ Yi 的中位数如果 x 增加1%将增加10%。 (The median of Yi would increase by 10% if x was increased by 1%.)</p>
<p>☐ Yi 的对数中位数如果 x 增加1%将增加0.1%。 (The median of log of Yi would increase by 0.1% if x was increased by 1%.)</p>
<p>✓ 以上皆非。 (None of the above.)</p>
<ul>
<li><strong>正确选项</strong>: ✓ 以上皆非。</li>
<li><strong>解析</strong>:
<ul>
<li>模型是 log(Yi)=β0+β1log(xi)+ϵi。</li>
<li>我们通常解释的是 xi <strong>百分比变化</strong>对 Yi <strong>中位数百分比变化</strong>的影响。</li>
<li>当 xi 增加1%时，Yi 的中位数大约增加 β1。题目中 β^1=0.1，所以当 xi 增加1%时，Yi 的中位数大约增加 0.1。</li>
<li>让我们逐个分析选项：
<ul>
<li>“Yi 的期望对数值如果 x 增加1个单位将不会改变。” 错误。这里 x 是绝对变化，且模型中是 log(x)。E[log(Yi)] 会改变。</li>
<li>“Yi 的期望对数值如果 x 增加1个单位将增加10%。” 错误。E[log(Yi)] 的变化是 β1×Δ(log(x))，不是简单的百分比。</li>
<li>“Yi 的期望对数值如果 x 增加1个单位将增加0.1。” 错误。E[log(Yi)] 的变化是 β1(log(x+1)−log(x))=0.1log((x+1)/x)，这不等于0.1。</li>
<li>“Yi 的中位数如果 x 增加1%将增加10%。” 错误。应该是增加约 0.1。</li>
<li>“Yi 的对数中位数如果 x 增加1%将增加0.1%。” 错误。Yi 的对数中位数是 E[log(Yi)]。当 x 增加1% (即 x 变为 1.01x) 时，log(x) 变为 log(1.01x)=log(1.01)+log(x)。所以 E[log(Yi)] 的变化是 β1log(1.01)=0.1×log(1.01)≈0.1×0.00995≈0.000995。这不是0.1%。</li>
</ul>
</li>
<li><strong>正确的陈述应该是</strong>：“Yi 的中位数如果 x 增加1%将大约增加 0.1。” 由于选项中没有这个陈述，所以选择“以上皆非”。</li>
</ul>
</li>
</ul>
<p><strong>C部分：简答题 (Question 3)</strong></p>
<p><strong>背景</strong>: 一个二次回归模型 y∼x+I(x2) 在R中拟合，输出摘要如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; summary(lm(y ~ x + I(x^2)))</span><br><span class="line">Call:</span><br><span class="line">lm(formula = y ~ x + I(x^2))</span><br><span class="line"></span><br><span class="line">Residuals:</span><br><span class="line">    Min      1Q  Median      3Q     Max</span><br><span class="line">-4.6231 -0.9615  0.0118  0.9832  3.9852</span><br><span class="line"></span><br><span class="line">Coefficients:</span><br><span class="line">            Estimate Std. Error t value Pr(&gt;|t|)</span><br><span class="line">(Intercept)  0.13947    0.05791   2.408 0.016200 *</span><br><span class="line">x            0.14603    0.04736   3.083 0.002103 **</span><br><span class="line">I(x^2)      -0.13152    0.03512  -3.744 0.000191 ***</span><br><span class="line">---</span><br><span class="line">Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><br><span class="line"></span><br><span class="line">Residual standard error: 1.478 on 997 degrees of freedom</span><br><span class="line">Multiple R-squared:  0.02245,   Adjusted R-squared:  0.02049</span><br><span class="line">F-statistic: 11.45 on 2 and 997 DF,  p-value: 1.212e-05</span><br></pre></td></tr></table></figure>
<p><strong>(a) 题目 (1分)</strong>: 数据框中有多少个数据点？</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li>根据残差标准误 (Residual standard error) 那一行，自由度 (degrees of freedom) 是997。</li>
<li>模型是 Y=β0+β1X+β2X2+ϵ，有3个参数被估计 (β0,β1,β2)。</li>
<li>残差自由度 dfresidual=n−p，其中 n 是数据点数量，p 是模型中参数的数量。</li>
<li>所以，997=n−3。</li>
<li>因此，n=997+3=1000。</li>
<li><strong>答案</strong>: 数据框中有1000个数据点。</li>
</ul>
</li>
</ul>
<p><strong>(b) 题目 (5分)</strong>: 计算 β2 (即 x2 的系数) 的95%置信区间。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li>β2 的95%置信区间公式为：β^2±tα/2,n−p×SE(β^2)。</li>
<li>从R输出中可知：
<ul>
<li>x2 的系数估计值 β^2=−0.13152 (I(x^2)的Estimate)。</li>
<li>x2 的系数标准误 SE(β^2)=0.03512 (I(x^2)的Std. Error)。</li>
<li>自由度 df=n−p=997。</li>
</ul>
</li>
<li>t0.025,997 的临界值。当自由度很大时（如大于100或200），t分布非常接近标准正态分布，所以可以使用 z0.025=1.96 作为近似。用R精确计算：<code>qt(0.975, df=997)</code> 结果约为 1.9623。题目解析中使用了1.96。</li>
<li>使用1.96：
<ul>
<li>下限: −0.13152−1.96×0.03512=−0.13152−0.0688352=−0.2003552≈−0.20<img src alt="img"></li>
<li>上限: −0.13152+1.96×0.03512=−0.13152+0.0688352=−0.0626848≈−0.06<img src alt="img"></li>
</ul>
</li>
<li>所以，β2 的95%置信区间约为 (-0.20, -0.06)。</li>
<li><strong>步骤评分点</strong>:
<ul>
<li>1分: 知道公式 β^2±tα/2×SE(β^2)。</li>
<li>1分: 正确读取 β^2=−0.13152。</li>
<li>1分: 正确读取 SE(β^2)=0.03512。</li>
<li>1分: 知道学生t分布可以被正态分布近似，因此 tα/2≈1.96。</li>
<li>1分: 最终答案 (-0.20, -0.06)。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>(c) 题目 (3分)</strong>: 在5%的显著性水平下，是否有证据反对 H0:β2=−0.1？请说明理由。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li><strong>判断</strong>: 没有足够的证据反对 H0:β2=−0.1。</li>
<li><strong>理由 (基于置信区间)</strong>:
<ul>
<li>我们在 (b) 中计算出 β2 的95%置信区间约为 (-0.20, -0.06)。</li>
<li>原假设的值 β2,0=−0.1。</li>
<li>因为 −0.1 包含在置信区间 (−0.20,−0.06) 内 (即 −0.20&lt;−0.1&lt;−0.06)。</li>
<li>当原假设的值落在参数的置信区间内时，我们没有足够的证据在相应的显著性水平下拒绝原假设。</li>
</ul>
</li>
<li><strong>步骤评分点</strong>:
<ul>
<li>1分: 正确判断“没有证据”。</li>
<li>1分: 知道使用置信区间与假设检验的对应关系。</li>
<li>1分: 知道因为 −0.1∈(−0.20,−0.06)，所以在5%水平下没有证据反对。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Question 4 (8分) - FEV (用力呼气量) 研究</strong></p>
<p><strong>背景</strong>: 研究吸烟与用力呼气量 (FEV) 的关系。FEV是肺功能指标。数据来自654名3-19岁青少年。变量：</p>
<ul>
<li><code>fev</code>: FEV测量值。</li>
<li><code>ht</code>: 身高 (英寸)。</li>
<li><code>smoke</code>: 1为吸烟者，0为不吸烟者。</li>
</ul>
<p><strong>输出信息</strong>:</p>
<ul>
<li>三个 <code>trendscatter</code> 图：<code>fev~ht</code>, <code>log(fev)~ht</code>, <code>log(fev)~log(ht)</code>。</li>
<li>三个模型的残差对拟合值图 (<code>plot(fit, which=1)</code>)：
<ul>
<li><code>fit1 = lm(fev ~ ht*smoke, ...)</code></li>
<li><code>fit2 = lm(log(fev) ~ ht*smoke, ...)</code> (对数线性模型)</li>
<li><code>fit3 = lm(log(fev) ~ log(ht)*smoke, ...)</code> (对数-对数模型)</li>
</ul>
</li>
<li>三个模型的系数置信区间 (原始尺度和反转换后的百分比变化尺度)。</li>
</ul>
<p><strong>(a) 题目 (2分)</strong>: 根据给定的输出，选择最佳模型。请说明理由。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li><strong>选择</strong>: 对数线性模型 (<code>fit2 = lm(log(fev) ~ ht*smoke, ...)</code>) 似乎是最佳的。</li>
<li><strong>理由</strong>:
<ul>
<li>从趋势散点图来看：
<ul>
<li><code>fev~ht</code> (左图) 显示出明显的曲线关系（向上弯曲）和异方差性（随身高增加，FEV的变异性增大）。这表明简单线性模型 <code>fit1</code> 可能不合适。</li>
<li><code>log(fev)~ht</code> (中图) 使得关系看起来更接近线性，并且散点的垂直散布（方差）似乎更均匀。</li>
<li><code>log(fev)~log(ht)</code> (右图) 也显示了线性关系，但与中图相比，其改善程度可能不那么明显，或者说中图的线性关系已经足够好。</li>
</ul>
</li>
<li>从残差图来看（虽然题目中只展示了 <code>fit1</code> 的一个残差图，但假设可以参考所有）：
<ul>
<li><code>fit1</code> (线性模型) 的残差图（题目中展示的第一个残差图）显示出明显的曲线模式（可能是U型或倒U型）和扇形散布（异方差），表明线性假设和等方差假设均不满足。</li>
<li>根据题目解析的提示“对数线性模型在散点图和残差图中都略优于对数-对数模型”，我们可以推断 <code>fit2</code> (对数线性模型) 的残差图表现最好，即残差随机分布在0附近，没有明显模式，且方差较为恒定。</li>
<li><code>fit3</code> (对数-对数模型) 的残差图可能也比 <code>fit1</code> 好，但可能不如 <code>fit2</code>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>答案</strong>: 对数线性模型 (<code>fit2 = lm(log(fev) ~ ht*smoke, ...)</code>) 似乎是最佳选择。理由是，原始的散点图和线性模型 (<code>fit1</code>) 的残差图都显示出曲线关系和拟合不足（或异方差）。而对数线性模型 (<code>fit2</code>) 的散点图（<code>log(fev)~ht</code>）和其对应的（未完全展示但可推断的）残差图表现出更好的线性关系和方差齐性，它在改善模型假设方面略优于对数-对数模型 (<code>fit3</code>)。</li>
</ul>
</li>
</ul>
<p><strong>(b) 题目 (3分)</strong>: 对于您选择的模型，写三句话（如同写执行摘要一样），第一句说明是否存在交互作用，第二句说明吸烟的影响，第三句量化身高的影响。</p>
<ul>
<li><strong>解析 (假设选择了 <code>fit2 = lm(log(fev) ~ ht\*smoke, ...)</code> 作为最佳模型)</strong>:
<ul>
<li>我们需要查看 <code>fit2</code> 的 <code>summary()</code> 输出（题目未直接给出，但给出了其置信区间 <code>confint(fit2)</code>）。从 <code>confint(fit2)</code> 的输出中，<code>ht:smoke</code> 项的置信区间是 <code>(-0.01409994, 0.009512541)</code>。这个区间包含了0，表明交互作用项<strong>不显著</strong>。</li>
<li>如果交互作用不显著，我们通常会移除它并拟合主效应模型 <code>lm(log(fev) ~ ht + smoke, ...)</code>。但题目要求基于“你选择的模型”，而 <code>fit2</code> 是包含交互作用的。如果严格按 <code>fit2</code> 解释：
<ul>
<li><strong>第一句 (交互作用)</strong>: FEV与身高之间的关系对于吸烟者和非吸烟者来说是相同的（或者说，没有足够的证据表明这种关系因吸烟状况而异，因为交互作用不显著）。</li>
<li><strong>第二句 (吸烟影响)</strong>: 在控制了身高后，吸烟状况对FEV的中位数没有显著影响（因为 <code>smoke</code> 主效应的置信区间 <code>(-0.63357410, 0.921653364)</code> 对应的指数减1后包含0，表明OR的CI包含1）。</li>
<li><strong>第三句 (身高影响)</strong>: 控制吸烟状况后，身高每增加一英寸，FEV的中位数预计增加5.1%到5.6%（来自 <code>ht</code> 项的置信区间 <code>(0.05014065, 0.054451541)</code>，反转换为百分比变化 100×(eCI−1)）。</li>
</ul>
</li>
<li><strong>更合理的做法 (如果允许模型简化)</strong>: 由于交互作用不显著，应简化模型为 <code>lm(log(fev) ~ ht + smoke, ...)</code>。然后根据简化模型的 <code>summary()</code> 来解释 <code>smoke</code> 和 <code>ht</code> 的主效应。
<ul>
<li>假设简化后 <code>smoke</code> 仍不显著，<code>ht</code> 显著。</li>
<li><strong>第一句 (交互作用)</strong>: FEV与身高之间的关系对于吸烟者和非吸烟者来说是相同的。</li>
<li><strong>第二句 (吸烟影响)</strong>: 在控制了身高后，我们没有发现吸烟状况对FEV有显著影响。</li>
<li><strong>第三句 (身高影响)</strong>: 我们估计，身高每额外增加一英寸，FEV的中位数会增加X%到Y% (这里的X和Y来自简化模型后 <code>ht</code> 系数的置信区间并反转换)。</li>
</ul>
</li>
<li><strong>题目解析中给出的答案似乎是基于 <code>fit1</code> (线性模型 <code>fev ~ ht\*smoke</code>) 或简化后的 <code>fit2</code> (如果交互和smoke主效应都不显著，则简化为 <code>log(fev) ~ ht</code>)，并且混合了解释：</strong>
<ul>
<li>“用力呼气量和身高之间的关系对于吸烟者和非吸烟者是相同的。” (这暗示交互作用不显著)</li>
<li>“在给定的数据中，我们没有发现吸烟对用力呼气量有任何影响。” (这暗示吸烟主效应不显著)</li>
<li>“我们估计身高每额外增加一英寸，用力呼气量的均值会增加0.12到0.14个单位。” (这是对 <code>fit1</code> 中 <code>ht</code> 系数的解释，针对均值，且假设交互和吸烟都不显著)</li>
<li>“我们估计身高每额外增加一英寸，用力呼气量的中位数会增加5.1%到5.6%。” (这是对 <code>fit2</code> 中 <code>ht</code> 系数的解释，针对中位数，且假设交互和吸烟都不显著)</li>
<li>“我们估计身高每增加1%，用力呼气量的中位数会增加3.0%到3.3%。” (这是对 <code>fit3</code> 中 <code>log(ht)</code> 系数的解释)</li>
</ul>
</li>
<li><strong>选择最佳答案句 (基于题目解析倾向于简化模型后的解释)</strong>:
<ol>
<li>FEV与身高之间的关系对于吸烟者和非吸烟者是相同的。</li>
<li>在给定的数据中，我们没有发现吸烟对FEV的中位数有显著影响。</li>
<li>我们估计，身高每额外增加一英寸，FEV的中位数会增加5.1%到5.6%。 (选择与 <code>fit2</code> 对应的解释)</li>
</ol>
</li>
</ul>
</li>
</ul>
<p><strong>(c) 题目 (3分)</strong>: 写下模型 <code>fit3</code> 的方程/公式，如同你在“方法与假设检查”部分所写的那样。</p>
<ul>
<li><strong>解析</strong>:
<ul>
<li><code>fit3</code> 是对数-对数模型，包含身高和吸烟状况的交互作用：<code>lm(log(fev) ~ log(ht)*smoke, ...)</code>。</li>
<li>模型方程可以写为： log(fevi)=β0+β1log(hti)+β2smokei+β3(log(hti)×smokei)+ϵi</li>
<li>其中：
<ul>
<li>fevi 是第 i 个个体的用力呼气量。</li>
<li>hti 是第 i 个个体的身高。</li>
<li>smokei 是一个指示变量，如果第 i 个个体吸烟则为1，否则为0。</li>
<li>ϵi∼iidNormal(0,σ2) 是误差项。</li>
<li>β0,β1,β2,β3 是固定但未知的模型参数。</li>
</ul>
</li>
<li><strong>步骤评分点</strong>:
<ul>
<li>1分: 写出模型的基本形式 log(fevi)=⋯+ϵi。</li>
<li>1分: 正确包含所有项，包括交互作用项 β3smokei⋅log(hti)，并说明误差项 ϵi∼Normal(0,σ2) 以及参数是固定但未知的。</li>
<li>1分: 正确定义指示变量 smokei (例如，吸烟为1，不吸烟为0)。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="代码总结">代码总结</h2>
<h3 id="Lecture-1">Lecture 1</h3>
<p>这个lecture主要介绍了R语言在统计分析中的一些基本操作和概念，特别是围绕<strong>简单线性回归</strong>的初步探讨。</p>
<ol>
<li>
<p><strong>数据生成与描述</strong>:</p>
<ul>
<li>
<p>使用 <code>rnorm()</code> 从正态分布中生成随机样本。</p>
</li>
<li>
<p>计算样本均值 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mean()</span><br></pre></td></tr></table></figure>
<p>,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sum()/length()</span><br></pre></td></tr></table></figure>
<p>) 作为总体均值</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">true.mu</span><br></pre></td></tr></table></figure>
<p>的最大似然估计 (MLE)。这与教材</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">STATS201 book SWU 2023.pdf</span><br></pre></td></tr></table></figure>
<p>第1章介绍的通过R分析数据的概念一致 。</p>
</li>
</ul>
</li>
<li>
<p><strong>单样本t检验</strong>:</p>
<ul>
<li>
<p>使用 <code>t.test(y, mu = value)</code> 进行单样本t检验，用于检验总体均值是否等于某个特定值。代码中用 <code>pi</code> 是一个占位符，实际应为数值。</p>
</li>
<li>
<p>提及了估计、假设检验和置信区间都可以通过</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">t.test()</span><br></pre></td></tr></table></figure>
<p>函数获得，并强调了样本量对检验效能和置信区间宽度的影响。这部分内容与教材第3章“零线性模型与单样本t检验的等价性”相关 。</p>
</li>
</ul>
</li>
<li>
<p><strong>相关性分析</strong>:</p>
<ul>
<li>使用 <code>plot(x,y)</code> 绘制散点图来可视化两个变量之间的关系。</li>
<li>使用 <code>cor(x,y)</code> 计算皮尔逊相关系数，用以衡量线性关联的强度和方向。</li>
<li>通过实例（<code>y</code>的均值依赖于<code>x</code>，或<code>y</code>的标准差依赖于<code>x</code>的绝对值）强调了相关系数<strong>仅能捕捉线性关联</strong>，对于非线性关系（如异方差引起的扇形散点图），相关系数可能很低，即使变量间存在明显模式。</li>
</ul>
</li>
<li>
<p><strong>简单线性回归 (Simple Linear Regression)</strong>:</p>
<ul>
<li>
<p>引入 <code>s20x</code> 包，这是课程特定的包。</p>
</li>
<li>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data.frame()</span><br></pre></td></tr></table></figure>
<p>创建数据框，这是R中存储表格数据的常用结构。对应教材第1章“将数据导入R - 创建数据框”部分 。</p>
</li>
<li>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(y ~ x, data = data.df)</span><br></pre></td></tr></table></figure>
<p>构建简单线性模型Y=β0+β1X+ϵ。这是本课程的核心内容之一，教材第1章“如何拟合直线到您的数据 - 简单线性模型”和第2章“简单线性回归的基础”详细讨论了这一点。</p>
</li>
<li>
<pre><code>summary(fit)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  函数提供了模型系数的估计值、标准误、t统计量、p值以及模型的R2、F统计量等重要信息。教材第1章和第2章均有详细解释 。</span><br><span class="line"></span><br><span class="line">- 手动计算 R2</span><br><span class="line"></span><br><span class="line">   (</span><br><span class="line"></span><br></pre></td></tr></table></figure>
(TSS-RSS)/TSS
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">)，并与</span><br><span class="line"></span><br></pre></td></tr></table></figure>
summary(fit)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">     中的结果对比。教材第1章“模型对预测有多好？ - R2”部分有讲解 。</span><br><span class="line"></span><br><span class="line">5. **模型诊断 (Model Diagnostics)**:</span><br><span class="line"></span><br><span class="line">   - `plot(fit, which = 1)` 生成残差对拟合值图，用于检查模型的**线性假设**和**方差齐性 (EOV)** 假设。</span><br><span class="line">   - `normcheck(fit)` (来自`s20x`包) 或 `plot(fit, which = 2)` (标准R的Q-Q图) 用于检查残差的**正态性假设**。</span><br><span class="line">   - `plot(fit, which = 4)` 生成Cook&#x27;s距离图，用于识别**强影响点**。</span><br><span class="line">   - 这些诊断方法是评估模型适用性的关键步骤，教材第2章“模型假设的标准检查 - 独立性、EOV和正态性”以及“使用cooks20x检查不当影响点”部分有详细说明 。</span><br><span class="line"></span><br><span class="line">6. **模型解释与预测**:</span><br><span class="line"></span><br><span class="line">   - `summary(fit)` 中的系数（斜率）解释。</span><br><span class="line">   - `confint(fit)` 计算模型参数（截距和斜率）的置信区间（95%）。</span><br><span class="line">   - `predict(fit, newdata = ..., interval = &quot;confidence&quot;)` 计算给定新 x 值时，E[Y∣x] 的置信区间。</span><br><span class="line">   - `predict(fit, newdata = ..., interval = &quot;prediction&quot;)` 计算给定新 x 值时，单个 Y 值的预测区间。</span><br><span class="line">   - 这些内容对应教材第1章“如何使用拟合模型进行预测”和第2章“置信区间和预测区间”。</span><br><span class="line"></span><br><span class="line">7. **配对t检验的等价性**:</span><br><span class="line"></span><br><span class="line">   - `t.test(x, y, paired = TRUE)` 执行配对t检验。</span><br><span class="line">   - `t.test(x-y)` 对差值进行单样本t检验，其结果与配对t检验等价。</span><br><span class="line">   - `summary(lm(x-y ~ 1, data = data.df))` 表明对差值拟合一个只有截距的模型（零模型），其截距的检验也与单样本t检验等价。这部分内容与教材第3章“零线性模型与单样本t检验的等价性”紧密相关 。</span><br><span class="line"></span><br><span class="line">```R</span><br><span class="line"># 真实参数未知但固定</span><br><span class="line">true.mu = 3</span><br><span class="line"># 从正态分布生成一个样本</span><br><span class="line">y = rnorm(100, mean = true.mu, sd = 1)</span><br><span class="line"># true.mu 的最大似然估计 (MLE)</span><br><span class="line">mean(y)</span><br><span class="line">sum(y) / length(y)</span><br><span class="line"># 假设检验 H_0: mu_y = pi (这里的pi应理解为一个特定的常数值，例如3.14159...，但在t检验中通常用于表示总体均值假设值)</span><br><span class="line"># 注意：实际代码中 t.test(y, mu = pi) 会引发错误，因为pi是内置常量。通常会用一个数值，如 t.test(y, mu = 3)</span><br><span class="line">t.test(y, mu = pi) # 概念上是检验均值是否等于某个值，但pi的使用在此处不标准，除非pi被重新赋值</span><br><span class="line"></span><br><span class="line"># 估计/假设检验/置信区间</span><br><span class="line"># 上述函数 (t.test) 均可提供</span><br><span class="line"># 注意检验的效能 (power)</span><br><span class="line"># 以及置信区间的宽度</span><br><span class="line"># 会随着样本量的改变而改变</span><br><span class="line"></span><br><span class="line"># 生成两个独立的变量用于展示关系</span><br><span class="line">y = rnorm(100, mean = 3, sd = 1)</span><br><span class="line">x = rnorm(100, mean = 0, sd = 1)</span><br><span class="line">plot(x,y) # 绘制散点图</span><br><span class="line"># 皮尔逊相关系数</span><br><span class="line">cor(x,y)</span><br><span class="line"># 创建一些依赖关系</span><br><span class="line">y = rnorm(100, mean = x, sd = 1) # y 的均值依赖于 x</span><br><span class="line">plot(x,y)</span><br><span class="line">cor(x,y)</span><br><span class="line"># 注意相关性仅能</span><br><span class="line"># 捕捉线性关联</span><br><span class="line">x = rnorm(1000, mean = 0, sd = 1)</span><br><span class="line">y = rnorm(1000, mean = 0, sd = abs(x)) # y 的标准差依赖于 x 的绝对值，创建了非线性关联 (异方差性)</span><br><span class="line">plot(x,y)</span><br><span class="line">cor(x,y) # 尽管存在明显模式，但线性相关系数可能接近0</span><br><span class="line"></span><br><span class="line">library(s20x) # 本课程需要此包</span><br><span class="line">y = rnorm(100, mean = 3, sd = 1)</span><br><span class="line">x = rnorm(100, mean = 0, sd = 1)</span><br><span class="line">data.df = data.frame(y=y,x=x) # 创建数据框</span><br><span class="line">fit = lm(y~x, data = data.df) # 构建线性模型 y = beta0 + beta1*x</span><br><span class="line">summary(fit) # 给出参数估计等信息</span><br><span class="line"># R^2 (决定系数)</span><br><span class="line">TSS = sum((y - mean(y))^2) # 总平方和 (Total Sum of Squares)</span><br><span class="line">RSS = sum((y-fitted.values(fit))^2) # 残差平方和 (Residual Sum of Squares)</span><br><span class="line">(TSS- RSS)/TSS # 计算 R^2，应与 summary(fit) 中的 R-squared 一致</span><br><span class="line"></span><br><span class="line"># 残差图</span><br><span class="line">plot(fit,which = 1) # 1. 残差 vs. 拟合值图，用于检查线性性和方差齐性 (EOV)</span><br><span class="line">normcheck(fit) # 3. 正态性检验图 (通常是Q-Q图和残差直方图)</span><br><span class="line"># plot(fit,which = 2) # 理论上 which=2 是 Q-Q plot，但 s20x 的 normcheck 更常用</span><br><span class="line">plot(fit,which = 4) # 4. Cook&#x27;s距离图，用于识别强影响点</span><br><span class="line"># 如何解释斜率</span><br><span class="line">summary(fit) # 回归系数的估计值、标准误、t值、p值</span><br><span class="line"># 置信区间</span><br><span class="line">confint(fit) # 参数的置信区间及其解释</span><br><span class="line"># 预测区间</span><br><span class="line">new.df = data.frame(x=pi) # 创建新的x值用于预测，同样pi的使用需注意</span><br><span class="line">predict(fit, newdata = new.df, # 对给定x值的均值y的置信区间</span><br><span class="line">        interval = &quot;confidence&quot;)</span><br><span class="line">predict(fit, newdata = new.df, # 对给定x值的单个y的预测区间</span><br><span class="line">        interval = &quot;prediction&quot;)</span><br><span class="line"></span><br><span class="line"># t.test 中的参数</span><br><span class="line">t.test(x,y,paired = TRUE) # 配对t检验</span><br><span class="line">t.test(x-y) # 这等同于对差值进行单样本t检验 (检验差值的均值是否为0)</span><br><span class="line"># 与零模型相同</span><br><span class="line">summary(lm(x-y~1, data = data.df)) # 对差值拟合截距模型，其截距的t检验与单样本t检验等价</span><br></pre></td></tr></table></figure>

</code></pre>
</li>
</ul>
</li>
</ol>
<h3 id="Lecture-2">Lecture 2</h3>
<p>这个lecture深入探讨了<strong>简单线性回归的基础 (The basics of simple linear regression)</strong>，重点在于理解模型参数的估计、推断（置信区间和预测区间）以及模型诊断。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第2章  的内容高度吻合。</p>
<ol>
<li>
<p><strong>参数估计与标准误</strong>:</p>
<ul>
<li>
<p>首先通过模拟生成已知真实参数 (<code>beta0</code>, <code>beta1</code>, <code>sd</code>) 的数据。</p>
</li>
<li>
<p>使用 <code>lm(y~x)</code> 拟合模型。</p>
</li>
<li>
<p>手动从公式计算残差标准误 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sigmahat</span><br></pre></td></tr></table></figure>
<p>) 和斜率的标准误 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sigma1hat</span><br></pre></td></tr></table></figure>
<p>)，并与</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">summary(fit)$sigma</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">summary(fit)$coeff[2,2]</span><br></pre></td></tr></table></figure>
<p>的输出进行比较，帮助理解这些估计量是如何得到的。这对应教材第2章中关于最小二乘法拟合模型后，如何得到参数估计及其标准误差的讨论 。</p>
</li>
</ul>
</li>
<li>
<p><strong>参数的置信区间</strong>:</p>
<ul>
<li>
<p>手动计算斜率 <code>beta1hat</code> 的95%置信区间，使用公式 $β^1±tα/2,n−p×SE(β^1)$。</p>
</li>
<li>
<p>将其与</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">confint(fit)</span><br></pre></td></tr></table></figure>
<p>函数的结果进行比较，验证手动计算的正确性。这在教材第2章“参数和拟合值的置信区间，以及个体预测的预测区间”部分有详细描述 。</p>
</li>
<li>
<p>通过模拟 (<code>for</code>循环) 展示了置信区间的<strong>覆盖率</strong> (coverage probability) 概念：在多次重复抽样和构建置信区间的过程中，大约有95%的95%置信区间会包含真实的参数值。</p>
</li>
</ul>
</li>
<li>
<p><strong>预测区间与均值置信区间</strong>:</p>
<ul>
<li>
<p>区分对条件均值 E[Y∣x] 的置信区间 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">interval = &quot;confidence&quot;</span><br></pre></td></tr></table></figure>
<p>) 和对单个新观测值 Ynew 的预测区间</p>
<p>(</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">interval = &quot;prediction&quot;</span><br></pre></td></tr></table></figure>
<p>)。教材第2章对此有详细解释 。</p>
</li>
<li>
<p>预测区间总是比均值置信区间更宽，因为它额外考虑了单个观测的随机波动 ϵ。</p>
</li>
<li>
<p>通过模拟展示了预测区间的覆盖率，即大约95%的95%预测区间会包含真实的未来观测值。</p>
</li>
</ul>
</li>
<li>
<p><strong>拟合值与残差</strong>:</p>
<ul>
<li><code>fitted.values(simple.fit)</code> 或 <code>coef(simple.fit)[1] + coef(simple.fit)[2] * sim_data$x</code> 用于获取拟合值 y^。</li>
<li><code>sim_data$y - yhat</code> 计算残差 e=y−y^。</li>
<li>这些概念是模型诊断的基础，教材第2章“拟合值和残差”部分有所提及 。</li>
</ul>
</li>
<li>
<p><strong>模型诊断图</strong>:</p>
<ul>
<li>
<pre><code>plot(simple.fit, which = 1)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  ：残差对拟合值图，用于检查线性性和方差齐性 (EOV)。教材第2章“检查模型假设 - 独立性、EOV和正态性” 。</span><br><span class="line"></span><br><span class="line">- `trendscatter(res ~ yhat)` (来自 `s20x` 包)：提供了另一种检查线性性和EOV的方法。</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  normcheck(simple.fit)</span><br></pre></td></tr></table></figure>

 (来自 

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s20x</span><br></pre></td></tr></table></figure>

 包)：用于检查残差的正态性。教材第2章 。

</code></pre>
</li>
<li>
<pre><code>cooks20x(simple.fit)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">(来自 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
s20x
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">      包)：用于检查强影响点。教材第2章“使用cooks20x检查不当影响点” 。</span><br><span class="line"></span><br><span class="line">   - 强调了样本量对诊断工具敏感性的影响：小样本时诊断图可能过于敏感。</span><br><span class="line"></span><br><span class="line">6. **违反模型假设的情况**:</span><br><span class="line"></span><br><span class="line">   - 模拟了违反方差齐性 (`sd = x`) 和违反残差正态性 (`error = error^2`) 的情况，并观察了这些违反对模型诊断图和回归线的影响。</span><br><span class="line">   - 指出在违反EOV时，回归线可能仍接近真实情况，但违反正态性（特别是当它导致均值结构改变时）可能会使回归线偏离真实情况。</span><br><span class="line"></span><br><span class="line">```r</span><br><span class="line"># 真实且未知的参数</span><br><span class="line">beta0 = 100</span><br><span class="line">beta1 = 0.05</span><br><span class="line"># 解释变量</span><br><span class="line">x = 1:200</span><br><span class="line"># 真实均值</span><br><span class="line">mu = beta0 + beta1*x</span><br><span class="line"># 响应变量 (带有随机误差)</span><br><span class="line">y = rnorm(200, mean = mu, sd = 10)</span><br><span class="line"># 构建模型</span><br><span class="line">fit = lm(y~x)</span><br><span class="line">summary(fit)</span><br><span class="line"># 从公式计算估计的标准差 sigma_hat (残差标准误)</span><br><span class="line">sqrt(sum((y - fitted.values(fit))^2)/(200-2)) # (n-p)，这里 p=2 (beta0, beta1)</span><br><span class="line"># 从summary对象中提取估计的标准差 sigma_hat</span><br><span class="line">sigmahat = summary(fit)$sigma</span><br><span class="line"># 从公式计算斜率估计的标准误 sigma1_hat</span><br><span class="line">sqrt(sigmahat^2/sum((x-mean(x))^2))</span><br><span class="line">sigma1hat = summary(fit)$coeff[2,2] # summary(fit)$coefficients 是系数矩阵，[2,2]是斜率的标准误</span><br><span class="line"># 提取斜率的估计值 beta1_hat</span><br><span class="line">beta1hat = summary(fit)$coeff[2,1] # [2,1]是斜率的估计值</span><br><span class="line"># 手动计算 beta1_hat 的95%置信区间下限和上限</span><br><span class="line">beta1hat + qt(0.025, df = 198) * sigma1hat # df = n-p = 200-2 = 198</span><br><span class="line">beta1hat + qt(0.975, df = 198) * sigma1hat</span><br><span class="line"># 与R内置函数结果比较</span><br><span class="line">confint(fit)</span><br><span class="line"></span><br><span class="line">rm(list=ls()) # 清除工作空间中的所有对象</span><br><span class="line"># 置信区间的覆盖率模拟</span><br><span class="line">count = 0</span><br><span class="line">N = 10000 # 模拟次数</span><br><span class="line">for (i in 1:N)&#123;</span><br><span class="line">  # 观测数量</span><br><span class="line">  sample.size = 30</span><br><span class="line">  # 预测变量</span><br><span class="line">  x = 1:sample.size;</span><br><span class="line">  # 真实截距和斜率</span><br><span class="line">  beta0 = 5; beta1 = 2;</span><br><span class="line">  # 真实误差项</span><br><span class="line">  error = rnorm(n = sample.size, mean = 0, sd = 3)</span><br><span class="line">  # 响应变量</span><br><span class="line">  y = beta0 + beta1 * x + error;</span><br><span class="line">  # 存入数据框</span><br><span class="line">  sim_data = data.frame(x=x, y=y);</span><br><span class="line">  # 拟合简单线性模型</span><br><span class="line">  simple.fit = lm(formula = y~x, data = sim_data)</span><br><span class="line">  # 获取斜率beta1的置信区间</span><br><span class="line">  beta1.CI = confint(simple.fit)[2, ] # 提取斜率的置信区间</span><br><span class="line">  # 如果真实beta1包含在计算得到的置信区间内，则计数</span><br><span class="line">  count = count + as.numeric((beta1.CI[1] &lt;= beta1) &amp; (beta1&lt;=beta1.CI[2]))</span><br><span class="line">&#125;</span><br><span class="line">count / N # 计算覆盖率，应接近0.95</span><br><span class="line"></span><br><span class="line"># 预测区间</span><br><span class="line">new.df = data.frame(x = 7) # 给定新的x值</span><br><span class="line"># 点预测值</span><br><span class="line">predict(simple.fit, newdata = new.df, interval = &quot;none&quot;) # 或者不指定 interval</span><br><span class="line"># 对 E[y|x=7] = beta0 + beta1 * 7 的置信区间</span><br><span class="line">predict(simple.fit, newdata = new.df, interval = &quot;confidence&quot;)</span><br><span class="line"># 对单个y (当x=7时) 的预测区间</span><br><span class="line">predict(simple.fit, newdata = new.df, interval = &quot;prediction&quot;)</span><br><span class="line"># y的条件均值 (真实值)</span><br><span class="line">beta0 + beta1 * 7</span><br><span class="line"># 某个随机误差</span><br><span class="line">(error = rnorm(n = 1, mean = 0, sd = 3))</span><br><span class="line"># 某个真实的y值</span><br><span class="line">beta0 + beta1 * 7 + error</span><br><span class="line"></span><br><span class="line"># 95%预测区间的思想类似于95%置信区间</span><br><span class="line">count = 0</span><br><span class="line">N = 1000 # 模拟次数</span><br><span class="line">sample.size = 30</span><br><span class="line">x = 1:sample.size;</span><br><span class="line">beta0 = 5; beta1 = 2;</span><br><span class="line">new.df = data.frame(x = 7) # 新的x值</span><br><span class="line">for (i in 1:N)&#123;</span><br><span class="line">  error = rnorm(n = sample.size, mean = 0, sd = 3) # 生成样本误差</span><br><span class="line">  y = beta0 + beta1 * x + error; # 生成样本y</span><br><span class="line">  sim_data = data.frame(x=x, y=y);</span><br><span class="line">  simple.fit = lm(formula = y~x, data = sim_data) # 拟合模型</span><br><span class="line">  x7.PI = predict(simple.fit, newdata = new.df, interval = &quot;prediction&quot;)[2:3] # 获取预测区间的下限和上限</span><br><span class="line">  error = rnorm(n = 1, mean = 0, sd = 3) # 为新的观测值生成一个随机误差</span><br><span class="line">  true.Y = beta0 + beta1 * 7 + error # 真实的新的Y值</span><br><span class="line">  # 如果真实的Y值包含在预测区间内，则计数</span><br><span class="line">  count = count + as.numeric((x7.PI[1] &lt;= true.Y) &amp; (true.Y&lt;=x7.PI[2]))</span><br><span class="line">&#125;</span><br><span class="line">count / N # 计算覆盖率，应接近0.95</span><br><span class="line"></span><br><span class="line">rm(list=ls()) # 清除工作空间</span><br><span class="line">library(s20x) # 加载s20x包</span><br><span class="line"># 观测数量 20</span><br><span class="line">sample.size = 20</span><br><span class="line"># 预测变量</span><br><span class="line">x = 1:sample.size;</span><br><span class="line"># 真实截距 5 和斜率 2</span><br><span class="line">beta0 = 5; beta1 = 2;</span><br><span class="line"># 真实误差 Normal(0, sigma = 3)</span><br><span class="line">error = rnorm(n = sample.size, mean = 0, sd = 3)</span><br><span class="line"># 响应变量</span><br><span class="line">y = beta0 + beta1 * x + error;</span><br><span class="line"># 存入数据框</span><br><span class="line">sim_data = data.frame(x=x, y=y);</span><br><span class="line"># 拟合简单线性模型</span><br><span class="line">simple.fit = lm(formula = y~x, data = sim_data)</span><br><span class="line"># 提取估计的截距和斜率</span><br><span class="line">coef(simple.fit)</span><br><span class="line"># 拟合值，即给定x的条件y值，</span><br><span class="line"># 其中x值是原始数据中观测到的</span><br><span class="line">coef(simple.fit)[1] + coef(simple.fit)[2] * sim_data$x</span><br><span class="line"># R中的内置函数</span><br><span class="line">yhat = fitted.values(simple.fit)</span><br><span class="line"># 残差</span><br><span class="line">res = sim_data$y - yhat</span><br><span class="line"># 将所有内容放入同一个数据框</span><br><span class="line">sim_data$yhat = yhat</span><br><span class="line">sim_data$res = res</span><br><span class="line">head(sim_data) # 显示数据框前几行</span><br><span class="line"># 绘制两个变量的散点图</span><br><span class="line">plot(y~x, data = sim_data)</span><br><span class="line"># 绘制估计的回归线</span><br><span class="line">abline(coef(simple.fit), col = &quot;red&quot;)</span><br><span class="line"># 绘制真实的双变量关系线</span><br><span class="line">abline(c(5, 2), col = &quot;blue&quot;, lty = 2) # c(intercept, slope)</span><br><span class="line"># 检查线性性和方差齐性 (EOV)</span><br><span class="line">plot(simple.fit, which = 1)</span><br><span class="line"># 更多用于线性性和EOV的可视化辅助</span><br><span class="line">trendscatter(res ~ yhat) # s20x包的函数，残差对拟合值的趋势散点图</span><br><span class="line"># 正态性检查</span><br><span class="line">normcheck(simple.fit) # s20x包的函数</span><br><span class="line"># 强影响点检查</span><br><span class="line">cooks20x(simple.fit) # s20x包的函数</span><br><span class="line"></span><br><span class="line"># 样本量 200</span><br><span class="line">sample.size = 200</span><br><span class="line"># 当样本量较小时，我们用于</span><br><span class="line"># 线性性、EOV和正态性的诊断工具可能过于敏感 (假阳性)</span><br><span class="line"># 当样本量足够大时，它们更可靠</span><br><span class="line"></span><br><span class="line"># 违反EOV (方差齐性) 的情况</span><br><span class="line">error = rnorm(n = sample.size, mean = 0, sd = x) # 误差标准差随x变化</span><br><span class="line"># 非恒定的散点，Q-Q图也可能偏离</span><br><span class="line"># 注意回归线仍然离真实情况不远</span><br><span class="line"># eovcheck中的误差带是不正确的 (可能是指s20x包中eovcheck函数的特定行为)</span><br><span class="line"></span><br><span class="line"># 违反正态性的情况</span><br><span class="line">error = error^2 # 使误差项非正态 (卡方分布)</span><br><span class="line"># 非恒定的散点，Q-Q图和直方图都表明有问题</span><br><span class="line"># 回归线不再接近真实情况</span><br></pre></td></tr></table></figure>

</code></pre>
</li>
</ul>
</li>
</ol>
<h3 id="Lecture-3：曲线拟合与分类解释变量-Fitting-Curves-and-Categorical-Explanatory-Variables">Lecture 3：曲线拟合与分类解释变量 (Fitting Curves and Categorical Explanatory Variables)</h3>
<p>这个lecture主要涵盖了两个重要主题：1) <strong>使用线性模型拟合曲线关系</strong>，特别是二次多项式回归；2) <strong>处理双水平分类解释变量</strong>。</p>
<ol>
<li>
<p><strong>拟合曲线关系 (Fitting Curves)</strong>:</p>
<ul>
<li>
<p>识别曲线关系</p>
<p>：通过散点图 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot()</span><br></pre></td></tr></table></figure>
<p>,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">trendscatter()</span><br></pre></td></tr></table></figure>
<p>) 和简单线性回归的残差对拟合值图 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot(fit, which=1)</span><br></pre></td></tr></table></figure>
<p>) 来识别数据中是否存在非线性（曲线）趋势。如果残差图显示出系统性模式（如U形或倒U形），则表明简单线性模型可能不适用。这部分对应教材</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">STATS201 book SWU 2023.pdf</span><br></pre></td></tr></table></figure>
<p>第4章“使用线性模型拟合曲线”的引言部分 。</p>
</li>
<li>
<p>二次模型：当检测到曲线关系时，可以尝试添加解释变量的二次项 (如X2) 到模型中，即</p>
<p>Y=β0+β1X+β2X2+ϵ。在R中，这通过</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">I(Assign^2)</span><br></pre></td></tr></table></figure>
<p>实现，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">I()</span><br></pre></td></tr></table></figure>
<p>函数确保</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">^2</span><br></pre></td></tr></table></figure>
<p>按数学上的平方运算执行，而不是R公式中的特殊含义。教材第4章对此有详细讨论。</p>
</li>
<li>
<p><strong>模型比较与可视化</strong>：比较简单线性模型和二次模型的残差图，看后者是否更好地消除了系统性模式。通过 <code>lines()</code> 和 <code>predict()</code> 函数在原始散点图上绘制两个模型的拟合线，直观比较拟合效果。</p>
</li>
<li>
<p><strong>模型外推风险</strong>：强调了将模型用于解释变量范围之外的数据进行预测（外推）是非常危险的，因为模型的形式在数据范围之外可能完全不适用。</p>
</li>
</ul>
</li>
<li>
<p><strong>处理双水平分类解释变量 (2-level Categorical Explanatory Variable)</strong>:</p>
<ul>
<li>
<p>数据类型转换：分类数据在R中应为因子 (factor)</p>
<p>类型。如果原始数据是字符型，需要使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">as.factor()</span><br></pre></td></tr></table></figure>
<p>进行转换。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">str()</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">summary()</span><br></pre></td></tr></table></figure>
<p>函数可以帮助查看和理解因子变量的结构和水平。这部分内容与教材第5章“具有2级分类（因子）解释变量的线性模型”的开始部分以及第1章中处理数据框的部分相关 。</p>
</li>
<li>
<p><strong>因子水平顺序</strong>：可以使用 <code>factor(variable, levels = c(&quot;level_preferred_as_baseline&quot;, &quot;other_level&quot;))</code> 来改变因子水平的顺序，这将影响 <code>lm()</code> 函数中选择哪个水平作为<strong>基准水平 (baseline level)</strong>。</p>
</li>
<li>
<p><strong>可视化</strong>：对于一个数值响应变量和一个二元分类解释变量，<code>plot(Y ~ X_factor)</code> 会自动生成并排的<strong>箱线图 (boxplots)</strong>，这是比较两组分布的有效方式。<code>summaryStats(Y, X_factor)</code> (来自<code>s20x</code>包) 可提供按组的描述性统计。</p>
</li>
<li>
<p>模型拟合与解释：</p>
<ul>
<li>可以直接在 <code>lm()</code> 中使用因子型解释变量，如 <code>lm(Exam ~ Attend, data = Stats20x.df)</code>。R会自动为因子变量创建<strong>指示变量 (indicator variables, or dummy variables)</strong>。如果因子有k个水平，通常会创建k-1个指示变量，其中一个水平作为基准。</li>
<li>对于双水平因子（如&quot;No&quot;, &quot;Yes&quot;），如果&quot;No&quot;是基准，则模型是 E[Exam]=β0+β1×IAttend=Yes。β0是&quot;No&quot;组的均值Exam，β1是&quot;Yes&quot;组相对于&quot;No&quot;组的均值Exam的差异。教材第5章详细解释了这一点。</li>
<li>也可以手动创建数值指示变量 (0/1)，然后用其进行回归，结果是等价的，但这通常不如直接使用因子方便。</li>
</ul>
</li>
<li>
<p><strong>模型诊断</strong>：对于这类模型，线性性假设通常不是主要关注点（因为解释变量只有两个有效值）。但<strong>方差齐性 (EOV)</strong> 和残差的<strong>正态性</strong>仍然需要通过 <code>plot(fit, which=1)</code> 和 <code>normcheck(fit)</code> 进行检查。</p>
</li>
<li>
<p><strong>置信区间与预测区间</strong>：<code>predict()</code> 函数可以用于获取各组的均值置信区间和个体预测区间。</p>
</li>
</ul>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">rm<span class="punctuation">(</span><span class="built_in">list</span><span class="operator">=</span>ls<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 清除工作空间</span></span><br><span class="line">library<span class="punctuation">(</span>s20x<span class="punctuation">)</span> <span class="comment"># 加载s20x包</span></span><br><span class="line">getwd<span class="punctuation">(</span><span class="punctuation">)</span> <span class="comment"># 获取当前工作目录</span></span><br><span class="line"><span class="comment"># setwd() # 设置工作目录 (如果需要)</span></span><br><span class="line">Stats20x.df <span class="operator">=</span> read.table<span class="punctuation">(</span> <span class="comment"># 加载数据</span></span><br><span class="line">  <span class="string">&quot;STATS20x.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 第一步是做探索性分析</span></span><br><span class="line">plot<span class="punctuation">(</span>Exam<span class="operator">~</span>Assign<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># Exam对Assign的散点图</span></span><br><span class="line"><span class="comment"># 有用的s20x绘图函数</span></span><br><span class="line">trendscatter<span class="punctuation">(</span>Exam<span class="operator">~</span>Assign<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># 带趋势线的散点图</span></span><br><span class="line"><span class="comment"># 第一个模型 (简单线性回归)</span></span><br><span class="line">examassign.fit <span class="operator">=</span> lm<span class="punctuation">(</span>Exam<span class="operator">~</span>Assign<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 检查线性性 + 方差齐性 (EOV)</span></span><br><span class="line">plot<span class="punctuation">(</span>examassign.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 有一些曲率的迹象</span></span><br><span class="line"><span class="comment"># 添加一个二次项</span></span><br><span class="line">examassign.fit2 <span class="operator">=</span> lm<span class="punctuation">(</span>Exam<span class="operator">~</span>Assign<span class="operator">+</span>I<span class="punctuation">(</span>Assign<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># I()确保^按数学运算处理</span></span><br><span class="line"><span class="comment"># 再次检查线性性 + EOV</span></span><br><span class="line">plot<span class="punctuation">(</span>examassign.fit2<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 看起来好一些了</span></span><br><span class="line"><span class="comment"># 图片通常是模型的一个很好的展示</span></span><br><span class="line">plot<span class="punctuation">(</span>Exam<span class="operator">~</span>Assign<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span></span><br><span class="line">x <span class="operator">=</span> <span class="number">0</span><span class="operator">:</span><span class="number">20</span> <span class="comment"># 用于评估 beta1_hat + beta2_hat * x 的x取值范围</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span> predict<span class="punctuation">(</span>examassign.fit<span class="punctuation">,</span></span><br><span class="line">                 data.frame<span class="punctuation">(</span>Assign<span class="operator">=</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 绘制简单线性模型拟合线</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span> predict<span class="punctuation">(</span>examassign.fit2<span class="punctuation">,</span></span><br><span class="line">                 data.frame<span class="punctuation">(</span>Assign<span class="operator">=</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span> <span class="comment"># 绘制二次模型拟合线</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 理解模型在数据范围之外的行为</span></span><br><span class="line">x <span class="operator">=</span> seq<span class="punctuation">(</span><span class="operator">-</span><span class="number">15</span><span class="punctuation">,</span> <span class="number">25</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">0.1</span><span class="punctuation">)</span> <span class="comment"># 更宽的x取值范围</span></span><br><span class="line">y <span class="operator">=</span> predict<span class="punctuation">(</span>examassign.fit2<span class="punctuation">,</span> data.frame<span class="punctuation">(</span>Assign<span class="operator">=</span>x<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 使用二次模型预测</span></span><br><span class="line">plot<span class="punctuation">(</span>y<span class="operator">~</span>x<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;l&quot;</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 绘制整个预测曲线</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">[</span>x<span class="operator">&gt;</span><span class="number">0</span> <span class="operator">&amp;</span> x<span class="operator">&lt;</span><span class="number">20</span><span class="punctuation">]</span><span class="punctuation">,</span> y<span class="punctuation">[</span>x<span class="operator">&gt;</span><span class="number">0</span> <span class="operator">&amp;</span> x<span class="operator">&lt;</span><span class="number">20</span><span class="punctuation">]</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span> <span class="comment"># 突出显示原始数据范围内的拟合</span></span><br><span class="line">abline<span class="punctuation">(</span>v<span class="operator">=</span><span class="built_in">range</span><span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Assign<span class="punctuation">)</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">2</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;grey&quot;</span><span class="punctuation">)</span> <span class="comment"># 标记原始数据x的范围</span></span><br><span class="line">abline<span class="punctuation">(</span>h<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">)</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">2</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;grey&quot;</span><span class="punctuation">)</span> <span class="comment"># 标记y的合理范围 (如考试分数0-100)</span></span><br><span class="line"><span class="comment"># 在数据范围之外使用模型总是存在风险的</span></span><br><span class="line"></span><br><span class="line">rm<span class="punctuation">(</span><span class="built_in">list</span><span class="operator">=</span>ls<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 清除工作空间</span></span><br><span class="line">library<span class="punctuation">(</span>s20x<span class="punctuation">)</span> <span class="comment"># 加载s20x包</span></span><br><span class="line">Stats20x.df <span class="operator">=</span> read.table<span class="punctuation">(</span></span><br><span class="line">  <span class="string">&quot;STATS20x.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 二元分类变量</span></span><br><span class="line">head<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># 查看Attend变量的前几行</span></span><br><span class="line"><span class="comment"># 字符型</span></span><br><span class="line">summary<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># 字符型变量的summary给出长度、类别、模式</span></span><br><span class="line"><span class="comment"># 将字符型转换为因子型</span></span><br><span class="line">Stats20x.df<span class="operator">$</span>Attend <span class="operator">=</span> as.factor<span class="punctuation">(</span></span><br><span class="line">  Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 存储方式不同</span></span><br><span class="line">str<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># 查看因子型变量的结构，会显示levels</span></span><br><span class="line">head<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># 因子型变量的summary给出各level的频数</span></span><br><span class="line"><span class="comment"># 我们可以改变level的顺序</span></span><br><span class="line">Stats20x.df<span class="operator">$</span>Attend <span class="operator">=</span> factor<span class="punctuation">(</span></span><br><span class="line"> Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">,</span> level <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Yes&quot;</span><span class="punctuation">,</span> <span class="string">&quot;No&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 将&quot;Yes&quot;设为第一个level (基准level会改变)</span></span><br><span class="line">summary<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># &quot;Yes&quot;在前，&quot;No&quot;在后</span></span><br><span class="line">head<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># Summary已经能提供很多信息了</span></span><br><span class="line">summaryStats<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Exam<span class="punctuation">,</span></span><br><span class="line">             Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># s20x包函数，按Attend分组计算Exam的描述统计</span></span><br><span class="line"><span class="comment"># 绘图更直观</span></span><br><span class="line">plot<span class="punctuation">(</span>Exam<span class="operator">~</span>Attend<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># 自动生成箱线图</span></span><br><span class="line"><span class="comment"># Hack数据格式以便使用trendscatter</span></span><br><span class="line">Stats20x.df<span class="operator">$</span>Attend2 <span class="operator">=</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span></span><br><span class="line">  Stats20x.df<span class="operator">$</span>Attend <span class="operator">==</span> <span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span> <span class="comment"># 创建数值型指示变量 (Yes=1, No通常为0，但这里取决于因子顺序)</span></span><br><span class="line"><span class="comment"># 上面代码取决于Attend的因子水平顺序。如果&quot;Yes&quot;是第一水平，则Yes为1，No为2。</span></span><br><span class="line"><span class="comment"># 如果要Yes=1, No=0，且原始因子水平是c(&quot;No&quot;, &quot;Yes&quot;)，则应为 as.numeric(Stats20x.df$Attend == &quot;Yes&quot;)</span></span><br><span class="line"><span class="comment"># 或者更稳妥的是: Stats20x.df$Attend2 = ifelse(Stats20x.df$Attend == &quot;Yes&quot;, 1, 0)</span></span><br><span class="line">with<span class="punctuation">(</span>Stats20x.df<span class="punctuation">,</span> table<span class="punctuation">(</span>Attend<span class="punctuation">,</span>Attend2<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 交叉表验证转换</span></span><br><span class="line">trendscatter<span class="punctuation">(</span>Exam<span class="operator">~</span>Attend2<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># 使用数值化的Attend2绘图</span></span><br><span class="line"><span class="comment"># 简单地对关系建模</span></span><br><span class="line">examattend2.fit <span class="operator">=</span> lm<span class="punctuation">(</span>Exam<span class="operator">~</span>Attend2<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># 使用数值型指示变量建模</span></span><br><span class="line">summary<span class="punctuation">(</span>examattend2.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 注意相似之处和不同之处</span></span><br><span class="line">examattend.fit <span class="operator">=</span> lm<span class="punctuation">(</span>Exam<span class="operator">~</span>Attend<span class="punctuation">,</span> data <span class="operator">=</span> Stats20x.df<span class="punctuation">)</span> <span class="comment"># 直接使用因子型变量建模</span></span><br><span class="line">summary<span class="punctuation">(</span>examattend.fit<span class="punctuation">)</span> <span class="comment"># R会自动处理因子变量的哑变量编码</span></span><br><span class="line"><span class="comment"># 在这种情况下不需要线性性检查 (因为解释变量只有两个点)</span></span><br><span class="line">plot<span class="punctuation">(</span>examattend.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图仍然可以看方差齐性</span></span><br><span class="line"><span class="comment"># EOV (方差齐性)</span></span><br><span class="line">summaryStats<span class="punctuation">(</span>Stats20x.df<span class="operator">$</span>Exam<span class="punctuation">,</span> Stats20x.df<span class="operator">$</span>Attend<span class="punctuation">)</span> <span class="comment"># 按组查看标准差</span></span><br><span class="line"><span class="comment"># 正态性</span></span><br><span class="line">normcheck<span class="punctuation">(</span>examattend.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 强影响点</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>examattend.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 解释</span></span><br><span class="line">coef<span class="punctuation">(</span>summary<span class="punctuation">(</span>examattend.fit<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 系数解释</span></span><br><span class="line">confint<span class="punctuation">(</span>examattend.fit<span class="punctuation">)</span> <span class="comment"># 置信区间</span></span><br><span class="line"><span class="comment"># 在这种情况下，两种类型的CI密切相关</span></span><br><span class="line">preds.df <span class="operator">=</span> data.frame<span class="punctuation">(</span>Attend <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;No&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Yes&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 注意level顺序</span></span><br><span class="line">predict<span class="punctuation">(</span>examattend.fit<span class="punctuation">,</span> preds.df<span class="punctuation">,</span> interval <span class="operator">=</span> <span class="string">&quot;confidence&quot;</span><span class="punctuation">)</span> <span class="comment"># 对均值的置信区间</span></span><br><span class="line">predict<span class="punctuation">(</span>examattend.fit<span class="punctuation">,</span> preds.df<span class="punctuation">,</span> interval <span class="operator">=</span> <span class="string">&quot;prediction&quot;</span><span class="punctuation">)</span> <span class="comment"># 对个体的预测区间</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Lecture-4：数据转换-Data-Transformations-特别是Log转换">Lecture 4：数据转换 (Data Transformations) - 特别是Log转换</h3>
<p>这个lecture的核心是<strong>数据转换 (Data Transformations)</strong>，特别是<strong>对数转换 (log transformation)</strong> 在处理右偏数据、非线性关系以及解释模型时的应用。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第6章“乘法线性模型 (Multiplicative linear models)”  紧密相关。</p>
<ol>
<li>
<p><strong>处理右偏数据与正态性</strong>:</p>
<ul>
<li>
<p>许多现实世界的数据（尤其是涉及金额、计数的数据，如房价 <code>Houses.df$price</code>、理发花费 <code>hair.df$hair</code>）是<strong>右偏 (right-skewed)</strong> 的。</p>
</li>
<li>
<p>直接对这类数据拟合线性模型可能会违反<strong>正态性假设</strong>和<strong>方差齐性假设</strong>。</p>
</li>
<li>
<p>对数转换 <code>log()</code>通常可以使右偏数据更对称，更接近正态分布，从而满足线性模型的假设。如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(log(price)~1, data=Houses.df)</span><br></pre></td></tr></table></figure>
<p>后</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">normcheck()</span><br></pre></td></tr></table></figure>
<p>显示改善。教材第6章 (第13页) 展示了对数转换如何使房价数据更接近正态。</p>
</li>
</ul>
</li>
<li>
<p><strong>均值 vs. 中位数</strong>:</p>
<ul>
<li>
<p>对于高度偏斜的数据，<strong>中位数 (median)</strong> 通常比均值 (mean) 更能代表数据的“典型”或中心趋势。</p>
</li>
<li>
<p>当对响应变量 Y 进行对数转换拟合模型 log(Y)=β0+β1X+ϵ 时，反转换 exp(β^0+β^1X) 得到的是 Y 的<strong>中位数估计</strong>，而不是均值估计。</p>
</li>
<li>
<p>因此，通过</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">exp(confint(LoggedPriceNull.fit))</span><br></pre></td></tr></table></figure>
<p>得到的置信区间是针对总体中位数的。教材第6章 (第14-17页) 详细解释了为何对数转换后的推断是关于中位数的。</p>
</li>
<li>
<p>代码中还演示了使用<strong>自助法 (bootstrap)</strong> 来估计均值的置信区间，并与基于CLT的<code>confint(HousesNull.fit)</code>结果比较。</p>
</li>
</ul>
</li>
<li>
<p><strong>处理非线性关系 (对数线性模型)</strong>:</p>
<ul>
<li>
<p>当响应变量与解释变量之间的关系不是直线，而是曲线（特别是指数增长或衰减型，如马自达汽车价格随车龄 <code>price ~ age</code> 的关系），对响应变量取对数 (<code>log(price) ~ age</code>) 可能使关系线性化。</p>
</li>
<li>
<p>这种模型 log(Y)=β0+β1X+ϵ 称为<strong>对数-线性模型 (log-linear model)</strong>。</p>
</li>
<li>
<p>拟合后，需要检查残差图 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot(LogPriceAge.fit, which = 1)</span><br></pre></td></tr></table></figure>
<p>) 和正态性 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">normcheck(LogPriceAge.fit)</span><br></pre></td></tr></table></figure>
<p>) 是否在新尺度上满足假设。教材第6章 (第35-37页) 展示了马自达数据的这个过程。</p>
</li>
</ul>
</li>
<li>
<p><strong>解释对数转换后的模型系数</strong>:</p>
<ul>
<li>
<p>对于对数-线性模型</p>
<p>log(Y)=β0+β1X:</p>
<ul>
<li>β^1 表示当 X 改变一个单位时，log(Y) 的平均变化。</li>
<li>exp(β^1) 表示当 X 改变一个单位时，Y 的中位数的<strong>乘性因子</strong>。</li>
<li>(exp(β^1)−1)×100% 表示当 X 改变一个单位时，Y 的中位数的<strong>百分比变化</strong>。</li>
</ul>
</li>
<li>
<p>代码中演示了如何计算和解释车龄每增加一年，马自达汽车价格中位数的百分比下降 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100 * (exp(confint(LogPriceAge.fit)[2,])-1)</span><br></pre></td></tr></table></figure>
<p>)。教材第6章 (第44页) 有详细的解释。</p>
</li>
<li>
<p>对于多年变化，如5年折旧，可以通过 (exp(β^1))5</p>
<p>或exp(5×β^1)来估计累积的乘性效应。教材第6章 (第47-49页)讨论了多年折旧。</p>
</li>
</ul>
</li>
<li>
<p><strong>对数转换与分类解释变量</strong>:</p>
<ul>
<li>当解释变量是分类变量时（如性别 <code>sex</code>），对数转换同样适用。</li>
<li>模型 log(Y)=β0+β1Isex=male 中，exp(β^1) 表示男性相对于女性（基准组）Y 中位数的比率。</li>
<li>(exp(β^1)−1)×100% 是男性相对于女性 Y 中位数的百分比差异。</li>
<li>代码中分析了理发花费 <code>log(hair)</code> 与性别 <code>sex</code> 的关系。</li>
</ul>
</li>
</ol>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library<span class="punctuation">(</span>s20x<span class="punctuation">)</span> <span class="comment"># 加载s20x包</span></span><br><span class="line">Houses.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;AkldHousePrices.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取奥克兰房价数据</span></span><br><span class="line">head<span class="punctuation">(</span>Houses.df<span class="operator">$</span>price<span class="punctuation">)</span> <span class="comment"># 查看房价数据前几行 (单位：千美元)</span></span><br><span class="line"><span class="comment"># 正态性存疑</span></span><br><span class="line">hist<span class="punctuation">(</span>Houses.df<span class="operator">$</span>price<span class="punctuation">,</span> breaks<span class="operator">=</span><span class="number">20</span><span class="punctuation">,</span>main<span class="operator">=</span><span class="string">&quot;&quot;</span><span class="punctuation">,</span>xlab<span class="operator">=</span><span class="string">&quot;Price ($1000)&quot;</span><span class="punctuation">)</span> <span class="comment"># 绘制房价直方图</span></span><br><span class="line">abline<span class="punctuation">(</span>v <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span>mean<span class="punctuation">(</span>Houses.df<span class="operator">$</span>price<span class="punctuation">)</span><span class="punctuation">,</span> median<span class="punctuation">(</span>Houses.df<span class="operator">$</span>price<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> <span class="string">&quot;green&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> lwd <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 添加均值和中位数线</span></span><br><span class="line">summary<span class="punctuation">(</span>Houses.df<span class="operator">$</span>price<span class="punctuation">)</span> <span class="comment"># 房价的描述统计</span></span><br><span class="line">HousesNull.fit<span class="operator">=</span>lm<span class="punctuation">(</span>price<span class="operator">~</span><span class="number">1</span><span class="punctuation">,</span> data<span class="operator">=</span>Houses.df<span class="punctuation">)</span> <span class="comment"># 拟合空模型 (仅截距)</span></span><br><span class="line">normcheck<span class="punctuation">(</span>HousesNull.fit<span class="punctuation">)</span> <span class="comment"># 对原始房价数据的残差进行正态性检验 (等同于对数据本身)</span></span><br><span class="line">nrow<span class="punctuation">(</span>Houses.df<span class="punctuation">)</span> <span class="comment"># 样本量是合理的 (94)</span></span><br><span class="line"><span class="comment"># 模拟样本均值 (自助法 Bootstrap)</span></span><br><span class="line"><span class="comment"># 注意：代码中 House.df$price 应该是 Houses.df$price (笔误)</span></span><br><span class="line">bootstrappedMeanPrices <span class="operator">=</span> double<span class="punctuation">(</span><span class="number">10000</span><span class="punctuation">)</span> <span class="comment"># 初始化一个长度为10000的向量</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">10000</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">  tmp.vec <span class="operator">=</span> sample<span class="punctuation">(</span>Houses.df<span class="operator">$</span>price<span class="punctuation">,</span> <span class="comment"># 此处修正了变量名</span></span><br><span class="line">                   size <span class="operator">=</span> nrow<span class="punctuation">(</span>Houses.df<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                   replace <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 有放回抽样</span></span><br><span class="line">  y.bar <span class="operator">=</span> mean<span class="punctuation">(</span>tmp.vec<span class="punctuation">)</span> <span class="comment"># 计算重抽样样本的均值</span></span><br><span class="line">  bootstrappedMeanPrices<span class="punctuation">[</span>i<span class="punctuation">]</span> <span class="operator">=</span> y.bar</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">hist<span class="punctuation">(</span>bootstrappedMeanPrices<span class="punctuation">)</span> <span class="comment"># 根据中心极限定理 (CLT)，样本均值的分布应接近正态</span></span><br><span class="line">quantile<span class="punctuation">(</span>bootstrappedMeanPrices<span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">.025</span><span class="punctuation">,</span> <span class="number">.975</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># Bootstrap置信区间</span></span><br><span class="line">confint<span class="punctuation">(</span>HousesNull.fit<span class="punctuation">)</span> <span class="comment"># 基于正态假设的均值置信区间 (与t检验结果一致)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Log转换</span></span><br><span class="line">LoggedPriceNull.fit<span class="operator">=</span>lm<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>price<span class="punctuation">)</span><span class="operator">~</span><span class="number">1</span><span class="punctuation">,</span> data<span class="operator">=</span>Houses.df<span class="punctuation">)</span> <span class="comment"># 对log(price)拟合空模型</span></span><br><span class="line">normcheck<span class="punctuation">(</span>LoggedPriceNull.fit<span class="punctuation">)</span> <span class="comment"># 检查log(price)的正态性</span></span><br><span class="line">confint<span class="punctuation">(</span>LoggedPriceNull.fit<span class="punctuation">)</span> <span class="comment"># log尺度上均值(也是中位数)的置信区间</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>LoggedPriceNull.fit<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 反转换回原始尺度，得到中位数的置信区间</span></span><br><span class="line"></span><br><span class="line">Mazda.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;mazda.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="built_in">T</span><span class="punctuation">)</span> <span class="comment"># 读取马自达汽车数据</span></span><br><span class="line">head<span class="punctuation">(</span>Mazda.df<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 我们需要自己创建一个名为age的新变量</span></span><br><span class="line">Mazda.df<span class="operator">$</span>age <span class="operator">=</span> <span class="number">91</span> <span class="operator">-</span> Mazda.df<span class="operator">$</span>year <span class="comment"># 数据收集于1991年</span></span><br><span class="line">head<span class="punctuation">(</span>Mazda.df<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 绘制数据散点图</span></span><br><span class="line">trendscatter<span class="punctuation">(</span>price <span class="operator">~</span> age<span class="punctuation">,</span> data <span class="operator">=</span> Mazda.df<span class="punctuation">)</span> <span class="comment"># s20x包函数，价格对车龄的趋势散点图</span></span><br><span class="line"><span class="comment"># 简单线性模型</span></span><br><span class="line">PriceAge.fit <span class="operator">=</span> lm<span class="punctuation">(</span>price <span class="operator">~</span> age<span class="punctuation">,</span> data <span class="operator">=</span> Mazda.df<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>PriceAge.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图，违反线性性假设</span></span><br><span class="line">normcheck<span class="punctuation">(</span>PriceAge.fit<span class="punctuation">)</span> <span class="comment"># 同时伴有正态性问题</span></span><br><span class="line"><span class="comment"># 尝试二次模型</span></span><br><span class="line">PriceAge.quad.fit <span class="operator">=</span> lm<span class="punctuation">(</span>price <span class="operator">~</span> age<span class="operator">+</span>I<span class="punctuation">(</span>age<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> Mazda.df<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>PriceAge.quad.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 线性性可以了，但是...</span></span><br><span class="line">normcheck<span class="punctuation">(</span>PriceAge.quad.fit<span class="punctuation">)</span> <span class="comment"># 并没有解决正态性问题 (通常是异方差)</span></span><br><span class="line"><span class="comment"># 对price取log可以移除非线性趋势</span></span><br><span class="line">trendscatter<span class="punctuation">(</span>price <span class="operator">~</span> age<span class="punctuation">,</span> data <span class="operator">=</span> Mazda.df<span class="punctuation">)</span> <span class="comment"># 原始数据</span></span><br><span class="line">trendscatter<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>price<span class="punctuation">)</span> <span class="operator">~</span> age<span class="punctuation">,</span> data <span class="operator">=</span> Mazda.df<span class="punctuation">)</span> <span class="comment"># log(price) vs age</span></span><br><span class="line">LogPriceAge.fit <span class="operator">=</span> lm<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>price<span class="punctuation">)</span> <span class="operator">~</span> age<span class="punctuation">,</span> data <span class="operator">=</span> Mazda.df<span class="punctuation">)</span> <span class="comment"># 对数线性模型</span></span><br><span class="line">plot<span class="punctuation">(</span>LogPriceAge.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 线性性和EOV看起来都不错了</span></span><br><span class="line">normcheck<span class="punctuation">(</span>LogPriceAge.fit<span class="punctuation">)</span> <span class="comment"># 正态性也可以了</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>LogPriceAge.fit<span class="punctuation">)</span> <span class="comment"># 没有过分影响的点</span></span><br><span class="line">summary<span class="punctuation">(</span>LogPriceAge.fit<span class="punctuation">)</span> <span class="comment"># 现在如何解释斜率？</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反转换 (Backtransform)</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>LogPriceAge.fit<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 获取截距和斜率(的指数)的置信区间</span></span><br><span class="line"><span class="comment"># 反转换为百分比差异</span></span><br><span class="line"><span class="number">100</span> <span class="operator">*</span> <span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>LogPriceAge.fit<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 对斜率的解释：车龄每增加1年，价格中位数变化的百分比</span></span><br><span class="line"><span class="comment"># 对斜率的解释: exp(beta1_hat) 是车龄增加1年时，价格中位数的乘性因子。</span></span><br><span class="line"><span class="comment"># (exp(beta1_hat) - 1) * 100% 是百分比变化。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5年内</span></span><br><span class="line"><span class="comment"># exp(confint(LogPriceAge.fit)[2,])^5 # 这是 (exp(beta1))^5 的置信区间，表示5年后的乘性因子</span></span><br><span class="line"><span class="comment"># 100 * (exp(confint(LogPriceAge.fit)[2,])^5 - 1) # 5年后价格中位数的总百分比变化</span></span><br><span class="line"><span class="comment"># 应该是 100 * (exp(5 * confint(LogPriceAge.fit)[2,]) - 1)</span></span><br><span class="line"><span class="comment"># 或者解释为：5年车龄的马自达汽车的中位数价格</span></span><br><span class="line"><span class="comment"># 预计比原始价值低52.76%到58.9%。(这里原文解释的是5年内的总折旧率)</span></span><br><span class="line"><span class="comment"># 更准确的说法: 5年车龄的马自达汽车的价值是新车价值的 exp(5*beta1_hat) 倍。</span></span><br><span class="line"><span class="comment"># 其置信区间是 exp(5 * lower_beta1) 到 exp(5 * upper_beta1)。</span></span><br><span class="line"><span class="comment"># 或者说，相比于车龄X的车，车龄X+5的车的价格中位数会变为原来的 exp(5*beta1) 倍。</span></span><br><span class="line"><span class="comment"># 所以这里原文的注释可能指 (exp(beta1_CI_lower))^5 和 (exp(beta1_CI_upper))^5</span></span><br><span class="line"><span class="comment"># 表示5年折旧的累积效应。</span></span><br><span class="line"><span class="comment"># 如教材P48所示，解释为 &quot;the median price of Mazdas drops between 52.8% and 58.9% over 5 years&quot;</span></span><br><span class="line"><span class="comment"># 计算方式是 100 * (exp(confint(LogPriceAge.fit)[2,])^5 - 1) 或 100 * (exp(5 * confint(LogPriceAge.fit)[2,]) -1)</span></span><br><span class="line"><span class="comment"># 后者更常见于直接解释5年总变化。前者是每年变化率的5次累积。</span></span><br><span class="line"><span class="comment"># 在教材P49， Case Study 6.1 中使用的是 exp(5 * confint(LogPriceAge.fit)[2,]) - 1</span></span><br><span class="line"><span class="comment"># 而代码中是 exp(confint(LogPriceAge.fit)[2,])^5 - 1。这两个在数值上可能略有差异但概念相似。</span></span><br><span class="line"></span><br><span class="line">survey.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;survey.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span> stringsAsFactors <span class="operator">=</span> <span class="built_in">T</span><span class="punctuation">)</span> <span class="comment"># 读取调查数据</span></span><br><span class="line"><span class="comment"># 为了使事情稍微不那么混乱，我们将数据放入其自己的数据框中</span></span><br><span class="line">hair.df <span class="operator">=</span> with<span class="punctuation">(</span>survey.df<span class="punctuation">,</span> data.frame<span class="punctuation">(</span>hair <span class="operator">=</span> hair<span class="punctuation">,</span> sex <span class="operator">=</span> sex<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 提取理发花费和性别</span></span><br><span class="line">plot<span class="punctuation">(</span>hair <span class="operator">~</span> sex<span class="punctuation">,</span> data <span class="operator">=</span> hair.df<span class="punctuation">)</span> <span class="comment"># 理发花费对性别的箱线图</span></span><br><span class="line"><span class="comment"># 女性在理发上的花费似乎比男性多。</span></span><br><span class="line"><span class="comment"># 我们可以看到数据是相当右偏的。方差齐性似乎也有问题。</span></span><br><span class="line"><span class="comment"># 也许取对数会有帮助。</span></span><br><span class="line">plot<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>hair<span class="punctuation">)</span> <span class="operator">~</span> sex<span class="punctuation">,</span> data <span class="operator">=</span> hair.df<span class="punctuation">)</span> <span class="comment"># log(理发花费) 对 性别的箱线图</span></span><br><span class="line"><span class="comment"># 对数尺度/对数线性模型</span></span><br><span class="line">hair.fit <span class="operator">=</span> lm<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>hair<span class="punctuation">)</span> <span class="operator">~</span> sex<span class="punctuation">,</span> data <span class="operator">=</span> hair.df<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>hair.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图</span></span><br><span class="line">normcheck<span class="punctuation">(</span>hair.fit<span class="punctuation">)</span> <span class="comment"># 正态性检查</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>hair.fit<span class="punctuation">)</span> <span class="comment"># 强影响点检查</span></span><br><span class="line">summary<span class="punctuation">(</span>hair.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 将反转换后的输出列绑定在一起</span></span><br><span class="line">cbind<span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>coef<span class="punctuation">(</span>hair.fit<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>hair.fit<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 估计值和置信区间 (中位数尺度)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>hair.fit<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 百分比差异解释</span></span><br><span class="line">pred.df <span class="operator">=</span> data.frame<span class="punctuation">(</span>sex <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;female&quot;</span><span class="punctuation">,</span> <span class="string">&quot;male&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 注意因子水平顺序</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>predict<span class="punctuation">(</span>hair.fit<span class="punctuation">,</span> pred.df<span class="punctuation">,</span> interval <span class="operator">=</span> <span class="string">&quot;confidence&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 预测各性别理发花费中位数的置信区间</span></span><br></pre></td></tr></table></figure>
<h3 id="Lecture-5：幂律模型-Power-Law-Models">Lecture 5：幂律模型 (Power Law Models)</h3>
<p>这个lecture主要讲解<strong>幂律模型 (Power law linear models)</strong>，通常通过对响应变量和解释变量都进行<strong>对数转换</strong>来实现。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第7章“幂律线性模型”  的内容一致。</p>
<ol>
<li><strong>幂律关系</strong>:
<ul>
<li>当变量之间的关系被假设为 Y=αXβ1 形式时，称为幂律关系。这种关系在生物学（如体重与长度）、物理学等领域很常见。</li>
<li>对这个等式两边取对数，得到 log(Y)=log(α)+β1log(X)。令β0=log(α)，则模型变为log(Y)=β0+β1log(X)。这正是关于log(Y)和 log(X)的一个简单线性模型，也称为对数-对数模型 (log-log model)。教材第7章 (第9-10页) 推导了这个过程。</li>
<li>代码中以鲷鱼的重量 (<code>wgt</code>) 与长度 (<code>len</code>) 为例，展示了原始尺度上非线性的关系，而在双对数尺度上 (<code>log(wgt) ~ log(len)</code>) 呈现线性关系。</li>
</ul>
</li>
<li><strong>模型拟合与诊断</strong>:
<ul>
<li>使用 <code>lm(log(wgt) ~ log(len), data=Snap.df)</code> 拟合模型。</li>
<li>与简单线性回归一样，需要进行模型诊断：
<ul>
<li><code>plot(Snap.lm, which=1)</code>：检查残差图的线性性和方差齐性。</li>
<li><code>normcheck(Snap.lm)</code>：检查残差的正态性。</li>
<li><code>cooks20x(Snap.lm)</code>：检查强影响点。</li>
</ul>
</li>
<li>教材第7章 (第12-14页) 展示了这些诊断步骤。</li>
</ul>
</li>
<li><strong>模型解释</strong>:
<ul>
<li>在对数-对数模型log(Y)=β0+β1log(X)中，斜率β1有特殊的解释：
<ul>
<li>β1 表示 X 每改变1%，Y 的中位数大约改变 β1% (这是一个近似，仅在 β1 较小且百分比变化较小时较准确)。</li>
<li>更精确地，如果 X 变为 kX（即 X 乘以因子 k），则 Y 的中位数变为 kβ1Ymedian。也就是说，Y 的中位数被乘以 kβ1。</li>
<li>代码中演示了当长度增加1% (即 k=1.01) 时，重量中位数乘以 1.01β1。以及当长度增加50% (即 k=1.5) 时，重量中位数乘以 1.5β1。教材第7章 (第24-26页) 详细解释了这种解释方法。</li>
</ul>
</li>
</ul>
</li>
<li><strong>预测</strong>:
<ul>
<li>在对数尺度上进行预测，然后使用 <code>exp()</code> 函数反转换回原始尺度，得到的是响应变量 Y 的<strong>中位数</strong>的点估计和区间估计。</li>
<li><code>predict(Snap.lm, newdata, interval=&quot;confidence&quot;)</code> 后取指数，得到的是给定 log(X) 时 E[log(Y)] (即 log(Median(Y))) 的置信区间。</li>
<li><code>predict(Snap.lm, newdata, interval=&quot;prediction&quot;)</code> 后取指数，得到的是给定 log(X) 时单个新 log(Y) 值的预测区间，反转换后是单个新 Y 值的预测区间（通常解释为中位数的预测）。</li>
<li>教材第7章 (第19页) 演示了如何预测30cm鲷鱼的重量中位数。</li>
</ul>
</li>
<li><strong>特定假设检验</strong>:
<ul>
<li>可以对斜率 β1 进行特定值的假设检验，例如，在生物学中，重量通常与长度的立方成正比（如果形状保持不变），这意味着在对数-对数模型中，β1 可能接近3。</li>
<li>代码中演示了如何手动计算t统计量和p值来检验H0:β1=3。这在教材第7章 (第20-21页) 也有提及。</li>
</ul>
</li>
</ol>
<p>总而言之，Lecture 5 介绍了如何通过双对数转换来处理幂律关系，将其转化为线性模型进行分析，并重点讲解了这种模型下系数的独特解释方式。</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">Snap.df<span class="operator">=</span>read.table<span class="punctuation">(</span><span class="string">&quot;SnapWgt.txt&quot;</span><span class="punctuation">,</span>header<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取鲷鱼(Snapper)重量数据</span></span><br><span class="line">str<span class="punctuation">(</span>Snap.df<span class="punctuation">)</span> <span class="comment"># 查看数据结构 (len: 长度, wgt: 重量)</span></span><br><span class="line">plot<span class="punctuation">(</span>wgt<span class="operator">~</span>len<span class="punctuation">,</span> data <span class="operator">=</span> Snap.df<span class="punctuation">)</span> <span class="comment"># 绘制重量对长度的散点图</span></span><br><span class="line">plot<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>wgt<span class="punctuation">)</span><span class="operator">~</span><span class="built_in">log</span><span class="punctuation">(</span>len<span class="punctuation">)</span><span class="punctuation">,</span> <span class="comment"># 绘制 log(重量) 对 log(长度) 的散点图</span></span><br><span class="line">     data<span class="operator">=</span>Snap.df<span class="punctuation">,</span></span><br><span class="line">     xlab<span class="operator">=</span><span class="string">&quot;log(Length)&quot;</span><span class="punctuation">,</span></span><br><span class="line">     ylab<span class="operator">=</span><span class="string">&quot;log(Weight)&quot;</span><span class="punctuation">)</span></span><br><span class="line">Snap.lm<span class="operator">=</span>lm<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>wgt<span class="punctuation">)</span><span class="operator">~</span><span class="built_in">log</span><span class="punctuation">(</span>len<span class="punctuation">)</span><span class="punctuation">,</span>data<span class="operator">=</span>Snap.df<span class="punctuation">)</span> <span class="comment"># 拟合对数-对数模型 (幂律模型)</span></span><br><span class="line">plot<span class="punctuation">(</span>Snap.lm<span class="punctuation">,</span>which<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差对拟合值图，检查线性性和EOV</span></span><br><span class="line">normcheck<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span> <span class="comment"># 正态性检查</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span> <span class="comment"># 强影响点检查</span></span><br><span class="line">summary<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span> <span class="comment"># 查看模型摘要</span></span><br><span class="line"><span class="comment"># 在对数尺度上绘图</span></span><br><span class="line">plot<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>wgt<span class="punctuation">)</span><span class="operator">~</span><span class="built_in">log</span><span class="punctuation">(</span>len<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">     data<span class="operator">=</span>Snap.df<span class="punctuation">,</span></span><br><span class="line">     xlab<span class="operator">=</span><span class="string">&quot;log(Length)&quot;</span><span class="punctuation">,</span></span><br><span class="line">     ylab<span class="operator">=</span><span class="string">&quot;log(Weight)&quot;</span><span class="punctuation">)</span></span><br><span class="line">abline<span class="punctuation">(</span>coef<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span><span class="punctuation">,</span>lty<span class="operator">=</span><span class="number">5</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 添加拟合线</span></span><br><span class="line"><span class="comment"># 在原始尺度上绘图</span></span><br><span class="line">plot<span class="punctuation">(</span>wgt<span class="operator">~</span>len<span class="punctuation">,</span> data <span class="operator">=</span> Snap.df<span class="punctuation">)</span></span><br><span class="line">pred.df <span class="operator">=</span> data.frame<span class="punctuation">(</span>len <span class="operator">=</span> <span class="number">20</span><span class="operator">:</span><span class="number">90</span><span class="punctuation">)</span> <span class="comment"># 创建用于预测的长度序列</span></span><br><span class="line">Snap.pred <span class="operator">=</span> <span class="built_in">exp</span><span class="punctuation">(</span>predict<span class="punctuation">(</span>Snap.lm<span class="punctuation">,</span> pred.df<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 预测log(wgt)并反转换回wgt (中位数)</span></span><br><span class="line">lines<span class="punctuation">(</span>pred.df<span class="operator">$</span>len<span class="punctuation">,</span> Snap.pred<span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 在原始尺度图上添加拟合曲线</span></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">Pred.df<span class="operator">=</span>data.frame<span class="punctuation">(</span>len<span class="operator">=</span><span class="number">30</span><span class="punctuation">)</span> <span class="comment"># 预测长度为30cm的鲷鱼</span></span><br><span class="line"><span class="comment"># 长度为30cm的鲷鱼的重量中位数</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>predict<span class="punctuation">(</span></span><br><span class="line">  Snap.lm<span class="punctuation">,</span>Pred.df<span class="punctuation">,</span></span><br><span class="line">  interval<span class="operator">=</span><span class="string">&quot;confidence&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 对log(wgt)的均值(即log(中位数))进行区间估计，然后反转换</span></span><br><span class="line"><span class="comment"># 一条长度为30cm的鲷鱼的重量 (个体预测)</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>predict<span class="punctuation">(</span></span><br><span class="line">  Snap.lm<span class="punctuation">,</span>Pred.df<span class="punctuation">,</span></span><br><span class="line">  interval<span class="operator">=</span><span class="string">&quot;prediction&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 对单个log(wgt)进行区间预测，然后反转换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重量和长度的关系是立方的吗？ (即检验 log(len) 的系数是否为3)</span></span><br><span class="line">summary<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span> <span class="comment"># 查看 log(len) 的系数估计值和标准误</span></span><br><span class="line">confint<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span> <span class="comment"># 查看 log(len) 系数的置信区间，看是否包含3</span></span><br><span class="line"><span class="comment"># 对应的p值是多少？(用于检验 H0: beta1 = 3)</span></span><br><span class="line">beta1 <span class="operator">=</span> coef<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span> <span class="comment"># 提取 log(len) 的系数估计值</span></span><br><span class="line">seBeta1 <span class="operator">=</span> summary<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span><span class="operator">$</span>coefficients<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="comment"># 提取 log(len) 系数的标准误</span></span><br><span class="line">hyp <span class="operator">=</span> <span class="number">3</span> <span class="comment"># 假设的系数值</span></span><br><span class="line">tstat <span class="operator">=</span> <span class="punctuation">(</span>beta1 <span class="operator">-</span> hyp<span class="punctuation">)</span><span class="operator">/</span>seBeta1 <span class="comment"># 计算t统计量</span></span><br><span class="line">tstat</span><br><span class="line">pval <span class="operator">=</span> <span class="number">2</span> <span class="operator">*</span> <span class="punctuation">(</span><span class="number">1</span> <span class="operator">-</span> pt<span class="punctuation">(</span> <span class="comment"># 计算双边p值</span></span><br><span class="line">  <span class="built_in">abs</span><span class="punctuation">(</span>tstat<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">  df <span class="operator">=</span> nrow<span class="punctuation">(</span>Snap.df<span class="punctuation">)</span> <span class="operator">-</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># df = n - p (p=2, 截距和斜率)</span></span><br><span class="line">pval</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如何解释beta1 (log(len)的系数)</span></span><br><span class="line">confint<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="comment"># log(len)系数的置信区间</span></span><br><span class="line"><span class="comment"># 如果x增加1%，那么y的中位数大约增加beta1%。</span></span><br><span class="line"><span class="comment"># (1+0.01)^beta1_hat approx 1 + beta1_hat*0.01</span></span><br><span class="line"><span class="comment"># 所以y的中位数乘以 (1+0.01)^beta1_hat，即增加了 ((1+0.01)^beta1_hat - 1)*100%</span></span><br><span class="line"><span class="number">1.01</span><span class="operator">^</span>confint<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="comment"># x增加1% (乘以1.01)，y的中位数乘以的因子范围</span></span><br><span class="line"><span class="comment"># 对于大的百分比变化不适用这种近似解释</span></span><br><span class="line"><span class="comment"># 如果x增加50% (乘以1.5)</span></span><br><span class="line"><span class="number">1.5</span><span class="operator">^</span>confint<span class="punctuation">(</span>Snap.lm<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="comment"># y的中位数乘以的因子范围</span></span><br><span class="line"><span class="comment"># (1.5^confint(Snap.lm)[2,] - 1) * 100% 是y的中位数增加的百分比范围</span></span><br></pre></td></tr></table></figure>
<h3 id="Lecture-6：包含数值和因子解释变量的线性模型-Linear-Models-with-Numeric-and-Factor-Explanatory-Variables-交互作用模型">Lecture 6：包含数值和因子解释变量的线性模型 (Linear Models with Numeric and Factor Explanatory Variables) - 交互作用模型</h3>
<p>这个lecture主要讨论了<strong>包含一个数值解释变量和一个因子（分类）解释变量的线性模型</strong>，特别是<strong>交互作用模型 (interaction model)</strong>。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第8章“包含数值和因子解释变量的线性模型 第1部分：交互作用模型”  的内容相对应。</p>
<ol>
<li>
<p><strong>交互作用的概念与可视化</strong>:</p>
<ul>
<li>
<p>当一个数值解释变量（如 <code>Test</code> 分数）对响应变量（如 <code>Exam</code> 分数）的影响<strong>取决于</strong>另一个因子解释变量（如 <code>Attend</code> 是否出勤）的水平时，就存在交互作用。</p>
</li>
<li>
<p>通过绘制不同因子水平下的散点图和拟合线来可视化交互作用。例如，</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot(Exam ~ Test, pch = substr(Attend, 1, 1), col = ...)</span><br></pre></td></tr></table></figure>
<p>后，分别为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Attend=&quot;Yes&quot;</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Attend=&quot;No&quot;</span><br></pre></td></tr></table></figure>
<p>的学生群体绘制拟合的回归线。如果这两条线不平行，则暗示可能存在交互作用。教材第8章 (第4-9页)展示了类似的可视化。</p>
</li>
</ul>
</li>
<li>
<p><strong>构建交互作用模型</strong>:</p>
<ul>
<li>
<p>在R的 <code>lm()</code> 函数中，交互作用可以通过 <code>X1 * X2</code> 的形式指定，这等价于 <code>X1 + X2 + X1:X2</code>。其中 <code>X1</code> 是数值变量，<code>X2</code> 是因子变量，<code>X1:X2</code> 是交互作用项。</p>
</li>
<li>
<p>模型形式为</p>
<p>Y=β0+β1X1+β2IX2=level2+β3(X1×IX2=level2)+ϵ</p>
<p>(以</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">X2</span><br></pre></td></tr></table></figure>
<p>为双水平因子为例)。</p>
<ul>
<li>对于基准水平 (如 <code>Attend=&quot;No&quot;</code>，IAttend=Yes=0)：Y=β0+β1X1+ϵ。</li>
<li>对于另一水平 (如 <code>Attend=&quot;Yes&quot;</code>，IAttend=Yes=1)：Y=(β0+β2)+(β1+β3)X1+ϵ。</li>
<li>β3 代表了因子不同水平下，数值变量 X1 的<strong>斜率差异</strong>。如果 β3 显著不为零，则表明存在交互作用。</li>
</ul>
</li>
<li>
<p>教材第8章 (第11-17页) 详细解释了模型的构建和参数含义。</p>
</li>
</ul>
</li>
<li>
<p><strong>模型诊断</strong>:</p>
<ul>
<li>
<p>与之前的线性模型一样，需要检查残差图 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot(fit, which=1)</span><br></pre></td></tr></table></figure>
<p>for EOV and linearity for each group's fit), 正态性 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">normcheck(fit)</span><br></pre></td></tr></table></figure>
<p>), 和强影响点 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cooks20x(fit)</span><br></pre></td></tr></table></figure>
<p>). 教材第8章 (第18-20页)也强调了这些步骤。</p>
</li>
</ul>
</li>
<li>
<p><strong>解释交互作用模型</strong>:</p>
<ul>
<li>
<p><code>summary(fit)</code> 会给出交互作用项 (<code>Test:AttendYes</code>) 的系数、标准误、t值和p值。</p>
</li>
<li>
<p>如果交互作用项显著 (p值较小)，则表明数值变量的效应确实因因子变量的水平而异。此时，不能孤立地解释主效应（如<code>Test</code>或<code>Attend</code>的系数），而应分别描述在因子各水平下数值变量的效应。</p>
</li>
<li>
<p>例如，对于<code>Attend=&quot;No&quot;</code>的学生，<code>Test</code>每增加1分，<code>Exam</code>平均增加 β^Test 分。对于<code>Attend=&quot;Yes&quot;</code>的学生，<code>Test</code>每增加1分，<code>Exam</code>平均增加 β^Test+β^Test:AttendYes 分。</p>
</li>
<li>
<pre><code>confint(fit)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">      可以给出所有参数（包括交互作用项）的置信区间。教材第8章 (第21-28页及32-35页) 提供了详细的解释和置信区间计算。</span><br><span class="line"></span><br><span class="line">5. **不显著的交互作用 (Simpson&#x27;s Paradox 类似情景)**:</span><br><span class="line"></span><br><span class="line">   - 第二个例子（甲状腺重量 `thyroid` vs. 体重 `body` 和处理方式 `trt`）展示了一个重要情况：初始观察 `plot(thyroid ~ trt)` 可能显示因子效应，但在调整了数值协变量 (`body`) 后，并且考虑到交互作用不显著时 (`summary(thyroid.fit)` 显示 `body:trt` 不显著)，主效应模型 (`lm(thyroid ~ body + trt)`) 可能显示因子不再显著。</span><br><span class="line">   - 这是因为因子不同水平间的响应差异可能完全由协变量的差异引起。例如，药物组的老鼠甲状腺较重，可能仅仅因为药物组的老鼠本身就比较重 (`plot(body ~ trt)` 显示药物组老鼠体重较大)。这种情况提示我们在解释因子效应时需要小心控制混淆变量。这部分内容实际上预示了第9章“不含交互作用的模型”的重要性。</span><br><span class="line"></span><br><span class="line">总结来说，Lecture 6 教授了如何在包含数值和因子解释变量时考虑和建模它们之间的交互作用，如何解释这种模型，以及在解释时需注意潜在的混淆效应。</span><br><span class="line"></span><br><span class="line">```r</span><br><span class="line">library(s20x) # 加载s20x包</span><br><span class="line">Stats20x.df = read.table(&quot;STATS20x.txt&quot;, header = T) # 读取数据</span><br><span class="line">Stats20x.df$Attend = as.factor(Stats20x.df$Attend) # 将Attend转换为因子</span><br><span class="line">plot(Exam ~ Test, data = Stats20x.df,</span><br><span class="line">     pch = substr(Attend, 1, 1), cex = 0.7, # 点的形状根据Attend的第一个字母 (&quot;Y&quot;或&quot;N&quot;)</span><br><span class="line">     col = ifelse(Attend == &quot;Yes&quot;, &quot;blue&quot;, &quot;red&quot;)) # 点的颜色根据Attend的值</span><br><span class="line"># Test分数与Exam分数的散点图，</span><br><span class="line"># 表明在每个出勤组（“Yes”或“No”）内，Test和Exam之间的正相关关系是合理线性的，</span><br><span class="line"># 但两组的斜率可能不同。</span><br><span class="line"></span><br><span class="line">## 模型构建和假设检查</span><br><span class="line">examTestAttend.fit = lm(Exam ~ Test * Attend, data = Stats20x.df) # 构建带交互作用的模型</span><br><span class="line"># Test * Attend 等价于 Test + Attend + Test:Attend</span><br><span class="line">plot(examTestAttend.fit, which = 1) # 残差对拟合值图</span><br><span class="line">normcheck(examTestAttend.fit) # 正态性检查</span><br><span class="line">cooks20x(examTestAttend.fit) # 强影响点检查</span><br><span class="line">summary(examTestAttend.fit) # 查看模型摘要</span><br><span class="line">confint(examTestAttend.fit) # 查看系数的置信区间</span><br><span class="line"></span><br><span class="line">## 可视化最终模型</span><br><span class="line">predAttend.df = data.frame(Test = 1:21, Attend = &quot;Yes&quot;) # 为&quot;Yes&quot;组创建预测数据</span><br><span class="line">predSlackers.df = data.frame(Test = 1:21, Attend = &quot;No&quot;) # 为&quot;No&quot;组创建预测数据</span><br><span class="line">plot(Exam ~ Test, data = Stats20x.df,pch = substr(Attend, 1, 1), cex = 0.7,</span><br><span class="line">     col = ifelse(Attend == &quot;Yes&quot;, &quot;blue&quot;, &quot;red&quot;)) # 重新绘制原始散点图</span><br><span class="line">lines(1:21, predict(examTestAttend.fit, predAttend.df), col = &quot;blue&quot;, lty = 2) # 绘制&quot;Yes&quot;组的拟合线</span><br><span class="line">lines(1:21, predict(examTestAttend.fit, predSlackers.df), col = &quot;red&quot;, lty = 2) # 绘制&quot;No&quot;组的拟合线</span><br><span class="line"></span><br><span class="line">thyroid.df = read.table(&quot;Thyroid.txt&quot;, header = TRUE) # 读取甲状腺数据</span><br><span class="line">head(thyroid.df) # 查看数据前几行</span><br><span class="line">str(thyroid.df) # 查看数据结构</span><br><span class="line"># 为药物组vs对照组创建一个因子变量</span><br><span class="line">thyroid.df$trt = with(</span><br><span class="line">  thyroid.df, factor(</span><br><span class="line">    ifelse(group == 1, &quot;control&quot;, &quot;drug&quot;))) # group=1为对照组，否则为药物组</span><br><span class="line"># 这似乎表明药物有用</span><br><span class="line">plot(thyroid ~ trt, data = thyroid.df, xlab = &quot;Treatment&quot;, ylab = &quot;Thyroid weights&quot;) # 甲状腺重量对处理方式的箱线图</span><br><span class="line"># 当以体重为条件时，效果不明显</span><br><span class="line">plot(thyroid ~ body, type = &quot;n&quot;, data = thyroid.df) # 空白散点图框架</span><br><span class="line">text(thyroid.df$body, thyroid.df$thyroid, thyroid.df$group) # 用组别标签绘制点</span><br><span class="line"># 我们应该调整体重差异</span><br><span class="line">thyroid.fit = lm(thyroid ~ body * trt, data = thyroid.df) # 拟合带交互作用的模型</span><br><span class="line">plot(thyroid.fit, which = 1) # 线性性和EOV似乎都还好</span><br><span class="line">normcheck(thyroid.fit) # 正态性可能有问题，可能需要使用模拟</span><br><span class="line">cooks20x(thyroid.fit) # 观测点12是否影响过大？</span><br><span class="line">cooks.distance(thyroid.fit)[12] # 非常接近0.4的阈值</span><br><span class="line">plot(thyroid ~ body, type = &quot;n&quot;, data = thyroid.df) # 重新绘制框架</span><br><span class="line">text(thyroid.df$body, thyroid.df$thyroid, 1:16) # 用观测序号绘制点，以识别影响点</span><br><span class="line"></span><br><span class="line"># 模型构建和假设检查</span><br><span class="line">summary(thyroid.fit) # 交互作用项不显著</span><br><span class="line">thyroid.fit2 = lm(thyroid ~ body + trt, data = thyroid.df) # 拟合无交互作用（加性）模型</span><br><span class="line">summary(thyroid.fit2) # 调整后的R平方证实了这一点</span><br><span class="line"># 处理(trt)似乎不显著</span><br><span class="line">thyroid.fit3 = lm(thyroid ~ body, data = thyroid.df) # 仅用body拟合模型</span><br><span class="line">summary(thyroid.fit3)</span><br><span class="line"># 为什么`trt`不显著？</span><br><span class="line">plot(body ~ trt, data = thyroid.df, main = &quot;&quot;, xlab = &quot;Treatment&quot;, ylab = &quot;Mouse weight&quot;) # 小鼠体重对处理方式的箱线图</span><br><span class="line"># 图中的差异是由于较重的小鼠甲状腺较重，</span><br><span class="line"># 而药物组的小鼠较重。</span><br></pre></td></tr></table></figure>

</code></pre>
</li>
</ul>
</li>
</ol>
<h3 id="Lecture-7">Lecture 7</h3>
<p>这个lecture继续探讨包含一个数值变量 (<code>IQ</code>) 和一个因子变量 (<code>method</code>) 的线性模型，重点关注当<strong>交互作用不显著</strong>时的模型简化和解释。这直接对应教材 <code>STATS201 book SWU 2023.pdf</code> 第9章“包含数值和因子解释变量的线性模型 第2部分：不含交互作用的模型”。</p>
<ol>
<li>
<p><strong>从交互作用模型到主效应模型</strong>:</p>
<ul>
<li>
<p>首先，如Lecture 6所示，拟合一个包含交互作用的模型 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(lang ~ IQ * method, data = teach.df)</span><br></pre></td></tr></table></figure>
<p>)。</p>
</li>
<li>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">anova(teach.fit)</span><br></pre></td></tr></table></figure>
<p>来检验交互作用项的显著性。如果交互作用不显著 (P值较大)，则根据奥卡姆剃刀原则 (Occam's Razor)或KISS原则 (Keep It Simple, Statistician)，应移除交互作用项，简化模型。教材第9章 (第14-18页) 详细讨论了模型选择和奥卡姆剃刀。</p>
</li>
<li>
<p>简化后的模型称为主效应模型或加性模型，形式为</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(lang ~ IQ + method, data = teach.df)</span><br></pre></td></tr></table></figure>
<p>。在这个模型中，数值变量 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">IQ</span><br></pre></td></tr></table></figure>
<p>) 的效应（斜率）对于因子变量 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">method</span><br></pre></td></tr></table></figure>
<p>) 的所有水平都是相同的，但因子变量的不同水平有不同的截距。在图形上表现为一组平行线。</p>
</li>
</ul>
</li>
<li>
<p><strong>解释主效应模型</strong>:</p>
<ul>
<li>
<pre><code>summary(teach.fit2)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">会给出主效应模型中 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
IQ
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">的系数 (共同斜率) 和 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
method
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   各水平相对于基准水平的截距差异。 </span><br><span class="line"></span><br><span class="line">- 例如，如果基准是 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
method1
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">     ，模型为E[lang]=β0+β1IQ+β2Imethod=2+β3Imethod=3。</span><br><span class="line"></span><br><span class="line">     - β1 是IQ对语言分数的共同效应。</span><br><span class="line">     - β0 是 `method1` 组在IQ为0时的期望语言分数 (通常IQ为0无实际意义，所以截距的绝对值解释需谨慎，但差异是有意义的)。</span><br><span class="line">     - β2 是 `method2` 组相对于 `method1` 组的平均语言分数差异 (控制IQ后)。</span><br><span class="line">     - β3 是 `method3` 组相对于 `method1` 组的平均语言分数差异 (控制IQ后)。</span><br><span class="line"></span><br><span class="line">   - 教材第9章 (第21页) 描述了这种模型的公式和参数解释。</span><br><span class="line"></span><br><span class="line">3. **更改基准水平 (Releveling)**:</span><br><span class="line"></span><br><span class="line">   - `summary()` 和 `confint()` 的输出是相对于当前基准水平的。为了得到所有因子水平之间的两两比较的置信区间（例如，`method2` vs `method3`），需要更改基准水平并重新拟合模型。</span><br><span class="line"></span><br><span class="line">   - 使用 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
relevel(factor_variable, ref = &quot;new_baseline_level&quot;)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">来更改基准。例如，</span><br><span class="line"></span><br></pre></td></tr></table></figure>
teach.df$method = relevel(teach.df$method, ref = &quot;2&quot;)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">将 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
method2
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">      设为新的基准。 </span><br><span class="line"></span><br><span class="line">   - 通过这种方式，可以获得所有配对比较的估计和置信区间。教材第9章 (第28-31页) 演示了如何更改基准水平并解释结果。</span><br><span class="line"></span><br><span class="line">4. **比较包含与不包含协变量的模型**:</span><br><span class="line"></span><br><span class="line">   - 代码中还比较了包含协变量 `IQ` 的模型 (`teach.fit2`) 和不包含 `IQ`、仅有 `method` 的模型 (`teach.fit5`，即单因素方差分析模型)。</span><br><span class="line">   - 通过比较 R2 或调整后的 R2 (`summary(model)$r.squared`, `summary(model)$adj.r.squared`)，可以看出加入协变量 `IQ` 后模型解释变异的百分比是否提高。</span><br><span class="line">   - 通常，如果协变量与响应变量相关，调整它会使得对因子效应的估计更精确（标准误更小），置信区间更窄。</span><br><span class="line"></span><br><span class="line">5. **模型诊断**:</span><br><span class="line"></span><br><span class="line">   - 对于最终选择的主效应模型，仍需进行标准的模型诊断，如检查残差图 (`plot(teach.fit2, which = 1)`), 正态性 (`normcheck(teach.fit2)`), 和强影响点 (`cooks20x(teach.fit2)`).</span><br><span class="line"></span><br><span class="line">总结：Lecture 7 侧重于在多解释变量（一个数值，一个因子）模型中，当交互作用不显著时，如何简化到主效应模型，并如何通过更改因子基准水平来全面解释因子各水平间的差异。同时，也展示了控制协变量的重要性。</span><br><span class="line"></span><br><span class="line">```r</span><br><span class="line">library(s20x) # 加载s20x包</span><br><span class="line">data(teach.df) # 加载s20x包中的teach.df数据集 (教学方法、IQ对语言分数的影响)</span><br><span class="line">head(teach.df) # 查看数据前几行</span><br><span class="line">str(teach.df) # 查看数据结构</span><br><span class="line"># 我们需要将method转换为因子变量</span><br><span class="line">teach.df$method = factor(teach.df$method)</span><br><span class="line">plot(lang ~ IQ, main = &quot;Language Score versus IQ (by method)&quot;, # 语言分数对IQ的散点图，按教学方法分组</span><br><span class="line">     pch = as.character(teach.df$method), data = teach.df) # 点的形状根据方法 (1,2,3)</span><br><span class="line"># 看起来有三条平行线。</span><br><span class="line"># 分数随着IQ的增加而增加，方法2的</span><br><span class="line">#得分最高，方法3得分最低。</span><br><span class="line"># 这些单独线条周围的变异性远低于</span><br><span class="line"># 在分开的图中看到的变异性。</span><br><span class="line"># 这里有方差分析 (Anova) 的思想!!!</span><br><span class="line">teach.fit = lm(lang ~ IQ * method, data = teach.df) # 拟合带交互作用的模型</span><br><span class="line">plot(teach.fit, which = 1) # 残差对拟合值图</span><br><span class="line">normcheck(teach.fit) # 正态性检查</span><br><span class="line">cooks20x(teach.fit) # 强影响点检查</span><br><span class="line">anova(teach.fit) # 方差分析表，用于检验交互作用的显著性</span><br><span class="line">teach.fit2 = lm(lang ~ IQ + method, data = teach.df) # 拟合无交互作用 (加性/主效应) 模型</span><br><span class="line">plot(teach.fit2, which = 1) # 新模型的残差图</span><br><span class="line">normcheck(teach.fit2) # 新模型的正态性检查</span><br><span class="line">cooks20x(teach.fit2) # 新模型的强影响点检查</span><br><span class="line">anova(teach.fit2) # 新模型的方差分析表</span><br><span class="line">summary(teach.fit2) # 查看模型摘要</span><br><span class="line">confint(teach.fit2) # 系数的置信区间</span><br><span class="line"># 无交互作用但截距不同</span><br><span class="line">plot(lang ~ IQ, main = &quot;Language Score versus IQ (by method)&quot;,</span><br><span class="line">     pch = as.character(teach.df$method), data = teach.df) # 重新绘制散点图</span><br><span class="line">abline(teach.fit2$coef[1], teach.fit2$coef[2], lty = 1) # 为基准方法绘制拟合线 (平行线)</span><br><span class="line">abline(teach.fit2$coef[1] + teach.fit2$coef[3], teach.fit2$coef[2], lty = 2) # 为第二个方法绘制拟合线</span><br><span class="line">abline(teach.fit2$coef[1] + teach.fit2$coef[4], teach.fit2$coef[2], lty = 4) # 为第三个方法绘制拟合线</span><br><span class="line"># 更改基准水平以获得方法1的置信区间 (相对于方法2)</span><br><span class="line">teach.df$method = relevel(teach.df$method, ref = &quot;2&quot;) # 将方法2设为基准</span><br><span class="line">teach.fit3 = lm(lang ~ IQ + method, data = teach.df) # 重新拟合模型</span><br><span class="line"># 为什么查看这些模型的截距是有意义的</span><br><span class="line">confint(teach.fit2) # 原基准 (方法1) 下的置信区间</span><br><span class="line">confint(teach.fit3) # 新基准 (方法2) 下的置信区间 (可以得到方法1和方法3相对于方法2的比较)</span><br><span class="line"># 如果不调整IQ会发生什么？(即只用method作为解释变量 - 单因素方差分析)</span><br><span class="line">teach.fit4 = lm(lang ~ method, data = teach.df) # 注意，此时method的基准是&quot;2&quot;</span><br><span class="line">teach.df$method = relevel(teach.df$method, ref = &quot;1&quot;) # 将基准改回方法1</span><br><span class="line">teach.fit5 = lm(lang ~ method, data = teach.df)</span><br><span class="line">teach.df$method = relevel(teach.df$method, ref = &quot;3&quot;) # 将基准改为方法3</span><br><span class="line">teach.fit6 = lm(lang ~ method, data = teach.df)</span><br><span class="line">confint(teach.fit5) # 仅包含method时，各水平与方法1的比较</span><br><span class="line">confint(teach.fit6) # 仅包含method时，各水平与方法3的比较</span><br><span class="line">summary(teach.fit2)$r.squared # 包含IQ和method的模型的R方</span><br><span class="line">summary(teach.fit5)$r.squared # 仅包含method的模型的R方 (比较R方大小)</span><br><span class="line"></span><br><span class="line">summary(teach.fit2)$adj.r.squared # 调整后的R方</span><br><span class="line">summary(teach.fit5)$adj.r.squared # 调整后的R方</span><br></pre></td></tr></table></figure>

</code></pre>
</li>
</ul>
</li>
</ol>
<h3 id="Lecture-8：多重线性回归与共线性问题-Multiple-Linear-Regression-and-Multicollinearity">Lecture 8：多重线性回归与共线性问题 (Multiple Linear Regression and Multicollinearity)</h3>
<p>这个lecture主要涉及<strong>多重线性回归 (Multiple Linear Regression)</strong>，即模型中包含多个解释变量（可以是数值型或因子型），并初步探讨了<strong>多重共线性 (Multicollinearity)</strong> 问题。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第10章 “多重线性回归模型” 的内容一致。代码后半部分还涉及了单因素方差分析 (One-way ANOVA) 和 <code>emmeans</code> 包的使用，这与第11章内容相关。</p>
<p>这个lecture主要涵盖<strong>多重线性回归 (Multiple Linear Regression)</strong> 的模型构建策略、<strong>多重共线性 (Multicollinearity)</strong> 问题，以及<strong>单因素方差分析 (One-way ANOVA)</strong> 中的<strong>多重比较 (Multiple Comparisons)</strong> 问题。</p>
<ol>
<li>
<p><strong>多重线性回归与模型构建策略 (婴儿出生体重案例)</strong>:</p>
<ul>
<li>
<p>初始模型与变量筛选</p>
<p>：当有多个潜在解释变量时，一种策略是先拟合一个包含所有变量的“完整模型” (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(bwt ~ ., data = Babies.df)</span><br></pre></td></tr></table></figure>
<p>)，然后逐步剔除不显著的变量。但这种方法可能因多重共线性而出问题。</p>
</li>
<li>
<p>创建新变量</p>
<p>：有时需要根据理论或数据洞察创建新的解释变量，如从母亲的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">height</span><br></pre></td></tr></table></figure>
<p>计算出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bmi</span><br></pre></td></tr></table></figure>
<p>。</p>
</li>
<li>
<p>探索性数据分析 (EDA)</p>
<p>：在构建复杂模型前，使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pairs20x()</span><br></pre></td></tr></table></figure>
<p>(或标准</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pairs()</span><br></pre></td></tr></table></figure>
<p>) 和单独的散点图 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot()</span><br></pre></td></tr></table></figure>
<p>,</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lowess()</span><br></pre></td></tr></table></figure>
<p>) 来检查变量间的关系、识别非线性、异常值和潜在的交互作用。</p>
</li>
<li>
<p><strong>处理特殊关系</strong>：婴儿出生体重与孕龄(<code>gestation</code>)的关系呈现“曲棍球棒”形状（先增加后平缓或略降）。代码通过创建一个指示变量 <code>OD</code> (是否逾期，以294天为界) 并拟合交互作用模型 <code>lm(bwt ~ gestation * OD, ...)</code> 来尝试捕捉这种分段线性关系。这在教材第10章 (第17-21页) 有案例说明。</p>
</li>
<li>
<p><strong>逐步回归思想</strong>：代码中通过一系列模型 (<code>bwt.fit</code> 到 <code>bwt.fit9</code>) 展示了逐步添加变量并检查其显著性的过程，这是一种模型选择的方法。移除异常影响点 (<code>Babies.df[-c(239, 820), ]</code>) 也是模型构建的重要一步。教材第10章 (第22-43页) 详细演示了婴儿出生体重数据的逐步建模过程。</p>
</li>
<li>
<p>模型诊断</p>
<p>：对最终选择的模型 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bwt.fit9</span><br></pre></td></tr></table></figure>
<p>) 进行了残差图、正态性和影响点检查。</p>
</li>
</ul>
</li>
<li>
<p><strong>多重共线性 (Multicollinearity)</strong>:</p>
<ul>
<li>
<p>当模型中包含高度相关的解释变量时（如同时包含</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weight</span><br></pre></td></tr></table></figure>
<p>、</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">height</span><br></pre></td></tr></table></figure>
<p>和由它们计算出的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bmi</span><br></pre></td></tr></table></figure>
<p>），会导致参数估计的标准误增大，使得原本可能显著的变量变得不显著。这就是多重共线性问题。</p>
</li>
<li>
<p>代码中 <code>bwt.fit6</code> 的 <code>summary</code> 显示 <code>weight</code>, <code>height</code>, <code>bmi</code> 可能都不显著，而 <code>bwt.fit7</code> (移除了<code>weight</code>) 中 <code>height</code> 和 <code>bmi</code> 变得显著。这是共线性的一个典型表现。教材第10章 (第38-41页) 讨论了这个问题。</p>
</li>
<li>
<p>解决方法通常是移除导致共线性的变量之一，或使用其他降维技术（本课程未深入）。</p>
</li>
</ul>
</li>
<li>
<p><strong>单因素方差分析 (One-way ANOVA) (果蝇寿命案例)</strong>:</p>
<ul>
<li>
<p>当解释变量是一个有多于两个水平的因子时 (如 <code>group</code>)，我们实际上是在进行单因素方差分析。</p>
</li>
<li>
<p><code>lm(days ~ group, data = Fruitfly.df)</code> 拟合模型。</p>
</li>
<li>
<pre><code>anova(ff.fit)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">给出总的F检验，判断因子 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
group
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">是否显著影响响应变量 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
days
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">     。 </span><br><span class="line"></span><br><span class="line">   - `summary(ff.fit)` 显示各因子水平与基准水平的差异。</span><br><span class="line"></span><br><span class="line">   - 这部分内容与教材第11章 “具有单个多于两个水平的因子解释变量的线性模型（单因素方差分析）” 相关。</span><br><span class="line"></span><br><span class="line">4. **多重比较问题与 `emmeans` (果蝇寿命案例)**:</span><br><span class="line"></span><br><span class="line">   - 当一个因子有多个水平，并且我们想比较所有水平两两之间的均值差异时，会遇到多重比较问题：进行多次假设检验会增加至少犯一次第一类错误（错误地拒绝真实的原假设）的概率。 </span><br><span class="line"></span><br><span class="line">   - 模拟代码部分（`for (i in 1:1000)&#123;...&#125;`) 试图通过模拟演示在原假设为真（即所有组均值相同）的情况下，随机选择的两个组（特别是均值差异最大的两个组）进行比较时，仍有可能观察到“显著”差异的现象，这突显了多重比较校正的必要性。第二个模拟（关于二次模型参数的置信区间覆盖率）说明了单个参数CI的覆盖率约为95%，但所有参数同时被其CI覆盖的概率会更低。</span><br><span class="line"></span><br><span class="line">   - `emmeans` 包 (`library(emmeans)`) 提供了进行多重比较校正的工具。</span><br><span class="line"></span><br><span class="line">   - `emmeans(ff.fit, ~group)` 计算各组的估计边际均值。</span><br><span class="line"></span><br><span class="line">   - ```</span><br><span class="line">     pairs(emmeans_object, infer=TRUE)</span><br></pre></td></tr></table></figure>

 进行所有两两比较，并默认使用Tukey方法调整p值和置信区间，以控制总体错误率。 

</code></pre>
</li>
<li>
<p>教材第11章 (第18-24页) 详细介绍了多重比较问题和使用 <code>emmeans</code> 进行Tukey调整的方法。</p>
</li>
</ul>
</li>
</ol>
<p>总结：Lecture 8 覆盖了多重回归中的实际建模步骤、共线性这一重要问题，并引入了 ANOVA 后进行多重比较的概念和工具，这些都是统计建模中非常核心和实用的技能。</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library<span class="punctuation">(</span>s20x<span class="punctuation">)</span> <span class="comment"># 加载s20x包</span></span><br><span class="line">Babies.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;babies_data.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="built_in">T</span><span class="punctuation">)</span> <span class="comment"># 读取婴儿出生体重数据</span></span><br><span class="line"><span class="comment"># 拟合完整模型 (包含所有变量)</span></span><br><span class="line">bwt.all.fit <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> .<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">)</span> <span class="comment"># &quot;.&quot;代表数据框中除响应变量外的所有其他变量</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.all.fit<span class="punctuation">)</span></span><br><span class="line">bwt.no.age.fit <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> .<span class="operator">-</span>age<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">)</span> <span class="comment"># 从模型中移除age变量</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.no.age.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 母亲体重过轻或严重超重</span></span><br><span class="line"><span class="comment"># 会对婴儿健康产生负面影响，</span></span><br><span class="line"><span class="comment"># 但身高或体重都不能直接衡量这一点。</span></span><br><span class="line"><span class="comment"># 让我们从这两个测量值创建BMI (身体质量指数)</span></span><br><span class="line">Babies.df<span class="operator">$</span>bmi <span class="operator">=</span> with<span class="punctuation">(</span>Babies.df<span class="punctuation">,</span> weight<span class="operator">/</span><span class="punctuation">(</span>height<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span> <span class="operator">*</span> <span class="number">703</span><span class="punctuation">)</span> <span class="comment"># 注意单位转换因子703 (磅/英寸^2)</span></span><br><span class="line"><span class="comment"># 重新拟合新的完整模型 (包含BMI)</span></span><br><span class="line">bwt.all.fit <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> .<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">)</span> <span class="comment"># 再次拟合所有变量，现在包括了bmi</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.all.fit<span class="punctuation">)</span></span><br><span class="line">bwt.no.age.fit <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> .<span class="operator">-</span>age<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">)</span> <span class="comment"># 再次移除age</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.no.age.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 多重共线性 (当解释变量高度相关时出现的问题)</span></span><br><span class="line"><span class="comment"># 比从完整模型开始更好的方法</span></span><br><span class="line"><span class="comment"># 进行一些探索性分析</span></span><br><span class="line">pairs20x<span class="punctuation">(</span>Babies.df<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">,</span> <span class="number">5</span><span class="punctuation">,</span> <span class="number">6</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 绘制数值变量间的成对关系图 (bwt, gestation, age, height, weight)</span></span><br><span class="line"><span class="comment"># 查看成对关系图，</span></span><br><span class="line"><span class="comment"># 我们发现`bwt`与母亲的`height`和`weight`之间关系较弱。</span></span><br><span class="line"><span class="comment"># 婴儿的孕龄(`gestation`)与其出生体重`bwt`之间有更强的关系，</span></span><br><span class="line"><span class="comment"># 这不足为奇，因为孩子在母亲子宫里的时间越长，</span></span><br><span class="line"><span class="comment"># 孩子就有越多的时间获得营养并成长——直到某个点，</span></span><br><span class="line"><span class="comment"># 然后关系某种程度上“变平”。42周 = 42 * 7 = 294天。</span></span><br><span class="line"><span class="comment"># 母亲的`age`与其孩子的`bwt`之间似乎没有任何关系。</span></span><br><span class="line"><span class="comment"># 让我们更深入地研究`bwt`和`gestation`之间的关系。</span></span><br><span class="line">plot<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;gray60&quot;</span><span class="punctuation">)</span> <span class="comment"># bwt对gestation的散点图</span></span><br><span class="line">lines<span class="punctuation">(</span>lowess<span class="punctuation">(</span>Babies.df<span class="operator">$</span>gestation<span class="punctuation">,</span>Babies.df<span class="operator">$</span>bwt<span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;tomato&quot;</span><span class="punctuation">,</span> lwd <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 添加loess平滑曲线</span></span><br><span class="line">text<span class="punctuation">(</span><span class="number">152</span><span class="punctuation">,</span> <span class="number">120</span><span class="punctuation">,</span> <span class="string">&quot;?&quot;</span><span class="punctuation">)</span> <span class="comment"># 标记可能的异常点</span></span><br><span class="line">text<span class="punctuation">(</span><span class="number">185</span><span class="punctuation">,</span> <span class="number">115</span><span class="punctuation">,</span> <span class="string">&quot;?&quot;</span><span class="punctuation">)</span> <span class="comment"># 标记可能的异常点</span></span><br><span class="line">abline<span class="punctuation">(</span>v <span class="operator">=</span> <span class="number">294</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;steelblue&quot;</span><span class="punctuation">,</span> lwd <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 在294天处画一条垂直线 (孕期转折点)</span></span><br><span class="line"><span class="comment"># 注意，这些图中似乎也有一些“奇怪”的数据点。</span></span><br><span class="line">which<span class="punctuation">(</span>Babies.df<span class="operator">$</span>gestation<span class="operator">&lt;</span><span class="number">200</span><span class="punctuation">)</span> <span class="comment"># 找出孕期小于200天的观测</span></span><br><span class="line"><span class="comment"># 分类(因子)数据变量与婴儿出生体重(`bwt`)之间关系不大。</span></span><br><span class="line">pairs20x<span class="punctuation">(</span>Babies.df<span class="punctuation">[</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">,</span> <span class="number">7</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># bwt, not.first.born, smokes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 像之前提到的那样创建OD (Overdue Days，是否逾期)</span></span><br><span class="line">Babies.df<span class="operator">$</span>OD <span class="operator">=</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span><span class="punctuation">(</span>Babies.df<span class="operator">$</span>gestation <span class="operator">&gt;</span> <span class="number">294</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 如果gestation &gt; 294则为1，否则为0</span></span><br><span class="line"><span class="built_in">range</span><span class="punctuation">(</span>Babies.df<span class="operator">$</span>gestation<span class="punctuation">[</span>Babies.df<span class="operator">$</span>OD <span class="operator">==</span> <span class="number">0</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 检查OD=0时的孕期范围</span></span><br><span class="line"><span class="built_in">range</span><span class="punctuation">(</span>Babies.df<span class="operator">$</span>gestation<span class="punctuation">[</span>Babies.df<span class="operator">$</span>OD <span class="operator">==</span> <span class="number">1</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 检查OD=1时的孕期范围</span></span><br><span class="line">bwt.fit <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">)</span> <span class="comment"># 拟合孕龄和是否逾期的交互作用模型 (分段线性)</span></span><br><span class="line">plot<span class="punctuation">(</span>bwt.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 线性性和EOV检查</span></span><br><span class="line">normcheck<span class="punctuation">(</span>bwt.fit<span class="punctuation">)</span> <span class="comment"># 正态性检查</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>bwt.fit<span class="punctuation">)</span> <span class="comment"># 影响点检查 (此时会发现239, 820号点影响大)</span></span><br><span class="line">bwt.fit2 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="number">239</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 移除239号点后重新拟合</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>bwt.fit2<span class="punctuation">)</span> <span class="comment"># 再次检查影响点 (此时会发现原820号点，现819号点影响大)</span></span><br><span class="line">bwt.fit3 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 移除239和820号点后重新拟合</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>bwt.fit3<span class="punctuation">)</span> <span class="comment"># 最终影响点检查</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit3<span class="punctuation">)</span> <span class="comment"># 交互作用是显著的</span></span><br><span class="line"></span><br><span class="line">gestation.seq<span class="operator">=</span><span class="number">201</span><span class="operator">:</span><span class="number">360</span> <span class="comment"># 用于预测的孕期序列</span></span><br><span class="line">ODdays.seq<span class="operator">=</span>ifelse<span class="punctuation">(</span>gestation.seq<span class="operator">&lt;=</span><span class="number">294</span><span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 对应的OD序列 (注意：这里应该是OD，而不是ODdays)</span></span><br><span class="line"><span class="comment"># 代码中ODdays.seq的定义与前面OD的0/1定义一致，用于指示是否进入第二段。</span></span><br><span class="line"><span class="comment"># 但模型bwt.fit3是用 gestation*OD 拟合的。如果想实现分段斜率，</span></span><br><span class="line"><span class="comment"># 需要创建类似 gestation_after_294 = ifelse(gestation &gt; 294, gestation - 294, 0) 的变量。</span></span><br><span class="line"><span class="comment"># 或者如教材P20所示的 (gestation-294)*OD。</span></span><br><span class="line"><span class="comment"># 当前的 gestation*OD 会产生不同的截距和斜率，但不是典型的分段点斜率突变模型。</span></span><br><span class="line"><span class="comment"># 假设这里的目的是可视化两个不同阶段的线性关系（如果OD是0或1）</span></span><br><span class="line">fit.seq<span class="operator">=</span>predict<span class="punctuation">(</span>bwt.fit3<span class="punctuation">,</span>new<span class="operator">=</span>data.frame<span class="punctuation">(</span>gestation<span class="operator">=</span>gestation.seq<span class="punctuation">,</span> OD<span class="operator">=</span>ODdays.seq<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>bwt<span class="operator">~</span>gestation<span class="punctuation">,</span>data<span class="operator">=</span>Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">,</span>ylab<span class="operator">=</span><span class="string">&quot;Birth weight (oz)&quot;</span><span class="punctuation">)</span> <span class="comment"># 修正笔误，使用清洗后的数据绘图</span></span><br><span class="line">lines<span class="punctuation">(</span>gestation.seq<span class="punctuation">,</span>fit.seq<span class="punctuation">,</span>col<span class="operator">=</span><span class="string">&quot;red&quot;</span><span class="punctuation">)</span>; abline<span class="punctuation">(</span>v<span class="operator">=</span><span class="number">294</span><span class="punctuation">,</span>lty<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span>col<span class="operator">=</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前模型是什么？逐步添加变量</span></span><br><span class="line">bwt.fit4 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD <span class="operator">+</span> weight<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit4<span class="punctuation">)</span></span><br><span class="line">bwt.fit5 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD <span class="operator">+</span> weight <span class="operator">+</span> height<span class="punctuation">,</span> data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit5<span class="punctuation">)</span></span><br><span class="line">bwt.fit6 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD <span class="operator">+</span> weight <span class="operator">+</span> height <span class="operator">+</span> bmi<span class="punctuation">,</span></span><br><span class="line">              data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># weight和height与bmi同时在模型中，可能导致共线性</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit6<span class="punctuation">)</span> <span class="comment"># 注意weight, height, bmi的显著性可能下降</span></span><br><span class="line">bwt.fit7 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD <span class="operator">+</span> height <span class="operator">+</span> bmi<span class="punctuation">,</span> <span class="comment"># 移除weight，保留height和bmi</span></span><br><span class="line">              data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit7<span class="punctuation">)</span></span><br><span class="line">bwt.fit8 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD <span class="operator">+</span> height <span class="operator">+</span> bmi <span class="operator">+</span> not.first.born<span class="punctuation">,</span></span><br><span class="line">              data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit8<span class="punctuation">)</span></span><br><span class="line">bwt.fit9 <span class="operator">=</span> lm<span class="punctuation">(</span>bwt <span class="operator">~</span> gestation <span class="operator">*</span> OD <span class="operator">+</span> height <span class="operator">+</span> bmi <span class="operator">+</span> not.first.born <span class="operator">+</span> smokes<span class="punctuation">,</span></span><br><span class="line">              data <span class="operator">=</span> Babies.df<span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">239</span><span class="punctuation">,</span> <span class="number">820</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 最终模型</span></span><br><span class="line">summary<span class="punctuation">(</span>bwt.fit9<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>bwt.fit9<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 最终模型的诊断</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>bwt.fit9<span class="punctuation">)</span></span><br><span class="line">normcheck<span class="punctuation">(</span>bwt.fit9<span class="punctuation">)</span></span><br><span class="line">confint<span class="punctuation">(</span>bwt.fit9<span class="punctuation">)</span> <span class="comment"># 最终模型参数的置信区间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 这部分代码切换到果蝇数据，与前面的婴儿体重数据无关 ---</span></span><br><span class="line"><span class="comment"># 在这项研究中，我们观察雄性果蝇的寿命与其</span></span><br><span class="line"><span class="comment"># 生殖活动的关系。</span></span><br><span class="line">Fruitfly.df <span class="operator">=</span> read.csv<span class="punctuation">(</span><span class="string">&quot;Fruitfly.csv&quot;</span><span class="punctuation">,</span> stringsAsFactors<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取果蝇数据</span></span><br><span class="line">plot<span class="punctuation">(</span>days <span class="operator">~</span> group<span class="punctuation">,</span> data <span class="operator">=</span> Fruitfly.df<span class="punctuation">)</span> <span class="comment"># 寿命对分组的箱线图</span></span><br><span class="line"><span class="comment"># + &quot;G1&quot;: 独居雄性,</span></span><br><span class="line"><span class="comment"># + &quot;G2&quot;: 与1只感兴趣的雌性同住的雄性,</span></span><br><span class="line"><span class="comment"># + &quot;G3&quot;: 与8只感兴趣的雌性同住的雄性,</span></span><br><span class="line"><span class="comment"># + &quot;G4&quot;: 与1只不感兴趣的雌性同住的雄性,</span></span><br><span class="line"><span class="comment"># + &quot;G5&quot;: 与8只不感兴趣的雌性同住的雄性,</span></span><br><span class="line">summaryStats<span class="punctuation">(</span>days <span class="operator">~</span> group<span class="punctuation">,</span> Fruitfly.df<span class="punctuation">)</span> <span class="comment"># 按组计算描述统计</span></span><br><span class="line">ff.fit <span class="operator">=</span> lm<span class="punctuation">(</span>days <span class="operator">~</span> group<span class="punctuation">,</span> data <span class="operator">=</span> Fruitfly.df<span class="punctuation">)</span> <span class="comment"># 单因素方差分析模型</span></span><br><span class="line">summary<span class="punctuation">(</span>ff.fit<span class="punctuation">)</span> <span class="comment"># 查看模型摘要</span></span><br><span class="line">anova<span class="punctuation">(</span>ff.fit<span class="punctuation">)</span> <span class="comment"># 查看方差分析表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 这部分是关于多重比较问题的模拟，与前面的具体数据集分析目的不同 ---</span></span><br><span class="line">m <span class="operator">=</span> <span class="number">6</span> <span class="comment"># 类别数量</span></span><br><span class="line">n <span class="operator">=</span> <span class="number">30</span> <span class="comment"># 每个类别中的样本量</span></span><br><span class="line">count1 <span class="operator">=</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span><span class="number">1000</span><span class="punctuation">)</span><span class="punctuation">&#123;</span> <span class="comment"># 模拟1000次</span></span><br><span class="line">  y <span class="operator">=</span> rnorm<span class="punctuation">(</span>m<span class="operator">*</span>n<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 生成数据 (所有组均值都为0)</span></span><br><span class="line">  w <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">LETTERS</span><span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span>m<span class="punctuation">]</span><span class="punctuation">,</span> each <span class="operator">=</span> n<span class="punctuation">)</span> <span class="comment"># 创建分组变量</span></span><br><span class="line">  table <span class="operator">=</span> aggregate<span class="punctuation">(</span>y<span class="punctuation">,</span> <span class="built_in">list</span><span class="punctuation">(</span>w<span class="punctuation">)</span><span class="punctuation">,</span> FUN<span class="operator">=</span>mean<span class="punctuation">)</span> <span class="comment"># 计算各组均值</span></span><br><span class="line">  min.index <span class="operator">=</span> which.min<span class="punctuation">(</span>table<span class="operator">$</span>x<span class="punctuation">)</span> <span class="comment"># 找到均值最小的组</span></span><br><span class="line">  max.index <span class="operator">=</span> which.max<span class="punctuation">(</span>table<span class="operator">$</span>x<span class="punctuation">)</span> <span class="comment"># 找到均值最大的组</span></span><br><span class="line">  sim_data <span class="operator">=</span> data.frame<span class="punctuation">(</span>y<span class="operator">=</span>y<span class="punctuation">,</span> w <span class="operator">=</span> w<span class="punctuation">)</span></span><br><span class="line">  <span class="comment"># model = lm(formula = y~w, data = sim_data)</span></span><br><span class="line">  <span class="comment"># 将均值最小和最大的组作为对比的基准 (这是一种特定的比较策略，可能用于放大差异)</span></span><br><span class="line">  w <span class="operator">=</span> factor<span class="punctuation">(</span>w<span class="punctuation">,</span> levels <span class="operator">=</span> unique<span class="punctuation">(</span><span class="built_in">LETTERS</span><span class="punctuation">[</span><span class="built_in">c</span><span class="punctuation">(</span>min.index<span class="punctuation">,</span>max.index<span class="punctuation">,</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>m<span class="punctuation">)</span><span class="punctuation">[</span><span class="operator">-</span><span class="built_in">c</span><span class="punctuation">(</span>min.index<span class="punctuation">,</span> max.index<span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 重新定义因子水平顺序</span></span><br><span class="line">  sim_data <span class="operator">=</span> data.frame<span class="punctuation">(</span>y<span class="operator">=</span>y<span class="punctuation">,</span> w <span class="operator">=</span> w<span class="punctuation">)</span></span><br><span class="line">  model <span class="operator">=</span> lm<span class="punctuation">(</span>formula <span class="operator">=</span> y<span class="operator">~</span>w<span class="punctuation">,</span> data <span class="operator">=</span> sim_data<span class="punctuation">)</span></span><br><span class="line">  summary<span class="punctuation">(</span>model<span class="punctuation">)</span></span><br><span class="line">  <span class="comment"># model = lm(formula = y~w, data = sim_data,</span></span><br><span class="line">  <span class="comment">#           subset = (w==(LETTERS[min.index])|w==(LETTERS[max.index]))) # 这行被注释掉了</span></span><br><span class="line">  <span class="comment"># 检查第二个系数（通常是与基准组的第一个比较）的置信区间是否包含0</span></span><br><span class="line">  <span class="comment"># 这里可能是想看在真实均值无差异的情况下，随机抽样有多少次会“发现”显著差异</span></span><br><span class="line">  <span class="comment"># 注意：这里count1的逻辑比较复杂，它是在测试第二个系数的置信区间是否覆盖0</span></span><br><span class="line">  <span class="comment"># 如果真实情况是所有组均值相同，那么任何比较的真实差异都是0。</span></span><br><span class="line">  <span class="comment"># 如果置信区间包含0，则表示未拒绝H0: diff=0。</span></span><br><span class="line">  <span class="comment"># 所以这里count1如果统计的是 (confint(model)[2,1] &lt; 0 &amp; confint(model)[2,2] &gt; 0)，即区间包含0，</span></span><br><span class="line">  <span class="comment"># 那么 count1/1000 应该是对 &quot;在H0为真时，未能拒绝H0的比例&quot;，但这不直接是alpha或beta。</span></span><br><span class="line">  <span class="comment"># 如果是 !as.numeric(...) 则是在统计第一类错误的发生率。</span></span><br><span class="line">  <span class="comment"># 原始代码 as.numeric(...) 表示CI包含0的次数。</span></span><br><span class="line">  count1 <span class="operator">=</span> count1 <span class="operator">+</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> <span class="number">0</span> <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> <span class="number">0</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">count1<span class="operator">/</span><span class="number">1000</span> <span class="comment"># 结果应接近0.95，如果count1是统计“CI包含0的次数”</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 另一段模拟代码，关于置信区间覆盖真实参数的比例 ---</span></span><br><span class="line">sample.size <span class="operator">=</span> <span class="number">20</span></span><br><span class="line">x <span class="operator">=</span> <span class="number">1</span><span class="operator">:</span>sample.size;</span><br><span class="line">beta0 <span class="operator">=</span> <span class="number">3</span>; beta1 <span class="operator">=</span> <span class="number">0.2</span>; beta2 <span class="operator">=</span> <span class="number">0.1</span> <span class="comment"># 真实参数</span></span><br><span class="line">m <span class="operator">=</span> <span class="number">10000</span> <span class="comment"># 模拟次数</span></span><br><span class="line">count0 <span class="operator">=</span> <span class="number">0</span> <span class="comment"># 记录beta0的CI覆盖真实值的次数</span></span><br><span class="line">count1 <span class="operator">=</span> <span class="number">0</span> <span class="comment"># 记录beta1的CI覆盖真实值的次数</span></span><br><span class="line">count2 <span class="operator">=</span> <span class="number">0</span> <span class="comment"># 记录beta2的CI覆盖真实值的次数</span></span><br><span class="line">count <span class="operator">=</span> <span class="number">0</span> <span class="comment"># 记录所有参数的CI同时覆盖真实值的次数</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> <span class="number">1</span><span class="operator">:</span>m<span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">  error <span class="operator">=</span> rnorm<span class="punctuation">(</span>n <span class="operator">=</span> sample.size<span class="punctuation">,</span> mean <span class="operator">=</span> <span class="number">0</span><span class="punctuation">,</span> sd <span class="operator">=</span> <span class="number">3</span><span class="punctuation">)</span></span><br><span class="line">  y <span class="operator">=</span> beta0 <span class="operator">+</span> beta1 <span class="operator">*</span> x <span class="operator">+</span> beta2 <span class="operator">*</span> x<span class="operator">^</span><span class="number">2</span> <span class="operator">+</span> error <span class="comment"># 二次模型</span></span><br><span class="line">  sim_data <span class="operator">=</span> data.frame<span class="punctuation">(</span>x<span class="operator">=</span>x<span class="punctuation">,</span> y<span class="operator">=</span>y<span class="punctuation">)</span></span><br><span class="line">  model <span class="operator">=</span> lm<span class="punctuation">(</span>formula <span class="operator">=</span> y<span class="operator">~</span>x<span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> sim_data<span class="punctuation">)</span> <span class="comment"># 拟合二次模型</span></span><br><span class="line">  <span class="comment"># confint(model) # 这里不需要打印</span></span><br><span class="line">  count0 <span class="operator">=</span> count0 <span class="operator">+</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> beta0 <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> beta0<span class="punctuation">)</span></span><br><span class="line">  count1 <span class="operator">=</span> count1 <span class="operator">+</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> beta1 <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> beta1<span class="punctuation">)</span></span><br><span class="line">  count2 <span class="operator">=</span> count2 <span class="operator">+</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> beta2 <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> beta2<span class="punctuation">)</span></span><br><span class="line">  count <span class="operator">=</span> count <span class="operator">+</span> <span class="built_in">as.numeric</span><span class="punctuation">(</span> <span class="comment"># 同时覆盖</span></span><br><span class="line">    confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> beta0 <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> beta0</span><br><span class="line">    <span class="operator">&amp;</span></span><br><span class="line">      confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> beta1 <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> beta1</span><br><span class="line">    <span class="operator">&amp;</span></span><br><span class="line">      confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span> <span class="operator">&lt;</span> beta2 <span class="operator">&amp;</span> confint<span class="punctuation">(</span>model<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span> <span class="operator">&gt;</span> beta2</span><br><span class="line">  <span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">count0<span class="operator">/</span>m <span class="comment"># beta0的CI覆盖率，应接近0.95</span></span><br><span class="line">count1<span class="operator">/</span>m <span class="comment"># beta1的CI覆盖率，应接近0.95</span></span><br><span class="line">count2<span class="operator">/</span>m <span class="comment"># beta2的CI覆盖率，应接近0.95</span></span><br><span class="line">count<span class="operator">/</span>m <span class="comment"># 所有参数联合CI覆盖率，会低于0.95 (Bonferroni不等式相关概念)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 回到果蝇数据，使用emmeans进行多重比较 ---</span></span><br><span class="line">library<span class="punctuation">(</span>emmeans<span class="punctuation">)</span> <span class="comment"># 加载emmeans包</span></span><br><span class="line">Fruitfly.emm <span class="operator">=</span> emmeans<span class="punctuation">(</span>ff.fit<span class="punctuation">,</span> <span class="operator">~</span>group<span class="punctuation">)</span> <span class="comment"># 计算各组的边际均值</span></span><br><span class="line"><span class="comment"># 查看所有成对比较：</span></span><br><span class="line">pairs<span class="punctuation">(</span>Fruitfly.emm<span class="punctuation">,</span> infer<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># infer=T 会给出调整后的p值和置信区间 (默认Tukey)</span></span><br><span class="line"><span class="comment"># 仅查看在5%水平下显著的比较：</span></span><br><span class="line">Fruitfly.pairs <span class="operator">=</span> data.frame<span class="punctuation">(</span>pairs<span class="punctuation">(</span>Fruitfly.emm<span class="punctuation">,</span> infer<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 转换为数据框以便筛选</span></span><br><span class="line">subset<span class="punctuation">(</span>Fruitfly.pairs<span class="punctuation">,</span> p.value<span class="operator">&lt;</span><span class="number">0.05</span><span class="punctuation">)</span> <span class="comment"># 筛选p值小于0.05的比较</span></span><br></pre></td></tr></table></figure>
<h3 id="Lecture-9：双因子方差分析-Two-way-ANOVA">Lecture 9：双因子方差分析 (Two-way ANOVA)</h3>
<p>这个lecture深入探讨了包含<strong>两个因子（分类）解释变量的线性模型</strong>，即<strong>双因素方差分析 (Two-way ANOVA)</strong>。核心内容包括交互作用的识别、建模、解释以及不同参数化方式的理解。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第12章“包含两个解释因子变量的线性模型（双因素方差分析）”的内容完全对应。</p>
<ol>
<li>
<p><strong>交互作用图 (Interaction Plots)</strong>:</p>
<ul>
<li>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">interactionPlots(Response ~ FactorA + FactorB, data = df)</span><br></pre></td></tr></table></figure>
<p>(来自</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s20x</span><br></pre></td></tr></table></figure>
<p>包) 来可视化两个因子变量对一个数值响应变量的联合影响。</p>
</li>
<li>
<p>图中的线条代表一个因子的不同水平，x轴是另一个因子的水平，y轴是响应变量的均值。</p>
</li>
<li>
<p>如果线条不平行，则暗示两个因子之间可能存在交互作用，意味着一个因子对响应变量的影响方式取决于另一个因子的水平。 如果线条大致平行，则交互作用可能不显著。</p>
</li>
<li>
<p>教材第12章 (第9-11页) 展示并解释了这种图。</p>
</li>
</ul>
</li>
<li>
<p><strong>双因素ANOVA模型的拟合与检验</strong>:</p>
<ul>
<li>
<p>交互作用模型</p>
<p>：首先拟合包含交互作用项的模型：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(Exam ~ Attend * Pass.test, data = Stats20x.df)</span><br></pre></td></tr></table></figure>
<p>，这等价于</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(Exam ~ Attend + Pass.test + Attend:Pass.test, ...)</span><br></pre></td></tr></table></figure>
<p>。</p>
</li>
<li>
<p>检验交互作用</p>
<p>：使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">anova(Exam.fit)</span><br></pre></td></tr></table></figure>
<p>查看方差分析表。交互作用项 (如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Attend:Pass.test</span><br></pre></td></tr></table></figure>
<p>) 的F检验的P值用于判断交互作用是否显著。</p>
</li>
<li>
<p>模型简化</p>
<p>：如果交互作用不显著 (P值较大)，则根据奥卡姆剃刀原则，移除交互作用项，拟合</p>
<p>主效应模型(或称加性模型)：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(Exam ~ Attend + Pass.test, data = Stats20x.df)</span><br></pre></td></tr></table></figure>
<p>。然后进一步检查各主效应的显著性。</p>
</li>
<li>
<p>代码中首先分析了 <code>Attend</code> 和 <code>Gender</code> 的模型，发现交互作用不显著，然后逐步简化。之后分析了 <code>Attend</code> 和 <code>Pass.test</code>，发现交互作用显著。</p>
</li>
<li>
<p>教材第12章 (第16页，第33-34页) 讨论了交互作用的检验和模型简化。</p>
</li>
</ul>
</li>
<li>
<p><strong>模型参数化与解释</strong>:</p>
<ul>
<li>
<p><strong>参考水平模型 (哑变量编码)</strong>：<code>summary(Exam.fit)</code> 显示的系数是相对于各因子的基准水平的。交互作用项的系数表示当两个因子都处于非基准水平时，相对于仅考虑主效应的额外效应或协同效应。教材第12章 (第18页，第40页) 有详细解释。</p>
</li>
<li>
<p>单元格均值模型 (Group Means Model)：通过</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(Exam ~ Attend:Pass.test - 1, ...)</span><br></pre></td></tr></table></figure>
<p>可以直接估计每个因子组合（单元格）的均值。代码中演示了这种模型的系数 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mu_ij</span><br></pre></td></tr></table></figure>
<p>) 与参考水平模型系数 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">beta_k</span><br></pre></td></tr></table></figure>
<p>) 之间的关系。教材第12章 (第19页) 提到了这种模型。</p>
</li>
<li>
<p><strong>效应模型 (Effects Model)</strong>：另一种参数化是将各效应表示为与总体均值的偏差 (主效应 αi,πj) 和交互效应 (alphapiij)。代码中手动计算了这些效应，并与 <code>emmeans</code> 的输出进行了概念上的联系。教材第12章 (第46-48页) 讨论了这种参数化。</p>
</li>
</ul>
</li>
<li>
<p><strong>多重比较 (Pairwise Comparisons)</strong>:</p>
<ul>
<li>
<p>当交互作用显著时，通常我们关心的是在某个因子的特定水平下，另一个因子不同水平之间的差异；或者比较特定的因子组合。</p>
</li>
<li>
<p><code>emmeans</code> 包用于计算调整后的边际均值 (Estimated Marginal Means) 并进行两两比较。</p>
</li>
<li>
<pre><code>pairs(emmeans(Exam.fit, ~Attend*Pass.test), infer=T)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">      会给出所有因子组合的两两比较，并使用Tukey等方法调整P值和置信区间以控制族错误率 (family-wise error rate)。</span><br><span class="line"></span><br><span class="line">   - `displayPairs()` (来自 `s20x` 包) 可以更清晰地展示特定条件下的两两比较。</span><br><span class="line"></span><br><span class="line">   - 教材第12章 (第22-25页) 详细介绍了如何使用 `emmeans` 和 `displayPairs`。</span><br><span class="line"></span><br><span class="line">总结：Lecture 9 详细介绍了双因素方差分析的完整流程，从可视化交互作用、拟合模型、检验交互作用的显著性，到不同模型参数化的理解和使用 `emmeans` 进行多重比较，这些都是分析具有多个分类预测变量数据的重要方法。</span><br><span class="line"></span><br><span class="line">```r</span><br><span class="line"># 我们希望确定规律上课</span><br><span class="line"># 并且通过测试是否与考试成绩相关。</span><br><span class="line"># 我们已经分别看到</span><br><span class="line"># 通过测试和规律上课</span><br><span class="line"># 会增加取得好考试成绩的机会</span><br><span class="line">library(s20x) # 加载s20x包</span><br><span class="line">Stats20x.df = read.table(&quot;STATS20x.txt&quot;, header = T) # 读取学生数据</span><br><span class="line">head(Stats20x.df) # 查看数据前几行</span><br><span class="line">interactionPlots(Exam ~ Attend + Gender, data = Stats20x.df) # 绘制 Exam 对 Attend 和 Gender 的交互作用图</span><br><span class="line"># 平行线表明没有交互作用</span><br><span class="line">Exam.gender.fit = lm(Exam ~ Attend*Gender, data = Stats20x.df) # 拟合 Attend 和 Gender 的交互作用模型</span><br><span class="line">plot(Exam.gender.fit, which = 1) # 残差图</span><br><span class="line">normcheck(Exam.gender.fit) # 正态性检查</span><br><span class="line">cooks20x(Exam.gender.fit) # 影响点检查</span><br><span class="line">anova(Exam.gender.fit) # 方差分析表，检查交互作用项 Attend:Gender 的显著性</span><br><span class="line">summary(Exam.gender.fit) # 注意这里的p值与anova表中交互作用项的p值应一致</span><br><span class="line">Exam.gender.no.intact.fit = lm(Exam ~ Attend+Gender, data = Stats20x.df) # 如果交互作用不显著，拟合主效应模型</span><br><span class="line">anova(Exam.gender.no.intact.fit) # 主效应模型的方差分析表</span><br><span class="line">summary(Exam.gender.no.intact.fit) # 主效应模型的摘要</span><br><span class="line">Exam.attend.fit = lm(Exam ~ Attend, data = Stats20x.df) # 如果Gender也不显著，进一步简化模型</span><br><span class="line">summary(Exam.attend.fit)</span><br><span class="line"></span><br><span class="line"># 现在考虑二元的test结果和出勤情况</span><br><span class="line"># 所以，我们需要使用`Test`变量</span><br><span class="line"># 创建变量`Pass.test`</span><br><span class="line">Stats20x.df$Pass.test = factor(</span><br><span class="line">  ifelse(Stats20x.df$Test &gt;= 10, &quot;pass&quot;, &quot;nopass&quot;)) # Test&gt;=10为pass，否则为nopass</span><br><span class="line"># 检查新变量`Pass.test`是否正确生成</span><br><span class="line">min(Stats20x.df$Test[Stats20x.df$Pass.test == &quot;pass&quot;])</span><br><span class="line">max(Stats20x.df$Test[Stats20x.df$Pass.test == &quot;nopass&quot;])</span><br><span class="line">interactionPlots(Exam ~ Pass.test + Attend, data = Stats20x.df) # Exam 对 Pass.test 和 Attend 的交互作用图</span><br><span class="line"># 也反过来看交互作用图：</span><br><span class="line">interactionPlots(Exam ~ Attend + Pass.test, data = Stats20x.df) # Exam 对 Attend 和 Pass.test 的交互作用图</span><br><span class="line"># 通过测试的学生的考试分数</span><br><span class="line"># 比未通过测试的学生的分数要高。</span><br><span class="line"># 然而，这种差异对于那些</span><br><span class="line"># 规律上课的学生来说似乎更大，</span><br><span class="line"># 这可以从交互图中不平行的彩色线条看出。</span><br><span class="line"># 换句话说，通过测试的影响似乎</span><br><span class="line"># 因出勤组而异，这表明</span><br><span class="line"># 两个预测变量之间可能存在交互作用。</span><br><span class="line">Exam.fit = lm(Exam ~ Attend * Pass.test, data = Stats20x.df) # 拟合Attend和Pass.test的交互作用模型</span><br><span class="line"># 模型公式也可以是 `Exam ~ Attend + Pass.test + Attend:Pass.test`</span><br><span class="line">plot(Exam.fit,1) # 残差图</span><br><span class="line">normcheck(Exam.fit) # 正态性检查</span><br><span class="line">cooks20x(Exam.fit) # 影响点检查</span><br><span class="line">anova(Exam.fit) # 查看交互作用 Attend:Pass.test 是否显著</span><br><span class="line">1-30968.4/(7630.8+11076.9+909.7+30968.4) # 手动计算R^2 ( (TSS-RSS)/TSS )</span><br><span class="line">summary(Exam.fit) # 模型摘要</span><br><span class="line"># 组均值模型 (无整体截距，为每个组合估计一个均值)</span><br><span class="line">Exam.fitNoBaseline=lm(</span><br><span class="line">  Exam~Attend:Pass.test-1,data=Stats20x.df) # Attend:Pass.test-1 表示所有组合的均值</span><br><span class="line">summary(Exam.fitNoBaseline)</span><br><span class="line"># 参数化：beta (效应编码/哑变量编码) 和 mu_ij (单元格均值编码) 之间的关系 (参考水平)</span><br><span class="line">beta0 = coef(Exam.fit)[1] # 基准组(AttendNo, TestNopass)的均值</span><br><span class="line">beta1 = coef(Exam.fit)[2] # AttendYes相对于AttendNo的效应 (当TestNopass时)</span><br><span class="line">beta2 = coef(Exam.fit)[3] # TestPass相对于TestNopass的效应 (当AttendNo时)</span><br><span class="line">beta3 = coef(Exam.fit)[4] # 交互作用效应</span><br><span class="line">mu11 = coef(Exam.fitNoBaseline)[1] # AttendNo,TestNopass 的均值</span><br><span class="line">print(c(mu11,beta0)) # mu11 应该等于 beta0</span><br><span class="line">mu21 = coef(Exam.fitNoBaseline)[2] # AttendYes, TestNopass 的均值</span><br><span class="line">print(c(mu21,beta1+beta0)) # mu21 应该等于 beta0+beta1</span><br><span class="line">mu12 = coef(Exam.fitNoBaseline)[3] # AttendNo, TestPass 的均值</span><br><span class="line">print(c(mu12,beta2+beta0)) # mu12 应该等于 beta0+beta2</span><br><span class="line">mu22 = coef(Exam.fitNoBaseline)[4] # AttendYes, TestPass 的均值</span><br><span class="line">print(c(mu22,beta3+beta2+beta1+beta0)) # mu22 应该等于 beta0+beta1+beta2+beta3</span><br><span class="line"># 多重比较</span><br><span class="line">library(emmeans) # 加载emmeans包</span><br><span class="line">Exam.pairs = pairs(</span><br><span class="line">  emmeans(Exam.fit, ~Attend*Pass.test), infer=T) # 计算所有处理组合的边际均值并进行两两比较</span><br><span class="line">Exam.pairs</span><br><span class="line"># 效应参数化 (与整体均值的偏差)</span><br><span class="line">mu = mean(Stats20x.df$Exam) # 总体均值</span><br><span class="line">mu1dot = mean(c(mu11,mu12)) # AttendNo的边际均值 (平均了Pass.test的水平)</span><br><span class="line">mu2dot = mean(c(mu21,mu22)) # AttendYes的边际均值</span><br><span class="line">emmeans(Exam.fit,~Attend) # 使用emmeans计算Attend的边际均值</span><br><span class="line">mudot1 = mean(c(mu11,mu21)) # TestNopass的边际均值 (平均了Attend的水平)</span><br><span class="line">mudot2 = mean(c(mu12,mu22)) # TestPass的边际均值</span><br><span class="line">emmeans(Exam.fit,~Pass.test) # 使用emmeans计算Pass.test的边际均值</span><br><span class="line"></span><br><span class="line"># 行效应 (Attend的效应)</span><br><span class="line">alpha1 = mu1dot - mu # AttendNo的效应 = AttendNo的边际均值 - 总体均值</span><br><span class="line">alpha2 = mu2dot - mu # AttendYes的效应</span><br><span class="line"># 列效应 (Pass.test的效应)</span><br><span class="line">pi1 = mudot1 - mu # TestNopass的效应</span><br><span class="line">pi2 = mudot2 - mu # TestPass的效应</span><br><span class="line"># 交互作用效应</span><br><span class="line">alphapi11 = mu11 - mu - alpha1 - pi1 # AttendNo &amp; TestNopass 的交互效应</span><br><span class="line">alphapi21 = mu21 - mu - alpha2 - pi1 # AttendYes &amp; TestNopass 的交互效应</span><br><span class="line">alphapi12 = mu12 - mu - alpha1 - pi2 # AttendNo &amp; TestPass 的交互效应</span><br><span class="line">alphapi22 = mu22 - mu - alpha2 - pi2 # AttendYes &amp; TestPass 的交互效应</span><br></pre></td></tr></table></figure>

</code></pre>
</li>
</ul>
</li>
</ol>
<h3 id="Lecture-10：广义线性模型-Generalized-Linear-Models-Poisson-和-Binomial-回归">Lecture 10：广义线性模型 (Generalized Linear Models) - Poisson 和 Binomial 回归</h3>
<p>这个lecture主要介绍<strong>广义线性模型 (Generalized Linear Models, GLMs)</strong>，特别是用于处理<strong>计数数据 (count data)</strong> 的 <strong>Poisson回归</strong>和用于处理<strong>比例/二元数据 (proportion/binary data)</strong> 的 <strong>Binomial回归 (Logistic回归)</strong>。它强调了在响应变量不满足正态分布和方差齐性假设时，直接使用普通线性模型（LM）的局限性，并引入了GLM作为更合适的工具。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第13章“使用泊松分布建模计数数据”和第15章“使用二项分布建模比例数据”的内容一致。</p>
<ol>
<li>
<p><strong>计数数据的特点与分布</strong>:</p>
<ul>
<li>
<p>Poisson 分布</p>
<p>: 介绍Poisson分布是描述在一定时间或空间内随机独立事件发生次数的经典概率分布，其特点是均值等于方差 (</p>
<p>μ=σ2</p>
<p>)。代码通过</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dpois()</span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rpois()</span><br></pre></td></tr></table></figure>
<p>展示了Poisson分布的形状如何随均值μ变化，并与正态分布进行比较，说明当μ较小时，Poisson分布是右偏的，当μ较大时，它近似于正态分布。这对应教材第13章“泊松分布” (第20-27页)。</p>
</li>
<li>
<p><strong>Binomial 分布</strong>: 简要提及了二项分布（<code>dbinom()</code>），并与正态分布比较，为后续的Logistic回归做铺垫。</p>
</li>
</ul>
</li>
<li>
<p><strong>传统线性模型 (LM) 处理计数数据的局限性</strong>:</p>
<ul>
<li>
<p>直接用LM拟合计数数据（如 <code>y ~ x</code>，其中y是计数）通常不合适，因为计数数据往往存在异方差（方差随均值变化）且响应变量非负整数。残差图会显示问题。</p>
</li>
<li>
<p>对响应变量取对数后用LM拟合 (<code>lm(log(Number) ~ Year, ...)</code>），即对数-线性模型，是一种常用技巧。这种模型解释的是响应变量的<strong>中位数</strong>的乘性变化。教材第6章已详细讨论。</p>
</li>
<li>
<p>代码通过CRAN R包提交数量的例子，展示了先使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm(log(Number) ~ Year)</span><br></pre></td></tr></table></figure>
<p>的方法，并解释了其系数和预测。</p>
</li>
<li>
<p>但这种方法也有缺点：log(0)无定义；对数转换可能无法完全解决异方差和正态性问题；影响点在对数尺度上可能与原始尺度不同。教材第13章 (第48页) 讨论了LM处理计数数据的局限性。</p>
</li>
</ul>
</li>
<li>
<p><strong>Poisson 回归 (GLM)</strong>:</p>
<ul>
<li>
<p>模型形式</p>
<p>: 假设响应变量Y服从Poisson分布，其均值μ=E[Y∣X]与解释变量X的关系通过连接函数 (link function)建立。对于Poisson回归，默认的连接函数是对数连接 (log link): log(μ)=β0+β1X。这意味着μ=exp(β0+β1X)，因此解释变量对均值μ的影响是乘性的。这与教材第13章 (第30页) 的描述一致。</p>
</li>
<li>
<p>R实现</p>
<p>: 使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glm(Number ~ Year, family = poisson, data = ...)</span><br></pre></td></tr></table></figure>
<p>。</p>
</li>
<li>
<p>解释: 系数β1的指数exp(β1)表示当X增加一个单位时，响应变量均值μ的乘性变化因子。(exp(β1)−1)×100%是均值的百分比变化。</p>
</li>
<li>
<p>模型诊断 - 过离散 (Overdispersion): Poisson分布假设均值等于方差。如果实际数据中方差远大于均值，则存在过离散。这可以通过比较模型的残差离散度 (Residual Deviance)与其自由度 (Residual df)来初步判断。如果 残差离散度 / 自由度 &gt;&gt; 1，或者</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 - pchisq(deviance, df.residual)</span><br></pre></td></tr></table></figure>
<p>的p值很小，则可能存在过离散。教材第13章 (第35页) 讨论了此检验。</p>
</li>
<li>
<p>处理过离散 - Quasi-Poisson</p>
<p>: 如果存在过离散，可以使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">family = quasipoisson</span><br></pre></td></tr></table></figure>
<p>重新拟合模型。这会调整参数估计的标准误，从而影响p值和置信区间，但系数估计本身不变。教材第13章 (第36-39页) 详细说明了Quasi-Poisson。</p>
</li>
</ul>
</li>
<li>
<p><strong>GLM的预测</strong>:</p>
<ul>
<li>
<p><code>predict(glm_fit, newdata, type = &quot;link&quot;)</code> 给出线性预测子 log(μ^) 的值。</p>
</li>
<li>
<pre><code>predict(glm_fit, newdata, type = &quot;response&quot;)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">      给出响应尺度上μ^的值 (exp(log(μ^)))。 </span><br><span class="line"></span><br><span class="line">   - `predictGLM()` (来自 `s20x` 包) 可以方便地给出响应尺度上的预测值及其置信区间。</span><br><span class="line"></span><br><span class="line">5. **GLM应用于多解释变量**:</span><br><span class="line"></span><br><span class="line">   - 数值与因子变量的交互作用 (地震数据)</span><br><span class="line"></span><br><span class="line">     : 与LM类似，可以在GLM中包含数值和因子变量及其交互作用，如 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
glm(Freq ~ Locn * Magnitude, family = poisson, ...)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  。解释方式也类似，但关注的是对均值的乘性效应和log-odds（对于二项）的线性效应。教材第14章 (第5-11页) 用地震数据为例。 </span><br><span class="line"></span><br><span class="line">- 双因子模型 (鲷鱼数据)</span><br><span class="line"></span><br><span class="line">  : 类似于双因素ANOVA，但使用 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
family = poisson
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  。首先检查交互作用，如果不显著则简化为主效应模型。解释时，关注各因子水平对响应变量均值的乘性影响。教材第14章 (第17-22页) 用鲷鱼数据为例。 </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  anova(glm_fit, test=&quot;Chisq&quot;)</span><br></pre></td></tr></table></figure>

 (或 &quot;F&quot; for quasipoisson) 用于检验GLM中各项的显著性，特别是因子变量或交互作用。

</code></pre>
</li>
</ul>
</li>
</ol>
<p>总结：Lecture 10 是从传统线性模型向广义线性模型过渡的关键一课，重点介绍了Poisson回归作为分析计数数据的标准方法，包括其模型设定、参数解释、过离散问题的识别与处理，以及如何将其应用于包含多种类型解释变量的场景。</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library<span class="punctuation">(</span>s20x<span class="punctuation">)</span> <span class="comment"># 加载s20x包</span></span><br><span class="line"><span class="comment"># 二项分布 (Binomial Distribution)</span></span><br><span class="line">p <span class="operator">=</span> <span class="number">0.75</span> <span class="comment"># 成功的概率</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>i <span class="keyword">in</span> seq<span class="punctuation">(</span><span class="number">10</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">,</span> by <span class="operator">=</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span> <span class="comment"># 试验次数n从10到100，步长为5</span></span><br><span class="line">  n <span class="operator">=</span> i; y <span class="operator">=</span> <span class="number">0</span><span class="operator">:</span>n <span class="comment"># 可能的成功次数</span></span><br><span class="line">  fy <span class="operator">=</span> dbinom<span class="punctuation">(</span>y<span class="punctuation">,</span>size <span class="operator">=</span> n<span class="punctuation">,</span>prob <span class="operator">=</span> p<span class="punctuation">)</span> <span class="comment"># 计算二项分布的概率质量函数</span></span><br><span class="line">  plot<span class="punctuation">(</span>y<span class="punctuation">,</span>fy<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;h&quot;</span><span class="punctuation">,</span> main <span class="operator">=</span> paste0<span class="punctuation">(</span><span class="string">&quot;n=&quot;</span><span class="punctuation">,</span>n<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 绘制概率分布图</span></span><br><span class="line">  <span class="comment"># 正态分布曲线比较 (二项分布的正态近似)</span></span><br><span class="line">  lines<span class="punctuation">(</span>y<span class="punctuation">,</span>dnorm<span class="punctuation">(</span>y<span class="punctuation">,</span> mean <span class="operator">=</span> n<span class="operator">*</span>p<span class="punctuation">,</span> sd <span class="operator">=</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>n<span class="operator">*</span>p<span class="operator">*</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">-</span>p<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 均值=np, 标准差=sqrt(np(1-p))</span></span><br><span class="line">  points<span class="punctuation">(</span>y<span class="punctuation">,</span> fy<span class="punctuation">,</span> pch <span class="operator">=</span> <span class="number">19</span><span class="punctuation">,</span> cex <span class="operator">=</span> <span class="number">0.25</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line">  legend<span class="punctuation">(</span><span class="string">&quot;topleft&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Normal&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Binomial&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">         col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> pch <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">19</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="comment"># 泊松分布 (Poisson Distribution)</span></span><br><span class="line">y <span class="operator">=</span> seq<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 可能的计数值</span></span><br><span class="line"><span class="keyword">for</span> <span class="punctuation">(</span>mu <span class="keyword">in</span> seq<span class="punctuation">(</span><span class="number">0.5</span><span class="punctuation">,</span> <span class="number">60</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span> <span class="comment"># 均值mu从0.5到60</span></span><br><span class="line">  fy <span class="operator">=</span> dpois<span class="punctuation">(</span>y<span class="punctuation">,</span>lambda <span class="operator">=</span> mu<span class="punctuation">)</span> <span class="comment"># 计算泊松分布的概率质量函数</span></span><br><span class="line">  plot<span class="punctuation">(</span>y<span class="punctuation">,</span>fy<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;h&quot;</span><span class="punctuation">,</span> main <span class="operator">=</span> paste0<span class="punctuation">(</span><span class="string">&quot;mu=&quot;</span><span class="punctuation">,</span>mu<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">  <span class="comment"># 正态分布曲线比较 (泊松分布的正态近似，当mu较大时)</span></span><br><span class="line">  lines<span class="punctuation">(</span>y<span class="punctuation">,</span>dnorm<span class="punctuation">(</span>y<span class="punctuation">,</span> mean <span class="operator">=</span> mu<span class="punctuation">,</span> sd <span class="operator">=</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>mu<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 均值=mu, 标准差=sqrt(mu)</span></span><br><span class="line">  points<span class="punctuation">(</span>y<span class="punctuation">,</span> fy<span class="punctuation">,</span> pch <span class="operator">=</span> <span class="number">19</span><span class="punctuation">,</span> cex <span class="operator">=</span> <span class="number">0.25</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line">  legend<span class="punctuation">(</span><span class="string">&quot;topright&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Normal&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Poisson&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">         col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> pch <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">19</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="comment"># 正态分布并非万能</span></span><br><span class="line"><span class="comment"># 当mu很小时，泊松分布与正态分布差异明显</span></span><br><span class="line">mu <span class="operator">=</span> <span class="number">0.5</span></span><br><span class="line">y <span class="operator">=</span> seq<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">6</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">fy <span class="operator">=</span> dpois<span class="punctuation">(</span>y<span class="punctuation">,</span>lambda <span class="operator">=</span> mu<span class="punctuation">)</span></span><br><span class="line">ny <span class="operator">=</span> dnorm<span class="punctuation">(</span>y<span class="punctuation">,</span> mean <span class="operator">=</span> mu<span class="punctuation">,</span> sd <span class="operator">=</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>mu<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>y<span class="punctuation">,</span> ny<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;b&quot;</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">,</span> ylab <span class="operator">=</span> <span class="string">&quot;fy&quot;</span><span class="punctuation">,</span></span><br><span class="line">     main <span class="operator">=</span> <span class="string">&quot;Normal Vs Poisson (mu=0.5)&quot;</span><span class="punctuation">,</span> ylim <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0.6</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">points<span class="punctuation">(</span>y<span class="punctuation">,</span> fy<span class="punctuation">,</span> pch <span class="operator">=</span> <span class="number">19</span><span class="punctuation">,</span> cex <span class="operator">=</span> <span class="number">0.5</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line">segments<span class="punctuation">(</span>y<span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span>y<span class="punctuation">,</span>fy<span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;topright&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Normal&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Poisson&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> pch <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">19</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 当n很小或p接近0或1时，二项分布与正态分布差异明显</span></span><br><span class="line">p <span class="operator">=</span> <span class="number">0.75</span></span><br><span class="line">n <span class="operator">=</span> <span class="number">6</span>; y <span class="operator">=</span> <span class="number">0</span><span class="operator">:</span>n</span><br><span class="line">fy <span class="operator">=</span> dbinom<span class="punctuation">(</span>y<span class="punctuation">,</span>size <span class="operator">=</span> n<span class="punctuation">,</span>prob <span class="operator">=</span> p<span class="punctuation">)</span></span><br><span class="line">ny <span class="operator">=</span> dnorm<span class="punctuation">(</span>y<span class="punctuation">,</span> mean <span class="operator">=</span> n<span class="operator">*</span>p<span class="punctuation">,</span> sd <span class="operator">=</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>n<span class="operator">*</span>p<span class="operator">*</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">-</span>p<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>y<span class="punctuation">,</span>ny<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;b&quot;</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">,</span>ylim <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">0.4</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">     main <span class="operator">=</span> <span class="string">&quot;Normal Vs Binomial (n=6,p=0.75)&quot;</span><span class="punctuation">)</span></span><br><span class="line">points<span class="punctuation">(</span>y<span class="punctuation">,</span> fy<span class="punctuation">,</span> pch <span class="operator">=</span> <span class="number">19</span><span class="punctuation">,</span> cex <span class="operator">=</span> <span class="number">0.5</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line">segments<span class="punctuation">(</span>y<span class="punctuation">,</span><span class="number">0</span><span class="punctuation">,</span>y<span class="punctuation">,</span>fy<span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;topleft&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Normal&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Binomial&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> pch <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">19</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在回归中差异更大</span></span><br><span class="line">x <span class="operator">=</span> <span class="operator">-</span><span class="number">30</span><span class="operator">:</span><span class="number">30</span></span><br><span class="line">beta0 <span class="operator">=</span> <span class="number">0.3</span></span><br><span class="line">beta1 <span class="operator">=</span> <span class="number">0.1</span></span><br><span class="line">mu <span class="operator">=</span> <span class="built_in">exp</span><span class="punctuation">(</span>beta0 <span class="operator">+</span> beta1<span class="operator">*</span> x<span class="punctuation">)</span> <span class="comment"># 泊松回归的均值通常通过指数连接函数建模</span></span><br><span class="line">y <span class="operator">=</span> rpois<span class="punctuation">(</span><span class="built_in">length</span><span class="punctuation">(</span>mu<span class="punctuation">)</span><span class="punctuation">,</span>lambda <span class="operator">=</span> mu<span class="punctuation">)</span> <span class="comment"># 生成泊松分布的响应变量</span></span><br><span class="line">sim.df <span class="operator">=</span> data.frame<span class="punctuation">(</span>x<span class="operator">=</span>x<span class="punctuation">,</span> y<span class="operator">=</span>y<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 必须使用灵活的线性模型 (如果用普通LM)</span></span><br><span class="line">fit.normal <span class="operator">=</span> lm<span class="punctuation">(</span>y<span class="operator">~</span>x<span class="punctuation">,</span> data <span class="operator">=</span> sim.df<span class="punctuation">)</span> <span class="comment"># 普通线性模型</span></span><br><span class="line">plot<span class="punctuation">(</span>fit.normal<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图可能显示异方差和非线性</span></span><br><span class="line">fit.quadratic.normal <span class="operator">=</span> lm<span class="punctuation">(</span>y<span class="operator">~</span>x<span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> sim.df<span class="punctuation">)</span> <span class="comment"># 二次模型</span></span><br><span class="line">plot<span class="punctuation">(</span>fit.quadratic.normal<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">fit.cubic.normal <span class="operator">=</span> lm<span class="punctuation">(</span>y<span class="operator">~</span>x<span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> sim.df<span class="punctuation">)</span> <span class="comment"># 三次模型</span></span><br><span class="line">plot<span class="punctuation">(</span>fit.cubic.normal<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">fit.quartic.normal <span class="operator">=</span> lm<span class="punctuation">(</span>y<span class="operator">~</span>x<span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">2</span><span class="punctuation">)</span><span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">3</span><span class="punctuation">)</span><span class="operator">+</span>I<span class="punctuation">(</span>x<span class="operator">^</span><span class="number">4</span><span class="punctuation">)</span><span class="punctuation">,</span> data <span class="operator">=</span> sim.df<span class="punctuation">)</span> <span class="comment"># 四次模型</span></span><br><span class="line">plot<span class="punctuation">(</span>fit.quartic.normal<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 通常拟合线会弯弯曲曲</span></span><br><span class="line">plot<span class="punctuation">(</span>x<span class="punctuation">,</span>y<span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span> fitted.values<span class="punctuation">(</span>fit.quadratic.normal<span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>x<span class="punctuation">,</span>y<span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span> fitted.values<span class="punctuation">(</span>fit.cubic.normal<span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>x<span class="punctuation">,</span>y<span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span> fitted.values<span class="punctuation">(</span>fit.quartic.normal<span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确的泊松假设</span></span><br><span class="line">fit.poisson <span class="operator">=</span> glm<span class="punctuation">(</span>y<span class="operator">~</span>x<span class="punctuation">,</span> family <span class="operator">=</span> poisson<span class="punctuation">,</span> data <span class="operator">=</span> sim.df<span class="punctuation">)</span> <span class="comment"># 使用glm和poisson族</span></span><br><span class="line">summary<span class="punctuation">(</span>fit.poisson<span class="punctuation">)</span></span><br><span class="line">beta0;beta1 <span class="comment"># 真实参数值</span></span><br><span class="line">plot<span class="punctuation">(</span>x<span class="punctuation">,</span>y<span class="punctuation">,</span> main <span class="operator">=</span> <span class="string">&quot;Poisson data&quot;</span><span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span>predict<span class="punctuation">(</span>fit.poisson<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span> <span class="comment"># 泊松回归拟合线 (响应尺度)</span></span><br><span class="line">lines<span class="punctuation">(</span>x<span class="punctuation">,</span> fitted.values<span class="punctuation">(</span>fit.quartic.normal<span class="punctuation">)</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 普通LM(四次)拟合线</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;topleft&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Normal&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Poisson&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 不仔细看差别不大</span></span><br><span class="line">summaryStats<span class="punctuation">(</span>fitted.values<span class="punctuation">(</span>fit.quartic.normal<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 普通LM的拟合值可能为负</span></span><br><span class="line"><span class="comment"># 当扩展到给定数据范围之外时，问题更大</span></span><br><span class="line">x.plot <span class="operator">=</span> <span class="operator">-</span><span class="number">60</span><span class="operator">:</span><span class="number">60</span> <span class="comment"># 更宽的x范围用于外推</span></span><br><span class="line">mu.plot <span class="operator">=</span> <span class="built_in">exp</span><span class="punctuation">(</span><span class="number">0.3</span> <span class="operator">+</span> <span class="number">0.1</span><span class="operator">*</span> x.plot<span class="punctuation">)</span></span><br><span class="line">y.plot <span class="operator">=</span> rpois<span class="punctuation">(</span><span class="built_in">length</span><span class="punctuation">(</span>mu.plot<span class="punctuation">)</span><span class="punctuation">,</span>lambda <span class="operator">=</span> mu.plot<span class="punctuation">)</span></span><br><span class="line">y.pois <span class="operator">=</span> predict<span class="punctuation">(</span>fit.poisson<span class="punctuation">,</span> newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span>x<span class="operator">=</span>x.plot<span class="punctuation">)</span><span class="punctuation">,</span>type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">y.norm <span class="operator">=</span> predict<span class="punctuation">(</span>fit.quartic.normal<span class="punctuation">,</span> newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span>x<span class="operator">=</span>x.plot<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 普通LM外推非常不可靠</span></span><br><span class="line">plot<span class="punctuation">(</span>x.plot<span class="punctuation">,</span> y.plot<span class="punctuation">,</span> main <span class="operator">=</span> <span class="string">&quot;Beyond Observed Poisson data&quot;</span><span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>x.plot<span class="punctuation">,</span> y.pois<span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span> <span class="comment"># 泊松回归外推</span></span><br><span class="line">lines<span class="punctuation">(</span>x.plot<span class="punctuation">,</span> y.norm<span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 普通LM外推</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;topleft&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Normal&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Poisson&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;red&quot;</span><span class="punctuation">,</span><span class="string">&quot;blue&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># setwd(&quot;~/Desktop&quot;) # 设置工作目录 (根据用户实际情况)</span></span><br><span class="line">CRAN.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;CRAN.txt&quot;</span><span class="punctuation">,</span> header<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">)</span> <span class="comment"># 读取CRAN R包提交数据</span></span><br><span class="line">CRAN.df</span><br><span class="line"><span class="comment">## 原始y的散点图</span></span><br><span class="line">plot<span class="punctuation">(</span>Number <span class="operator">~</span> Year<span class="punctuation">,</span> data <span class="operator">=</span> CRAN.df<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## log(y)的散点图</span></span><br><span class="line">plot<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>Number<span class="punctuation">)</span> <span class="operator">~</span> Year<span class="punctuation">,</span> data <span class="operator">=</span> CRAN.df<span class="punctuation">)</span></span><br><span class="line">CRAN.fit <span class="operator">=</span> lm<span class="punctuation">(</span><span class="built_in">log</span><span class="punctuation">(</span>Number<span class="punctuation">)</span> <span class="operator">~</span> Year<span class="punctuation">,</span> data <span class="operator">=</span> CRAN.df<span class="punctuation">)</span> <span class="comment"># 对log(Number)拟合线性模型</span></span><br><span class="line">plot<span class="punctuation">(</span>CRAN.fit<span class="punctuation">,</span>which<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图</span></span><br><span class="line">normcheck<span class="punctuation">(</span>CRAN.fit<span class="punctuation">)</span> <span class="comment"># 正态性检查</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>CRAN.fit<span class="punctuation">)</span> <span class="comment"># 影响点检查</span></span><br><span class="line">summary<span class="punctuation">(</span>CRAN.fit<span class="punctuation">)</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>CRAN.fit<span class="operator">$</span>coef<span class="punctuation">[</span><span class="string">&quot;Year&quot;</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 年份的乘性效应因子 (中位数)</span></span><br><span class="line"><span class="comment">## 置信区间</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>CRAN.fit<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 年份效应的百分比变化 (中位数)</span></span><br><span class="line">predCRAN.df <span class="operator">=</span> data.frame<span class="punctuation">(</span>Year <span class="operator">=</span> <span class="number">2017</span><span class="punctuation">)</span> <span class="comment"># 预测2017年</span></span><br><span class="line">pred2017 <span class="operator">=</span> predict<span class="punctuation">(</span>CRAN.fit<span class="punctuation">,</span> predCRAN.df<span class="punctuation">,</span> interval <span class="operator">=</span> <span class="string">&quot;confidence&quot;</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## 对数尺度上的预测</span></span><br><span class="line">pred2017</span><br><span class="line"><span class="comment">## 反转换得到2017年提交数量的中位数预测</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>pred2017<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 泊松拟合</span></span><br><span class="line">CRAN.pois.fit <span class="operator">=</span> glm<span class="punctuation">(</span>Number <span class="operator">~</span> Year<span class="punctuation">,</span> family <span class="operator">=</span> poisson<span class="punctuation">,</span> data <span class="operator">=</span> CRAN.df<span class="punctuation">)</span> <span class="comment"># 使用glm进行泊松回归</span></span><br><span class="line">summary<span class="punctuation">(</span>CRAN.pois.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 如果泊松假设为真，残差应近似标准正态</span></span><br><span class="line">plot<span class="punctuation">(</span>CRAN.pois.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 查看残差图 (注意glm的残差类型与lm不同)</span></span><br><span class="line"><span class="comment"># 离散度检验：原假设是模型充分（即泊松假设适合，特别是均值=方差）</span></span><br><span class="line"><span class="comment"># 通常查看残差离散度 (Residual Deviance) 与自由度的比值</span></span><br><span class="line"><span class="number">1</span> <span class="operator">-</span> pchisq<span class="punctuation">(</span><span class="number">402.61</span><span class="punctuation">,</span><span class="number">10</span><span class="punctuation">)</span> <span class="comment"># summary(CRAN.pois.fit)$deviance, summary(CRAN.pois.fit)$df.residual</span></span><br><span class="line"><span class="comment"># p值非常小，表明模型不充分，可能存在过离散 (overdispersion)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 补救措施：使用类泊松 (quasipoisson)</span></span><br><span class="line">CRAN.quasi.pois.fit <span class="operator">=</span> glm<span class="punctuation">(</span></span><br><span class="line">  Number <span class="operator">~</span> Year<span class="punctuation">,</span> family <span class="operator">=</span> quasipoisson<span class="punctuation">,</span> data <span class="operator">=</span> CRAN.df<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>CRAN.quasi.pois.fit<span class="punctuation">)</span> <span class="comment"># 注意此时会估计一个离散参数 (Dispersion parameter)</span></span><br><span class="line"><span class="comment"># 解释</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>CRAN.quasi.pois.fit<span class="operator">$</span>coef<span class="punctuation">[</span><span class="string">&quot;Year&quot;</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 年份的乘性效应因子 (均值)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>CRAN.quasi.pois.fit<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 年份效应的百分比变化 (均值)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>CRAN.fit<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 与之前log(LM)的结果比较 (解释的是中位数)</span></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">new.CRAN.df <span class="operator">=</span> data.frame<span class="punctuation">(</span>Year <span class="operator">=</span> <span class="number">2017</span><span class="punctuation">)</span></span><br><span class="line">predictGLM<span class="punctuation">(</span>CRAN.quasi.pois.fit<span class="punctuation">,</span> new.CRAN.df<span class="punctuation">)</span> <span class="comment"># 预测 (默认link尺度，即log(均值))</span></span><br><span class="line">pred2017.quasi<span class="operator">=</span>predict<span class="punctuation">(</span>CRAN.quasi.pois.fit<span class="punctuation">,</span> new.CRAN.df<span class="punctuation">,</span> se.fit<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 获取预测值和标准误</span></span><br><span class="line"><span class="comment"># log(均值)的置信区间</span></span><br><span class="line">lower <span class="operator">=</span> pred2017.quasi<span class="operator">$</span>fit<span class="operator">-</span><span class="number">1.96</span><span class="operator">*</span>pred2017.quasi<span class="operator">$</span>se.fit</span><br><span class="line">upper <span class="operator">=</span> pred2017.quasi<span class="operator">$</span>fit<span class="operator">+</span><span class="number">1.96</span><span class="operator">*</span>pred2017.quasi<span class="operator">$</span>se.fit</span><br><span class="line"><span class="comment"># 均值的置信区间</span></span><br><span class="line">pred2017.ci.mean<span class="operator">=</span><span class="built_in">exp</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span>lower<span class="punctuation">,</span> upper<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">pred2017.ci.mean</span><br><span class="line"><span class="comment"># exp(predictGLM(CRAN.quasi.pois.fit, new.CRAN.df)) # 等价于上面的手动计算</span></span><br><span class="line">predictGLM<span class="punctuation">(</span>CRAN.quasi.pois.fit<span class="punctuation">,</span> new.CRAN.df<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span> <span class="comment"># 直接在响应尺度上预测和获取CI</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 泊松回归的预测区间不作考核</span></span><br><span class="line"><span class="comment"># --- 处理数值和分类变量 (地震数据) ---</span></span><br><span class="line">Quakes.df<span class="operator">=</span>read.table<span class="punctuation">(</span><span class="string">&quot;EarthquakeMagnitudes.txt&quot;</span><span class="punctuation">,</span>header<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取地震数据</span></span><br><span class="line">Quakes.df<span class="operator">$</span>Locn<span class="operator">=</span>as.factor<span class="punctuation">(</span>Quakes.df<span class="operator">$</span>Locn<span class="punctuation">)</span> <span class="comment"># 将地点转换为因子</span></span><br><span class="line">subset<span class="punctuation">(</span>Quakes.df<span class="punctuation">,</span>subset<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span>Locn<span class="operator">==</span><span class="string">&quot;SC&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">4</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="comment"># 打印南加州前4行</span></span><br><span class="line">subset<span class="punctuation">(</span>Quakes.df<span class="punctuation">,</span>subset<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span>Locn<span class="operator">==</span><span class="string">&quot;WA&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">4</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="comment"># 打印华盛顿州前4行</span></span><br><span class="line">plot<span class="punctuation">(</span>Freq<span class="operator">~</span>Magnitude<span class="punctuation">,</span>data<span class="operator">=</span>Quakes.df<span class="punctuation">,</span>pch<span class="operator">=</span>substr<span class="punctuation">(</span>Locn<span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 频数对震级的散点图，按地点区分</span></span><br><span class="line">Quake.fit <span class="operator">=</span> glm<span class="punctuation">(</span>Freq <span class="operator">~</span> Locn <span class="operator">*</span> Magnitude<span class="punctuation">,</span> <span class="comment"># 拟合带交互作用的泊松回归</span></span><br><span class="line">                 family <span class="operator">=</span> poisson<span class="punctuation">,</span> data <span class="operator">=</span> Quakes.df<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>Quake.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图</span></span><br><span class="line">plot<span class="punctuation">(</span>Quake.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">4</span><span class="punctuation">)</span> <span class="comment"># Cook&#x27;s距离图</span></span><br><span class="line"><span class="number">1</span> <span class="operator">-</span> pchisq<span class="punctuation">(</span>summary<span class="punctuation">(</span>Quake.fit<span class="punctuation">)</span><span class="operator">$</span>deviance<span class="punctuation">,</span> summary<span class="punctuation">(</span>Quake.fit<span class="punctuation">)</span><span class="operator">$</span>df.residual<span class="punctuation">)</span> <span class="comment"># 离散度检验 (p值大，模型可接受)</span></span><br><span class="line">summary<span class="punctuation">(</span>Quake.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 南加州的震级效应</span></span><br><span class="line">Quake.cis <span class="operator">=</span> confint<span class="punctuation">(</span>Quake.fit<span class="punctuation">)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>Quake.cis<span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 震级每增加1，频数均值变化的百分比 (SC)</span></span><br><span class="line"><span class="comment"># 更改基准到华盛顿州</span></span><br><span class="line">Quakes.df<span class="operator">$</span>Locn2<span class="operator">=</span>factor<span class="punctuation">(</span>Quakes.df<span class="operator">$</span>Locn<span class="punctuation">,</span>levels<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;WA&quot;</span><span class="punctuation">,</span><span class="string">&quot;SC&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">Quake2.fit<span class="operator">=</span>glm<span class="punctuation">(</span>Freq<span class="operator">~</span>Locn2<span class="operator">*</span>Magnitude<span class="punctuation">,</span>family<span class="operator">=</span>poisson<span class="punctuation">,</span>data<span class="operator">=</span>Quakes.df<span class="punctuation">)</span></span><br><span class="line">Quake.WA.ci <span class="operator">=</span> confint<span class="punctuation">(</span>Quake2.fit<span class="punctuation">)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>Quake.WA.ci<span class="punctuation">[</span><span class="number">3</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 震级每增加1，频数均值变化的百分比 (WA)</span></span><br><span class="line"><span class="comment"># 展示结果的最佳方式</span></span><br><span class="line">no.sc <span class="operator">=</span> predictGLM<span class="punctuation">(</span>Quake.fit<span class="punctuation">,</span> subset<span class="punctuation">(</span>Quakes.df<span class="punctuation">,</span>subset<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span>Locn<span class="operator">==</span><span class="string">&quot;SC&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                   type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span> <span class="comment"># 预测SC的频数均值</span></span><br><span class="line">mag.sc <span class="operator">=</span> subset<span class="punctuation">(</span>Quakes.df<span class="punctuation">,</span>subset<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span>Locn<span class="operator">==</span><span class="string">&quot;SC&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="operator">$</span>Magnitude</span><br><span class="line">no.was <span class="operator">=</span> predictGLM<span class="punctuation">(</span>Quake.fit<span class="punctuation">,</span> subset<span class="punctuation">(</span>Quakes.df<span class="punctuation">,</span>subset<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span>Locn<span class="operator">==</span><span class="string">&quot;WA&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                   type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span> <span class="comment"># 预测WA的频数均值</span></span><br><span class="line">mag.was <span class="operator">=</span> subset<span class="punctuation">(</span>Quakes.df<span class="punctuation">,</span>subset<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span>Locn<span class="operator">==</span><span class="string">&quot;WA&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="operator">$</span>Magnitude</span><br><span class="line">plot<span class="punctuation">(</span>Freq<span class="operator">~</span>Magnitude<span class="punctuation">,</span>data<span class="operator">=</span>Quakes.df<span class="punctuation">,</span>pch<span class="operator">=</span>substr<span class="punctuation">(</span>Locn<span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>mag.sc<span class="punctuation">,</span> no.sc<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;blue&quot;</span><span class="punctuation">)</span> <span class="comment"># 绘制SC的拟合线</span></span><br><span class="line">lines<span class="punctuation">(</span>mag.was<span class="punctuation">,</span> no.was<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 绘制WA的拟合线</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;topright&quot;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Southern California&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Washington&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       lty <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;blue&quot;</span><span class="punctuation">,</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 鲷鱼数据 (两个分类变量)，类似于双因素方差分析 ---</span></span><br><span class="line">Snap.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;SnapperCROPvsHAHEI.txt&quot;</span><span class="punctuation">,</span> header <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取鲷鱼数据</span></span><br><span class="line">interactionPlots<span class="punctuation">(</span>Freq <span class="operator">~</span> Locn <span class="operator">*</span> Reserve<span class="punctuation">,</span> data <span class="operator">=</span> Snap.df<span class="punctuation">)</span> <span class="comment"># 交互作用图 (注意是原始频数)</span></span><br><span class="line"><span class="comment"># 从交互作用模型开始</span></span><br><span class="line">Snap.glm <span class="operator">=</span> glm<span class="punctuation">(</span>Freq <span class="operator">~</span> Locn <span class="operator">*</span> Reserve<span class="punctuation">,</span> family <span class="operator">=</span> poisson<span class="punctuation">,</span> data <span class="operator">=</span> Snap.df<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>Snap.glm<span class="punctuation">)</span></span><br><span class="line"><span class="number">1</span> <span class="operator">-</span> pchisq<span class="punctuation">(</span>summary<span class="punctuation">(</span>Snap.glm<span class="punctuation">)</span><span class="operator">$</span>deviance<span class="punctuation">,</span> summary<span class="punctuation">(</span>Snap.glm<span class="punctuation">)</span><span class="operator">$</span>df.residual<span class="punctuation">)</span> <span class="comment"># 离散度检验 (p值大，模型可接受)</span></span><br><span class="line">anova<span class="punctuation">(</span>Snap.glm<span class="punctuation">,</span> test<span class="operator">=</span><span class="string">&quot;Chisq&quot;</span><span class="punctuation">)</span> <span class="comment"># 检验交互作用的显著性 (Chisq检验用于GLM)</span></span><br><span class="line">Snap2.glm <span class="operator">=</span> glm<span class="punctuation">(</span>Freq <span class="operator">~</span> Locn <span class="operator">+</span> Reserve<span class="punctuation">,</span> family <span class="operator">=</span> poisson<span class="punctuation">,</span> data <span class="operator">=</span> Snap.df<span class="punctuation">)</span> <span class="comment"># 如果交互不显著，拟合主效应模型</span></span><br><span class="line">anova<span class="punctuation">(</span>Snap2.glm<span class="punctuation">,</span> test<span class="operator">=</span><span class="string">&quot;Chisq&quot;</span><span class="punctuation">)</span> <span class="comment"># 主效应模型的ANOVA表</span></span><br><span class="line"><span class="number">1</span> <span class="operator">-</span> pchisq<span class="punctuation">(</span>summary<span class="punctuation">(</span>Snap2.glm<span class="punctuation">)</span><span class="operator">$</span>deviance<span class="punctuation">,</span> summary<span class="punctuation">(</span>Snap2.glm<span class="punctuation">)</span><span class="operator">$</span>df.residual<span class="punctuation">)</span> <span class="comment"># 再次检查离散度</span></span><br><span class="line">summary<span class="punctuation">(</span>Snap2.glm<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>Snap2.glm<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>Snap2.glm<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 解释主效应 (均值的百分比变化)</span></span><br></pre></td></tr></table></figure>
<h3 id="Lecture-11">Lecture 11</h3>
<p>这个lecture主要讲解<strong>Logistic回归</strong>，它是广义线性模型的一种，专门用于处理<strong>二元（是/否，0/1）响应变量</strong>或<strong>比例数据</strong>（成功次数/总试验次数）。核心概念包括odds、log-odds以及如何解释模型系数。这与教材 <code>STATS201 book SWU 2023.pdf</code> 第15章“使用二项分布建模比例数据”和第16章“列联表分析” 的内容相关。</p>
<p>这个lecture全面介绍了<strong>广义线性模型 (GLM)</strong> 的另一个重要分支：<strong>Logistic回归</strong>，用于分析二元响应变量或比例数据。同时，它也开始引入<strong>列联表分析</strong>的概念，并展示了Poisson回归和Logistic回归在特定情况下（饱和模型分析关联性）的等价性。</p>
<ol>
<li>
<p><strong>二元数据与Binomial/Bernoulli分布</strong>:</p>
<ul>
<li>当响应变量只有两个可能的结果（如成功/失败，是/否，0/1）时，称为二元数据 (binary data)。单次试验服从Bernoulli分布。多次独立重复的Bernoulli试验中成功次数服从Binomial分布。 教材第15章 (第4-5页) 对此有介绍。</li>
<li>代码中篮球投篮数据 (<code>bb.df</code>) 的 <code>basket</code> 变量 (0或1) 是典型的二元数据。</li>
</ul>
</li>
<li>
<p><strong>Odds 和 Log-Odds (Logit)</strong>:</p>
<ul>
<li>Odds (几率)：事件发生的概率与不发生的概率之比，即Odds=p/(1−p)。</li>
<li>Log-Odds (对数几率，logit)：logit(p)=log(Odds)=log(p/(1−p))。Log-odds的取值范围是(−∞,+∞)，这使得它适合作为线性模型的响应部分。</li>
<li>从log-odds反推概率p的函数是logistic函数：p=exp(log-odds)/(1+exp(log-odds))=plogis(log-odds)。</li>
<li>教材第15章 (第7-9页) 详细解释了odds和log-odds。</li>
</ul>
</li>
<li>
<p><strong>Logistic回归模型</strong>:</p>
<ul>
<li>
<p>模型形式：logit(pi)=β0+β1Xi1+...+βkXik。这意味着解释变量与成功概率pi的对数几率呈线性关系。</p>
</li>
<li>
<p>R实现：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glm(response ~ predictors, family = binomial, data = df)</span><br></pre></td></tr></table></figure>
<p>。</p>
<ul>
<li>
<p>对于<strong>未分组的二元数据</strong> (如 <code>bb.df</code>): <code>response</code> 是0/1变量。</p>
</li>
<li>
<p>对于分组数据</p>
<p>(如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">bb.grouped.df</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Space.df</span><br></pre></td></tr></table></figure>
<p>):</p>
<ul>
<li>
<pre><code>response
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">可以是 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
cbind(success_count, failure_count)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  。 </span><br><span class="line"></span><br><span class="line">- 或者 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
response
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">是成功比例，并使用 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
weights = total_count
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">       参数。 </span><br><span class="line"></span><br><span class="line">- 系数解释:</span><br><span class="line"></span><br><span class="line">  - βj 表示当其他变量不变时，Xj 每增加一个单位，log(Odds) 的变化量。</span><br><span class="line"></span><br><span class="line">  - exp(βj) 表示当其他变量不变时，Xj 每增加一个单位，Odds 的**乘性变化因子 (Odds Ratio, OR)**。</span><br><span class="line"></span><br><span class="line">  - (exp(βj)−1)×100% 是 Odds 的百分比变化。</span><br><span class="line"></span><br><span class="line">  - 代码中 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p>100*(exp(coef(bb.fit2)[2])-1)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">和 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>100*(exp(bb.ci2[2,])-1)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">        就是这种解释。教材第15章 (第27-28页)。</span><br><span class="line"></span><br><span class="line">4. **模型诊断与过离散 (针对分组数据)**:</span><br><span class="line"></span><br><span class="line">   - 对于分组的二项数据，可以像Poisson回归一样使用残差离散度来检查模型拟合优度。如果 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>1 - pchisq(deviance, df.residual)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">   的p值很小，可能表明存在过离散(或欠离散)。 </span><br><span class="line"></span><br><span class="line">- 处理过离散：使用 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>family = quasibinomial</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">     。这会调整标准误和p值，但不改变系数估计。 </span><br><span class="line"></span><br><span class="line">   - 对于未分组的二元数据，残差图和离散度检验的解释性有限。 </span><br><span class="line"></span><br><span class="line">5. **预测**:</span><br><span class="line"></span><br><span class="line">   - `predict(glm_fit, newdata, type = &quot;link&quot;)` 给出log-odds尺度的预测。</span><br><span class="line"></span><br><span class="line">   - ```</span><br><span class="line">     predict(glm_fit, newdata, type = &quot;response&quot;)</span><br></pre></td></tr></table></figure>
<p>给出概率尺度的预测。</p>
</li>
<li>
<p><code>predictGLM()</code> (来自 <code>s20x</code> 包) 可以方便地给出这两个尺度上的预测值及其置信区间。</p>
</li>
</ul>
</li>
<li>
<p><strong>挑战者号航天飞机灾难案例</strong>:</p>
<ul>
<li>
<p>这是一个著名的错误应用线性模型分析比例数据导致灾难性后果的案例。直接对比例数据 (O环损坏数/总数) 用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">lm()</span><br></pre></td></tr></table></figure>
<p>拟合是错误的，因为它可能预测出小于0或大于1的概率，且未考虑二项分布的特性。</p>
</li>
<li>
<p>正确的做法是使用Logistic回归 (</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glm(cbind(Failure, 6-Failure) ~ Temp, family = binomial, ...)</span><br></pre></td></tr></table></figure>
<p>)。 教材第15章 (第45-52页) 详细描述了此案例。</p>
</li>
</ul>
</li>
<li>
<p><strong>列联表分析初探</strong>:</p>
<ul>
<li>
<p>当所有变量都是分类变量时，数据可以总结为列联表 (contingency table)。</p>
</li>
<li>
<p>可以使用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">xtabs()</span><br></pre></td></tr></table></figure>
<p>创建列联表，并用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plot()</span><br></pre></td></tr></table></figure>
<p>(马赛克图) 或</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">barplot()</span><br></pre></td></tr></table></figure>
<p>可视化。</p>
</li>
<li>
<p>关联性研究:</p>
<ul>
<li>
<p>Poisson回归方法</p>
<p>: 可以将列联表中的频数作为响应变量，用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glm(Freq ~ FactorA * FactorB, family = poisson, ...)</span><br></pre></td></tr></table></figure>
<p>拟合。交互作用项的系数exp(βinteraction)即为Odds Ratio (OR)，表示一个因子水平下，另一个因子不同水平间的odds比率。</p>
</li>
<li>
<p>Logistic回归方法 (等价性)</p>
<p>: 如果将其中一个因子视为响应（如Pass/Fail），另一个因子视为解释变量，用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">glm(cbind(Pass_counts, Fail_counts) ~ Factor_predictor, family=binomial, ...)</span><br></pre></td></tr></table></figure>
<p>拟合，其解释变量的系数exp(βpredictor)也解释为Odds Ratio，且与Poisson模型交互作用项的OR在饱和模型中是等价的。</p>
</li>
<li>
<p>代码中演示了对 <code>AttendPass</code> 数据分别用这两种GLM方法以及直接计算OR，结果相似。</p>
</li>
<li>
<p>教材第16章 (第18-29页) 详细讨论了这种等价性。</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>总结：Lecture 11 是 GLM 的重要组成部分，系统介绍了 Logistic 回归的原理、实现、解释和预测，并通过实例（包括著名的挑战者号案例）强调了其在处理二元/比例数据时的正确性和重要性。同时，也开始将 GLM 应用于列联表的关联性分析，为更复杂的分类数据分析打下基础。</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Logistic 回归 - 篮球数据示例</span></span><br><span class="line">library<span class="punctuation">(</span>s20x<span class="punctuation">)</span> <span class="comment"># 加载s20x包</span></span><br><span class="line">bb.df <span class="operator">=</span> read.csv<span class="punctuation">(</span><span class="string">&quot;basketball.csv&quot;</span><span class="punctuation">)</span> <span class="comment"># 读取篮球投篮数据</span></span><br><span class="line">head<span class="punctuation">(</span>bb.df<span class="punctuation">,</span> <span class="number">10</span><span class="punctuation">)</span> <span class="comment"># 查看数据前10行</span></span><br><span class="line">bb.fit <span class="operator">=</span> glm<span class="punctuation">(</span>basket <span class="operator">~</span> distance <span class="operator">*</span> gender<span class="punctuation">,</span> <span class="comment"># basket(0或1) 对 距离和性别的交互作用模型</span></span><br><span class="line">             family <span class="operator">=</span> binomial<span class="punctuation">,</span> data <span class="operator">=</span> bb.df<span class="punctuation">)</span> <span class="comment"># 使用二项分布族 (logistic连接函数是默认)</span></span><br><span class="line">plot<span class="punctuation">(</span>bb.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span> lty<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 残差图 (对于未分组二元数据，解释性有限)</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.fit<span class="punctuation">)</span> <span class="comment"># 查看模型摘要</span></span><br><span class="line"><span class="number">1</span><span class="operator">-</span>pchisq<span class="punctuation">(</span><span class="number">46.202</span><span class="punctuation">,</span> <span class="number">56</span><span class="punctuation">)</span> <span class="comment"># 离散度检验的近似 (对于未分组数据，解释需谨慎)</span></span><br><span class="line">bb.fit1 <span class="operator">=</span> glm<span class="punctuation">(</span>basket <span class="operator">~</span> distance <span class="operator">+</span> gender<span class="punctuation">,</span> family <span class="operator">=</span> binomial<span class="punctuation">,</span> data <span class="operator">=</span> bb.df<span class="punctuation">)</span> <span class="comment"># 移除交互作用</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.fit1<span class="punctuation">)</span></span><br><span class="line">bb.fit2 <span class="operator">=</span> glm<span class="punctuation">(</span>basket <span class="operator">~</span> distance<span class="punctuation">,</span> family <span class="operator">=</span> binomial<span class="punctuation">,</span> data <span class="operator">=</span> bb.df<span class="punctuation">)</span> <span class="comment"># 仅保留距离作为预测变量</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.fit2<span class="punctuation">)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>coef<span class="punctuation">(</span>bb.fit2<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 距离每增加1单位，odds变化的百分比</span></span><br><span class="line"><span class="punctuation">(</span>bb.ci2 <span class="operator">=</span> confint<span class="punctuation">(</span>bb.fit2<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 系数的置信区间 (log-odds尺度)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>bb.ci2<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">,</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 距离每增加1单位，odds变化百分比的置信区间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">predn.df<span class="operator">=</span>data.frame<span class="punctuation">(</span>distance <span class="operator">=</span> <span class="number">1</span><span class="operator">:</span><span class="number">3</span><span class="punctuation">)</span> <span class="comment"># 创建新的距离值用于预测</span></span><br><span class="line">bb.logit.pred <span class="operator">=</span> predict<span class="punctuation">(</span>bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> predn.df<span class="punctuation">)</span> <span class="comment"># 预测 (默认在log-odds尺度，即线性预测子)</span></span><br><span class="line">bb.logit.pred <span class="comment"># 这是在log-odds尺度上</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>bb.logit.pred<span class="punctuation">)</span> <span class="comment"># 转换到odds尺度</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>bb.logit.pred<span class="punctuation">)</span> <span class="operator">/</span> <span class="punctuation">(</span><span class="number">1</span><span class="operator">+</span><span class="built_in">exp</span><span class="punctuation">(</span>bb.logit.pred<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 从odds转换到概率尺度 p = odds/(1+odds)</span></span><br><span class="line">plogis<span class="punctuation">(</span>bb.logit.pred<span class="punctuation">)</span> <span class="comment"># 内置函数，从log-odds转换到概率尺度 (logistic函数)</span></span><br><span class="line">predict<span class="punctuation">(</span>bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> predn.df<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span> <span class="comment"># 直接在概率尺度上预测</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 概率的置信区间</span></span><br><span class="line">bb.logit.predses <span class="operator">=</span> predict<span class="punctuation">(</span> <span class="comment"># 获取log-odds尺度预测的标准误</span></span><br><span class="line">  bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> predn.df<span class="punctuation">,</span> se.fit <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span><span class="operator">$</span>se.fit</span><br><span class="line"><span class="comment"># log-odds的置信区间上下限</span></span><br><span class="line">lower <span class="operator">=</span> bb.logit.pred <span class="operator">-</span> <span class="number">1.96</span><span class="operator">*</span>bb.logit.predses</span><br><span class="line">upper <span class="operator">=</span> bb.logit.pred <span class="operator">+</span> <span class="number">1.96</span><span class="operator">*</span>bb.logit.predses</span><br><span class="line">ci.logit <span class="operator">=</span> cbind<span class="punctuation">(</span>lower<span class="punctuation">,</span> upper<span class="punctuation">)</span> <span class="comment"># log-odds的置信区间</span></span><br><span class="line"><span class="comment"># 转换为概率的置信区间</span></span><br><span class="line">ci.prob <span class="operator">=</span> plogis<span class="punctuation">(</span>ci.logit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 或者使用s20x包中的predictGLM</span></span><br><span class="line">predictGLM<span class="punctuation">(</span>bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> predn.df<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;link&quot;</span><span class="punctuation">)</span> <span class="comment"># log-odds尺度的CI</span></span><br><span class="line">predictGLM<span class="punctuation">(</span>bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> predn.df<span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span> <span class="comment"># 概率尺度的CI</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 展示模型</span></span><br><span class="line">plot<span class="punctuation">(</span>basket <span class="operator">~</span> distance<span class="punctuation">,</span> xlim <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">,</span> ylim<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="comment"># 绘制原始0/1数据点</span></span><br><span class="line">     ylab <span class="operator">=</span> <span class="string">&quot;fitted values (Est prob)&quot;</span><span class="punctuation">,</span> data <span class="operator">=</span> bb.df<span class="punctuation">)</span></span><br><span class="line">distance.plot <span class="operator">=</span> seq<span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">0.1</span><span class="punctuation">)</span> <span class="comment"># 创建平滑的距离序列</span></span><br><span class="line">phat <span class="operator">=</span> predict<span class="punctuation">(</span>bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span> <span class="comment"># 预测概率</span></span><br><span class="line">  distance<span class="operator">=</span>distance.plot<span class="punctuation">)</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>distance.plot<span class="punctuation">,</span> phat<span class="punctuation">)</span> <span class="comment"># 绘制拟合的S形曲线</span></span><br><span class="line">ci.plot <span class="operator">=</span> predictGLM<span class="punctuation">(</span>bb.fit2<span class="punctuation">,</span> newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span> <span class="comment"># 获取概率的置信带</span></span><br><span class="line">  distance<span class="operator">=</span>distance.plot<span class="punctuation">)</span><span class="punctuation">,</span> type<span class="operator">=</span><span class="string">&quot;response&quot;</span><span class="punctuation">)</span></span><br><span class="line">lines<span class="punctuation">(</span>distance.plot<span class="punctuation">,</span> ci.plot<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 置信带下限</span></span><br><span class="line">lines<span class="punctuation">(</span>distance.plot<span class="punctuation">,</span> ci.plot<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">]</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 置信带上限</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 未分组 vs 分组数据</span></span><br><span class="line">head<span class="punctuation">(</span>bb.df<span class="punctuation">)</span> <span class="comment"># 未分组数据</span></span><br><span class="line">success.tbl <span class="operator">=</span> xtabs<span class="punctuation">(</span>basket<span class="operator">~</span>distance<span class="operator">+</span>gender<span class="punctuation">,</span> data <span class="operator">=</span> bb.df<span class="punctuation">)</span> <span class="comment"># 创建成功次数的列联表</span></span><br><span class="line">success.tbl <span class="comment"># 成功次数表</span></span><br><span class="line"><span class="built_in">sum</span><span class="punctuation">(</span>success.tbl<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># 总成功次数</span></span><br><span class="line"><span class="built_in">sum</span><span class="punctuation">(</span>bb.df<span class="operator">$</span>basket<span class="punctuation">)</span> <span class="comment"># 与上面应一致</span></span><br><span class="line">str<span class="punctuation">(</span>success.tbl<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">(</span>dist.unique <span class="operator">=</span> <span class="built_in">attr</span><span class="punctuation">(</span>success.tbl<span class="punctuation">,</span> <span class="string">&quot;dimnames&quot;</span><span class="punctuation">)</span><span class="operator">$</span>distance<span class="punctuation">)</span> <span class="comment"># 获取距离的唯一值</span></span><br><span class="line"><span class="punctuation">(</span>distance <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">as.numeric</span><span class="punctuation">(</span>dist.unique<span class="punctuation">)</span><span class="punctuation">,</span> times <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 重复以匹配性别</span></span><br><span class="line"><span class="punctuation">(</span>gender.unique <span class="operator">=</span> <span class="built_in">attr</span><span class="punctuation">(</span>success.tbl<span class="punctuation">,</span> <span class="string">&quot;dimnames&quot;</span><span class="punctuation">)</span><span class="operator">$</span>gender<span class="punctuation">)</span> <span class="comment"># 获取性别的唯一值</span></span><br><span class="line"><span class="punctuation">(</span>gender <span class="operator">=</span> <span class="built_in">rep</span><span class="punctuation">(</span>gender.unique<span class="punctuation">,</span> each <span class="operator">=</span> <span class="number">3</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 重复以匹配距离</span></span><br><span class="line">fail.tbl <span class="operator">=</span> xtabs<span class="punctuation">(</span><span class="number">1</span><span class="operator">-</span>basket<span class="operator">~</span>distance<span class="operator">+</span>gender<span class="punctuation">,</span> data <span class="operator">=</span> bb.df<span class="punctuation">)</span> <span class="comment"># 创建失败次数的列联表</span></span><br><span class="line">bb.grouped.df <span class="operator">=</span> data.frame<span class="punctuation">(</span> <span class="comment"># 创建分组后的数据框</span></span><br><span class="line">  success <span class="operator">=</span> success.tbl<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">]</span><span class="punctuation">,</span> fail <span class="operator">=</span> fail.tbl<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  distance <span class="operator">=</span> distance<span class="punctuation">,</span> gender <span class="operator">=</span> gender<span class="punctuation">)</span></span><br><span class="line">bb.grouped.fit <span class="operator">=</span> glm<span class="punctuation">(</span> <span class="comment"># 使用 cbind(success, fail) 作为响应变量</span></span><br><span class="line">  cbind<span class="punctuation">(</span>success<span class="punctuation">,</span> fail<span class="punctuation">)</span> <span class="operator">~</span> distance <span class="operator">*</span> gender<span class="punctuation">,</span></span><br><span class="line">  family <span class="operator">=</span> binomial<span class="punctuation">,</span>data <span class="operator">=</span> bb.grouped.df<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.grouped.fit<span class="punctuation">)</span> <span class="comment"># 分组数据拟合结果</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.fit<span class="punctuation">)</span> <span class="comment"># 未分组数据拟合结果 (应相同)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第三种拟合logistic回归的方式 (使用比例和权重)</span></span><br><span class="line">bb.grouped.df<span class="operator">$</span>n <span class="operator">=</span> bb.grouped.df<span class="operator">$</span>success <span class="operator">+</span> bb.grouped.df<span class="operator">$</span>fail <span class="comment"># 总试验次数</span></span><br><span class="line">bb.grouped.df<span class="operator">$</span>prop <span class="operator">=</span> bb.grouped.df<span class="operator">$</span>success<span class="operator">/</span>bb.grouped.df<span class="operator">$</span>n <span class="comment"># 成功比例</span></span><br><span class="line">bb.grouped.prop.fit <span class="operator">=</span> glm<span class="punctuation">(</span></span><br><span class="line">  prop <span class="operator">~</span> distance <span class="operator">*</span> gender<span class="punctuation">,</span> weights <span class="operator">=</span> n<span class="punctuation">,</span> <span class="comment"># 使用成功比例作为响应，总次数作为权重</span></span><br><span class="line">  family <span class="operator">=</span> binomial<span class="punctuation">,</span>data <span class="operator">=</span> bb.grouped.df<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.grouped.prop.fit<span class="punctuation">)</span> <span class="comment"># 结果应与前两种方式一致</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.grouped.fit<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>bb.fit<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不使用logistic回归的真正危险 (挑战者号航天飞机案例)</span></span><br><span class="line">Space.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;ChallengerShuttle.txt&quot;</span><span class="punctuation">,</span> head <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取挑战者号数据</span></span><br><span class="line">Space.df <span class="comment"># 已知有6个O型环</span></span><br><span class="line">Space.df<span class="operator">$</span>prop <span class="operator">=</span> Space.df<span class="operator">$</span>Failure<span class="operator">/</span><span class="number">6</span> <span class="comment"># 计算失败比例</span></span><br><span class="line"><span class="comment"># 错误方法1: 移除0失败的数据点后用普通线性模型拟合</span></span><br><span class="line">Space.no.zero.fit <span class="operator">=</span> lm<span class="punctuation">(</span>prop<span class="operator">~</span>Temp<span class="punctuation">,</span> data <span class="operator">=</span> subset<span class="punctuation">(</span>Space.df<span class="punctuation">,</span> Failure<span class="operator">&gt;</span><span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>Space.no.zero.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 错误方法2: 直接用普通线性模型拟合比例</span></span><br><span class="line">Space.fit <span class="operator">=</span> lm<span class="punctuation">(</span>prop<span class="operator">~</span>Temp<span class="punctuation">,</span> data <span class="operator">=</span> Space.df<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>Space.fit<span class="punctuation">)</span> <span class="comment"># 这个模型的预测可能超出[0,1]范围</span></span><br><span class="line">plot<span class="punctuation">(</span>Space.fit<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 残差图可能显示问题</span></span><br><span class="line">normcheck<span class="punctuation">(</span>Space.fit<span class="punctuation">)</span> <span class="comment"># 正态性假设不成立</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>Space.fit<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 正确方法: logistic回归</span></span><br><span class="line">Space.logistic.fit<span class="operator">=</span>glm<span class="punctuation">(</span></span><br><span class="line">  cbind<span class="punctuation">(</span>Failure<span class="punctuation">,</span> <span class="number">6</span><span class="operator">-</span>Failure<span class="punctuation">)</span><span class="operator">~</span>Temp<span class="punctuation">,</span> family <span class="operator">=</span> binomial<span class="punctuation">,</span> data <span class="operator">=</span> Space.df<span class="punctuation">)</span> <span class="comment"># Failure是成功(这里指O环损坏)次数, 6-Failure是未损坏次数</span></span><br><span class="line">summary<span class="punctuation">(</span>Space.logistic.fit<span class="punctuation">)</span></span><br><span class="line">predictGLM<span class="punctuation">(</span>Space.logistic.fit<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">,</span> <span class="comment"># 预测在31华氏度时O环损坏的概率</span></span><br><span class="line">           newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span>Temp<span class="operator">=</span><span class="number">31</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>prop<span class="operator">~</span>Temp<span class="punctuation">,</span> ylim <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> xlim <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">20</span><span class="punctuation">,</span><span class="number">100</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">     data <span class="operator">=</span> Space.df<span class="punctuation">)</span> <span class="comment"># 绘制原始比例数据</span></span><br><span class="line">Temp.plot <span class="operator">=</span> seq<span class="punctuation">(</span><span class="number">20</span><span class="punctuation">,</span> <span class="number">100</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 创建温度序列</span></span><br><span class="line">FV <span class="operator">=</span> predictGLM<span class="punctuation">(</span>Space.logistic.fit<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">,</span></span><br><span class="line">                newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span>Temp<span class="operator">=</span>Temp.plot<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 预测损坏概率</span></span><br><span class="line">lines<span class="punctuation">(</span>Temp.plot<span class="punctuation">,</span> FV<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> lty <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 绘制拟合的logistic曲线</span></span><br><span class="line">abline<span class="punctuation">(</span>v <span class="operator">=</span> <span class="number">31</span><span class="punctuation">,</span> col <span class="operator">=</span> <span class="string">&quot;red&quot;</span><span class="punctuation">)</span> <span class="comment"># 标记挑战者号发射时的温度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类二项 (Quasibinomial) - 处理过离散 (当响应是比例时)</span></span><br><span class="line">Haddock.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;Haddock.dat&quot;</span><span class="punctuation">,</span> head <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 读取黑线鳕数据 (已分组)</span></span><br><span class="line">Haddock.glm <span class="operator">=</span> glm<span class="punctuation">(</span>cbind<span class="punctuation">(</span>codend<span class="punctuation">,</span>cover<span class="punctuation">)</span> <span class="operator">~</span> forklen<span class="punctuation">,</span> family <span class="operator">=</span> binomial<span class="punctuation">,</span> <span class="comment"># codend是网到的，cover是逃逸的</span></span><br><span class="line">                  data <span class="operator">=</span> Haddock.df<span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>Haddock.glm<span class="punctuation">,</span> which <span class="operator">=</span> <span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">cooks20x<span class="punctuation">(</span>Haddock.glm<span class="punctuation">)</span></span><br><span class="line">Haddock.df<span class="punctuation">[</span><span class="number">6</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="comment"># 查看某个可能有影响的点</span></span><br><span class="line"><span class="comment"># summary(Haddock.glm) # 查看初始模型</span></span><br><span class="line"><span class="comment"># Haddock.glm = glm(cbind(codend,cover) ~ forklen, family = binomial, # 尝试移除影响点</span></span><br><span class="line"><span class="comment">#                   data = Haddock.df[-6,])</span></span><br><span class="line"><span class="comment"># cooks20x(Haddock.glm) # 再次检查</span></span><br><span class="line"><span class="comment"># 检查离散度：如果p值小，则存在过离散或欠离散</span></span><br><span class="line"><span class="number">1</span> <span class="operator">-</span> pchisq<span class="punctuation">(</span>summary<span class="punctuation">(</span>Haddock.glm<span class="punctuation">)</span><span class="operator">$</span>deviance<span class="punctuation">,</span></span><br><span class="line">           summary<span class="punctuation">(</span>Haddock.glm<span class="punctuation">)</span><span class="operator">$</span>df.residual<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 如果p值小，我们可以尝试 family = quasibinomial</span></span><br><span class="line">Haddock.quasi.glm <span class="operator">=</span> glm<span class="punctuation">(</span></span><br><span class="line">  cbind<span class="punctuation">(</span>codend<span class="punctuation">,</span>cover<span class="punctuation">)</span> <span class="operator">~</span> forklen<span class="punctuation">,</span></span><br><span class="line">  family <span class="operator">=</span> quasibinomial<span class="punctuation">,</span> <span class="comment"># 使用类二项分布族</span></span><br><span class="line">  data <span class="operator">=</span> Haddock.df<span class="punctuation">)</span> <span class="comment"># 教材中使用的是 data = Haddock.df[-6,]</span></span><br><span class="line">summary<span class="punctuation">(</span>Haddock.quasi.glm<span class="punctuation">)</span> <span class="comment"># 类二项模型摘要</span></span><br><span class="line">summary<span class="punctuation">(</span>Haddock.glm<span class="punctuation">)</span> <span class="comment"># 与二项模型比较 (系数估计相同，标准误不同)</span></span><br><span class="line"><span class="number">100</span><span class="operator">*</span><span class="punctuation">(</span><span class="built_in">exp</span><span class="punctuation">(</span>confint<span class="punctuation">(</span>Haddock.glm<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">-</span> <span class="number">1</span><span class="punctuation">)</span> <span class="comment"># 解释系数 (odds百分比变化)</span></span><br><span class="line">forklen.plot <span class="operator">=</span> seq<span class="punctuation">(</span><span class="number">10</span><span class="punctuation">,</span> <span class="number">60</span><span class="punctuation">,</span> by <span class="operator">=</span> <span class="number">0.5</span><span class="punctuation">)</span> <span class="comment"># 创建鱼叉长度序列</span></span><br><span class="line">FV <span class="operator">=</span> predictGLM<span class="punctuation">(</span>Haddock.glm<span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;response&quot;</span><span class="punctuation">,</span> <span class="comment"># 预测捕获概率</span></span><br><span class="line">                newdata <span class="operator">=</span> data.frame<span class="punctuation">(</span>forklen<span class="operator">=</span>forklen.plot<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">plot<span class="punctuation">(</span>forklen.plot<span class="punctuation">,</span> FV<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> type <span class="operator">=</span> <span class="string">&quot;l&quot;</span><span class="punctuation">,</span></span><br><span class="line">     xlab <span class="operator">=</span> <span class="string">&quot;Fork Length (cm)&quot;</span><span class="punctuation">,</span> ylab <span class="operator">=</span> <span class="string">&quot;Prob&quot;</span><span class="punctuation">)</span> <span class="comment"># 绘制拟合的logistic曲线</span></span><br><span class="line">prop <span class="operator">=</span> Haddock.df<span class="operator">$</span>codend<span class="operator">/</span><span class="punctuation">(</span>Haddock.df<span class="operator">$</span>codend<span class="operator">+</span>Haddock.df<span class="operator">$</span>cover<span class="punctuation">)</span> <span class="comment"># 计算原始数据中的比例</span></span><br><span class="line">points<span class="punctuation">(</span>Haddock.df<span class="operator">$</span>forklen<span class="punctuation">,</span> prop<span class="punctuation">)</span> <span class="comment"># 添加原始数据点</span></span><br><span class="line">rm<span class="punctuation">(</span><span class="built_in">list</span><span class="operator">=</span>ls<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 清除工作空间</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 列联表 (关联性研究)</span></span><br><span class="line"><span class="comment"># 例如：眼睛和头发颜色</span></span><br><span class="line">Genetics<span class="operator">=</span>matrix<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">284</span><span class="punctuation">,</span> <span class="number">577</span><span class="punctuation">,</span> <span class="number">613</span><span class="punctuation">,</span> <span class="number">2002</span><span class="punctuation">)</span><span class="punctuation">,</span>ncol<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span> <span class="comment"># 创建矩阵</span></span><br><span class="line">rownames<span class="punctuation">(</span>Genetics<span class="punctuation">)</span> <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Hair brown&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Hair other&quot;</span><span class="punctuation">)</span></span><br><span class="line">colnames<span class="punctuation">(</span>Genetics<span class="punctuation">)</span> <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Eyes brown&quot;</span><span class="punctuation">,</span> <span class="string">&quot;Eyes other&quot;</span><span class="punctuation">)</span></span><br><span class="line">gen.tbl<span class="operator">=</span>as.table<span class="punctuation">(</span>Genetics<span class="punctuation">)</span> <span class="comment"># 转换为表格对象</span></span><br><span class="line">gen.tbl</span><br><span class="line">plot<span class="punctuation">(</span>gen.tbl<span class="punctuation">)</span> <span class="comment"># 绘制马赛克图</span></span><br><span class="line">barplot<span class="punctuation">(</span>gen.tbl<span class="punctuation">)</span> <span class="comment"># 绘制条形图</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;topleft&quot;</span><span class="punctuation">,</span> legend<span class="operator">=</span>rownames<span class="punctuation">(</span>gen.tbl<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">       fill<span class="operator">=</span>grey<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">2</span><span class="operator">/</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 疫苗和压痛程度</span></span><br><span class="line">vaccines<span class="operator">=</span>matrix<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">21</span><span class="punctuation">,</span><span class="number">16</span><span class="punctuation">,</span><span class="number">11</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">22</span><span class="punctuation">,</span><span class="number">19</span><span class="punctuation">,</span><span class="number">9</span><span class="punctuation">)</span><span class="punctuation">,</span>nrow<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span>byrow<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">)</span></span><br><span class="line">rownames<span class="punctuation">(</span>vaccines<span class="punctuation">)</span><span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Trmt A&quot;</span><span class="punctuation">,</span><span class="string">&quot;Trmt B&quot;</span><span class="punctuation">)</span></span><br><span class="line">colnames<span class="punctuation">(</span>vaccines<span class="punctuation">)</span><span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;none&quot;</span><span class="punctuation">,</span><span class="string">&quot;mild&quot;</span><span class="punctuation">,</span><span class="string">&quot;moderate&quot;</span><span class="punctuation">,</span><span class="string">&quot;severe&quot;</span><span class="punctuation">)</span></span><br><span class="line">vax.tbl<span class="operator">=</span>as.table<span class="punctuation">(</span>vaccines<span class="punctuation">)</span></span><br><span class="line">vax.tbl</span><br><span class="line">plot<span class="punctuation">(</span>vax.tbl<span class="punctuation">)</span> <span class="comment"># 马赛克图</span></span><br><span class="line">n.levels <span class="operator">=</span> ncol<span class="punctuation">(</span>vax.tbl<span class="punctuation">)</span></span><br><span class="line">barplot<span class="punctuation">(</span>t<span class="punctuation">(</span>vax.tbl<span class="punctuation">)</span><span class="punctuation">,</span> col<span class="operator">=</span>grey<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>n.levels<span class="operator">/</span>n.levels<span class="punctuation">)</span><span class="punctuation">,</span> ylim <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">60</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 转置后绘制堆叠条形图</span></span><br><span class="line">legend<span class="punctuation">(</span><span class="string">&quot;top&quot;</span><span class="punctuation">,</span> legend<span class="operator">=</span>colnames<span class="punctuation">(</span>vax.tbl<span class="punctuation">)</span><span class="punctuation">,</span> horiz <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span></span><br><span class="line">       fill<span class="operator">=</span>grey<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span>n.levels<span class="operator">/</span>n.levels<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 课程通过与否和出勤情况</span></span><br><span class="line">AP.df <span class="operator">=</span> read.table<span class="punctuation">(</span><span class="string">&quot;AttendPass.txt&quot;</span><span class="punctuation">,</span>header<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">)</span> <span class="comment"># 读取出勤与通过数据</span></span><br><span class="line">head<span class="punctuation">(</span>AP.df<span class="punctuation">,</span> <span class="number">8</span><span class="punctuation">)</span></span><br><span class="line">AP.tbl <span class="operator">=</span> xtabs<span class="punctuation">(</span><span class="operator">~</span>Attend<span class="operator">+</span>Pass<span class="punctuation">,</span> data <span class="operator">=</span> AP.df<span class="punctuation">)</span> <span class="comment"># 创建列联表</span></span><br><span class="line">barplot<span class="punctuation">(</span>t<span class="punctuation">(</span>AP.tbl<span class="punctuation">)</span><span class="punctuation">,</span>legend<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 绘制条形图</span></span><br><span class="line">plot<span class="punctuation">(</span>AP.tbl<span class="punctuation">)</span> <span class="comment"># 绘制马赛克图</span></span><br><span class="line">prop.table<span class="punctuation">(</span>AP.tbl<span class="punctuation">)</span> <span class="comment"># 计算比例表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 多维列联表</span></span><br><span class="line"><span class="comment"># 2. 响应变量和解释变量可能清晰也可能不清晰</span></span><br><span class="line"><span class="comment"># 3. 试验次数n是否固定可能清晰也可能不清晰</span></span><br><span class="line"><span class="comment"># 4. 在研究关联性时这些不一定重要</span></span><br><span class="line"><span class="comment"># 泊松回归和二项回归在关联性研究中的等价性</span></span><br><span class="line">freq.df <span class="operator">=</span> data.frame<span class="punctuation">(</span> <span class="comment"># 将列联表转换为长格式数据框</span></span><br><span class="line">  Y<span class="operator">=</span>AP.tbl<span class="punctuation">[</span><span class="number">1</span><span class="operator">:</span><span class="number">4</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="comment"># 频数</span></span><br><span class="line">  Attend<span class="operator">=</span><span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">attr</span><span class="punctuation">(</span>AP.tbl<span class="punctuation">,</span> <span class="string">&quot;dimnames&quot;</span><span class="punctuation">)</span><span class="operator">$</span>Attend<span class="punctuation">,</span> times <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">  Pass<span class="operator">=</span><span class="built_in">rep</span><span class="punctuation">(</span><span class="built_in">attr</span><span class="punctuation">(</span>AP.tbl<span class="punctuation">,</span> <span class="string">&quot;dimnames&quot;</span><span class="punctuation">)</span><span class="operator">$</span>Pass<span class="punctuation">,</span> each <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">AP.tbl</span><br><span class="line">freq.df<span class="operator">$</span>Attend <span class="operator">=</span> factor<span class="punctuation">(</span>freq.df<span class="operator">$</span>Attend<span class="punctuation">,</span> levels <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;not.attend&quot;</span><span class="punctuation">,</span> <span class="string">&quot;attend&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 设置因子水平</span></span><br><span class="line">freq.df<span class="operator">$</span>Pass <span class="operator">=</span> factor<span class="punctuation">(</span>freq.df<span class="operator">$</span>Pass<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># AP.tbl = AP.tbl[2:1,] # 这行会改变AP.tbl的行顺序，影响下面AP.grouped.df的创建，可能导致与freq.df不一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合泊松模型 (响应变量是频数Y)</span></span><br><span class="line">AP.pois<span class="operator">=</span>glm<span class="punctuation">(</span>Y<span class="operator">~</span>Attend<span class="operator">*</span>Pass<span class="punctuation">,</span>family<span class="operator">=</span>poisson<span class="punctuation">,</span>data<span class="operator">=</span>freq.df<span class="punctuation">)</span> <span class="comment"># 注意这里freq.df的Attend顺序</span></span><br><span class="line">summary<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合Logistic回归 (未分组数据)</span></span><br><span class="line">AP.logistic <span class="operator">=</span> glm<span class="punctuation">(</span></span><br><span class="line">  <span class="built_in">as.numeric</span><span class="punctuation">(</span>Pass<span class="operator">==</span><span class="string">&quot;pass&quot;</span><span class="punctuation">)</span><span class="operator">~</span>Attend<span class="punctuation">,</span> <span class="comment"># Pass==&quot;pass&quot;会产生0/1响应</span></span><br><span class="line">  family <span class="operator">=</span> binomial<span class="punctuation">,</span>data <span class="operator">=</span> AP.df<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合Logistic回归 (分组数据)</span></span><br><span class="line"><span class="comment"># AP.tbl的行顺序可能因上一条注释掉的命令而与freq.df不同，需要注意</span></span><br><span class="line">AP.grouped.df <span class="operator">=</span> data.frame<span class="punctuation">(</span> <span class="comment"># 从原始AP.tbl创建分组数据</span></span><br><span class="line">  pass <span class="operator">=</span> AP.tbl<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">,</span> fail <span class="operator">=</span> AP.tbl<span class="punctuation">[</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> <span class="comment"># 假设AP.tbl的列是 fail, pass</span></span><br><span class="line">  Attend <span class="operator">=</span> rownames<span class="punctuation">(</span>AP.tbl<span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># Attend是行名</span></span><br><span class="line">AP.grouped.df<span class="operator">$</span>Attend <span class="operator">&lt;-</span> factor<span class="punctuation">(</span>AP.grouped.df<span class="operator">$</span>Attend<span class="punctuation">,</span> levels <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;not.attend&quot;</span><span class="punctuation">,</span> <span class="string">&quot;attend&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="comment"># 确保因子水平一致</span></span><br><span class="line"></span><br><span class="line">AP.grouped.logistic <span class="operator">=</span> glm<span class="punctuation">(</span>cbind<span class="punctuation">(</span>pass<span class="punctuation">,</span>fail<span class="punctuation">)</span><span class="operator">~</span>Attend<span class="punctuation">,</span></span><br><span class="line">                  family <span class="operator">=</span> binomial<span class="punctuation">,</span>data <span class="operator">=</span> AP.grouped.df<span class="punctuation">)</span> <span class="comment"># 使用成功/失败次数</span></span><br><span class="line"></span><br><span class="line">summary<span class="punctuation">(</span>AP.logistic<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>AP.grouped.logistic<span class="punctuation">)</span></span><br><span class="line">summary<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span> <span class="comment"># 比较三个模型的系数</span></span><br><span class="line">confint<span class="punctuation">(</span>AP.logistic<span class="punctuation">)</span></span><br><span class="line">confint<span class="punctuation">(</span>AP.grouped.logistic<span class="punctuation">)</span></span><br><span class="line">confint<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span> <span class="comment"># 交互作用项的系数 (log-odds ratio) 应与logistic中Attend的系数相似</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不用模型计算Odds Ratio</span></span><br><span class="line">coef<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span></span><br><span class="line">coef<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">4</span><span class="punctuation">]</span> <span class="comment"># Poisson模型中交互作用项的系数 (log odds ratio)</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>coef<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">4</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="comment"># Odds Ratio</span></span><br><span class="line">AP.tbl <span class="comment"># 原始列联表 (注意行顺序)</span></span><br><span class="line"><span class="comment"># (OR=(27*83)/(17*19)) # 手动计算OR ( (n00*n11)/(n01*n10) )，需确保AP.tbl顺序正确</span></span><br><span class="line"><span class="comment"># 例如，如果AP.tbl是:</span></span><br><span class="line"><span class="comment">#            Pass</span></span><br><span class="line"><span class="comment"># Attend     fail pass</span></span><br><span class="line"><span class="comment"># not.attend   27   19</span></span><br><span class="line"><span class="comment"># attend       17   83</span></span><br><span class="line"><span class="comment"># OR = (n_attend_pass * n_not.attend_fail) / (n_attend_fail * n_not.attend_pass)</span></span><br><span class="line"><span class="comment"># OR = (83 * 27) / (17 * 19)</span></span><br><span class="line"><span class="punctuation">(</span>OR <span class="operator">=</span> <span class="punctuation">(</span>AP.tbl<span class="punctuation">[</span><span class="string">&quot;attend&quot;</span><span class="punctuation">,</span><span class="string">&quot;pass&quot;</span><span class="punctuation">]</span> <span class="operator">*</span> AP.tbl<span class="punctuation">[</span><span class="string">&quot;not.attend&quot;</span><span class="punctuation">,</span><span class="string">&quot;fail&quot;</span><span class="punctuation">]</span><span class="punctuation">)</span> <span class="operator">/</span> <span class="punctuation">(</span>AP.tbl<span class="punctuation">[</span><span class="string">&quot;attend&quot;</span><span class="punctuation">,</span><span class="string">&quot;fail&quot;</span><span class="punctuation">]</span> <span class="operator">*</span> AP.tbl<span class="punctuation">[</span><span class="string">&quot;not.attend&quot;</span><span class="punctuation">,</span><span class="string">&quot;pass&quot;</span><span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">logOR.se<span class="operator">=</span><span class="built_in">sqrt</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">/</span><span class="number">17</span><span class="operator">+</span><span class="number">1</span><span class="operator">/</span><span class="number">83</span><span class="operator">+</span><span class="number">1</span><span class="operator">/</span><span class="number">27</span><span class="operator">+</span><span class="number">1</span><span class="operator">/</span><span class="number">19</span><span class="punctuation">)</span> <span class="comment"># log(OR)的标准误近似公式</span></span><br><span class="line">logOR.CI<span class="operator">=</span><span class="built_in">log</span><span class="punctuation">(</span>OR<span class="punctuation">)</span> <span class="operator">+</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="operator">-</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="operator">*</span><span class="number">1.96</span><span class="operator">*</span>logOR.se <span class="comment"># log(OR)的置信区间</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>logOR.CI<span class="punctuation">)</span> <span class="comment"># OR的置信区间，应与模型结果相似</span></span><br><span class="line"><span class="built_in">exp</span><span class="punctuation">(</span>confint.default<span class="punctuation">(</span>AP.pois<span class="punctuation">)</span><span class="punctuation">[</span><span class="number">4</span><span class="punctuation">,</span><span class="punctuation">]</span> <span class="punctuation">)</span> <span class="comment"># 从Poisson模型得到的OR置信区间</span></span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>数据库原理与应用（外教版）</title>
    <url>/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/</url>
    <content><![CDATA[<p>数据库原理与应用（外教版）</p>
<h2 id="4-22">4.22</h2>
<p>1.主码（primary key）：</p>
<p>主码也称为主键，是表中的一个或多个字段（列）的组合，其值能够唯一地标识表中的每一行记录，即表中的任意两条记录的主码值都不相同，并且主码不允许包含空值（NULL）。</p>
<p>例如，在一个 <code>Students</code> 表中，包含 <code>StudentID</code>（学生编号）、<code>Name</code>（姓名）、<code>Age</code>（年龄）等字段，<code>StudentID</code> 就可以设置为主码。因为每个学生的编号都是唯一的，通过 <code>StudentID</code> 能够准确地确定每一个学生的记录。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> Students(</span><br><span class="line">    StudentID <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    Name <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    Age <span class="type">int</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>主码的作用主要有：</p>
<ul>
<li><strong>保证数据的唯一性</strong>：确保表中的每一行数据都是独一无二的，避免出现重复记录。</li>
<li><strong>作为表的唯一标识</strong>：方便在数据库操作中（如查询、更新、删除等）准确地定位和操作特定的记录。</li>
<li><strong>建立表之间的联系</strong>：与其他表的外码配合，建立表之间的关联关系。</li>
</ul>
<p>2.外码（foreign key）：</p>
<p>外码也叫外键，是一个表中的一个或多个字段（列），它的值与另一个表（称为主表或父表）的主码或候选码相对应。外码用于建立两个表之间的关联关系，通过外码可以实现数据的参照完整性。</p>
<p>例如，有一个 <code>Orders</code> 表，包含 <code>OrderID</code>（订单编号）、<code>CustomerID</code>（客户编号）、<code>OrderDate</code>（订单日期）等字段，同时有一个 <code>Customers</code> 表，包含 <code>CustomerID</code>（客户编号）、<code>CustomerName</code>（客户姓名）等字段。在 <code>Orders</code> 表中的 <code>CustomerID</code> 就可以设置为外码，它参照了 <code>Customers</code> 表中的 <code>CustomerID</code> 主码。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> Customers(</span><br><span class="line">    CustomerID <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    CustomerName <span class="type">varchar</span>(<span class="number">50</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create table</span> Orders(</span><br><span class="line">    OrderID <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    customerID <span class="type">int</span>,</span><br><span class="line">    Orderdate <span class="type">date</span>,</span><br><span class="line">    <span class="keyword">foreign key</span> (CustomerID) <span class="keyword">references</span> Customers(CustomerID)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>外码的作用主要有：</p>
<ul>
<li><strong>建立表间关系</strong>：明确地表示出两个表之间的关联，使得数据库中的数据能够更好地组织和管理。</li>
<li><strong>保证数据的一致性和完整性</strong>：当对主表中的主码进行修改或删除操作时，数据库管理系统会根据外码约束规则进行相应的处理，以确保相关表中的数据保持一致。例如，当删除 <code>Customers</code> 表中一个客户的记录时，如果 <code>Orders</code> 表中有该客户的订单记录，根据外码约束的设置（如级联删除、拒绝删除等），可以保证数据的完整性不被破坏。</li>
</ul>
<p>3.关系代数</p>
<p>（1）选择select（$\sigma_F(R)$）</p>
<p>选择操作是从关系中挑选出满足给定条件的元组。设 R 为一个关系，F 是一个条件表达式，那么选择操作可表示为$ \sigma_{F}(R)$。</p>
<p>示例：假设存在一个关系 <code>Students</code>，包含字段 <code>StudentID</code>、<code>Name</code>、<code>Age</code>。若要挑选出年龄大于 20 的学生，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM Students WHERE Age &gt; 20;</code></p>
<p>在关系代数里，可表示成$ \sigma_{Age &gt; 20}(Students)$。</p>
<p>（2）投影projection（ $\pi_{A_1, A_2, \cdots, A_n}(R)$）</p>
<p>投影操作是从关系里选取特定的属性列，并且去除重复的元组。设$ R $为一个关系，$A_1, A_2, \cdots, A_n$是 R 的属性，那么投影操作可表示为 $\pi_{A_1, A_2, \cdots, A_n}(R)$。</p>
<p>示例：还是以 <code>Students</code> 关系为例，若要获取所有学生的姓名和学号，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT StudentID, Name FROM Students;</code></p>
<p>在关系代数里，可表示成 $\pi_{StudentID, Name}(Students)$。</p>
<p>（3）连接union（$R \cup S$）</p>
<p>并操作要求参与运算的两个关系 R 和 S 具有相同的属性列，并且属性的域也相同。它会把两个关系的元组合并，同时去除重复的元组。</p>
<p>示例：假设有两个关系 <code>ClassA</code> 和 <code>ClassB</code>，它们的结构一样，都包含 <code>StudentID</code> 和 <code>Name</code> 字段。若要把两个班级的学生信息合并，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM ClassA UNION SELECT * FROM ClassB;</code></p>
<p>在关系代数里，可表示成 $ClassA \cup ClassB$。</p>
<p>（4）内连接intersect($R\cap S$)</p>
<p>交操作同样要求参与运算的两个关系 R 和 S 具有相同的属性列，并且属性的域也相同。它会返回两个关系中相同的元组。</p>
<p>示例：对于 <code>ClassA</code> 和 <code>ClassB</code> 关系，若要找出同时在两个班级的学生，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM ClassA INTERSECT SELECT * FROM ClassB;</code></p>
<p>在关系代数里，可表示成$ClassA \cap ClassB$。</p>
<p>（5）差difference（$R-S$）</p>
<p>差操作要求参与运算的两个关系 R 和 S 具有相同的属性列，并且属性的域也相同。它会返回在关系 R 中但不在关系 S 中的元组。</p>
<p>示例：对于 <code>ClassA</code> 和 <code>ClassB</code> 关系，若要找出只在 <code>ClassA</code> 而不在 <code>ClassB</code> 的学生，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM ClassA EXCEPT SELECT * FROM ClassB;</code></p>
<p>在关系代数里，可表示成 $ClassA - ClassB$。</p>
<p>（6）笛卡尔积product（$R\times S$）</p>
<p>笛卡尔积操作会把关系 R 中的每个元组和关系 S 中的每个元组进行组合，形成一个新的关系。新关系的属性列是 R 和 S 的属性列的并集。</p>
<p>示例：假设存在关系 <code>Employees</code>（包含 <code>EmployeeID</code>、<code>EmployeeName</code>）和 <code>Departments</code>（包含 <code>DepartmentID</code>、<code>DepartmentName</code>），若要得到员工和部门的所有可能组合，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM Employees, Departments;</code></p>
<p>在关系代数里，可表示成$Employees \times Departments$。</p>
<p>（7）连接join（$R \bowtie S$）</p>
<p>连接操作是从两个关系的笛卡尔积中选取满足一定条件的元组。常见的连接类型有等值连接、自然连接等。</p>
<p>等值连接</p>
<p>等值连接是在笛卡尔积的基础上，选取两个关系中指定属性相等的元组。设 R 和 S 是两个关系，A 是 R 的属性，B 是 S 的属性，等值连接可表示为$R \bowtie_{A = B} S$。</p>
<p>示例：假设有关系 <code>Orders</code>（包含 <code>OrderID</code>、<code>CustomerID</code>）和 <code>Customers</code>（包含 <code>CustomerID</code>、<code>CustomerName</code>），若要获取订单及其对应的客户信息，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM Orders </code></p>
<p><code>JOIN Customers ON Orders.CustomerID = Customers.CustomerID;</code></p>
<p>在关系代数里，可表示成</p>
<p>$Orders \bowtie_{Orders.CustomerID = Customers.CustomerID} Customers$。</p>
<p>自然连接</p>
<p>自然连接是一种特殊的等值连接，它会自动选取两个关系中相同名称的属性进行等值比较，并且去除重复的属性列。</p>
<p>示例：对于上述的 <code>Orders</code> 和 <code>Customers</code> 关系，使用自然连接获取订单及其对应的客户信息，操作如下：</p>
<p><code>-- SQL 示例 </code></p>
<p><code>SELECT * FROM Orders NATURAL JOIN Customers;</code></p>
<p>在关系代数里，可表示成 $Orders \bowtie Customers$ 。</p>
<p>4.SQL Query</p>
<p>SQL 查询是用来从数据库里获取数据的语句。一个基本的 SQL 查询包含 <code>SELECT</code>、<code>FROM</code> 和 <code>WHERE</code> 子句。</p>
<p>5.Doucument Data Model（NOSQL）</p>
<p>文档数据模型是 NoSQL 数据库里常见的数据模型，数据以文档的形式存储，通常是 JSON、BSON 或者 XML 格式。文档数据库中的文档可以包含嵌套结构，并且不同文档的结构可以不同。</p>
<p>6.Vector Data Model/Mechine Learning Model</p>
<p>向量数据模型是把数据表示成向量的形式，常用于机器学习、信息检索等领域。例如，在文本处理中，可以把文档或者单词表示成向量，通过向量之间的相似度来衡量文档或者单词之间的相关性。</p>
<p>7.SQL简介:</p>
<p>major commands(e.g. select update delete insert);DML,DDL,TCL,DCL</p>
<ul>
<li><strong>SELECT</strong>：从数据库中选取数据。</li>
</ul>
<p><code>select * from Employees;</code></p>
<ul>
<li><strong>UPDATE</strong>：更新数据库中的数据。</li>
</ul>
<p><code>update empolyees set Salary=6000 where EmployeeID=1;</code></p>
<ul>
<li><strong>DELETE</strong>：从数据库中删除数据。</li>
</ul>
<p><code>delete from Employees where EmployeeID=1;</code></p>
<ul>
<li><strong>INSERT</strong>：向数据库中插入新的数据。</li>
</ul>
<p><code>insert into Employees(EmployeeID,Name,Salary) values(2,'Bob',5500); </code></p>
<ul>
<li>数据操作语言<strong>Data Manipulation Language</strong>（DML）</li>
</ul>
<p>DML 用于操作数据库中的数据，主要包括 <code>SELECT</code>、<code>INSERT</code>、<code>UPDATE</code> 和 <code>DELETE</code> 语句。</p>
<ul>
<li>数据定义语言<strong>Data Definition Language</strong>（DDL）</li>
</ul>
<p>DDL 用于定义数据库的结构，例如创建、修改和删除数据库对象（如表、视图、索引等）。常见的 DDL 语句有 <code>CREATE</code>、<code>ALTER</code> 和 <code>DROP</code>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一个新的表</span></span><br><span class="line"><span class="keyword">CREATE TABLE</span> Employees (</span><br><span class="line">    EmployeeID <span class="type">INT</span> <span class="keyword">PRIMARY KEY</span>,</span><br><span class="line">    Name <span class="type">VARCHAR</span>(<span class="number">50</span>),</span><br><span class="line">    Salary <span class="type">DECIMAL</span>(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<ul>
<li>事务控制语言<strong>Transaction Control Language</strong>（TCL）</li>
</ul>
<p>TCL 用于管理数据库中的事务，例如提交事务、回滚事务等。常见的 TCL 语句有 <code>COMMIT</code>、<code>ROLLBACK</code> 和 <code>SAVEPOINT</code>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">start</span> transaction;</span><br><span class="line"><span class="keyword">update</span> Employees <span class="keyword">set</span> Salary<span class="operator">=</span><span class="number">6000</span> <span class="keyword">where</span> EmployeeID<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>数据控制语言<strong>Data Control Language</strong>（DCL）</li>
</ul>
<p>DCL 用于控制用户对数据库对象的访问权限，例如授予和撤销权限。常见的 DCL 语句有 <code>GRANT</code> 和 <code>REVOKE</code>。</p>
<p><code>grant select on Employees to 'user'@'localhost'</code></p>
<p>8.Select Syntax</p>
<p>（1）distinct</p>
<p><code>DISTINCT</code> 关键字用于去除查询结果中的重复行。</p>
<p><code>select distinct Department from Employees;</code></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250422103948934.png" alt="image-20250422103948934"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250422103959905.png" alt="image-20250422103959905"></p>
<p>（2）where操作对应有and/or/not，like，is null/is not null等operator操作（is for filtering records）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> Employees <span class="keyword">where</span> Salary<span class="operator">&gt;</span><span class="number">5000</span> <span class="keyword">and</span> Department<span class="operator">=</span><span class="string">&#x27;IT&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> Employees <span class="keyword">where</span> Name <span class="keyword">like</span> <span class="string">&#x27;A%&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> Employees <span class="keyword">where</span> ManagerID <span class="keyword">is</span> <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure>
<p><code>select * from Employees where Name like 'A%';</code></p>
<p>这条语句使用了 <code>LIKE</code> 操作符，用于从 <code>Employees</code> 表中筛选出 <code>Name</code> 列以字母 <code>A</code> 开头的所有记录。<code>LIKE</code> 操作符是用于模糊匹配的，<code>%</code> 是通配符，表示任意数量（包括零个）的任意字符。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250422104145155.png" alt="image-20250422104145155"></p>
<p><code>SELECT * FROM Employees WHERE ManagerID IS NULL;</code></p>
<p>这条语句使用了 <code>IS NULL</code> 操作符，用于从 <code>Employees</code> 表中筛选出 <code>ManagerID</code> 列为 <code>NULL</code> 的所有记录。<code>IS NULL</code> 用于判断某列的值是否为 <code>NULL</code>。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250422104233490.png" alt="image-20250422104233490"></p>
<p>（3）分组group by</p>
<p><code>GROUP BY</code> 子句用于对查询结果进行分组，通常与聚合函数（如 <code>SUM</code>、<code>AVG</code>、<code>COUNT</code> 等）一起使用。</p>
<p><code>select Department,count(*) from Employees group by Department;</code></p>
<p>（4）having(is for filtering aggregated results)</p>
<p><code>HAVING</code> 子句用于过滤分组后的结果，与 <code>WHERE</code> 子句不同的是，<code>HAVING</code> 子句可以使用聚合函数。</p>
<p><code>select Department,AVG(Salary) from Employees group by Department having AVG(Salary)&gt;5000</code></p>
<p>（5）order by</p>
<p><code>ORDER BY</code> 子句用于对查询结果进行排序，可以按升序（<code>ASC</code>）或降序（<code>DESC</code>）排列。</p>
<p><code>select * from Employees order by Salary desc;</code></p>
<p>（6）limit（select top）</p>
<p><code>LIMIT</code> 关键字（在 MySQL 中）或 <code>SELECT TOP</code> 关键字（在 SQL Server 中）用于限制查询结果的行数。</p>
<p><code>select * from Employees limit 10;</code></p>
<p>（7）Alias（AS）</p>
<p><code>AS</code> 关键字用于为列或表指定别名，使查询结果更易读。</p>
<p><code>select Name as EmployName,Salary as AnnualSalary from Employees;</code></p>
<p>（8）join：</p>
<p>（inner）join（also a inner join）：returns records that have matching values in both tables，left（outer）join，right（outer）join，full（outer）join，cross join</p>
<p>①inner join</p>
<p><code>INNER JOIN</code> 也被称为简单的 <code>JOIN</code>，它返回两个表中在连接条件上具有匹配值的记录。只有当两个表中指定列的值相等时，对应的行才会被包含在结果集中。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250423142752778.png" alt="image-20250423142752778"></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 执行 INNER JOIN 查询</span></span><br><span class="line"><span class="keyword">SELECT</span> Employees.EmployeeName, Departments.DepartmentName</span><br><span class="line"><span class="keyword">FROM</span> Employees</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> Departments <span class="keyword">ON</span> Employees.DepartmentID <span class="operator">=</span> Departments.DepartmentID;</span><br></pre></td></tr></table></figure>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250423142907232-1745389748192-1.png" alt="image-20250423142907232"></p>
<p>②left（outer） join（就是保证左表信息全部出来）</p>
<p><code>LEFT JOIN</code> 会返回左表（<code>FROM</code> 子句中第一个指定的表）中的所有记录，以及右表中匹配的记录。如果右表中没有与左表记录匹配的行，则右表的列值将显示为 <code>NULL</code>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 执行 LEFT JOIN 查询</span></span><br><span class="line"><span class="keyword">SELECT</span> Employees.EmployeeName, Departments.DepartmentName</span><br><span class="line"><span class="keyword">FROM</span> Employees</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> Departments <span class="keyword">ON</span> Employees.DepartmentID <span class="operator">=</span> Departments.DepartmentID;</span><br></pre></td></tr></table></figure>
<p>③right join</p>
<p>RIGHT JOIN 与 LEFT JOIN 相反，它返回右表中的所有记录，以及左表中匹配的记录。如果左表中没有与右表记录匹配的行，则左表的列值将显示为 NULL。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 执行 RIGHT JOIN 查询</span></span><br><span class="line"><span class="keyword">SELECT</span> Employees.EmployeeName, Departments.DepartmentName</span><br><span class="line"><span class="keyword">FROM</span> Employees</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> Departments <span class="keyword">ON</span> Employees.DepartmentID <span class="operator">=</span> Departments.DepartmentID;</span><br></pre></td></tr></table></figure>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250423143114397.png" alt="image-20250423143114397"></p>
<p>④full join</p>
<p><code>FULL JOIN</code> 返回两个表中的所有记录。如果某一行在另一个表中没有匹配项，则对应的列将显示为 <code>NULL</code>。在 MySQL 中不直接支持 <code>FULL JOIN</code>，但可以使用 <code>UNION</code> 操作来模拟。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- MySQL 模拟 FULL JOIN</span></span><br><span class="line"><span class="keyword">SELECT</span> Employees.EmployeeName, Departments.DepartmentName</span><br><span class="line"><span class="keyword">FROM</span> Employees</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span> Departments <span class="keyword">ON</span> Employees.DepartmentID <span class="operator">=</span> Departments.DepartmentID</span><br><span class="line"><span class="keyword">UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> Employees.EmployeeName, Departments.DepartmentName</span><br><span class="line"><span class="keyword">FROM</span> Employees</span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">JOIN</span> Departments <span class="keyword">ON</span> Employees.DepartmentID <span class="operator">=</span> Departments.DepartmentID;</span><br></pre></td></tr></table></figure>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250423143220424.png" alt="image-20250423143220424"></p>
<p>⑤cross join</p>
<p><code>CROSS JOIN</code> 会生成两个表的笛卡尔积，即左表中的每一行都会与右表中的每一行组合，结果集的行数为左表行数乘以右表行数。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 执行 CROSS JOIN 查询</span></span><br><span class="line"><span class="keyword">SELECT</span> Employees.EmployeeName, Departments.DepartmentName</span><br><span class="line"><span class="keyword">FROM</span> Employees</span><br><span class="line"><span class="keyword">CROSS</span> <span class="keyword">JOIN</span> Departments;</span><br></pre></td></tr></table></figure>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250423143414571.png" alt="image-20250423143414571"></p>
<blockquote>
<p><mark>不同 JOIN 操作显示信息的说明</mark></p>
<p>在前面的 <code>JOIN</code> 查询示例里，<code>EmployeeID</code> 和 <code>DepartmentID</code> 没有显示，是因为在 <code>SELECT</code> 子句中仅指定了 <code>EmployeeName</code> 和 <code>DepartmentName</code> 这两列。<code>SELECT</code> 子句的作用是明确要从查询结果里获取哪些列，要是没有在 <code>SELECT</code> 子句中列出某列，那该列就不会出现在最终的查询结果中。</p>
<p>不同的 <code>JOIN</code> 操作（如 <code>INNER JOIN</code>、<code>LEFT JOIN</code>、<code>RIGHT JOIN</code>、<code>FULL JOIN</code>、<code>CROSS JOIN</code>）本身并不决定显示哪些信息，显示的信息完全由 <code>SELECT</code> 子句来决定。下面是修改后的示例，把 <code>EmployeeID</code> 和 <code>DepartmentID</code> 也包含在 <code>SELECT</code> 子句中。</p>
</blockquote>
<h2 id="4-23">4.23</h2>
<p>1.procedure（存储过程）</p>
<p>存储过程是一组为了完成特定功能的 SQL 语句集，经编译后存储在数据库中。用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。</p>
<p>2.case（if-else）</p>
<p><code>CASE</code> 语句用于在 SQL 中实现条件逻辑，类似于其他编程语言中的 <code>if - else</code> 语句。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 根据员工的薪水给出不同的评价</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">	EmployeeName,</span><br><span class="line">	Salary,</span><br><span class="line">	<span class="keyword">case</span></span><br><span class="line">		<span class="keyword">when</span> Salary<span class="operator">&gt;</span><span class="number">5000</span> <span class="keyword">then</span> &quot;High Salary&quot;</span><br><span class="line">		<span class="keyword">when</span> Salary<span class="operator">&gt;</span><span class="number">3000</span> <span class="keyword">then</span> &quot;Medium Salary&quot;</span><br><span class="line">		<span class="keyword">Else</span> &quot;Low Salary&quot;</span><br><span class="line">	<span class="keyword">end</span> <span class="keyword">as</span> SalaryGrade</span><br><span class="line"><span class="keyword">from</span> Employees;</span><br></pre></td></tr></table></figure>
<p>3.nested query（sub query）</p>
<p>子查询是指在一个 SQL 查询中嵌套另一个查询。子查询可以出现在 <code>SELECT</code>、<code>FROM</code>、<code>WHERE</code> 等子句中。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询薪水高于平均薪水的员工</span></span><br><span class="line"><span class="keyword">select</span> EmployeeName,Salary</span><br><span class="line"><span class="keyword">from</span> Employees</span><br><span class="line"><span class="keyword">where</span> Salary<span class="operator">&gt;</span>(<span class="keyword">select</span> <span class="built_in">avg</span>(Salary) <span class="keyword">from</span> Employees);</span><br></pre></td></tr></table></figure>
<p>4.exists</p>
<p><code>EXISTS</code> 是一个布尔运算符，用于检查子查询是否返回任何行。如果子查询返回至少一行，则 <code>EXISTS</code> 返回 <code>TRUE</code>，否则返回 <code>FALSE</code>。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> CustomerName</span><br><span class="line"><span class="keyword">from</span> Customers</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">exists</span>(<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> Orders <span class="keyword">where</span> Orders.CustomerID<span class="operator">=</span>Customers.CustomerID);</span><br></pre></td></tr></table></figure>
<p>5.in</p>
<p><code>IN</code> 运算符用于指定多个可能的值，用于在 <code>WHERE</code> 子句中筛选出符合这些值的记录。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查询部门 ID 为 1 或 2 的员工</span></span><br><span class="line"><span class="keyword">select</span> EmployeeName</span><br><span class="line"><span class="keyword">from</span> Employees</span><br><span class="line"><span class="keyword">where</span> DepartmentID <span class="keyword">in</span>(<span class="number">1</span>,<span class="number">2</span>);</span><br></pre></td></tr></table></figure>
<p>6.insert into</p>
<p><code>INSERT INTO</code> 语句用于向表中插入新记录。</p>
<p><code>insert into Employees(EmployeeName,DepartmentID) values ('Eve','2');</code></p>
<p>7.update</p>
<p><code>UPDATE</code> 语句用于修改表中的现有记录。</p>
<p><code>update set where</code></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> Customers</span><br><span class="line"><span class="keyword">set</span> ContactName<span class="operator">=</span><span class="string">&#x27;Juan&#x27;</span></span><br><span class="line"><span class="keyword">where</span> Country<span class="operator">=</span><span class="string">&#x27;Mexico&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>8.delete</p>
<p><code>DELETE</code> 语句用于从表中删除记录。</p>
<p><code>delete from Employees where DepartmentID=3;</code></p>
<p>DML has ended</p>
<hr>
<p>9.DDL（数据定义语言） review</p>
<p>DDL 用于定义数据库的结构和对象，常见的 DDL 语句有 <code>CREATE</code>、<code>ALTER</code> 和 <code>DROP</code>。</p>
<p>10.numeric data types（数值数据类型）</p>
<p>（1）int or integer</p>
<p>用于存储整数，通常占用 4 个字节，范围根据数据库系统略有不同。</p>
<p>（2）smallint</p>
<p>用于存储较小的整数，通常占用 2 个字节，范围比 <code>int</code> 小。</p>
<p>（3）decimal（p，s）</p>
<p>用于存储精确的小数，<code>p</code> 表示精度（总位数），<code>s</code> 表示小数位数。</p>
<p>（4）float</p>
<p>用于存储单精度浮点数，通常占用 4 个字节，精度有限。</p>
<p>（5）double</p>
<p>用于存储双精度浮点数，通常占用 8 个字节，精度比 <code>float</code> 高。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一个包含不同数值类型的表</span></span><br><span class="line"><span class="keyword">CREATE TABLE</span> NumericTable (</span><br><span class="line">    IntColumn <span class="type">INT</span>,</span><br><span class="line">    SmallIntColumn <span class="type">SMALLINT</span>,</span><br><span class="line">    DecimalColumn <span class="type">DECIMAL</span>(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">    FloatColumn <span class="type">FLOAT</span>,</span><br><span class="line">    DoubleColumn <span class="keyword">DOUBLE</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>11.text data type（文本数据类型）</p>
<p>（1）char（n）</p>
<p>用于存储固定长度的字符串，<code>n</code> 表示字符串的长度。如果存储的字符串长度小于 <code>n</code>，则会用空格填充。</p>
<p>（2）varchar（n）</p>
<p>用于存储可变长度的字符串，<code>n</code> 表示字符串的最大长度。只占用实际存储的字符串长度的空间。</p>
<p>（3）text</p>
<p>用于存储大量的文本数据，长度没有固定限制。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> TextTable(</span><br><span class="line">    CharColumn <span class="type">char</span>(<span class="number">10</span>),</span><br><span class="line">    VarcharColumn <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    TextColumn text</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>12.constraints</p>
<p>（1）not null</p>
<p>确保列中不允许存储 <code>NULL</code> 值。</p>
<p>（2）unique</p>
<p>确保列中的值是唯一的。</p>
<p>（3）default</p>
<p>为列指定默认值，如果插入记录时没有提供该列的值，则使用默认值。</p>
<p>（4）check</p>
<p>用于限制列中的值必须满足指定的条件。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建一个包含约束的表</span></span><br><span class="line"><span class="keyword">create table</span> ConstraintTable(</span><br><span class="line">    ID <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    Name <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">not null</span>,</span><br><span class="line">    Email <span class="type">varchar</span>(<span class="number">100</span>)<span class="keyword">unique</span>,</span><br><span class="line">    Age <span class="type">int</span> <span class="keyword">default</span> <span class="number">18</span>,</span><br><span class="line">    Salary <span class="type">decimal</span>(<span class="number">10</span>,<span class="number">2</span>) <span class="keyword">check</span>(Salary<span class="operator">&gt;</span><span class="number">0</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>13.auto increment</p>
<p><code>AUTO_INCREMENT</code> 用于为表中的某一列自动生成唯一的整数序列，通常用于主键列。</p>
<p><code>AUTO_INCREMENT</code> 是数据库中的一个重要特性，主要用于为表中的某一列自动生成唯一的整数序列，通常与主键列搭配使用。每当向表中插入一条新记录时，该列的值会自动递增。在 MySQL 里，一个表只能有一个 <code>AUTO_INCREMENT</code> 列，并且该列必须被定义为主键或者包含在一个唯一索引里。以下是创建带有 <code>AUTO_INCREMENT</code> 列的表的示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> users(</span><br><span class="line">    id <span class="type">int</span> auto_increment <span class="keyword">primary key</span>,</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    email <span class="type">varchar</span>(<span class="number">100</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>14.foreign key straint（外键约束）</p>
<p>child table and parent table</p>
<p>外键约束用于在两个表之间建立关联，关联的表分别称为子表（child table）和父表（parent table）。父表包含主键列，子表包含引用父表主键的外键列。外键约束保证了数据的引用完整性，也就是子表中的外键值必须与父表中主键值相匹配，或者为 <code>NULL</code>。以下是创建带有外键约束的表的示例：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">--- 创建父表</span></span><br><span class="line"><span class="keyword">create table</span> departments(</span><br><span class="line">    dept_id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    dept_name <span class="type">varchar</span>(<span class="number">50</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">--- 创建子表</span></span><br><span class="line"><span class="keyword">create table</span> employees(</span><br><span class="line">    emp_id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    emp_name <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    dept_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">foreign key</span> (dept_id) <span class="keyword">references</span> departments(dept_id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>15.referential action（引用操作）</p>
<p>当父表中的记录被更新或者删除时，引用操作决定了子表中的相关记录要如何处理。常见的引用操作有以下几种：</p>
<p>（1）cascade（级联）</p>
<p>当父表中的记录被更新或者删除时，子表中所有引用该记录的外键值也会被相应地更新或者删除。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> employees(</span><br><span class="line">    emp_id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    emp_name <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    dept_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">foreign key</span> (dept_id) <span class="keyword">references</span> departments(dept_id)</span><br><span class="line">    <span class="keyword">on</span> <span class="keyword">update</span> cascade</span><br><span class="line">    <span class="keyword">on</span> <span class="keyword">delete</span> cascade</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>（2）set null</p>
<p>当父表中的记录被更新或者删除时，子表中所有引用该记录的外键值会被设置为 <code>NULL</code>。不过，这要求外键列允许为 <code>NULL</code>。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> employees(</span><br><span class="line">    emp_id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    emp_name <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    dept_id <span class="type">int</span> <span class="keyword">null</span>,</span><br><span class="line">    <span class="keyword">foreign key</span>(dept_id) <span class="keyword">references</span> departments(dept_id)</span><br><span class="line">    <span class="keyword">on</span> <span class="keyword">update</span> <span class="keyword">set</span> <span class="keyword">null</span></span><br><span class="line">    <span class="keyword">on</span> <span class="keyword">delete</span> <span class="keyword">set</span> <span class="keyword">null</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>（3）restrict</p>
<p>在父表中有相关记录被子表引用时，禁止对父表中的记录进行更新或者删除操作。这是默认的引用操作。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create table</span> employees(</span><br><span class="line">    emp_id <span class="type">int</span> <span class="keyword">primary key</span>,</span><br><span class="line">    emp_name <span class="type">varchar</span>(<span class="number">50</span>),</span><br><span class="line">    dept_id <span class="type">int</span>,</span><br><span class="line">    <span class="keyword">foreign key</span>(dept_id) <span class="keyword">references</span> departments(dept_id)</span><br><span class="line">    <span class="keyword">on</span> <span class="keyword">update</span> restrict</span><br><span class="line">    <span class="keyword">on</span> <span class="keyword">delete</span> restrict</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p><mark>16.index</mark>（索引）（节约搜索时间提高搜索效率）</p>
<p><code>create index</code></p>
<p>索引是一种数据结构，它能够加快数据库表中数据的查询速度。通过创建索引，数据库可以更快地定位到满足查询条件的记录，从而节省搜索时间，提高搜索效率。不过，索引并非适用于所有场景：</p>
<ul>
<li><strong>频繁更新</strong>：在对表进行频繁更新操作时，索引也需要相应地更新，这会降低更新操作的性能。</li>
<li><strong>数据量小</strong>：当表中的数据量较小时，专门创建索引的开销可能会超过索引带来的性能提升，因此没必要专门建立索引。</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> index idx_name <span class="keyword">on</span> table_name (column1, column2,...);</span><br></pre></td></tr></table></figure>
<p>17.sql view（视图）</p>
<p>视图是一个虚拟表，它并不实际存储数据，而是基于一个或多个表的查询结果。视图可以简化复杂的查询，提高数据的安全性，并且允许用户以不同的方式查看数据。创建视图的语法如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> view_name <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> column1,column2,...</span><br><span class="line"><span class="keyword">from</span> table_name</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">condition</span>;</span><br></pre></td></tr></table></figure>
<p>18.alter value（修改表结构）</p>
<p><code>ALTER TABLE</code> 语句用于修改已存在表的结构，常见的操作有以下几种：</p>
<p>（1）<code>alter table add</code></p>
<p>用于向表中添加新的列。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> users <span class="keyword">add</span> <span class="keyword">column</span> phone <span class="type">varchar</span>(<span class="number">20</span>);</span><br></pre></td></tr></table></figure>
<p>（2）<code>alter table modify</code></p>
<p>用于修改表中现有列的定义，例如修改列的数据类型。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> users modify <span class="keyword">column</span> phone <span class="type">varchar</span>(<span class="number">30</span>);</span><br></pre></td></tr></table></figure>
<p>（3）<code>alter table drop</code></p>
<p>用于删除表中的列。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter table</span> users <span class="keyword">drop</span> <span class="keyword">column</span> phone;</span><br></pre></td></tr></table></figure>
<p>19.transaction（事务）</p>
<p>事务是一组不可分割的数据库操作序列，这些操作要么全部执行成功，要么全部不执行。事务具有原子性、一致性、隔离性和持久性（ACID）四个特性。常见的事务操作有以下几种：</p>
<p>（1）commit</p>
<p>用于提交事务，将事务中所做的所有修改永久保存到数据库中。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">start</span> transaction;</span><br><span class="line"><span class="comment">--- 执行一系列数据库操作</span></span><br><span class="line">insect <span class="keyword">into</span> users (name,email) <span class="keyword">values</span> (<span class="string">&#x27;join&#x27;</span>,<span class="string">&#x27;join@example.com&#x27;</span>);</span><br><span class="line"><span class="keyword">update</span> accounts <span class="keyword">set</span> balance<span class="operator">=</span>balance<span class="number">-100</span> <span class="keyword">where</span> user_id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<p>（2）rollback</p>
<p>用于回滚事务，撤销事务中所做的所有修改。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">start</span> transaction;</span><br><span class="line"><span class="comment">--- 执行一系列数据库操作</span></span><br><span class="line"><span class="keyword">insert into</span> users (name,email) <span class="keyword">values</span> (<span class="string">&#x27;join&#x27;</span>,<span class="string">&#x27;join@example.com&#x27;</span>)</span><br><span class="line"><span class="keyword">update</span> accounts <span class="keyword">set</span> balance<span class="operator">=</span>balance<span class="number">-100</span> <span class="keyword">where</span> user_id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">rollback</span>;</span><br></pre></td></tr></table></figure>
<p>（3）savepoint</p>
<p>用于在事务中设置保存点，允许在事务执行过程中部分回滚。示例如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">start</span> transaction;</span><br><span class="line"><span class="comment">--- 执行一系列数据库操作</span></span><br><span class="line"><span class="keyword">insert into</span> users (name,email) <span class="keyword">values</span> (<span class="string">&#x27;join&#x27;</span>,<span class="string">&#x27;join@example.com&#x27;</span>);</span><br><span class="line"><span class="keyword">savepoint</span> sp1;</span><br><span class="line"><span class="keyword">update</span> account <span class="keyword">set</span> balance<span class="operator">=</span>balance<span class="number">-100</span> <span class="keyword">where</span> user_id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="comment">-- 发现错误，回滚到保存点</span></span><br><span class="line"><span class="keyword">rollback</span> <span class="keyword">to</span> sp1;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>数据库是如何发现错误进行回滚的？</p>
<p>数据库发现错误并触发回滚的机制主要依赖于==<strong>事务控制逻辑</strong><mark>和</mark><strong>错误检测机制</strong>==</p>
<p>数据库通过 <strong>应用层主动检测</strong> 和 <strong>数据库层自动检测</strong> 两种方式发现错误：</p>
<p>一、错误发现的途径</p>
<ol>
<li>
<p><strong>应用层主动检测（业务逻辑层面）</strong></p>
<ul>
<li>应用程序在执行数据库操作前或后，通过业务规则判断是否出现异常（例如：转账时余额不足、数据格式不符合要求等）。</li>
<li>若检测到错误（如代码中通过条件判断、异常捕获等逻辑），应用会主动调用数据库的回滚接口（如 <code>ROLLBACK</code> 或 <code>ROLLBACK TO SAVEPOINT</code>），显式触发回滚。</li>
</ul>
</li>
<li>
<p><strong>数据库层自动检测（约束与系统层面）</strong></p>
</li>
</ol>
<ul>
<li><strong>完整性约束违反</strong>：数据库内置的约束（如主键唯一性、外键引用、CHECK 约束、非空约束等）会在数据操作时自动校验。若违反约束（例如插入重复主键、外键引用不存在的记录），数据库会立即抛出错误（如 SQL 错误码）。</li>
<li><strong>系统错误</strong>：数据库在执行操作时遇到内部错误（如磁盘空间不足、事务超时、锁冲突等），会自动生成错误信号并中断事务。</li>
</ul>
<p>二、回滚机制的核心原理</p>
<p>当错误被检测到后，数据库通过 <strong>事务日志（Transaction Log）</strong> 和 <strong>保存点（Savepoint）</strong> 实现回滚，具体逻辑如下：</p>
<ol>
<li><strong>事务日志的作用</strong>
<ul>
<li>数据库在事务执行过程中，会将每一步操作（如插入、更新、删除）记录到 <strong>撤销日志（Undo Log）</strong> 中，记录数据修改前的原始状态。</li>
<li>回滚时，数据库根据撤销日志逆向执行操作，将数据恢复到错误发生前的状态（例如：对 <code>UPDATE</code> 操作，回滚会将数据恢复为修改前的值）。</li>
</ul>
</li>
<li><strong>完全回滚与部分回滚</strong>
<ul>
<li><strong>完全回滚（<code>ROLLBACK</code>）</strong>：撤销事务开始后所有未提交的操作，将整个事务回退到初始状态（即 <code>START TRANSACTION</code> 之前的状态）。</li>
<li><strong>部分回滚（<code>ROLLBACK TO SAVEPOINT</code>）</strong>：通过 <code>SAVEPOINT</code> 在事务中标记一个中间点，回滚时仅撤销该保存点之后的操作，保存点之前的操作仍保留（需后续通过 <code>COMMIT</code> 提交）。</li>
</ul>
</li>
<li><strong>事务原子性的保证</strong>
<ul>
<li>无论错误是由应用层还是数据库层触发，事务的原子性要求确保：要么所有操作都提交，要么所有未提交的操作都被回滚，避免数据处于中间状态。</li>
</ul>
</li>
</ol>
</blockquote>
<p>20.motivation（动机）</p>
<p>在数据库设计和开发中，运用上述这些特性和技术主要有以下几个动机：</p>
<ul>
<li><strong>数据完整性</strong>：利用 <code>AUTO_INCREMENT</code>、外键约束和引用操作可以保证数据的完整性和一致性，防止出现无效或者不一致的数据。</li>
<li><strong>性能优化</strong>：使用索引能够提高查询性能，尤其是在处理大量数据时，能够显著缩短查询时间。</li>
<li><strong>灵活性和可维护性</strong>：视图和 <code>ALTER TABLE</code> 语句可以让数据库结构更加灵活，便于后续的维护和扩展。</li>
<li><strong>数据安全性</strong>：事务机制可以确保在并发操作时数据的安全性和一致性，避免数据冲突和错误。</li>
</ul>
<p>##　　 4.24</p>
<p>1.sanitize input（输入净化）</p>
<p>输入净化是一种重要的安全措施，主要用于防止恶意用户通过输入特殊字符或代码来攻击数据库，像 SQL 注入攻击。在将用户输入的数据用于数据库查询之前，需要对输入进行处理，过滤掉可能导致安全问题的字符或代码。例如，在使用 Python 的 <code>sqlite3</code> 库时，可以使用参数化查询，它会自动对输入进行净化：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">conn = sqlite3.connect(<span class="string">&#x27;example.db&#x27;</span>)</span><br><span class="line">cursor = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户输入</span></span><br><span class="line">username = <span class="built_in">input</span>(<span class="string">&quot;请输入用户名: &quot;</span>)</span><br><span class="line">password = <span class="built_in">input</span>(<span class="string">&quot;请输入密码: &quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用参数化查询</span></span><br><span class="line">query = <span class="string">&quot;SELECT * FROM users WHERE username =? AND password =?&quot;</span></span><br><span class="line">cursor.execute(query, (username, password))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取结果</span></span><br><span class="line">results = cursor.fetchall()</span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭连接</span></span><br><span class="line">conn.close()    </span><br></pre></td></tr></table></figure>
<p>2.prevention（预防）</p>
<p>预防在数据库安全和性能优化方面都非常关键。主要包括以下几个方面：</p>
<ul>
<li><strong>安全预防</strong>：通过输入净化、访问控制、加密等手段，防止 SQL 注入、跨站脚本攻击（XSS）、数据泄露等安全问题。</li>
<li><strong>性能预防</strong>：合理设计数据库结构、创建适当的索引、优化查询语句等，避免出现性能瓶颈，如查询缓慢、死锁等问题。</li>
</ul>
<p>3.ORM: Object-Relational Mapping（对象关系映射）</p>
<p>对象关系映射（ORM）是一种编程技术，它允许开发者使用面向对象的方式来操作数据库，而无需编写原生的 SQL 语句。ORM 框架会自动将对象的操作转换为相应的 SQL 语句，从而简化了数据库开发过程。例如，在 Python 中使用 SQLAlchemy 这个 ORM 框架：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> Column, Integer, String, create_engine</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.orm <span class="keyword">import</span> sessionmaker</span><br><span class="line"><span class="keyword">from</span> sqlalchemy.ext.declarative <span class="keyword">import</span> declarative_base</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建基类</span></span><br><span class="line">Base = declarative_base()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">User</span>(<span class="title class_ inherited__">Base</span>):</span><br><span class="line">    __tablename__ = <span class="string">&#x27;users&#x27;</span></span><br><span class="line">    <span class="built_in">id</span> = Column(Integer, primary_key=<span class="literal">True</span>)</span><br><span class="line">    name = Column(String)</span><br><span class="line">    email = Column(String)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据库引擎</span></span><br><span class="line">engine = create_engine(<span class="string">&#x27;sqlite:///example.db&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建表</span></span><br><span class="line">Base.metadata.create_all(engine)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话</span></span><br><span class="line">Session = sessionmaker(bind=engine)</span><br><span class="line">session = Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建用户对象</span></span><br><span class="line">new_user = User(name=<span class="string">&#x27;John&#x27;</span>, email=<span class="string">&#x27;john@example.com&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加到会话</span></span><br><span class="line">session.add(new_user)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交会话</span></span><br><span class="line">session.commit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询用户</span></span><br><span class="line">users = session.query(User).<span class="built_in">all</span>()</span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">    <span class="built_in">print</span>(user.name, user.email)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭会话</span></span><br><span class="line">session.close()    </span><br></pre></td></tr></table></figure>
<p>4.ER Model</p>
<p>实体 - 关系模型是一种用于数据库设计的概念模型，它通过实体、属性、关系等元素来描述现实世界中的数据及其关系。</p>
<p>（1）Entities（实体）</p>
<p>实体是现实世界中可区别的事物或对象，例如学生、课程、员工等。在 ER 图中，实体通常用矩形表示。</p>
<p>（2）Attributes：the details about each thing（属性）</p>
<p>属性是实体的特征或描述信息，例如学生的姓名、年龄、学号等。在 ER 图中，属性通常用椭圆形表示，并与对应的实体相连。</p>
<p>（2）Relations（can also have attributes）（关系）</p>
<p>关系表示实体之间的联系，例如学生和课程之间的选课关系、员工和部门之间的所属关系等。在 ER 图中，关系通常用菱形表示，并与相关的实体相连。关系也可以有自己的属性，例如选课关系可以有成绩这个属性。</p>
<p>（3）Participation（total，partical participation）（参与度）</p>
<p>参与度描述了实体在关系中出现的情况，分为完全参与和部分参与：</p>
<ul>
<li><strong>完全参与（Total Participation）</strong>：表示实体集中的每个实体都必须参与到某个关系中。在 ER 图中，用双线表示。</li>
<li><strong>部分参与（Partial Participation）</strong>：表示实体集中的部分实体参与到某个关系中。在 ER 图中，用单线表示。</li>
</ul>
<p>（4）generalization and specialization（<strong>泛化</strong>与<strong>特化</strong>）</p>
<ul>
<li><strong>Generalization（泛化）</strong>：是从多个具体实体中抽象出共同特征，形成一个更通用的实体。例如，从本科生、研究生中抽象出学生这个通用实体。</li>
<li><strong>Specialization（特化）</strong>：是将一个通用实体根据某些特征细分为多个具体实体。例如，将学生实体细分为本科生、研究生。</li>
</ul>
<p>（5）weak entities（弱实体）</p>
<p>弱实体是依赖于其他实体（称为强实体）而存在的实体，它没有自己的主键，而是通过与强实体的关系来确定其唯一性。例如，订单明细依赖于订单而存在，订单明细就是一个弱实体。在 ER 图中，弱实体用双矩形表示。</p>
<p>（6）ternary relationships（三元关系）</p>
<p>三元关系是指三个实体之间的关系。例如，教师、课程和学生之间的关系，一个教师可以教授多门课程，一门课程可以有多个学生选修，一个学生可以选修多门课程。在 ER 图中，三元关系用菱形表示，并与三个相关的实体相连。</p>
<h2 id="4-29">4.29</h2>
<p>1.Database Design</p>
<p>需求分析-&gt;概念设计-&gt;逻辑设计-&gt;物理设计</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510210337610.png" alt="image-20250510210337610"></p>
<p>2.ER图常见物象</p>
<p>（1）Entities实体</p>
<p>表示存储有关某个事物的数据的真实对象或概念，表示为矩形</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510210509270.png" alt="image-20250510210509270"></p>
<p>（2）Attributes属性</p>
<p>是实体的具体特征，如员工实体可能有ID，姓名，薪水等属性；属性可以是简单的，复合的，派生的，多值的（多个东西实际上对应一种名字，只是名头不一样），表示为连接到实体的椭圆</p>
<p>（3）Relationships关系</p>
<p>关系是实体之间的关联，例如员工“works in”部门，分为1：1（一对一，如一个人have一张机票），1：M（一对多，如一个老师teach多个学生），M：N（多对多哦，如一个学生enroll注册多个课程，一个课程有多个学生）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510211849132.png" alt="image-20250510211849132"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510211855671.png" alt="image-20250510211855671"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510211901064.png" alt="image-20250510211901064"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510211911012.png" alt="image-20250510211911012"></p>
<p>（4）Participation参与</p>
<p>参与是确认是否所有实体都得有这个关系，表示为不同形式的连线。如果每个实体都需要参与就画double line双实线，有些可能不参与就画single line单实线。例如每位学生必须enroll注册至少一门课程，而不是所有课程都必须被enroll</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510212232034.png" alt="image-20250510212232034"></p>
<p>（5）Keys键</p>
<p>primary keys主键就在实体的名称下面加下划线（are underlined）</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510212323974.png" alt="image-20250510212323974"></p>
<p>（5）Generalization&amp;Specialization泛化和专业化：grouping similar things</p>
<p>Generalization泛化：当几个实体相似时，我们可以将它们归类为一个通用类型</p>
<p>Specialization特化：将一般实体拆分为更具体的子实体</p>
<p>Inheritance继承：子实体从通用化实体继承属性和关系</p>
<p>三角形标注ISA表示继承，如学生和老师继承人的属性</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510213141605.png" alt="image-20250510213141605"></p>
<p>（6）Weak Entities弱实体</p>
<p>弱实体依附于其他实体存在，否则将无意义。表示为双矩形。实体与弱实体间的关系表示为双菱形，又称identity relationship标识关系</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510213433472.png" alt="image-20250510213433472"></p>
<p>（7）Recursive Relationship递归关系</p>
<p>指实体内部之间有交互动作，用循环箭头表示</p>
<p>如alicebob都是员工，员工bob要向经理（也算employee）的alice汇报工作</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510213942400.png" alt="image-20250510213942400"></p>
<p>（8）Ternary Relationships三元关系</p>
<p>•同时涉及三个实体的关系。你不能只看两个来完全理解这种关系。</p>
<p>• Example: A member want to reserves some equipment at a certain time frame. who? what? when?</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510214138658.png" alt="image-20250510214138658"></p>
<p>（9）（Min，Max）大小限制</p>
<p>• （min， max） 表示法告诉我们实体可以参与关系的次数 - 至少 （min） 和最多 （max）。• 任何人都可以注册课程，但最多可容纳 50 人。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510214230173.png" alt="image-20250510214230173"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250510214250575.png" alt="image-20250510214250575"></p>
<p>3.ER图绘制流程</p>
<p>（1）<strong>Mapping of Regular Entity Types（常规实体类型的映射）</strong><br>
将每个常规实体类型转换为一个关系模式，实体的属性成为关系模式的属性，实体的主键作为关系模式的主键。例如，“学生” 实体（属性：学号、姓名、年龄，学号为主键）映射为 “学生表”，包含这些属性且学号为主键。</p>
<p>（2）<strong>Mapping of Weak Entity Types（弱实体类型的映射）</strong><br>
弱实体依赖强实体存在，无独立主键。映射时，弱实体与所依赖强实体的主键共同构成其关系模式的主键。如 “订单明细”（弱实体）依赖 “订单”（强实体），订单号（订单主键） + 明细项编号可作为订单明细表的主键，同时包含弱实体自身属性。</p>
<p>（3）<strong>Mapping of Binary 1:1 Relationship Types（二元一对一关系类型的映射）</strong><br>
可将关系合并到任一端实体的关系模式中（如作为一个属性），或单独创建关系模式（包含两端实体主键及关系自身属性，若有）。例如，“部门” 与 “经理”（1:1），可将经理主键加入部门表，或新建表包含部门与经理主键。</p>
<p>（4）<strong>Mapping of Binary 1:N Relationship Types（二元一对多关系类型的映射）</strong>（对于1：N来说，就是要在动作的接收者那加一个动作发出者的主键作为外键放后面即可）<br>
将一端（1 端）实体的主键加入多端（N 端）实体的关系模式中。如 “部门”（1 端）与 “员工”（N 端），把部门编号（部门主键）加入员工表作为外键，体现一对多关系。</p>
<p>（5）<strong>Mapping of Binary M:N Relationship Types（二元多对多关系类型的映射）</strong>（对于M：N，就是要新建一个表table名字为动作的名字，然后表中让动作发出接受者的主键作为外键，这样就能形成m：n的关系）<br>
需创建新关系模式，包含两端实体主键及关系自身属性（若有）。例如 “学生” 与 “课程”（多对多），创建 “选课表”，包含学生学号、课程编号，还可含成绩等属性。</p>
<p>（6）<strong>Mapping of Multivalued attributes（多值属性的映射）</strong><br>
多值属性（如一个人多个电话号码）不能直接存于关系模式，需新建关系模式。如创建 “电话表”，包含个人主键与电话号码属性。</p>
<p>（7）<strong>Mapping of Ternary Relationship（三元关系的映射）</strong><br>
创建新关系模式，包含三个实体的主键及关系自身属性（若有）。例如 “供应商 - 项目 - 零件” 的供应关系，创建表包含供应商编号、项目编号、零件编号，以及供应数量等属性。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/05cd82aa4503ad50ed57d1976d79780.jpg" alt="05cd82aa4503ad50ed57d1976d79780"></p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/795b3dc0b26f16d0748643ce504f868.jpg" alt="795b3dc0b26f16d0748643ce504f868"></p>
<blockquote>
<p>如何解读ER图？</p>
<ul>
<li><strong>长方形（实体）</strong></li>
</ul>
<p>（1）<strong>Branch（分行）</strong>：代表银行分行这一实体，具有属性 <code>branch#</code>（分行编号，下划线表示主键，唯一标识分行）和 <code>name</code>（分行名称）。</p>
<p>（2）<strong>Account（账户）</strong>：代表银行账户实体，属性有 <code>acct#</code>（账户编号，主键）和 <code>balance</code>（账户余额）。</p>
<p>（3）<strong>Customer（客户）</strong>：代表银行客户实体，属性包括 <code>cust#</code>（客户编号，主键）、<code>name</code>（客户姓名）和 <code>address</code>（客户地址）。</p>
<ul>
<li><strong>菱形（关系）</strong></li>
</ul>
<p>（1）<strong>HeldAt（开设在）</strong>：表示账户（Account）与分行（Branch）之间的关系，即账户开设在某个分行。</p>
<p>（2）<strong>HomeBranch（归属分行）</strong>：描述客户（Customer）与分行（Branch）的关系，即客户的归属分行。</p>
<p>（3）<strong>Owns（拥有）</strong>：体现客户（Customer）与账户（Account）的关系，即客户拥有账户。</p>
<ul>
<li><strong>椭圆形（属性）</strong></li>
</ul>
<p>椭圆形用于表示实体的特征，如 <code>branch#</code>、<code>name</code> 是 <code>Branch</code> 实体的属性；<code>acct#</code>、<code>balance</code> 是 <code>Account</code> 实体的属性；<code>cust#</code>、<code>name</code>、<code>address</code> 是 <code>Customer</code> 实体的属性。</p>
<ul>
<li><strong>连线粗细与箭头</strong></li>
</ul>
<p>（1）图中 <code>Account</code> 到 <code>HeldAt</code> 的连线较粗，通常在 ER 图中，这种视觉差异可能用于强调关系的重要性或突出显示特定关联，但并非严格的标准规范。</p>
<p>（2）箭头表示关系的方向或参与度。例如，<code>HeldAt</code> 到 <code>Branch</code> 的箭头，表明 <code>Account</code> 通过 <code>HeldAt</code> 关系指向 <code>Branch</code>，即账户开设在分行，一个账户只对应一个分行（从图中关系方向看，体现了一种单向的关联指向）。而 <code>Owns</code> 关系连接 <code>Customer</code> 和 <code>Account</code> 无箭头，暗示这是一种多对多的关系（一个客户可拥有多个账户，一个账户可被多个客户拥有）。</p>
</blockquote>
<blockquote>
<p>如何作答“如何解读ER图”的问题？</p>
<blockquote>
<p>[!success]</p>
<p>解读此 ER 图时，答案应包含以下信息：图中的实体及其属性、实体间的关系、关系的含义、主键标识（下划线）等。</p>
</blockquote>
<p>图中包含三个实体：</p>
<ul>
<li><strong>Branch（分行）</strong>：具有属性 <code>branch#</code>（分行编号，主键，以下划线标识）和 <code>name</code>（分行名称）。</li>
<li><strong>Account（账户）</strong>：具有属性 <code>acct#</code>（账户编号，主键）和 <code>balance</code>（账户余额）。</li>
<li><strong>Customer（客户）</strong>：具有属性 <code>cust#</code>（客户编号，主键）、<code>name</code>（客户姓名）和 <code>address</code>（客户地址）。</li>
</ul>
<p>实体间的关系如下：</p>
<ul>
<li><strong>HeldAt（开设在）</strong>：表示 <code>Account</code>（账户）与 <code>Branch</code>（分行）的关系，即账户开设在某个分行。</li>
<li><strong>Owns（拥有）</strong>：表示 <code>Customer</code>（客户）与 <code>Account</code>（账户）的关系，即客户拥有账户。</li>
<li><strong>HomeBranch（归属分行）</strong>：表示 <code>Customer</code>（客户）与 <code>Branch</code>（分行）的关系，即客户的归属分行。</li>
</ul>
<p>通过这些元素，该 ER 图构建了一个简单的银行相关数据模型，清晰展示了分行、账户、客户及其属性与相互关系，为后续数据库设计（如关系模式转换）提供了概念基础。</p>
</blockquote>
<p>4.Heuristic Guideline（启发式准则）</p>
<p>（1）Heuristic Guideline 1</p>
<ul>
<li><strong>内容</strong>：关系中的每个元组应代表一个实体或关系实例，且仅通过外键引用其他实体。不同实体（如学生、课程）的属性不应混合在同一关系中。</li>
<li><strong>示例</strong>：图中表格将 <code>StudentID</code>（学生编号）、<code>StudentName</code>（学生姓名）、<code>CourseName</code>（课程名称）、<code>Instructor</code>（教师）混合，这种设计不符合准则，被标记为错误（打叉）。</li>
</ul>
<blockquote>
<p>将这些属性放在同一表中，会使关系语义混乱，违背 “一个关系代表一种实体或关系实例” 的原则。例如，该表既想表示学生信息，又想表示课程信息，导致元组含义不清晰，因此被标记为错误。</p>
</blockquote>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250429103002135.png" alt="image-20250429103002135"></p>
<p>（2）Heuristic Guideline 2</p>
<ul>
<li><strong>内容</strong>：避免冗余信息。冗余信息可能导致更新不一致。</li>
<li><strong>示例</strong>：若数学课程的教师 <code>Prof. A</code> 被 <code>Prof. X</code> 取代，由于教师信息在每个学生 - 课程对中重复存储，若未更新所有相关行，会导致数据冲突或错误。</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250429103706006.png" alt="image-20250429103706006"></p>
<p>不能把Prof.A变成Prof.X</p>
<p>（3）Heuristic Guideline 3</p>
<ul>
<li><strong>内容</strong>：设计的模式应避免插入、删除和更新异常。</li>
<li><strong>示例</strong>：对于 <code>EMP_PROJ(Emp#, Proj#, Ename, Pname, No_hours)</code> 表，删除一个项目会误删所有参与该项目的员工；若一个员工是某项目的唯一参与者，删除该员工会误删项目，这就是删除异常。</li>
</ul>
<p>（4）Heuristic Guideline 4</p>
<ul>
<li><strong>内容</strong>：关系设计应使元组的 <code>NULL</code> 值尽可能少。<code>NULL</code> 会导致歧义（未知还是不适用），并使查询复杂易错。</li>
<li><strong>示例</strong>：查询考试不及格的学生时，需添加 <code>grade is not null</code> 条件，因为 <code>grade</code> 为 <code>NULL</code> 不代表不及格，体现了 <code>NULL</code> 对查询的影响。</li>
</ul>
<p>（5）Heuristic Guideline 5</p>
<ul>
<li><strong>内容</strong>：设计关系模式时，应能通过适当相关的属性（主键、外键）对的相等条件进行连接，以抑制虚假元组产生。若表间无匹配键属性或值不一致，连接会出错或产生虚假结果。</li>
<li><strong>核心</strong>：确保表间通过合理的键（如主键 - 外键）关联，避免连接错误或无效结果。</li>
</ul>
<p>5.Database Normalization</p>
<p>数据库规范化通过将大表拆分为更小的相关表，并使用键定义它们之间的清晰关系，以减少数据冗余、消除异常（插入、删除、更新异常）。规范化遵循一系列范式（Normal Forms）：</p>
<p>（1）1NF（First Normal Form）第一范式</p>
<p>表中无重复组或数组，所有列值为原子值（不可再分）。</p>
<ul>
<li>
<p><strong>原子性</strong>：每列值为不可分割的最小单元（如字符串、数字，而非列表 / 集合）。</p>
</li>
<li>
<p><strong>数据类型一致性</strong>：每列仅存储单一类型的值。</p>
</li>
<li></li>
<li>
<p>非 1NF 表</p>
<p>（旧 users 表）：</p>
</li>
<li>
<p>| userid                                   | fullname   |<br>
| ---------------------------------------- | ---------- |<br>
| 1                                        | John Doe   |<br>
| 2                                        | Mary Smith |<br>
| <code>fullname</code>包含空格分隔的姓名，非原子值。 |            |</p>
</li>
<li>
<p>1NF 表</p>
<p>（新 users 表）：</p>
<p>| userid                         | firstname | lastname |<br>
| ------------------------------ | --------- | -------- |<br>
| 1                              | John      | Doe      |<br>
| 2                              | Mary      | Smith    |<br>
| 姓名拆分为独立列，符合原子性。 |           |          |</p>
</li>
</ul>
<blockquote>
<p><strong>每个格子只能装 “单一数据”，不能 “抱团”</strong>：表中的每个 “单元格” 只能存一个 <strong>独立、不可分割</strong> 的值，不能存 “一堆数据”。</p>
<ul>
<li>
<p>比如：“姓名” 列不能存 “张三 李四”（两个人名挤一起），也不能存 “张 / 三”（用斜线分隔），必须拆成 “姓” 和 “名” 两列。</p>
</li>
<li>
<p>再比如：“课程” 列不能写 “数学，物理”（多个课程用逗号隔开），这属于 “重复组”，要么拆成多行（每个课程一行），要么单独建 “选课表”。</p>
</li>
<li>
<p><strong>目的</strong>：让表看起来 “干干净净”，每个格子都规规矩矩只存一个值，方便后续操作。</p>
</li>
<li>
<p>反例</p>
<p>（非 1NF）：</p>
<p>| 学生 ID | 姓名     | 课程       |<br>
| ------- | -------- | ---------- |<br>
| 1       | 张三李四 | 数学，物理 |</p>
</li>
<li>
<p>正例</p>
<p>（1NF）：</p>
<p>| 学生 ID | 姓   | 名   | 课程 |<br>
| ------- | ---- | ---- | ---- |<br>
| 1       | 张   | 三   | 数学 |<br>
| 1       | 张   | 三   | 物理 |</p>
</li>
</ul>
</blockquote>
<p>（2）2NF（Second Normal Form）</p>
<p>满足 1NF，且每个非键列完全函数依赖于主键（无部分依赖）。</p>
<ul>
<li>先满足 1NF。</li>
<li><strong>完全函数依赖</strong>：非主键列依赖于整个主键（若主键为复合键，不能仅依赖部分键）。</li>
</ul>
<p><strong>非 2NF 表</strong>（Students 表）：</p>
<p>| IDSt                                                         | LastName | IDProf | Prof   |<br>
| ------------------------------------------------------------ | -------- | ------ | ------ |<br>
| 1                                                            | Mueller  | 3      | Schmid |<br>
| 主键为<code>(IDSt, IDProf)</code>，但<code>Prof</code>仅依赖<code>IDProf</code>，存在部分依赖。 |          |        |        |</p>
<p><strong>2NF 表</strong>（分解后）：</p>
<ul>
<li>
<p>Students 表：</p>
<p>| IDSt | LastName |<br>
| ---- | -------- |<br>
| 1    | Mueller  |</p>
</li>
<li>
<p>Professors 表：</p>
<p>| IDProf                             | Prof   |<br>
| ---------------------------------- | ------ |<br>
| 3                                  | Schmid |<br>
| 消除部分依赖，非键列完全依赖主键。 |        |</p>
</li>
</ul>
<blockquote>
<p><strong>2NF（第二范式）：非主键列必须 “全靠主键”，不能 “只靠一半”</strong></p>
<ul>
<li>
<p><strong>前提</strong>：先满足 1NF，且表有 <strong>主键</strong>（可能是单个字段，也可能是多个字段的 “复合主键”）。</p>
</li>
<li>
<p>核心规则：表中 “非主键列” 必须完全依赖整个主键，不能只依赖主键的“一部分”。</p>
<ul>
<li>比如：主键是 “学生 ID + 课程 ID”（复合主键，唯一标识一个学生选的一门课），但 “学生姓名” 只依赖 “学生 ID”，“课程名称” 只依赖 “课程 ID”。这时候 “学生姓名” 和 “课程名称” 只依赖主键的 “一半”，属于 “部分依赖”，不符合 2NF。</li>
</ul>
</li>
<li>
<p>解决办法：把 “只依赖一半主键” 的字段拆出去，单独建表。</p>
<ul>
<li>
<p>原表（非 2NF）：</p>
<p>| 学生 ID | 课程 ID | 学生姓名 | 课程名称 | 成绩 |<br>
| ------- | ------- | -------- | -------- | ---- |<br>
| 1       | 101     | 张三     | 数学     | 90   |</p>
</li>
<li>
<p>拆分成三张表（2NF）：</p>
<ul>
<li>
<p>学生表（主键：学生 ID）：</p>
<p>| 学生 ID | 学生姓名 |<br>
| ------- | -------- |<br>
| 1       | 张三     |</p>
</li>
<li>
<p>课程表（主键：课程 ID）：</p>
<p>| 课程 ID | 课程名称 |<br>
| ------- | -------- |<br>
| 101     | 数学     |</p>
</li>
<li>
<p>选课表（主键：学生 ID + 课程 ID）：</p>
<p>| 学生 ID | 课程 ID | 成绩 |<br>
| ------- | ------- | ---- |<br>
| 1       | 101     | 90   |</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>目的</strong>：避免 “部分依赖” 导致的数据冗余（比如同一个学生姓名在多个课程记录中重复出现）。</p>
</li>
</ul>
</blockquote>
<p>（3）3NF（Third Normal Form）</p>
<p>满足 2NF，且无非主属性对主键的传递依赖。</p>
<ul>
<li>先满足 2NF。</li>
<li><strong>无传递依赖</strong>：非主属性仅依赖于候选键，不依赖于其他非主属性。</li>
</ul>
<p><strong>非 3NF 表</strong>（Vendor 表）：</p>
<p>| ID                                                           | Name    | Account No | Bank Code No | Bank   |<br>
| ------------------------------------------------------------ | ------- | ---------- | ------------ | ------ |<br>
| 1                                                            | Vendor1 | 12345      | B001         | Bank A |<br>
| <code>Bank</code>依赖于<code>Bank Code No</code>，而<code>Bank Code No</code>依赖于<code>ID</code>，存在传递依赖。 |         |            |              |        |</p>
<p><strong>3NF 表</strong>（分解后）：</p>
<ul>
<li>
<p>Vendor 表：</p>
<p>| ID   | Name    | Account No | Bank Code No |<br>
| ---- | ------- | ---------- | ------------ |<br>
| 1    | Vendor1 | 12345      | B001         |</p>
</li>
<li>
<p>Bank 表：</p>
<p>| Bank Code No                         | Bank   |<br>
| ------------------------------------ | ------ |<br>
| B001                                 | Bank A |<br>
| 消除传递依赖，非主属性仅依赖候选键。 |        |</p>
</li>
</ul>
<blockquote>
<h3 id="3NF（第三范式）：非主键列之间不能-“间接依赖”，只能-“直接靠主键”"><strong>3NF（第三范式）：非主键列之间不能 “间接依赖”，只能 “直接靠主键”</strong></h3>
<ul>
<li>
<p><strong>前提</strong>：先满足 2NF。</p>
</li>
<li>
<p>核心规则</p>
<p>：表中 “非主键列” 之间不能存在</p>
<p>传递依赖</p>
<p>，即不能有 “字段 A → 字段 B → 字段 C” 这种间接依赖关系，字段 C 必须直接依赖主键，而不是依赖另一个非主键字段。</p>
<ul>
<li>比如：员工表中有 “员工 ID（主键）、部门 ID、部门名称”，“部门名称” 依赖 “部门 ID”，而 “部门 ID” 依赖 “员工 ID”，形成 “员工 ID → 部门 ID → 部门名称” 的传递依赖，不符合 3NF。</li>
</ul>
</li>
<li>
<p>解决办法</p>
<p>：把 “传递依赖” 的字段拆出去，单独建表。</p>
<ul>
<li>
<p>原表（非 3NF）：</p>
<p>| 员工 ID | 部门 ID | 部门名称 | 员工姓名 |<br>
| ------- | ------- | -------- | -------- |<br>
| 1       | D01     | 人事部   | 张三     |</p>
</li>
<li>
<p>拆分成两张表（3NF）：</p>
<ul>
<li>
<p>员工表（主键：员工 ID）：</p>
<p>| 员工 ID | 部门 ID | 员工姓名 |<br>
| ------- | ------- | -------- |<br>
| 1       | D01     | 张三     |</p>
</li>
<li>
<p>部门表（主键：部门 ID）：</p>
</li>
<li>
<p>| 部门 ID | 部门名称 |<br>
| ------- | -------- |<br>
| D01     | 人事部   |</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>目的</strong>：避免 “传递依赖” 导致的更新异常（比如修改部门名称时，需要更新所有相关员工记录，容易漏改）。</p>
</li>
</ul>
</blockquote>
<blockquote>
<p>[!success]</p>
<p><strong>一句话总结三个范式的关系</strong></p>
<ul>
<li><strong>1NF</strong>：先让每个格子 “单身”（只存一个值）。</li>
<li><strong>2NF</strong>：再让非主键列 “全靠主键”（不能只依赖主键的一部分）。</li>
<li><strong>3NF</strong>：最后让非主键列 “直接靠主键”（不能绕弯子依赖其他非主键列）。</li>
</ul>
</blockquote>
<p>（4）BCNF（Boyce-Codd Normal Form）（<strong>Boyce</strong> 和 <strong>Codd</strong> 是两个人的姓氏，分别是 <strong>Raymond F. Boyce</strong> 和 <strong>Edgar F. Codd</strong>（数据库领域的大佬，Codd 还是关系型数据库的创始人，提出了关系模型和前三范式）</p>
<p>3NF 的严格版本，每个决定因素（Determinant）均为候选键。</p>
<ul>
<li><strong>BCNF</strong>：<strong>所有 “决定因素” 都必须是候选键</strong>，包括主键和其他候选键，杜绝任何非候选键决定其他字段的情况（哪怕是主属性之间的依赖）。</li>
</ul>
<p>若存在函数依赖$X \rightarrow Y$，则X必须是候选键（包括单键或复合键）。</p>
<p>解决 3NF 中可能存在的主属性之间的依赖问题（如候选键之间的依赖）。</p>
<ul>
<li>若表中存在依赖$AB \rightarrow C$和$A \rightarrow C$，且A不是候选键，则违反 BCNF，需进一步分解。</li>
</ul>
<p>4.Functional Dependency（函数依赖）</p>
<p>若表中属性X的值唯一决定属性Y的值$X \rightarrow Y$，则称Y函数依赖于X。<br>
<strong>类型</strong>：</p>
<ul>
<li>
<p><strong>完全函数依赖</strong>：Y依赖于整个主键（如复合主键((StudentID, CourseID) \rightarrow Grade)）。</p>
</li>
<li>
<p><strong>部分函数依赖</strong>：Y仅依赖于主键的一部分（如(StudentID \rightarrow StudentName)，在复合主键中违反 2NF）。</p>
</li>
<li>
<p><strong>传递依赖</strong>：Y依赖于非主属性（如(ID \rightarrow Bank Code No \rightarrow Bank)，违反 3NF）。</p>
</li>
</ul>
<p>函数依赖判断范式的核心依据，通过分析函数依赖消除冗余和异常，指导表的分解。</p>
<h2 id="4-30">4.30</h2>
<p>1.Storage Management（存储管理）</p>
<p>（1）数据库存储基本结构</p>
<ul>
<li><strong>存储形式</strong>：数据库以文件形式存放在磁盘上，文件由多个 <strong>“页面（Page）”</strong> 组成（页面是磁盘与内存交互的最小单位，类似 “数据块”）。</li>
<li><strong>页面作用</strong>：每个页面存储多个 <strong>元组（Tuple，即表中的一行数据）</strong>，页面大小固定（如 256 字节、4KB 等）。</li>
<li><strong>存储介质分类</strong></li>
</ul>
<blockquote>
<ul>
<li><strong>易失性存储</strong>（Volatile）：断电数据丢失，如 DRAM（内存）。</li>
<li><strong>非易失性存储</strong>（Non-Volatile）：持久化存储，如硬盘（HDD）、固态硬盘（SSD）。</li>
</ul>
<p>SSD：固态硬盘（Solid State Drive）。HDD：混合硬盘（hybrid harddrive，HHD）</p>
</blockquote>
<ul>
<li>存储层次结构</li>
</ul>
<blockquote>
<p>访问速度（从快到慢）：寄存器 → L1/L2缓存 → 主内存 → SSD → HDD → 磁带/光盘</p>
<p>容量（从小到大）：寄存器（几字节）→ 缓存（KB/MB）→ 主存（GB）→ SSD/HDD（TB）→ 磁带（PB级）</p>
<p>特性：越上层越贵、速度越快、容量越小；越下层越便宜、速度越慢、容量越大。</p>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250506083447472.png" alt="image-20250506083447472"></p>
</blockquote>
<ul>
<li><strong>HDD vs SSD 物理结构对比</strong></li>
</ul>
<blockquote>
<ul>
<li><strong>HDD</strong>：依赖磁头臂移动和盘片旋转，顺序访问快（因磁头无需频繁寻道），随机访问慢（寻道时间长），抗震性低（工作时抗 55g 冲击）。</li>
<li><strong>SSD</strong>：基于 NAND 闪存，无机械部件，顺序 / 随机访问均快于 HDD，抗震性强（抗 1500g 冲击），但写入寿命有限（约 10³-10⁵次擦写循环）。</li>
</ul>
</blockquote>
<p>（2）页面管理（Page Management）</p>
<ul>
<li><strong>页头元数据</strong>：每个页面包含 <strong>页 ID（唯一标识）、校验和（数据完整性检查）、版本号（并发控制）、元组状态数组（标记元组是否有效 / 删除）</strong>。</li>
<li><strong>槽式页面（Slotted Page）核心机制</strong></li>
</ul>
<blockquote>
<ul>
<li><strong>槽数组（Slot Array）</strong>：存储每个元组的起始偏移量和状态（如是否被删除），页头记录 “已用槽数”，方便快速定位元组。</li>
<li><strong>可变长度元组处理</strong>：元组数据区需额外存储长度信息，删除元组后通过槽数组标记空闲槽位，新元组可复用任意空闲槽（非仅页尾，区别于固定长度元组）。</li>
</ul>
</blockquote>
<ul>
<li><strong>NULL 值与大值存储</strong></li>
</ul>
<blockquote>
<ol>
<li><strong>NULL 表示</strong></li>
</ol>
<ul>
<li>
<p><strong>全局位图</strong>：页头设一个位图，每一位对应一个元组的某个属性是否为 NULL（节省空间，适合多 NULL 场景）。</p>
</li>
<li>
<p><strong>占位符值</strong>：为数据类型定义特殊值（如 INT 类型用 - 9999 表示 NULL，需业务层约定）。</p>
</li>
<li>
<p><strong>属性级标记</strong>：每个属性前加 1 位标记位（1 = 有效，0=NULL），简单但增加存储开销。</p>
</li>
</ul>
<ol start="2">
<li><strong>大值处理</strong></li>
</ol>
<p>当元组超过页面大小时，DBMS 会：</p>
<ol>
<li>使用<strong>溢出页</strong>（Overflow Page）：主页面存指针，大值数据分块存在后续页面（如 PostgreSQL 的 TOAST 技术）。</li>
<li>存储到<strong>外部文件</strong>：元组仅存文件路径，适用于超大对象（如二进制文件、长文本）。</li>
</ol>
</blockquote>
<ul>
<li>存储方式
<ul>
<li><strong>页头（Page Header）</strong>：记录页面基本信息（如已存元组数量、空闲空间等）。</li>
<li><strong>槽位（Slot Entries）</strong>：每个槽位记录一个元组在页面中的位置，类似 “索引标签”，方便快速定位元组。</li>
<li><strong>元组存储</strong>：元组按顺序写入页面，固定长度元组直接连续存放，可变长度元组需额外记录长度信息。</li>
</ul>
</li>
<li>操作逻辑
<ul>
<li><strong>追加元组</strong>：直接在页面末尾写入新元组，页头更新元组数量（类似在笔记本新页末尾写新内容）。</li>
<li><strong>删除元组</strong>：标记元组空间为 “可用”，后续新元组可复用该空间（类似划掉笔记本上的旧内容，留空位置写新内容）。</li>
</ul>
</li>
</ul>
<p>（3）页面存储效率</p>
<ul>
<li>顺序访问 vs 随机访问</li>
</ul>
<p>磁盘读取连续存储的页面（顺序访问）比分散存储的页面（随机访问）快得多（类比：在书架上按顺序找书 vs 随机翻找，前者更快）。</p>
<ul>
<li>固定长度元组优势</li>
</ul>
<p>固定长度元组（如每个元组占 20 字节）的存储和查询效率更高，无需额外处理长度信息（可变长度元组需多一步 “量尺寸”）。</p>
<ul>
<li>对齐访问</li>
</ul>
<blockquote>
<ul>
<li>数据在内存中需按数据类型长度对齐（如 8 字节数据存放在 8 的倍数地址），否则：ARM/RISC-V 等 CPU 会触发<strong>访问异常</strong>，X86 虽允许但会导致<strong>性能骤降</strong>（需多次非对齐读写）。</li>
<li>解决方案</li>
</ul>
<p><strong>填充（Padding）</strong>：在属性间补空字节，强制对齐（如 16 位属性后补 2 字节，占 4 字节）。</p>
<p><strong>字段重排序</strong>：按数据类型长度降序排列属性（长字段优先，减少填充开销）。</p>
</blockquote>
<ul>
<li><strong>记录标识符（RID，Row Identifier）</strong></li>
</ul>
<blockquote>
<p>DBMS 自动为每个元组生成唯一物理地址，用于快速定位：</p>
<ul>
<li>PostgreSQL：<code>CTID</code>（6 字节，包含页号和槽位号）。</li>
<li>SQLite：<code>ROWID</code>（8 字节，隐含表的物理存储位置）。</li>
<li>应用场景：索引底层常依赖 RID 加速数据定位，但业务层不应依赖（物理位置可能变更）。</li>
</ul>
</blockquote>
<p>（4）典型计算示例（页面容量计算）</p>
<p><strong>问题1</strong>：若页面大小 256 字节，页头占 16 字节，每个槽位占 4 字节，每个元组占 20 字节（固定长度），最多存多少元组？</p>
<p>$页头＋(槽位+元组数据)\times元组数≤页面大小$</p>
<p>$16+(4+20)\times n\le240$，$n\le 10$</p>
<p><strong>问题2</strong>：<br>
页面大小 512 字节，页头 20 字节，槽位 4 字节 / 个，记录大小依次为 [32, 28, 50, 40, 64, 45, 36]（可变长度），按顺序插入，能存多少条？</p>
<p>计算公式还是$页头＋(槽位+元组数据)\times元组数≤页面大小$</p>
<p>可以发现$20+7\times (4+\sum元组_i)=343&lt;512$，因此能插入7条。</p>
<blockquote>
<p><mark>通俗解释：数据库如何在磁盘上 “存东西”？</mark></p>
<p>想象数据库是一个大账本，磁盘是账本的存储抽屉，每个 <strong>“页面” 就是账本中的一页纸</strong>：</p>
<ol>
<li><strong>每页纸的结构</strong>：
<ul>
<li>页头（纸的顶部）：写着 “这页有 3 条记录”“还剩 50 字节空白”。</li>
<li>槽位（纸的边缘标签）：每个标签标着 “第 1 条记录从第 10 字节开始，占 20 字节”，方便快速翻到对应位置。</li>
<li>记录（纸上的内容）：每条记录像一行字，固定长度的记录（如 “姓名 20 字”）整齐排列，可变长度的记录（如 “备注”）需要额外标记长度。</li>
</ul>
</li>
<li><strong>往纸上写东西</strong>：
<ul>
<li><strong>新增记录</strong>：直接在纸的末尾空白处写新内容，页头更新 “现在有 4 条记录”（类似在笔记本新行写东西）。</li>
<li><strong>删除记录</strong>：划掉某条记录，标记这块空白可以写新内容，下次写记录时优先用空出来的位置（避免浪费纸张）。</li>
</ul>
</li>
<li><strong>找东西的效率</strong>：
<ul>
<li>如果记录按顺序写在纸上（如按学号排序），找起来很快（顺序访问）；如果记录乱序分布（随机访问），就像在笔记本里乱翻，效率低。</li>
</ul>
</li>
</ol>
<p>通过这种 “分页管理”，数据库能高效地在磁盘上存储、查询和修改数据，就像用账本有条理地记录信息一样。</p>
</blockquote>
<h2 id="5-6">5.6</h2>
<p>1.Clock</p>
<blockquote>
<ul>
<li><strong>数据结构</strong>：环形队列 + 参考位（1 位），指针循环扫描，替代 LRU 的时间戳数组，空间效率更高。</li>
<li>优化点
<ul>
<li>避免 LRU 的 “Belady 异常”（容量增大时命中率可能下降）。</li>
<li>比 LRU 更易实现，适合内存受限场景（如嵌入式 DBMS）。</li>
</ul>
</li>
<li><strong>变种：Second Chance 算法</strong>（等价于 Clock）：<br>
若首次扫描到参考位 1，清 0 并跳过；再次扫描到 0 时才淘汰（给近期访问过的页面第二次机会）。</li>
</ul>
</blockquote>
<p>2.Localization（缓冲区本地化）</p>
<blockquote>
<ul>
<li>
<p><strong>核心目标</strong>：防止单个查询（如全表扫描）占用过多缓冲池资源，导致高频访问页面被挤出。</p>
</li>
<li>
<p>实现方式</p>
<p>（以 PostgreSQL 为例）：</p>
<ol>
<li><strong>查询级缓冲区配额</strong>：为每个查询分配固定数量的缓冲区帧（如最多使用 100 个页面），超出后强制释放。</li>
<li><strong>局部性感知替换</strong>：将缓冲区分为 “全局区” 和 “局部区”，前者存储高频共享页面，后者临时存储查询专属页面（查询结束后释放）。</li>
</ol>
</li>
<li>
<p>优势</p>
<ul>
<li><strong>隔离资源竞争</strong>：OLTP 查询（高频小数据量）和 OLAP 查询（低频大数据量）的缓冲区使用互不干扰。</li>
<li><strong>减少 “缓冲区污染”</strong>：全表扫描加载的页面仅在查询生命周期内存在，不会长期占用缓冲区。</li>
</ul>
</li>
<li>
<p><strong>与 LRU-K 对比</strong>：<br>
LRU-K 通过历史访问次数优化淘汰策略，Localization 则从<strong>查询粒度</strong>控制资源分配，二者可结合使用（如 PostgreSQL 同时支持本地化和 LRU 变种）。</p>
</li>
</ul>
</blockquote>
<p>3.缓冲区管理其他技术</p>
<ul>
<li>
<p>Buffer Pool 组成</p>
<ul>
<li><strong>帧（Frame）</strong>：缓冲区中的固定大小存储单元，与磁盘页面一一对应。</li>
<li>页表（Page Table）：记录页面是否在内存中，包含元数据：
<ul>
<li><strong>脏标志（Dirty Flag）</strong>：页面被修改后标记为脏，淘汰前需写回磁盘（写回策略比写通更高效，减少 I/O 次数）。</li>
<li><strong>固定标志（Pin Flag）</strong>：防止重要页面被淘汰（如索引根节点），查询使用时 “固定” 页面，用完释放。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>替换策略对比</strong>：</p>
</li>
</ul>
<p>| 策略           | 核心逻辑                                      | 优势               | 缺点                 |<br>
| -------------- | --------------------------------------------- | ------------------ | -------------------- |<br>
| LRU            | 淘汰最久未访问的页面                          | 简单有效           | 对顺序扫描不友好     |<br>
| Clock          | 基于 1 位参考位，扫描淘汰未访问页面           | 省内存，易实现     | 精度低于 LRU         |<br>
| LRU-K          | 记录最近 K 次访问时间，淘汰最不可能访问的页面 | 平衡频率和近期访问 | 复杂度高，内存开销大 |<br>
| MySQL 两段 LRU | 新页面先放 “旧列表”，再次访问移到 “新列表”    | 避免短期热点       |                      |</p>
<ul>
<li><strong>预取优化（Prefetching）</strong><br>
DBMS 根据查询模式提前加载相邻页面（如顺序扫描时预取下一页面），减少 I/O 等待时间，尤其适合 HDD（机械寻道时间长）。</li>
</ul>
<blockquote>
<p>在数据库和多线程编程中，锁是一种非常重要的机制，用于解决多个进程或线程同时访问共享资源时可能出现的数据不一致问题。下面通俗地介绍一下锁和乐观锁，并举例分析：</p>
<ul>
<li><strong>锁的基本概念</strong>：可以把锁想象成一个房间的钥匙。当一个人（线程或进程）想要进入房间（访问共享资源）并对房间里的东西进行操作时，他需要先拿到钥匙。在他拿着钥匙的期间，其他人就不能进入房间，直到他把钥匙还回来。在编程里，锁就是用来控制对共享资源的访问顺序的工具，确保同一时间只有一个 “人” 能访问和修改共享资源，避免出现混乱。</li>
<li><strong>悲观锁</strong>：悲观锁就像一个非常谨慎的人，他总是觉得别人会跟他抢东西。在数据库操作中，悲观锁认为在自己使用共享资源的过程中，一定会有其他线程来修改这个资源。所以，在访问资源前，它就会先把资源锁起来，其他线程只能等待锁被释放后才能访问。例如，在银行转账的场景中，A 向 B 转账 100 元。假设数据库里记录 A 账户余额的这一行数据就是共享资源，使用悲观锁时，当系统要读取 A 的余额时，就会马上给这行数据加上锁。在整个转账操作（读取 A 余额、扣除 100 元、写入新余额、读取 B 余额、增加 100 元、写入 B 新余额）过程中，其他任何操作都不能修改 A 和 B 的余额数据。只有等转账操作全部完成，锁被释放后，其他线程才能对这些数据进行操作。这样能保证数据的一致性，但缺点是如果有很多线程都要访问这些数据，就会有很多线程处于等待状态，性能会受到影响。</li>
<li><strong>乐观锁</strong>：乐观锁则像一个乐观的人，他总是觉得在自己操作共享资源的时候，别人不会来抢。乐观锁假设在大多数情况下，多个线程不会同时修改共享资源。所以，它不会在一开始就锁定资源，而是在更新资源的时候，检查一下在自己读取资源之后，有没有其他线程修改过这个资源。如果没有，就正常更新；如果有，就采取一些措施，比如重新读取数据再尝试更新。还是以银行转账为例，系统读取 A 的余额时，不会加锁。在准备更新 A 的余额时，会检查 A 的余额在读取之后有没有被其他线程修改过。可以通过版本号或者时间戳来实现这种检查。假设给 A 的账户数据添加一个版本号，初始值是 1。系统读取 A 余额时，同时获取版本号 1。当要更新 A 余额时，再次检查版本号，如果还是 1，就说明没有其他线程修改过，允许更新，并把版本号更新为 2；如果版本号已经不是 1 了，就说明有其他线程修改过，那么就重新读取 A 的余额和最新版本号，然后再尝试更新。乐观锁的优点是并发性能好，因为大多数时候不需要等待锁的释放。但缺点是如果并发冲突比较频繁，就会导致很多次更新失败，需要不断重试，也会影响性能。</li>
</ul>
</blockquote>
<p><strong>问题 1</strong>（LRU缓存算法）</p>
<p>你有一个容量为 3 的 LRU 缓存，在执行以下操作后，缓存的内容是什么？<br>
操作步骤：</p>
<ol>
<li>Get(1)</li>
<li>Put(1)</li>
<li>Get(2)</li>
<li>Get(3)</li>
<li>Put(3)</li>
<li>Get(2)</li>
<li>Get(4)</li>
<li>Get(1)</li>
<li>Put(1)</li>
<li>Get(5)</li>
</ol>
<blockquote>
<p>问题1使用的是 LRU 算法，LRU 关注的是数据的访问顺序，通过将最近访问的数据置于缓存靠前位置（最近使用），最久未访问的数据置于靠后位置（最久未使用），来决定淘汰策略。当有新数据插入时，若缓存未满，新数据直接插入到最近使用的位置（也就是最前面）；若缓存已满，则淘汰最久未使用的数据（最末尾的数据） ，然后将新数据插入到最前面。</p>
<p>在 “put (1)” 和 “put (2)” 时，因为缓存未满，所以直接按访问顺序从前到后插入，先插入 1，再插入 2，就变成了 [2, 1]，这里的顺序是基于最近使用的顺序，2 是最新使用的，所以在前面，1 在后面。</p>
<p>1.知识点讲解</p>
<p>LRU（Least Recently Used）缓存算法即最近最少使用算法。它的核心思想是，当缓存容量已满，需要淘汰数据时，优先淘汰最久未被使用的数据。就好比你有一个小书架，空间有限，新的书放不下了，你就会把很久都没看过的书拿走，给新书腾地方。在缓存中，每次访问一个数据，就相当于这本书被你拿出来看了，它就变成了最近使用的，会被放到更靠前的位置；而一直没被访问的数据，就会逐渐往后移，当缓存满了，最靠后的那个（最久未使用的）就会被淘汰。</p>
<p>2.答案解析</p>
<ol>
<li><strong>Get(1)</strong>：缓存中原本没有 1，所以未命中，从别处获取 1 放入缓存。此时缓存只有 1，所以缓存内容是 [1] ，也没有数据被淘汰。</li>
<li><strong>Put(1)</strong>：1 已经在缓存中，对其进行更新，更新后它依然是最近使用的，所以缓存内容还是 [1] ，没有淘汰数据。</li>
<li><strong>Get(2)</strong>：缓存中没有 2，未命中，获取 2 放入缓存。因为 2 是新访问的，所以放在最前面，此时缓存内容变为 [2, 1] ，无淘汰数据。</li>
<li><strong>Get(3)</strong>：缓存中没有 3，未命中，获取 3 放入缓存。3 成为最近使用的，所以缓存内容变为 [3, 2, 1] ，没有数据被淘汰。</li>
<li><strong>Put(3)</strong>：3 已经在缓存中，更新 3 后它还是最近使用的，缓存内容保持 [3, 2, 1] ，无淘汰。</li>
<li><strong>Get(2)</strong>：缓存中有 2，命中。将 2 提升到最近使用的位置，也就是放到最前面，缓存内容变为 [2, 3, 1] ，没有数据被淘汰。</li>
<li><strong>Get(4)</strong>：缓存中没有 4，未命中。但缓存容量已满，需要淘汰数据。根据 LRU 算法，最久未使用的是 1，所以淘汰 1，放入 4，缓存内容变为 [4, 2, 3] 。</li>
<li><strong>Get(1)</strong>：缓存中没有 1，未命中。缓存容量还是满的，此时最久未使用的是 3，淘汰 3，放入 1，缓存内容变为 [1, 4, 2] 。</li>
<li><strong>Put(1)</strong>：1 已经在缓存中，更新后缓存内容不变，还是 [1, 4, 2] ，无淘汰。</li>
<li><strong>Get(5)</strong>：缓存中没有 5，未命中。缓存满了，淘汰最久未使用的 2，放入 5，缓存内容变为 [5, 1, 4] 。</li>
</ol>
</blockquote>
<p>| 步骤 | 操作   | 动作                        | 缓存内容（最近使用 -&gt; 最久未使用） | 是否淘汰   |<br>
| ---- | ------ | --------------------------- | ---------------------------------- | ---------- |<br>
| 1    | Get(1) | 未命中：获取 1              | [1]                                | 否         |<br>
| 2    | Put(1) | 更新 1（已在最近使用位置）  | [1]                                | 否         |<br>
| 3    | Get(2) | 未命中：获取 2              | [2, 1]                             | 否         |<br>
| 4    | Get(3) | 未命中：获取 3              | [3, 2, 1]                          | 否         |<br>
| 5    | Put(3) | 更新 3（已在最近使用位置）  | [3, 2, 1]                          | 否         |<br>
| 6    | Get(2) | 命中：提升 2 到最近使用位置 | [2, 3, 1]                          | 否         |<br>
| 7    | Get(4) | 未命中：获取 4（淘汰 1）    | [4, 2, 3]                          | 是，淘汰 1 |<br>
| 8    | Get(1) | 未命中：获取 1（淘汰 3）    | [1, 4, 2]                          | 是，淘汰 3 |<br>
| 9    | Put(1) | 更新 1（已在最近使用位置）  | [1, 4, 2]                          | 否         |<br>
| 10   | Get(5) | 未命中：获取 5（淘汰 2）    | [5, 1, 4]                          | 是，淘汰 2 |</p>
<p><strong>问题 2</strong>（时钟手LRU算法）</p>
<p>给定一个容量为 4 页的缓存，使用时钟手 LRU 算法进行管理。在执行以下操作后，缓存中存在哪些页面，它们的参考位是什么？<br>
操作步骤：</p>
<ol>
<li>Get(1)</li>
<li>Get(2)</li>
<li>Get(3)</li>
<li>Get(4)</li>
<li>Get(2)</li>
<li>Get(5)</li>
</ol>
<blockquote>
<p>问题2采用时钟手 LRU 算法，它以环形缓冲区的形式组织页面，主要依据页面的参考位（是否被访问过）来决定淘汰页面，对页面插入顺序没有像 LRU 那样严格按照最近使用顺序来排列。</p>
<p>“Get (2)” 时插入 2 并设参考位为 1，缓存变为 [1:1, 2:1]，这只是简单地将新插入的页面添加到缓存中，没有涉及到按照特定顺序调整位置的操作 。这里只是单纯记录每个页面的参考位状态，在需要淘汰页面时，才会根据时钟手扫描参考位的结果进行操作，与第一题 LRU 算法中时刻维护严格的最近使用顺序不同。</p>
<p>1.知识点讲解</p>
<p>时钟手 LRU 算法是 LRU 算法的一种优化。每个页面都有一个参考位（1 位二进制数），当页面被访问时，参考位设为 1。页面组织在一个环形缓冲区中，有一个类似时钟指针的东西按顺序扫描这些页面。当需要淘汰页面时，指针开始扫描，遇到参考位为 1 的页面，就把参考位清 0 并跳过它；遇到参考位为 0 的页面，就淘汰该页面。这就像你在一个环形书架上整理书，每个书有个小标签（参考位），你拿着一个标记棒（时钟指针）依次检查。如果书的标签是 1（被看过），你就把标签擦掉（清 0）继续检查下一本；如果标签是 0（没被看过），就把这本书拿走（淘汰）。</p>
<p>2.答案解析</p>
<ol>
<li><strong>Get(1)</strong>：插入 1 并把 1 的参考位设为 1，此时缓存只有 1:1，没有淘汰数据。</li>
<li>c</li>
<li><strong>Get(3)</strong>：插入 3 并把 3 的参考位设为 1，缓存变为 [1:1, 2:1, 3:1] ，无淘汰。</li>
<li><strong>Get(4)</strong>：插入 4 并把 4 的参考位设为 1，缓存变为 [1:1, 2:1, 3:1, 4:1] ，无淘汰。</li>
<li><strong>Get(2)</strong>：命中 2，将 2 的参考位设为 1 ，缓存内容不变还是 [1:1, 2:1, 3:1, 4:1] ，无淘汰。</li>
<li><strong>Get(5)</strong>：缓存已满，需要淘汰。时钟手从 1 开始扫描，1 的参考位是 1，清 0 后移动到 2；2 的参考位是 1，清 0 后移动到 3；3 的参考位是 1，清 0 后移动到 4；4 的参考位是 1，清 0 后又回到 1，此时 1 的参考位已经是 0 了，所以淘汰 1，插入 5 并把 5 的参考位设为 1，最终缓存内容是 [5:1, 2:0, 3:0, 4:0] 。</li>
</ol>
<blockquote>
<p>[!success]</p>
<p>在时钟手 LRU 算法的情境下，写成 [2:1, 1:1] 从存储内容的角度来说是可以的。因为该算法重点在于通过参考位和时钟手扫描来管理页面淘汰，对页面在缓存中的排列顺序并没有严格的 “最近使用 - 最久未使用” 顺序要求 。</p>
<p>在这个算法里，只要缓存未满，新插入页面时，你可以将其放在缓存中的任意位置（当然，一般习惯上会添加在末尾等位置，但这并非强制规定）。当缓存满了需要淘汰页面时，时钟手会按顺序扫描页面的参考位来决定淘汰对象，而不是依据页面在缓存中的排列顺序。所以 [2:1, 1:1] 和 [1:1, 2:1] 在表示缓存内容上是等效的，都记录了页面 1 和页面 2 已被访问（参考位为 1）且在缓存中的状态 。</p>
<p>不过，在实际应用和分析问题时，为了保持一致性和便于理解，通常会按照一定的顺序（如插入顺序或某种约定顺序）来书写缓存内容，但这并不影响算法的本质逻辑。</p>
</blockquote>
</blockquote>
<p>| 步骤 | 操作   | 动作                        | 缓存内容（带参考位）                                         | 是否淘汰   | 时钟手移动                                |<br>
| ---- | ------ | --------------------------- | ------------------------------------------------------------ | ---------- | ----------------------------------------- |<br>
| 1    | Get(1) | 插入 1（参考位设为 1）      | [1:1]                                                        | 否         | -                                         |<br>
| 2    | Get(2) | 插入 2（参考位设为 1）      | [1:1, 2:1]                                                   | 否         | -                                         |<br>
| 3    | Get(3) | 插入 3（参考位设为 1）      | [1:1, 2:1, 3:1]                                              | 否         | -                                         |<br>
| 4    | Get(4) | 插入 4（参考位设为 1）      | [1:1, 2:1, 3:1, 4:1]                                         | 否         | -                                         |<br>
| 5    | Get(2) | 命中（将 2 的参考位设为 1） | [1:1, 2:1, 3:1, 4:1]                                         | 否         | -                                         |<br>
| 6    | Get(5) | 未命中：需要淘汰            | 缓存已满，时钟手从 1 开始。1:R=1→清 0，移动；2:R=1→清 0，移动；3:R=1→清 0，移动；4:R=1→清 0，移动；1:R=0（之前已清 0）→淘汰页面 1，插入 5 并设参考位为 1。最终缓存内容：[5:1, 2:0, 3:0, 4:0] | 是，淘汰 1 | 依次检查 1、2、3、4 页后，淘汰 1 并插入 5 |</p>
<p><strong>问题 3</strong>（分代缓存策略）</p>
<p>假设年轻代容量为 2，老年代容量为 2，在执行以下操作后，年轻代和老年代列表中的页面情况如何？<br>
操作步骤：</p>
<ol>
<li>get(1)</li>
<li>get(2)</li>
<li>get(1)</li>
<li>get(3)</li>
<li>get(2)</li>
<li>get(4)</li>
<li>get(5)</li>
</ol>
<blockquote>
<p>问题3的分代缓存策略将缓存分为年轻代和老年代。新数据默认插入老年代，老年代中的数据若再次被访问则移动到年轻代。</p>
<p>在 “put (1)” 和 “put (2)” 时，数据都是新插入老年代，老年代的数据顺序按照插入顺序排列（从新到旧），所以是 [2, 1] ，2 是后插入的，相对较新，在前面，1 在后面。这种顺序和 LRU 算法中按照最近使用排序的逻辑不同，它是基于插入和访问的特定规则来管理数据在不同代缓存中的位置。</p>
<p>1.知识点讲解</p>
<p>这种缓存策略把缓存分为年轻代和老年代。新数据一开始会插入到老年代，如果老年代满了，就淘汰最老的数据。当老年代中的数据再次被访问时，会被移动到年轻代。年轻代也有容量限制，满了之后也会淘汰数据。这就像把书架分成了新书架（年轻代）和旧书架（老年代），新书先放旧书架，如果旧书架满了就扔掉最旧的书。旧书架上的书被重新翻看了，就把它放到新书架上，新书架满了也会扔掉最旧的书。</p>
<p>2.答案解析</p>
<ol>
<li><strong>put(1)</strong>：1 插入到老年代，年轻代没有数据，老年代是 [1] 。</li>
<li><strong>put(2)</strong>：2 插入到老年代，老年代变为 [2, 1] ，年轻代还是空的。</li>
<li><strong>get(1)</strong>：1 在老年代，被访问后移动到年轻代，年轻代变为 [1] ，老年代变为 [2] 。</li>
<li><strong>put(3)</strong>：3 插入到老年代，年轻代是 [1] ，老年代变为 [3, 2] 。</li>
<li><strong>get(2)</strong>：2 在老年代，被访问后移动到年轻代，由于年轻代容量为 2，放入 2 后满了，年轻代变为 [2, 1] ，老年代变为 [3] 。</li>
<li><strong>put(4)</strong>：4 插入到老年代，老年代满了，淘汰最老的 2，老年代变为 [4, 3] ，年轻代还是 [2, 1] 。</li>
<li><strong>put(5)</strong>：5 插入到老年代，老年代又满了，淘汰最老的 3，老年代变为 [5, 4] ，年轻代保持 [2, 1] 。</li>
</ol>
</blockquote>
<p>| 步骤 | 操作   | 年轻代列表（最近 -&gt; 最久） | 老年代列表（最近 -&gt; 最久） | 备注                                   |<br>
| ---- | ------ | -------------------------- | -------------------------- | -------------------------------------- |<br>
| 1    | put(1) | []                         | [1]                        | 1 插入到老年代                         |<br>
| 2    | put(2) | []                         | [2, 1]                     | 2 插入到老年代                         |<br>
| 3    | get(1) | [1]                        | [2]                        | 1 移动到年轻代                         |<br>
| 4    | put(3) | [1]                        | [3, 2]                     | 3 插入到老年代                         |<br>
| 5    | get(2) | [2, 1]                     | [3]                        | 2 移动到年轻代（年轻代已满）           |<br>
| 6    | put(4) | [2, 1]                     | [4, 3]                     | 4 插入到老年代（老年代淘汰最老的页面） |<br>
| 7    | put(5) | [2, 1]                     | [5, 4]                     | 5 插入到老年代（老年代淘汰 3）         |</p>
<p>4.B、B+树</p>
<p>（1）B树基础概念</p>
<p>B树是一种自平衡树数据结构，支持高效的搜索、插入、删除操作，时间复杂度均为 <strong>O(log n)</strong>。每个节点可包含多个键（分隔符）和子节点，适用于磁盘等外存设备，减少 I/O 次数。</p>
<p>核心特性</p>
<ul>
<li><strong>平衡特性</strong>：所有叶子节点位于同一深度，保证树高均衡。</li>
<li><strong>节点填充</strong>：除根节点外，每个节点至少包含 <strong>⌈m/2⌉-1</strong> 个键（m 为节点最大键数），至多 <strong>m-1</strong> 个键，确保节点 “半满”，减少分裂 / 合并频率。</li>
<li><strong>键有序性</strong>：节点内键按升序排列，子节点键范围由父节点键划分（左子树键 &lt; 分隔符键 ≤ 右子树键）。</li>
</ul>
<p>（2）节点结构与树型特征</p>
<p>①节点组成</p>
<ul>
<li><strong>根节点</strong>：至少 1 个键，至多 m-1 个键（m 为节点最大容量）。</li>
<li><strong>内部节点</strong>：含 k 个键，对应 k+1 个子节点，键作为子节点范围的分隔符。</li>
<li><strong>叶子节点</strong>：无子女，存储键及对应值（B 树）或仅键（B + 树，值存于叶子）。</li>
</ul>
<blockquote>
<p>根节点：[20]</p>
<p>内部节点：[10, 35]（左子树&lt;10，中间10≤键&lt;35，右子树≥35）</p>
<p>叶子节点：[6, 10, 20, 31, 38, 44]（键有序排列）</p>
</blockquote>
<p>②与二叉搜索树（BST）的区别</p>
<ul>
<li>BST 每个节点至多 2 个子节点，B 树可含多个子节点，树高更低（降低 I/O 次数）。</li>
<li>BST 无节点填充限制，B 树通过 “半满” 规则减少树高波动，保证性能稳定。</li>
</ul>
<p>（3）B + 树（数据库常用变种）</p>
<p>①与B树核心区别</p>
<ul>
<li><strong>值存储位置</strong>：B 树所有节点存储键和值；B + 树仅叶子节点存储值，内部节点仅存键（用于索引导航），空间利用率更高，查询更稳定（必达叶子节点）。</li>
<li><strong>叶子节点连接</strong>：B + 树叶子节点通过双向指针串联（如<code>Prev</code>/<code>Next</code>），支持高效范围查询（如<code>SELECT * FROM table WHERE id BETWEEN 10 AND 20</code>），无需回溯父节点。</li>
</ul>
<p>②插入与删除算法</p>
<blockquote>
<p>1.<strong>插入流程</strong>（以最大键数 3 为例，插入 6 到节点 [4,12]）</p>
<p>（1）找到目标叶子节点，插入键并排序（如 [4,12]→[4,6,12]，未溢出则完成）。</p>
<p>（2）若节点满（如插入 30 到 [15,25,35]，变为 4 个键）：</p>
<ul>
<li>分裂为左右节点（左 [15,25]，右 [30,35]）。</li>
<li>中间键（30）上移至父节点，若无父节点则创建新根（形成分层结构）。</li>
</ul>
<p>2.<strong>删除流程</strong>（如删除叶子节点 [5,10] 中的 5，导致下溢）</p>
<ul>
<li>
<p>若删除后节点键数 &lt;⌈m/2⌉-1，尝试向 “富裕” 的兄弟节点借键（如从右兄弟 [25,30,35] 借 25，合并到左节点 [10,25]）。</p>
</li>
<li>
<p>借键失败则合并节点（如左兄弟 [10] 与右兄弟 [25,30,35] 合并，父节点删除分隔符 20，可能触发递归调整）。</p>
</li>
</ul>
</blockquote>
<p><strong>例题 1：插入操作（最大节点容量m=3）</strong></p>
<p><strong>初始树结构</strong>：单个叶子节点 <code>[15, 25, 35]</code><br>
<strong>操作</strong>：插入 30</p>
<p><strong>知识点讲解：B + 树的插入与删除规则</strong></p>
<p>B + 树是数据库中常用的索引结构，具有以下核心特性：</p>
<ol>
<li><strong>节点容量</strong>：每个节点最多存储 m 个键（m 为节点最大容量），最少存储$\lceil m/2 \rceil$个键（根节点除外）。</li>
<li><strong>插入规则:</strong>
<ul>
<li>找到目标叶子节点，插入键值对。</li>
<li>若节点已满（键数超过m），则<strong>分裂</strong>为左右两个节点，中间键上移到父节点，递归处理父节点。</li>
</ul>
</li>
<li><strong>删除规则:</strong>
<ul>
<li>找到目标叶子节点，删除键值对。</li>
<li>若节点键数不足（少于$\lceil m/2 \rceil - 1$），尝试向<strong>兄弟节点借键</strong>或与兄弟节点<strong>合并</strong>，递归处理父节点。</li>
</ul>
</li>
</ol>
<p><strong>答案解析</strong></p>
<ol>
<li><strong>插入前检查</strong>：
<ul>
<li>叶子节点当前键：<code>[15, 25, 35]</code>，容量 (m=3)，已满。</li>
<li>插入 30 后，键变为 <code>[15, 25, 30, 35]</code>，超过容量，触发<strong>分裂</strong>。</li>
</ul>
</li>
<li><strong>分裂节点</strong>：
<ul>
<li>按中间位置（第2个键，索引从 1 开始）将键分为左右两部分：
<ul>
<li>左节点：前 2 个键 <code>[15, 25]</code>（保留较小的一半）</li>
<li>右节点：后 2 个键 <code>[30, 35]</code>（保留较大的一半）</li>
</ul>
</li>
<li>中间键 30 上移到父节点。由于原节点是根节点，分裂后<strong>创建新的根节点</strong> <code>[30]</code>，左右子节点分别为左、右分裂节点。</li>
</ul>
</li>
</ol>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250507144701216.png" alt="image-20250507144701216"></p>
<p><strong>例题 2：删除操作（最大节点容量 m=3）</strong></p>
<p><strong>初始树结构</strong>：</p>
<ul>
<li>根节点：<code>[20]</code></li>
<li>左子节点（叶子）：<code>[5, 10]</code></li>
<li>右子节点（叶子）：<code>[25, 30, 35]</code> <strong>操作</strong>：依次删除 5 和 10</li>
</ul>
<p><strong>答案解析</strong></p>
<ol>
<li><strong>删除 5</strong>：
<ul>
<li>左子节点 <code>[5, 10]</code> 删除 5 后，剩余键 <code>[10]</code>。</li>
<li>节点容量 (m=3)，最小允许键数为$\lceil 3/2 \rceil - 1 = 1$，当前键数为 1，<strong>未触发不足</strong>，暂不处理。</li>
</ul>
</li>
<li><strong>删除 10</strong>：
<ul>
<li>左子节点 <code>[10]</code> 删除 10 后，变为空节点，触发<strong>节点不足</strong>。</li>
<li>尝试向兄弟节点（右子节点 <code>[25, 30, 35]</code>）借键，或与兄弟节点合并。</li>
<li>由于左子节点已空，选择<strong>合并</strong>：将父节点的键 <code>20</code> 下移到左子节点，与右子节点合并为 <code>[20, 25, 30, 35]</code>，超过容量，需再次分裂？</li>
<li>从父节点下移键 <code>20</code>，与右子节点的键合并后重新分配，最终左子节点为 <code>[20, 25]</code>，右子节点为 <code>[30, 35]</code>，父节点更新为 <code>[25]</code></li>
</ul>
</li>
</ol>
<h2 id="5-7">5.7</h2>
<p>1.哈希表（Hash Table）</p>
<p>（1）定义与特性</p>
<ul>
<li><strong>本质</strong>：通过哈希函数将键映射到数组索引，实现快速键值对查找的数据结构。</li>
<li><strong>核心操作</strong>：插入、删除、查询的平均时间复杂度为O(1)，空间复杂度为O(n)。</li>
<li><strong>内部结构</strong>：基于无序数组，通过哈希函数$h(key)$计算索引，例如$h(key) = key\mod N$（N 为数组大小）。</li>
</ul>
<p>（2）与B+树对比</p>
<p>| <strong>特性</strong>       | <strong>哈希表</strong>                           | <strong>B + 树</strong>                     |<br>
| -------------- | ------------------------------------ | ------------------------------ |<br>
| <strong>有序性</strong>     | 无序（仅支持精确匹配）               | 有序（支持范围查询、排序）     |<br>
| <strong>查询复杂度</strong> | 平均 O(1)                            | O($\log n$)                    |<br>
| <strong>适用场景</strong>   | 高频精确查询（如 Redis、PostgreSQL） | 范围查询、排序（如数据库索引） |</p>
<p>（3）<strong>核心问题与解决方案</strong></p>
<p><strong>核心挑战</strong></p>
<ul>
<li><strong>问题 1</strong>：键空间大，无法预先分配足够内存。</li>
<li><strong>问题 2</strong>：哈希冲突（不同键映射到相同索引）。</li>
</ul>
<p><strong>解决方案</strong></p>
<ul>
<li><strong>哈希函数</strong>：设计快速、低冲突的函数（如 xxhash），将任意长度的键映射为固定长度整数。</li>
<li><strong>冲突处理策略</strong>：线性探测（Linear Probing）、链地址法（Separate Chaining）、布谷鸟哈希（Cuckoo Hashing）等。</li>
</ul>
<p>（4）<strong>冲突处理策略</strong></p>
<p><strong>线性探测（Linear Probing）</strong></p>
<ul>
<li><strong>原理</strong>：冲突时顺序查找下一个空闲槽位插入，删除时标记 “墓碑”（Tombstone）以避免查找中断。</li>
</ul>
<p><strong>链地址法（Separate Chaining）</strong></p>
<ul>
<li><strong>原理</strong>：每个槽位维护一个链表，冲突时将键值对添加到链表末尾。</li>
</ul>
<p><strong>布谷鸟哈希（Cuckoo Hashing）</strong></p>
<ul>
<li><strong>原理</strong>：使用多个哈希函数（如$h_1, h_2$）生成多个候选槽位，插入时选择空闲槽位，若冲突则驱逐现有元素并重新哈希。</li>
</ul>
<p>（5）核心公式与算法</p>
<p><strong>哈希函数</strong>：$h(key) = key \mod N$（简单示例）。</p>
<p><strong>线性探测插入</strong>：若槽$h(key)$被占，依次探测$h(key)+1, h(key)+2, \dots$直至找到空闲槽。</p>
<p><strong>布谷鸟哈希插入</strong>：对键$k$，计算$h_1(k)$和$h_2(k)$，优先选择空闲槽，冲突时驱逐旧元素并递归重新哈希。</p>
<p>（6）例题</p>
<p><strong>问题 1：线性探测冲突处理策略例题</strong></p>
<p><strong>知识点讲解</strong><br>
线性探测是哈希表冲突处理的一种策略，当插入键值对时若发生哈希冲突（即目标槽位已被占用），则按顺序探测下一个空闲槽位（如槽位 + 1，+2，…，循环到开头）。删除时需标记 “墓碑”（Tombstone），避免查找时因空洞中断。</p>
<p><strong>核心规则</strong></p>
<ol>
<li>插入：计算哈希值，若槽位空闲则插入；否则顺序探测下一个槽位，直到找到空闲位置。</li>
<li>删除：标记槽位为 “已删除”（墓碑），插入时可重用该槽位，但查找时需跳过墓碑继续探测。</li>
</ol>
<p><strong>例题</strong></p>
<p><strong>哈希函数</strong>：(h(x) = x \mod 7) <strong>表大小</strong>：7 个槽位（索引 0-6） <strong>操作序列</strong>：</p>
<ol>
<li>插入：10, 20, 15, 7, 32, 24</li>
<li>删除：15, 7</li>
</ol>
<p><strong>答案解析</strong></p>
<ol>
<li><strong>插入 10</strong>：
<ul>
<li>(h(10) = 10 \mod 7 = 3)，槽 3 空闲，插入。</li>
<li>表状态：<code>[N, N, N, 10, N, N, N]</code></li>
</ul>
</li>
<li><strong>插入 20</strong>：
<ul>
<li>(h(20) = 20 \mod 7 = 6)，槽 6 空闲，插入。</li>
<li>表状态：<code>[N, N, N, 10, N, N, 20]</code></li>
</ul>
</li>
<li><strong>插入 15</strong>：
<ul>
<li>(h(15) = 15 \mod 7 = 1)，槽 1 空闲，插入。</li>
<li>表状态：<code>[N, 15, N, 10, N, N, 20]</code></li>
</ul>
</li>
<li><strong>插入 7</strong>：
<ul>
<li>(h(7) = 7 \mod 7 = 0)，槽 0 空闲，插入。</li>
<li>表状态：<code>[7, 15, N, 10, N, N, 20]</code></li>
</ul>
</li>
<li><strong>插入 32</strong>：
<ul>
<li>(h(32) = 32 \mod 7 = 4)，槽 4 空闲，插入。</li>
<li>表状态：<code>[7, 15, N, 10, 32, N, 20]</code></li>
</ul>
</li>
<li><strong>插入 24</strong>：
<ul>
<li>(h(24) = 24 \mod 7 = 3)，槽 3 被 10 占用，探测槽 4（被 32 占用），槽 5 空闲，插入槽 5。</li>
<li>表状态：<code>[7, 15, N, 10, 32, 24, 20]</code></li>
</ul>
</li>
<li><strong>删除 15</strong>：
<ul>
<li>槽 1 标记为 “Deleted”（墓碑），表状态：<code>[7, Deleted, N, 10, 32, 24, 20]</code></li>
</ul>
</li>
<li><strong>删除 7</strong>：
<ul>
<li>槽 0 标记为 “Deleted”（墓碑），表状态：<code>[Deleted, Deleted, N, 10, 32, 24, 20]</code></li>
</ul>
</li>
</ol>
<p>| 步骤 | 操作    | 哈希值计算        | 槽位状态（0-6）                         | 冲突处理 / 备注            |<br>
| ---- | ------- | ----------------- | --------------------------------------- | -------------------------- |<br>
| 1    | 插入 10 | (10 \mod 7 = 3) | <code>[N, N, N, 10, N, N, N]</code>                | 无冲突，直接插入           |<br>
| 2    | 插入 20 | (20 \mod 7 = 6) | <code>[N, N, N, 10, N, N, 20]</code>               | 无冲突，直接插入           |<br>
| 3    | 插入 15 | (15 \mod 7 = 1) | <code>[N, 15, N, 10, N, N, 20]</code>              | 无冲突，直接插入           |<br>
| 4    | 插入 7  | (7 \mod 7 = 0)  | <code>[7, 15, N, 10, N, N, 20]</code>              | 无冲突，直接插入           |<br>
| 5    | 插入 32 | (32 \mod 7 = 4) | <code>[7, 15, N, 10, 32, N, 20]</code>             | 无冲突，直接插入           |<br>
| 6    | 插入 24 | (24 \mod 7 = 3) | <code>[7, 15, N, 10, 32, 24, 20]</code>            | 槽 3 冲突，探测到槽 5 插入 |<br>
| 7    | 删除 15 | -                 | <code>[7, Deleted, N, 10, 32, 24, 20]</code>       | 标记墓碑，不影响探测       |<br>
| 8    | 删除 7  | -                 | <code>[Deleted, Deleted, N, 10, 32, 24, 20]</code> | 标记墓碑，槽位可被重用     |</p>
<p><strong>问题 2：链地址法（分离链接）冲突处理策略例题</strong></p>
<p><strong>知识点讲解</strong><br>
链地址法为每个哈希槽维护一个链表，冲突时将键值对添加到链表末尾。查询时遍历链表，删除时从链表中移除节点。</p>
<ul>
<li><strong>核心规则</strong>
<ol>
<li>插入：计算哈希值，将键值对添加到对应槽位的链表末尾。</li>
<li>查询 / 删除：计算哈希值，遍历链表查找目标键，删除时断开节点链接。</li>
</ol>
</li>
</ul>
<p><strong>例题</strong></p>
<p><strong>哈希函数</strong>：(h(x) = x \mod 7) <strong>表大小</strong>：7 个桶（0-6） <strong>操作序列</strong>：</p>
<ol>
<li>插入：15, 11, 27, 8, 3</li>
<li>查询：8（命中）</li>
<li>删除：11</li>
</ol>
<p><strong>答案解析</strong></p>
<ol>
<li><strong>插入 15</strong>：
<ul>
<li>(h(15) = 15 \mod 7 = 1)，桶 1 链表为空，插入链表：<code>[15]</code>。</li>
<li>表状态：桶 1: <code>15</code>，其余桶为空。</li>
</ul>
</li>
<li><strong>插入 11</strong>：
<ul>
<li>(h(11) = 11 \mod 7 = 4)，桶 4 链表为空，插入链表：<code>[11]</code>。</li>
<li>表状态：桶 1: <code>15</code>，桶 4: <code>11</code>，其余桶为空。</li>
</ul>
</li>
<li><strong>插入 27</strong>：
<ul>
<li>(h(27) = 27 \mod 7 = 6)，桶 6 链表为空，插入链表：<code>[27]</code>。</li>
<li>表状态：桶 1: <code>15</code>，桶 4: <code>11</code>，桶 6: <code>27</code>，其余桶为空。</li>
</ul>
</li>
<li><strong>插入 8</strong>：
<ul>
<li>(h(8) = 8 \mod 7 = 1)，桶 1 已有 15，冲突，添加到链表末尾：<code>[15 → 8]</code>。</li>
<li>表状态：桶 1: <code>15 → 8</code>，桶 4: <code>11</code>，桶 6: <code>27</code>，其余桶为空。</li>
</ul>
</li>
<li><strong>插入 3</strong>：
<ul>
<li>(h(3) = 3 \mod 7 = 3)，桶 3 链表为空，插入链表：<code>[3]</code>。</li>
<li>表状态：桶 1: <code>15 → 8</code>，桶 3: <code>3</code>，桶 4: <code>11</code>，桶 6: <code>27</code>，其余桶为空。</li>
</ul>
</li>
<li><strong>查询 8</strong>：
<ul>
<li>(h(8) = 1)，遍历桶 1 链表，找到 8，命中。</li>
</ul>
</li>
<li><strong>删除 11</strong>：
<ul>
<li>(h(11) = 4)，遍历桶 4 链表，移除 11，链表为空。</li>
<li>表状态：桶 1: <code>15 → 8</code>，桶 3: <code>3</code>，桶 6: <code>27</code>，其余桶为空。</li>
</ul>
</li>
</ol>
<p><strong>步骤表格</strong></p>
<p>| 步骤 | 操作    | 哈希值计算        | 桶状态（链表）               | 冲突处理 / 备注            |<br>
| ---- | ------- | ----------------- | ---------------------------- | -------------------------- |<br>
| 1    | 插入 15 | (15 \mod 7 = 1) | 桶 1: <code>[15]</code>                 | 无冲突，新建链表           |<br>
| 2    | 插入 11 | (11 \mod 7 = 4) | 桶 4: <code>[11]</code>                 | 无冲突，新建链表           |<br>
| 3    | 插入 27 | (27 \mod 7 = 6) | 桶 6: <code>[27]</code>                 | 无冲突，新建链表           |<br>
| 4    | 插入 8  | (8 \mod 7 = 1)  | 桶 1: <code>[15 → 8]</code>             | 冲突，添加到链表末尾       |<br>
| 5    | 插入 3  | (3 \mod 7 = 3)  | 桶 3: <code>[3]</code>                  | 无冲突，新建链表           |<br>
| 6    | 查询 8  | -                 | 桶 1 链表命中 8              | 遍历链表找到目标键         |<br>
| 7    | 删除 11 | -                 | 桶 4 链表移除 11，变为空链表 | 断开节点链接，不影响其他键 |</p>
<p><strong>问题 3：布谷鸟哈希冲突处理策略例题</strong></p>
<p><strong>知识点讲解</strong><br>
布谷鸟哈希使用多个哈希函数（如 2 个）生成候选槽位，插入时优先选择空闲槽位，若冲突则驱逐现有元素并重新哈希。</p>
<ul>
<li><strong>核心规则</strong>
<ol>
<li>每个键有 2 个候选槽位：(h_1(key)) 和 (h_2(key))。</li>
<li>插入时，若两个槽位均被占，驱逐其中一个元素，递归重新哈希被驱逐的元素。</li>
</ol>
</li>
</ul>
<p><strong>例题</strong></p>
<p><strong>哈希函数</strong>：</p>
<ul>
<li>(h_1(x) = x \mod 7)</li>
<li>(h_2(x) = (x \div 7) \mod 7)（整数除法取商再模 7） <strong>表大小</strong>：7 个槽位（0-6） <strong>操作序列</strong>：插入 5, 12, 26, 19, 13, 48</li>
</ul>
<p><strong>答案解析（本质就是占了坑的就全部驱逐重新选坑）</strong></p>
<ol>
<li><strong>插入 5</strong>：
<ul>
<li>(h_1(5)=5)，(h_2(5)=0)（5÷7=0），槽 5 空闲，插入槽 5。</li>
<li>表状态：<code>[N, N, N, N, N, 5, N]</code></li>
</ul>
</li>
<li><strong>插入 12</strong>：
<ul>
<li>(h_1(12)=5)（冲突），(h_2(12)=1)（12÷7=1），槽 1 空闲，插入槽 1。</li>
<li>表状态：<code>[N, 12, N, N, N, 5, N]</code></li>
</ul>
</li>
<li><strong>插入 26</strong>：
<ul>
<li>(h_1(26)=26 \mod 7=5)（冲突），(h_2(26)=3)（26÷7=3），槽 3 空闲，插入槽 3。</li>
<li>表状态：<code>[N, 12, N, 26, N, 5, N]</code></li>
</ul>
</li>
<li><strong>插入 19</strong>：
<ul>
<li>(h_1(19)=19 \mod 7=5)（冲突），(h_2(19)=2)（19÷7=2），槽 2 空闲，插入槽 2。</li>
<li>表状态：<code>[N, 12, 19, 26, N, 5, N]</code></li>
</ul>
</li>
<li><strong>插入 13</strong>：
<ul>
<li>(h_1(13)=13 \mod 7=6)，(h_2(13)=1)（13÷7=1），槽 6 空闲，插入槽 6。</li>
<li>表状态：<code>[N, 12, 19, 26, N, 5, 13]</code></li>
</ul>
</li>
<li><strong>插入 48</strong>：
<ul>
<li>(h_1(48)=48 \mod 7=6)（冲突，槽 6 有 13），(h_2(48)=6)（48÷7=6），两个候选槽位均为 6，冲突。</li>
<li>驱逐 13，重新哈希 13：(h_1(13)=6)（冲突），(h_2(13)=1)（槽 1 有 12），驱逐 12，重新哈希 12：(h_1(12)=5)（槽 5 有 5），(h_2(12)=1)（冲突），继续驱逐 5，重新哈希 5：(h_1(5)=5)（冲突），(h_2(5)=0)，槽 0 空闲，插入 5 到槽 0。</li>
<li>最终表状态：<code>[5, 13, 19, 26, N, 12, 48]</code></li>
</ul>
</li>
</ol>
<p><strong>步骤表格</strong></p>
<p>| 步骤 | 操作    | 候选槽位         | 表状态（0-6）                | 冲突处理 / 备注                                              |<br>
| ---- | ------- | ---------------- | ---------------------------- | ------------------------------------------------------------ |<br>
| 1    | 插入 5  | (h_1=5, h_2=0) | <code>[N, N, N, N, N, 5, N]</code>      | 槽 5 空闲，直接插入                                          |<br>
| 2    | 插入 12 | (h_1=5, h_2=1) | <code>[N, 12, N, N, N, 5, N]</code>     | 槽 5 冲突，插入 h₂=1（空闲）                                 |<br>
| 3    | 插入 26 | (h_1=5, h_2=3) | <code>[N, 12, N, 26, N, 5, N]</code>    | 槽 5 冲突，插入 h₂=3（空闲）                                 |<br>
| 4    | 插入 19 | (h_1=5, h_2=2) | <code>[N, 12, 19, 26, N, 5, N]</code>   | 槽 5 冲突，插入 h₂=2（空闲）                                 |<br>
| 5    | 插入 13 | (h_1=6, h_2=1) | <code>[N, 12, 19, 26, N, 5, 13]</code>  | 槽 6 空闲，直接插入 h₁=6                                     |<br>
| 6    | 插入 48 | (h_1=6, h_2=6) | <code>[5, 13, 19, 26, N, 12, 48]</code> | 槽 6 冲突，驱逐 13→h₂=1（冲突 12），驱逐 12→h₁=5（冲突 5），驱 |</p>
<p><strong>总结对比</strong></p>
<p>| <strong>策略</strong>   | <strong>核心机制</strong>           | <strong>优点</strong>             | <strong>缺点</strong>               | <strong>典型场景</strong>         |<br>
| ---------- | ---------------------- | -------------------- | ---------------------- | -------------------- |<br>
| 线性探测   | 顺序查找下一个空闲槽位 | 实现简单，空间紧凑   | 聚集现象，墓碑管理     | 小规模、低冲突场景   |<br>
| 链地址法   | 每个槽位维护链表       | 冲突处理简单，无聚集 | 链表过长影响性能       | 高冲突、动态数据场景 |<br>
| 布谷鸟哈希 | 多哈希函数 + 驱逐机制  | 高空间利用率，查询快 | 实现复杂，可能无限循环 | 内存敏感、高负载场景 |</p>
<h2 id="5-13">5.13</h2>
<p>| 存储过程名称           | 参数                                        | 功能描述                                                     |<br>
| ---------------------- | ------------------------------------------- | ------------------------------------------------------------ |<br>
| BackupCriticalData     | 无                                          | 备份关键数据表 (user, booking, customizeditinerary)，并记录备份日志 |<br>
| ConfirmBooking         | p_booking_id                                | 确认预订，将状态从 &quot;pending&quot; 改为 &quot;confirmed&quot;，支付状态改为 &quot;paid&quot; |<br>
| CopyItinerary          | p_itinerary_id, p_user_id                   | 为用户复制已有行程，包括基本信息和所有行程项目               |<br>
| GenerateItinerary      | p_user_id, p_city, p_start_date, p_end_date | 创建简单行程，根据城市和日期自动添加热门景点和酒店           |<br>
| generate_itinerary     | p_user_id, p_city, p_start_date, p_end_date | 创建个性化行程，根据用户偏好 (旅行风格、预算) 推荐景点和酒店 |<br>
| recommend_scenic_spots | p_user_id, p_city                           | 根据用户兴趣和旅行风格推荐特定城市的景点                     |<br>
| RecordStrategyView     | p_strategy_id                               | 记录攻略查看，增加攻略查看次数，提升相关城市景点热度         |<br>
| RecoverData            | p_recovery_point                            | 从备份恢复数据到指定时间点，恢复 user、booking 和 customizeditinerary 表 |<br>
| SearchScenic           | p_city, p_keyword                           | 搜索特定城市的景点，基于关键词匹配名称、描述或标签           |<br>
| UpdateScenicHotScore   | 无                                          | 更新所有景点的热度评分，基于评论、预订和收藏情况             |</p>
<p>| 触发器名称                  | 作用时机       | 目标表格     | 功能描述                 |<br>
| --------------------------- | -------------- | ------------ | ------------------------ |<br>
| after_booking_status_update | 修改预订状态后 | booking      | 自动发送状态更新通知     |<br>
| after_review_insert         | 添加新评论后   | review       | 根据评分自动调整景点热度 |<br>
| after_strategylike_insert   | 添加新点赞后   | strategylike | 自动增加攻略的点赞计数   |<br>
| after_strategylike_delete   | 删除点赞后     | strategylike | 自动减少攻略的点赞计数   |</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 触发器</span></span><br><span class="line">use <span class="number">016</span>_yhy;</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CREATE</span> <span class="keyword">TRIGGER</span> after_strategylike_insert;</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">CREATE</span> <span class="keyword">TRIGGER</span> after_strategylike_delete;</span><br><span class="line"><span class="keyword">SELECT</span> strategy_id, title, like_count <span class="keyword">FROM</span> strategy;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 存储过程</span></span><br><span class="line">use <span class="number">016</span>_yhy;</span><br><span class="line"><span class="keyword">SELECT</span> scenic_id, name, hot_score <span class="keyword">FROM</span> scenic <span class="keyword">ORDER</span> <span class="keyword">BY</span> hot_score <span class="keyword">DESC</span> LIMIT <span class="number">10</span>;</span><br><span class="line"><span class="keyword">CALL</span> UpdateScenicHotScore();</span><br><span class="line"><span class="keyword">SELECT</span> scenic_id, name, hot_score <span class="keyword">FROM</span> scenic <span class="keyword">ORDER</span> <span class="keyword">BY</span> hot_score <span class="keyword">DESC</span> LIMIT <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 并发控制:乐观锁</span></span><br><span class="line"><span class="comment">-- 重置房间状态</span></span><br><span class="line"><span class="keyword">UPDATE</span> room <span class="keyword">SET</span> available_count <span class="operator">=</span> <span class="number">5</span>, version <span class="operator">=</span> <span class="number">0</span> <span class="keyword">WHERE</span> room_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 连接1</span></span><br><span class="line"><span class="keyword">START</span> TRANSACTION;</span><br><span class="line"><span class="keyword">UPDATE</span> room </span><br><span class="line"><span class="keyword">SET</span> available_count <span class="operator">=</span> available_count <span class="operator">-</span> <span class="number">2</span>,</span><br><span class="line">    version <span class="operator">=</span> version <span class="operator">+</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> room_id <span class="operator">=</span> <span class="number">1</span> </span><br><span class="line"><span class="keyword">AND</span> version <span class="operator">=</span> <span class="number">0</span>;  </span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 连接2</span></span><br><span class="line"><span class="keyword">START</span> TRANSACTION;</span><br><span class="line"><span class="keyword">UPDATE</span> room </span><br><span class="line"><span class="keyword">SET</span> available_count <span class="operator">=</span> available_count <span class="operator">-</span> <span class="number">1</span>,</span><br><span class="line">    version <span class="operator">=</span> version <span class="operator">+</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">WHERE</span> room_id <span class="operator">=</span> <span class="number">1</span> </span><br><span class="line"><span class="keyword">AND</span> version <span class="operator">=</span> <span class="number">0</span>;  </span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 1. 备份用户数据</span></span><br><span class="line"><span class="keyword">CREATE TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> backup_user <span class="keyword">LIKE</span> <span class="keyword">user</span>;</span><br><span class="line"><span class="keyword">TRUNCATE</span> backup_user;</span><br><span class="line"><span class="keyword">INSERT INTO</span> backup_user <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> <span class="keyword">user</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2. 记录操作前数据状态 </span></span><br><span class="line"><span class="keyword">SELECT</span> status <span class="keyword">INTO</span> <span class="variable">@old_status</span> <span class="keyword">FROM</span> <span class="keyword">user</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3. 模拟错误操作</span></span><br><span class="line"><span class="keyword">UPDATE</span> <span class="keyword">user</span> <span class="keyword">SET</span> status <span class="operator">=</span> <span class="string">&#x27;banned&#x27;</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4. 记录操作日志</span></span><br><span class="line"><span class="keyword">INSERT INTO</span> transactionlog (</span><br><span class="line">    operation_type, table_name, record_id, </span><br><span class="line">    old_data, new_data, user_id, operation_time</span><br><span class="line">) <span class="keyword">VALUES</span> (</span><br><span class="line">    <span class="string">&#x27;UPDATE&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="number">1</span>, </span><br><span class="line">    <span class="built_in">JSON_OBJECT</span>(<span class="string">&#x27;status&#x27;</span>, <span class="variable">@old_status</span>), </span><br><span class="line">    <span class="built_in">JSON_OBJECT</span>(<span class="string">&#x27;status&#x27;</span>, <span class="string">&#x27;banned&#x27;</span>), </span><br><span class="line">    <span class="number">1</span>, NOW()</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 5. 从备份恢复 </span></span><br><span class="line"><span class="keyword">UPDATE</span> <span class="keyword">user</span> u </span><br><span class="line"><span class="keyword">JOIN</span> backup_user bu <span class="keyword">ON</span> u.user_id <span class="operator">=</span> bu.user_id</span><br><span class="line"><span class="keyword">SET</span> u.status <span class="operator">=</span> bu.status</span><br><span class="line"><span class="keyword">WHERE</span> u.user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 6. 验证恢复结果</span></span><br><span class="line"><span class="keyword">SELECT</span> user_id, username, status <span class="keyword">FROM</span> <span class="keyword">user</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 7. 记录恢复操作</span></span><br><span class="line"><span class="keyword">INSERT INTO</span> recoverylog (</span><br><span class="line">    recovery_time, table_name, recovery_point, status</span><br><span class="line">) <span class="keyword">VALUES</span> (</span><br><span class="line">    NOW(), <span class="string">&#x27;user&#x27;</span>, NOW(), <span class="string">&#x27;success&#x27;</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">查看当前用户数据</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line">USE <span class="number">016</span>_yhy;</span><br><span class="line"><span class="keyword">SELECT</span> user_id, username, email, status <span class="keyword">FROM</span> <span class="keyword">user</span> LIMIT <span class="number">5</span>;</span><br><span class="line">执行关键数据备份</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="comment">-- 执行备份存储过程</span></span><br><span class="line"><span class="keyword">CALL</span> BackupCriticalData();</span><br><span class="line">查看备份日志</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> backuplog <span class="keyword">ORDER</span> <span class="keyword">BY</span> backup_id <span class="keyword">DESC</span> LIMIT <span class="number">1</span>;</span><br><span class="line">模拟数据损坏（例如误删用户）</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="comment">-- 记录当前时间用于恢复点</span></span><br><span class="line"><span class="keyword">SELECT</span> NOW() <span class="keyword">INTO</span> <span class="variable">@recovery_point</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除一个用户(请替换为实际存在的用户ID)</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> <span class="keyword">user</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 确认用户已删除</span></span><br><span class="line"><span class="keyword">SELECT</span> user_id, username, email, status <span class="keyword">FROM</span> <span class="keyword">user</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">执行数据恢复</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="comment">-- 使用之前记录的时间点恢复数据</span></span><br><span class="line"><span class="keyword">CALL</span> RecoverData(<span class="variable">@recovery_point</span>);</span><br><span class="line">查看恢复日志</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> recoverylog <span class="keyword">ORDER</span> <span class="keyword">BY</span> recovery_id <span class="keyword">DESC</span> LIMIT <span class="number">1</span>;</span><br><span class="line">验证数据已恢复</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="comment">-- 确认用户数据已恢复</span></span><br><span class="line"><span class="keyword">SELECT</span> user_id, username, email, status <span class="keyword">FROM</span> <span class="keyword">user</span> <span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">检查恢复操作效果</span><br><span class="line">Apply <span class="keyword">to</span> whole.sql</span><br><span class="line"><span class="comment">-- 查看是否有新的通知生成</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> notification </span><br><span class="line"><span class="keyword">WHERE</span> user_id <span class="operator">=</span> <span class="number">1</span> <span class="keyword">AND</span> created_at <span class="operator">&gt;</span> <span class="variable">@recovery_point</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> notification_id <span class="keyword">DESC</span> LIMIT <span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<p>| 表名 | 锁类型 | 实现字段 | 实现方式       | 触发条件                                                 |<br>
| ---- | ------ | -------- | -------------- | -------------------------------------------------------- |<br>
| room | 乐观锁 | version  | 整数版本号自增 | 在预订房间后通过 after_booking_room 触发器自动增加版本号 |</p>
<p>| 视图名称                  | 功能描述                               | 基础表                                       | 主要字段                                               |<br>
| ------------------------- | -------------------------------------- | -------------------------------------------- | ------------------------------------------------------ |<br>
| view_bookingdetails       | 预订详情信息，包括用户、景点和酒店信息 | booking, user, scenic, hotel                 | 预订信息、用户信息、景点信息、酒店信息                 |<br>
| view_cityscenicstats      | 各城市景点统计数据                     | scenic                                       | 城市、景点数量、平均价格、最低价格、最高价格、平均热度 |<br>
| view_hotstrategies        | 热门攻略列表，按热度排序               | strategy, user                               | 攻略信息、作者信息、点赞和浏览量                       |<br>
| view_itineraryitemdetails | 行程项目详细信息                       | itineraryitem, scenic, hotel, transport      | 行程项目信息、景点信息、酒店信息、交通信息             |<br>
| view_personalizedscenic   | 个性化景点推荐                         | user, userpreference, scenic                 | 用户信息、偏好城市内的景点信息                         |<br>
| view_useractivity         | 用户活动统计                           | user, strategy, booking, customizeditinerary | 用户信息、攻略数量、预订数量、行程数量、活跃度评分     |<br>
| view_useritineraries      | 用户行程统计信息                       | customizeditinerary, itineraryitem           | 行程信息、项目总数、景点数、酒店数、交通数、活动数     |</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">&lt;</span>svg width<span class="operator">=</span>&quot;480&quot; height<span class="operator">=</span>&quot;840&quot; xmlns<span class="operator">=</span>&quot;http://www.w3.org/2000/svg&quot;<span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 整体大框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;20&quot; y<span class="operator">=</span>&quot;20&quot; width<span class="operator">=</span>&quot;440&quot; height<span class="operator">=</span>&quot;820&quot; rx<span class="operator">=</span>&quot;15&quot; ry<span class="operator">=</span>&quot;15&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#333366&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 标题 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;50&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;24&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;#333366&quot; font<span class="operator">-</span>weight<span class="operator">=</span>&quot;bold&quot;<span class="operator">&gt;</span>旅游系统前台功能架构<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 核心功能标题 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;70&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;60&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#5B6BBF&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;110&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;20&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>用户旅游服务中心<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第一功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;145&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;180&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#4A9BDC&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第一功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;150&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#4A9BDC&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;183&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>景点与酒店管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第一模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;210&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;240&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>景点浏览与搜索<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;210&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;240&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>景点详情查看<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;270&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;300&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>酒店房型查询<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;270&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;300&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>酒店筛选与比较<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第二功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;335&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;180&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#4CAF50&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第二功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;340&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#4CAF50&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;373&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>个性化行程系统<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第二模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;400&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;430&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>行程创建与编辑<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;400&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;430&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>拖拽式规划<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;460&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;490&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>地图路线可视化<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;460&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;490&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>行程分享与保存<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第三功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;525&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;180&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#FF9966&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第三功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;530&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#FF9966&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;563&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>预订与支付系统<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第三模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;590&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;620&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>景点门票预订<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;590&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;620&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>酒店房间预订<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;650&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;680&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>在线支付处理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;650&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;680&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>订单管理与跟踪<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第四功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;715&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;120&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#9C27B0&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第四功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;720&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#9C27B0&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;753&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>社区互动平台<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第四模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;780&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#8E24AA&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;810&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>攻略浏览与发布<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;780&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#8E24AA&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;810&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>评论与点赞功能<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line"><span class="operator">&lt;</span><span class="operator">/</span>svg<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">&lt;</span>svg width<span class="operator">=</span>&quot;480&quot; height<span class="operator">=</span>&quot;840&quot; xmlns<span class="operator">=</span>&quot;http://www.w3.org/2000/svg&quot;<span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 整体大框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;20&quot; y<span class="operator">=</span>&quot;20&quot; width<span class="operator">=</span>&quot;440&quot; height<span class="operator">=</span>&quot;820&quot; rx<span class="operator">=</span>&quot;15&quot; ry<span class="operator">=</span>&quot;15&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#333366&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 标题 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;50&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;24&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;#333366&quot; font<span class="operator">-</span>weight<span class="operator">=</span>&quot;bold&quot;<span class="operator">&gt;</span>旅游系统后台管理架构<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 核心功能标题 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;70&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;60&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#5B6BBF&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;110&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;20&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>综合管理控制中心<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第一功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;145&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;180&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#4A9BDC&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第一功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;150&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#4A9BDC&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;183&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>数据仪表盘<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第一模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;210&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;240&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>运营数据统计<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;210&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;240&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>预订转化分析<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;270&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;300&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>热门景点统计<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;270&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3A7CA5&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;300&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>收入报表分析<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第二功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;335&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;180&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#4CAF50&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第二功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;340&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#4CAF50&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;373&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>内容管理系统<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第二模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;400&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;430&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>景点信息管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;400&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;430&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>酒店信息管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;460&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;490&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>攻略内容管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;460&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#3E9641&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;490&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>行程模板管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第三功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;525&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;180&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#FF9966&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第三功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;530&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#FF9966&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;563&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>用户与订单管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第三模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;590&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;620&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>用户账户管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;590&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;620&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>用户权限配置<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;650&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;680&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>订单处理与跟踪<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;650&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#E67E22&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;680&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>退款与异常处理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第四功能模块虚线框 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;35&quot; y<span class="operator">=</span>&quot;715&quot; width<span class="operator">=</span>&quot;410&quot; height<span class="operator">=</span>&quot;120&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;none&quot; stroke<span class="operator">=</span>&quot;#9C27B0&quot; stroke<span class="operator">-</span>width<span class="operator">=</span>&quot;2&quot; stroke<span class="operator">-</span>dasharray<span class="operator">=</span>&quot;5,3&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第四功能模块 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;720&quot; width<span class="operator">=</span>&quot;400&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;10&quot; ry<span class="operator">=</span>&quot;10&quot; fill<span class="operator">=</span>&quot;#9C27B0&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;240&quot; y<span class="operator">=</span>&quot;753&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;18&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>系统维护与安全<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span><span class="operator">!</span><span class="comment">-- 第四模块子功能 --&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;40&quot; y<span class="operator">=</span>&quot;780&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#8E24AA&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;135&quot; y<span class="operator">=</span>&quot;810&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>数据备份与恢复<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="operator">&lt;</span>rect x<span class="operator">=</span>&quot;250&quot; y<span class="operator">=</span>&quot;780&quot; width<span class="operator">=</span>&quot;190&quot; height<span class="operator">=</span>&quot;50&quot; rx<span class="operator">=</span>&quot;5&quot; ry<span class="operator">=</span>&quot;5&quot; fill<span class="operator">=</span>&quot;#8E24AA&quot; <span class="operator">/</span><span class="operator">&gt;</span></span><br><span class="line">  <span class="operator">&lt;</span>text x<span class="operator">=</span>&quot;345&quot; y<span class="operator">=</span>&quot;810&quot; font<span class="operator">-</span>family<span class="operator">=</span>&quot;&#x27;Microsoft YaHei&#x27;, sans-serif&quot; font<span class="operator">-</span>size<span class="operator">=</span>&quot;14&quot; text<span class="operator">-</span>anchor<span class="operator">=</span>&quot;middle&quot; fill<span class="operator">=</span>&quot;white&quot;<span class="operator">&gt;</span>系统日志管理<span class="operator">&lt;</span><span class="operator">/</span>text<span class="operator">&gt;</span></span><br><span class="line"><span class="operator">&lt;</span><span class="operator">/</span>svg<span class="operator">&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="5-14">5.14</h2>
<p>1.布隆过滤器（Bloom Filter）</p>
<p>（1）定义与特性</p>
<ul>
<li>
<p><strong>本质</strong>：一种概率型数据结构，用于高效判断元素是否存在于集合中，基于多个哈希函数和位数组实现。</p>
</li>
<li>
<p>核心特性</p>
<ul>
<li><strong>无假阴性</strong>：若查询返回 “不存在”，则元素必定不在集合中（False Negative Rate = 0）。</li>
<li><strong>可能有假阳性</strong>：若查询返回 “存在”，元素可能不存在（由哈希冲突导致，False Positive Rate &gt; 0）。</li>
</ul>
</li>
<li>
<p>核心操作</p>
<ul>
<li><strong>插入</strong>：通过多个哈希函数计算元素的哈希值，将位数组对应位置置为 1。</li>
<li><strong>查询</strong>：检查所有哈希函数对应位是否全为 1，全为 1 则 “可能存在”，否则 “必定不存在”。</li>
</ul>
</li>
<li>
<p><strong>空间效率</strong>：仅需维护一个位数组，空间复杂度远低于存储完整元素集合。</p>
</li>
</ul>
<p>（2）核心问题与解决方案</p>
<p><strong>核心挑战</strong></p>
<ul>
<li><strong>问题 1</strong>：哈希冲突导致假阳性（False Positive）。</li>
<li><strong>问题 2</strong>：位数组大小和哈希函数数量需平衡，以降低假阳性率。</li>
</ul>
<p><strong>解决方案</strong></p>
<ul>
<li>参数优化</li>
</ul>
<p>设位数组大小为$m$，元素数量为$n$，哈希函数数量为$k$，最优$k = (\ln 2) \cdot (m/n)$，最小假阳性率为$ (1 - e^{-kn/m})^k$。</p>
<ul>
<li><strong>多哈希函数</strong>：使用独立哈希函数减少冲突概率，如$h_1(x), h_2(x), \dots, h_k(x)$</li>
</ul>
<p>（3）工作原理与示例</p>
<p>插入流程：对元素 x，计算k个哈希值$h_i(x)\mod m(i=1,2,\dots,k)$。将位数组中$h_i(x)$对应的位置置为 1。</p>
<p><strong>查询流程</strong></p>
<ol>
<li>对元素 x，计算 k 个哈希值，检查对应位是否全为 1。</li>
<li>若全为 1，返回 “可能存在”；否则返回 “不存在”。</li>
</ol>
<p><strong>示例（文档中的案例）</strong>（内容ai识别有误但可以辅助理解）</p>
<ul>
<li><strong>位数组大小</strong>：8（索引 0-7），<strong>哈希函数</strong>：2 个$(h_1(x) = x%8，h_2(x) = (x//2)%8)$。</li>
<li><strong>插入 “RZA”</strong>：$h_1=2，h_2=4$，置位 2、4 为 1。</li>
<li><strong>插入 “GZA”</strong>：$h_1=3，h_2=1$，置位 3、1 为 1。</li>
<li><strong>查询 “ODB”</strong>：$h_1=3，h_2=6$，对应位 3 为 1、6 为 0，返回 “不存在”（实际未插入，正确）。</li>
<li><strong>查询 “假阳性案例”</strong>：某元素未插入，但所有哈希位被其他元素置 1（也就是置1的这两个位置分别是两个元素1的一部分，1个元素有两个1，组合在一起并没有实际的含义），返回 “存在”（如文档中 “ODB” 示例可能因哈希冲突误判）。</li>
</ul>
<p><img src="/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%EF%BC%88%E5%A4%96%E6%95%99%E7%89%88%EF%BC%89/image-20250514144309539.png" alt="image-20250514144309539"></p>
<p><strong>问题：布隆过滤器插入操作例题</strong></p>
<p><strong>知识点讲解</strong></p>
<p>布隆过滤器是一种基于哈希函数和位数组的数据结构，用于高效判断元素是否存在于集合中。其核心思想是通过多个哈希函数将元素映射到位数组的不同位置，并将这些位置置为 1。插入时，所有哈希函数对应的位都会被标记；查询时，只有当所有哈希位均为 1 时，才认为元素 “可能存在”，否则 “必定不存在”。</p>
<p><strong>核心规则</strong></p>
<ol>
<li>插入操作
<ul>
<li>对元素 x，计算每个哈希函数$h_i(x)$的值，将位数组中索引为$h_i(x)$的位置置为 1（即使该位置已为 1，仍保持为 1）。</li>
</ul>
</li>
<li><strong>无假阴性</strong>：若查询时任何一个哈希位为 0，元素必定不存在；若所有哈希位为 1，元素可能存在（可能因哈希冲突导致假阳性）。</li>
</ol>
<p><strong>例题</strong></p>
<p><strong>布隆过滤器设置</strong>：</p>
<ul>
<li><strong>位数组大小</strong>：10 位（索引 0–9）</li>
<li><strong>哈希函数数量</strong>：2 个</li>
<li>哈希函数
<ul>
<li>$h_1(x) = x % 10$</li>
<li>$h_2(x) = (x \times 3) % 10$</li>
</ul>
</li>
<li><strong>操作序列</strong>：插入数字 5、7、12</li>
</ul>
<p><strong>答案解析</strong></p>
<ol>
<li><strong>插入数字 5</strong>：
<ul>
<li>$h_1(5) = 5 % 10 = 5$</li>
<li>$h_2(5) = (5 \times 3) % 10 = 15 % 10 = 5$</li>
<li>置位：索引 5（两个哈希函数均映射到 5，只需置位一次）。</li>
<li>位数组状态：<code>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</code></li>
</ul>
</li>
<li><strong>插入数字 7</strong>：
<ul>
<li>(h_1(7) = 7 % 10 = 7)</li>
<li>(h_2(7) = (7 \times 3) % 10 = 21 % 10 = 1)</li>
<li>置位：索引 1 和 7。</li>
<li>位数组状态：<code>[0, 1, 0, 0, 0, 1, 0, 1, 0, 0]</code></li>
</ul>
</li>
<li><strong>插入数字 12</strong>：
<ul>
<li>(h_1(12) = 12 % 10 = 2)</li>
<li>(h_2(12) = (12 \times 3) % 10 = 36 % 10 = 6)</li>
<li>置位：索引 2 和 6。</li>
<li>最终位数组状态：<code>[0, 1, 1, 0, 0, 1, 1, 1, 0, 0]</code>（与答案一致）</li>
</ul>
</li>
</ol>
<p><strong>步骤表格</strong></p>
<p>| 步骤 | 操作    | 哈希值计算       | 置位索引 | 位数组状态（0-9）       | 备注                 |<br>
| ---- | ------- | ---------------- | -------- | ----------------------- | -------------------- |<br>
| 1    | 插入 5  | (h_1=5, h_2=5) | 5        | <code>[0,0,0,0,0,1,0,0,0,0]</code> | 两哈希函数均映射到 5 |<br>
| 2    | 插入 7  | (h_1=7, h_2=1) | 1, 7     | <code>[0,1,0,0,0,1,0,1,0,0]</code> | 无冲突，直接置位     |<br>
| 3    | 插入 12 | (h_1=2, h_2=6) | 2, 6     | <code>[0,1,1,0,0,1,1,1,0,0]</code> | 无冲突               |</p>
<p>2.跳跃列表（Skip List）</p>
<p>（1）定义与特性</p>
<ul>
<li><strong>本质</strong>：基于多层有序链表的索引结构，通过分层指针实现快速搜索，常用于内存数据库索引（如 Redis）。</li>
<li>核心结构
<ul>
<li><strong>多层链表</strong>：每层为有序链表，高层节点稀疏（约为下层的 1/2），底层包含所有节点。</li>
<li><strong>随机层数</strong>：插入时通过 “抛硬币” 随机决定节点层数（概率 1/2 逐层递增，避免手动平衡）。</li>
</ul>
</li>
<li><strong>时间复杂度</strong>：搜索、插入、删除均为 (O(\log n))（近似平衡树效率）。</li>
</ul>
<p>（2）与 B + 树对比</p>
<p>| <strong>特性</strong>        | <strong>跳跃列表</strong>                       | <strong>B + 树</strong>                   |<br>
| --------------- | ---------------------------------- | ---------------------------- |<br>
| <strong>存储结构</strong>    | 内存有序链表（多层索引）           | 磁盘友好的树结构             |<br>
| <strong>插入 / 删除</strong> | 无需全局重新平衡（仅局部指针调整） | 可能触发树的分裂 / 合并      |<br>
| <strong>局部性</strong>      | 差（节点分散，缓存不友好）         | 好（节点按页存储，局部性强） |<br>
| <strong>适用场景</strong>    | 内存型数据库、高频更新场景         | 磁盘数据库、范围查询为主     |</p>
<p>（3）核心操作流程</p>
<p><strong>搜索流程</strong></p>
<ol>
<li>从最高层的头节点开始，沿当前层向右移动，直到下一个节点键值大于目标键。</li>
<li>若无法右移，下降到下一层，重复步骤 1，直到底层。</li>
<li>底层找到目标键则成功，否则不存在。</li>
</ol>
<p><strong>插入流程</strong></p>
<ol>
<li>找到插入位置（同搜索流程，记录各层前驱节点）。</li>
<li>插入底层节点，更新前驱节点的 next 指针。</li>
<li>抛硬币决定是否将节点提升到更高层（概率 1/2，直到达到最大层数），逐层更新指针。</li>
</ol>
<p>（4）例题：插入节点与层数决策</p>
<p><strong>场景</strong>：跳跃列表当前底层节点为 [11, 17, 20]，层数最大为 3 层，插入节点 19。</p>
<ol>
<li><strong>底层搜索</strong>：17 &lt; 19 &lt; 20，插入位置在 17 和 20 之间。</li>
<li><strong>抛硬币结果</strong>：第一次正面（层数提升至 2 层），第二次反面（停止提升）。</li>
<li>更新指针
<ul>
<li>底层：17→19→20。</li>
<li>2 层：若原 2 层节点为 [11, 20]，插入 19 需判断是否在 2 层存在（因 19 未被提升到 2 层，故 2 层无变化）。</li>
</ul>
</li>
</ol>
<p>（5）优缺点</p>
<ul>
<li><strong>优点</strong>：实现简单，插入 / 删除高效（无需平衡），适合动态数据。</li>
<li><strong>缺点</strong>：内存占用较高（每层节点需额外指针），磁盘访问效率低（非本地化存储）。</li>
</ul>
<p>3.倒排索引（Inverted Index）</p>
<p>（1）定义与特性</p>
<ul>
<li><strong>本质</strong>：一种反向映射索引，将 “术语” 映射到 “包含该术语的文档集合”，用于高效文本搜索。</li>
<li>核心结构
<ul>
<li><strong>词典（Dictionary）</strong>：存储所有唯一术语，如 “car”“vehicle”。</li>
<li>** postings 列表（Postings List）**：每个术语对应一个文档列表，记录文档 ID 及出现位置 / 频率。</li>
</ul>
</li>
<li><strong>应用场景</strong>：全文检索（如 SQL 中的<code>LIKE '%关键词%'</code>）、搜索引擎、文档检索系统。</li>
</ul>
<p>（2）核心问题与解决方案</p>
<p><strong>核心挑战</strong></p>
<ul>
<li><strong>问题 1</strong>：术语分词与标准化（如大小写、单复数处理）。</li>
<li><strong>问题 2</strong>：高效处理多术语组合查询（如 “AND”“OR” 操作）。</li>
</ul>
<p><strong>解决方案</strong></p>
<ul>
<li><strong>分词与归一化</strong>：使用分词器（如 Lucene 的 Standard Analyzer）将文本拆分为词元（Token），并转换为小写、去除停用词。</li>
<li>倒排列表合并
<ul>
<li><strong>AND 操作</strong>：取多个 postings 列表的交集（如归并排序遍历）。</li>
<li><strong>OR 操作</strong>：取多个 postings 列表的并集。</li>
</ul>
</li>
</ul>
<p>（3）示例：构建倒排索引</p>
<p><strong>文档集合</strong>：</p>
<ol>
<li>“Winter is coming.”</li>
<li>“Ours is the fury.”</li>
<li>“The choice is yours.”</li>
</ol>
<p><strong>倒排索引表</strong></p>
<p>| 术语   | 文档 ID 列表 | 频率 |<br>
| ------ | ------------ | ---- |<br>
| coming | [1]          | 1    |<br>
| fury   | [2]          | 1    |<br>
| is     | [1, 2, 3]    | 3    |<br>
| the    | [2, 3]       | 2    |<br>
| yours  | [3]          | 1    |</p>
<p><strong>查询 “is the”</strong>：</p>
<ul>
<li>取 “is” 的文档列表 [1,2,3] 和 “the” 的文档列表 [2,3] 的交集，结果为 [2,3]。</li>
</ul>
<p>（4）与正向索引对比</p>
<p>| <strong>特性</strong>     | <strong>倒排索引</strong>                  | <strong>正向索引（如 B + 树）</strong>  |<br>
| ------------ | ----------------------------- | -------------------------- |<br>
| <strong>映射方向</strong> | 术语→文档（反向）             | 文档→术语（正向）          |<br>
| <strong>优势</strong>     | 快速回答 “哪些文档包含某术语” | 快速定位单个文档内容       |<br>
| <strong>适用场景</strong> | 文本搜索、关键词查询          | 文档内容检索、精确主键查询 |</p>
<p>（5）扩展技术</p>
<ul>
<li><strong>同义词处理</strong>：引入同义词词典（如 “car” 映射 “vehicle”），扩展查询匹配范围。</li>
<li><strong>短语查询</strong>：在 postings 列表中记录术语位置，支持 “邻近词” 搜索（如 “New York” 需连续出现）。</li>
</ul>
<p>4.向量数据库（Vector Database）</p>
<h2 id="5-14（2）sort">5.14（2）sort</h2>
<p>1.排序（Sorting）</p>
<p>（1）定义与特性</p>
<ul>
<li>
<p><strong>本质</strong>：将数据集合按升序或降序排列的过程，数据库中通过 <code>ORDER BY</code> 实现，大规模数据需考虑内存限制采用外部排序。</p>
</li>
<li>
<p>核心操作</p>
<ul>
<li><strong>数据库排序</strong>：<code>ORDER BY</code> 排序结果集，<code>LIMIT</code> 限制返回记录数，平均时间复杂度依赖数据规模和算法。</li>
<li><strong>内部排序</strong>：在内存中直接排序（如选择排序、插入排序、快速排序），时间复杂度范围为 (O(n^2)) 到 (O(n \log n))。</li>
<li><strong>外部排序</strong>：处理超出内存的数据，通过分块排序和归并实现（如外部归并排序）。</li>
</ul>
</li>
<li>
<p>内部结构</p>
<ul>
<li>内部排序基于数组或链表操作，外部排序依赖磁盘分块和归并，利用优先队列优化 Top-N 排序。</li>
</ul>
</li>
</ul>
<p>（2）与其他排序方法对比</p>
<p>| <strong>特性</strong>       | <strong>内部排序（如快速排序）</strong> | <strong>Top-N 排序（优先队列）</strong>    | <strong>外部归并排序</strong>                |<br>
| -------------- | -------------------------- | ----------------------------- | ------------------------------- |<br>
| <strong>数据规模</strong>   | 适合内存内数据             | 适合内存内数据                | 适合超大规模磁盘数据            |<br>
| <strong>时间复杂度</strong> | (O(n \log n))            | (O(n \log k))（k 为 Top-N） | (O((n/m) \log (n/m) \cdot m)) |<br>
| <strong>内存依赖</strong>   | 需全部数据在内存中         | 仅需维护大小为 k 的堆         | 分块处理，每次仅加载部分数据    |<br>
| <strong>适用场景</strong>   | 中等规模数据排序           | 快速获取前 k 小 / 大元素      | 处理 TB 级无法内存容纳的数据    |</p>
<p>（3）核心问题与解决方案</p>
<p><strong>核心挑战</strong></p>
<ul>
<li><strong>问题 1</strong>：数据量超过内存容量，无法直接加载排序（如 128 TB 数据）。</li>
<li><strong>问题 2</strong>：数据库中 <code>ORDER BY</code> 结合 <code>LIMIT</code> 时，全量排序效率低。</li>
</ul>
<p><strong>解决方案</strong></p>
<ul>
<li><strong>外部归并排序</strong>：分治策略，先排序内存可容纳的块，再逐次归并有序块（如 2 路归并）。</li>
<li><strong>Top-N 排序</strong>：使用优先队列（最大堆），仅维护当前 Top-N 元素，避免全量排序。</li>
</ul>
<p>（4）核心算法步骤</p>
<p><strong>Top-N 最小元素算法（最大堆）</strong></p>
<ol>
<li>初始化大小为 N 的最大堆。</li>
<li>遍历数据，若堆中元素数小于 N，直接插入堆。</li>
<li>否则，若当前元素小于堆顶（最大元素），删除堆顶并插入当前元素，调整堆结构。</li>
</ol>
<p><strong>外部归并排序（2 路）</strong></p>
<ol>
<li><strong>排序阶段（Pass 0）</strong>：将数据分块读入内存，排序后写回磁盘（生成 1-page 有序块）。</li>
<li><strong>归并阶段（Pass 1, 2, ...）</strong>：每次合并两个相邻有序块，生成双倍大小的有序块，直至所有数据有序。</li>
</ol>
<p>（5）核心公式与算法</p>
<p><strong>优先队列操作</strong></p>
<ul>
<li>插入元素：时间复杂度 (O(\log k))（k 为堆大小）。</li>
<li>删除堆顶：时间复杂度 (O(\log k))。</li>
</ul>
<p><strong>外部归并阶段数计算</strong></p>
<p>设数据总页数为 n，内存可容纳 m 页，则归并阶段数为 (\lceil \log_{k}(n/m) \rceil)（k 为归并路数，此处 k=2）</p>
<p><strong>问题 1：Top-N 最小元素算法例题（最大堆）</strong></p>
<p><strong>知识点讲解</strong><br>
使用最大堆维护当前 Top-N 元素，堆顶为当前最大元素。新元素若小于堆顶，则替换堆顶并调整堆，确保堆中始终为最小的 N 个元素。</p>
<p><strong>核心规则</strong></p>
<ol>
<li>堆大小固定为 N，存储当前最小的 N 个元素。</li>
<li>新元素小于堆顶时，删除堆顶并插入新元素；否则跳过。</li>
</ol>
<p><strong>例题</strong></p>
<p><strong>目标</strong>：查找数组 <code>[3, 8, 4, 9, 1, 5, 7, 11]</code> 中前 3 小的元素。</p>
<p><strong>步骤解析</strong></p>
<ol>
<li><strong>初始化堆</strong>：堆大小为 3，初始为空。</li>
<li>插入前 3 个元素（3, 8, 4）
<ul>
<li>插入 3，堆：<code>[3]</code>（堆大小 1）。</li>
<li>插入 8，堆：<code>[3, 8]</code>（堆大小 2）。</li>
<li>插入 4，堆：调整为最大堆 <code>[8, 3, 4]</code>（堆顶 8 为当前最大）。</li>
</ul>
</li>
<li>处理后续元素（9, 1, 5, 7, 11）
<ul>
<li>9 &gt; 堆顶 8，跳过，堆不变。</li>
<li>1 &lt; 堆顶 8，删除 8，插入 1，调整堆为 <code>[4, 3, 1]</code>（堆顶 4）。</li>
<li>5 &gt; 堆顶 4，跳过，堆不变。</li>
<li>7 &gt; 堆顶 4，跳过，堆不变。</li>
<li>11 &gt; 堆顶 4，跳过，堆不变。</li>
</ul>
</li>
<li><strong>最终堆元素</strong>：1, 3, 4（排序后为 [1, 3, 4]）。</li>
</ol>
<p><strong>步骤表格</strong></p>
<p>| 步骤 | 操作    | 堆状态（最大堆） | 对比结果        | 备注                     |<br>
| ---- | ------- | ---------------- | --------------- | ------------------------ |<br>
| 1    | 插入 3  | <code>[3]</code>            | -               | 堆大小不足 3，直接插入   |<br>
| 2    | 插入 8  | <code>[8, 3]</code>         | -               | 堆大小不足 3，直接插入   |<br>
| 3    | 插入 4  | <code>[8, 3, 4]</code>      | -               | 堆大小达 3，调整为最大堆 |<br>
| 4    | 处理 9  | <code>[8, 3, 4]</code>      | 9 &gt; 8，跳过     | 不更新堆                 |<br>
| 5    | 处理 1  | <code>[4, 3, 1]</code>      | 1 &lt; 8，替换堆顶 | 删除 8，插入 1，调整堆   |<br>
| 6    | 处理 5  | <code>[4, 3, 1]</code>      | 5 &gt; 4，跳过     | 不更新堆                 |<br>
| 7    | 处理 7  | <code>[4, 3, 1]</code>      | 7 &gt; 4，跳过     | 不更新堆                 |<br>
| 8    | 处理 11 | <code>[4, 3, 1]</code>      | 11 &gt; 4，跳过    | 不更新堆                 |</p>
<p><strong>问题 2：2 路外部归并排序例题</strong></p>
<p><strong>知识点讲解</strong><br>
外部归并排序分排序阶段和归并阶段，每次归并两个有序块，逐步生成更大有序块，适用于数据量超过内存的场景。</p>
<p><strong>核心规则</strong></p>
<ol>
<li><strong>排序阶段</strong>：将数据分块读入内存，排序后写回磁盘（生成初始有序块）。</li>
<li><strong>归并阶段</strong>：每次合并两个相邻有序块，生成双倍大小的有序块，直至所有数据有序。</li>
</ol>
<p><strong>例题</strong><br>
<strong>数据</strong>：磁盘上的数组 <code>[17, 3, 9, 22, 13, 7, 5, 11]</code>，内存仅能容纳 4 个元素。</p>
<p><strong>步骤解析</strong></p>
<ol>
<li>排序阶段（Pass 0）
<ul>
<li>分块 1：<code>[17, 3, 9, 22]</code> → 排序后 <code>[3, 9, 17, 22]</code>（写入磁盘）。</li>
<li>分块 2：<code>[13, 7, 5, 11]</code> → 排序后 <code>[5, 7, 11, 13]</code>（写入磁盘）。</li>
</ul>
</li>
<li>归并阶段（Pass 1）
<ul>
<li>合并两个有序块：
<ul>
<li>比较 3（块 1）和 5（块 2），取 3，块 1 指针后移。</li>
<li>比较 9（块 1）和 5（块 2），取 5，块 2 指针后移。</li>
<li>比较 9（块 1）和 7（块 2），取 7，块 2 指针后移。</li>
<li>比较 9（块 1）和 11（块 2），取 9，块 1 指针后移。</li>
<li>比较 17（块 1）和 11（块 2），取 11，块 2 指针后移。</li>
<li>比较 17（块 1）和 13（块 2），取 13，块 2 指针后移。</li>
<li>取块 1 剩余元素 17、22。</li>
</ul>
</li>
<li>生成最终有序数组：<code>[3, 5, 7, 9, 11, 13, 17, 22]</code>。</li>
</ul>
</li>
</ol>
<p><strong>步骤表格</strong></p>
<p>| 阶段   | 操作     | 内存中数据块      | 磁盘写入结果          | 备注                       |<br>
| ------ | -------- | ----------------- | --------------------- | -------------------------- |<br>
| Pass 0 | 分块排序 | 块 1: [17,3,9,22] | 块 1: [3,9,17,22]     | 生成初始有序块             |<br>
| Pass 0 | 分块排序 | 块 2: [13,7,5,11] | 块 2: [5,7,11,13]     | 生成初始有序块             |<br>
| Pass 1 | 2 路归并 | 块 1 + 块 2 合并  | [3,5,7,9,11,13,17,22] | 合并两个有序块生成最终结果 |</p>
<p><strong>总结对比</strong></p>
<p>| <strong>算法</strong>         | <strong>核心机制</strong>          | <strong>优点</strong>                            | <strong>缺点</strong>                    | <strong>典型场景</strong>                   |<br>
| ---------------- | --------------------- | ----------------------------------- | --------------------------- | ------------------------------ |<br>
| <strong>Top-N 排序</strong>   | 最大堆维护 Top-N 元素 | 避免全量排序，节省时间              | 需维护堆结构                | 数据库 <code>ORDER BY + LIMIT</code> 场景 |<br>
| <strong>外部归并排序</strong> | 分块排序后逐次归并    | 处理超大规模数据                    | 多次磁盘 I/O 操作           | 处理 TB 级数据排序             |<br>
| <strong>快速排序</strong>     | 分治 + 分区交换       | 平均时间复杂度低（(O(n \log n))） | 最坏情况 (O(n^2))，不稳定 | 内存中大规模数据排序           |</p>
<h2 id="5-14（3）transaction-and-ACID（事务和ACID特性）">5.14（3）transaction and ACID（事务和ACID特性）</h2>
<p>ACID是数据库事务的四个基本特性，分别是<strong>原子性</strong>（Atomicity）、<strong>一致性</strong>（Consistency）、<strong>隔离性</strong>（Isolation）和<strong>持久性</strong>（Durability）。</p>
<p>1.事务（Transaction）与 ACID 特性</p>
<p>（1）定义与特性</p>
<ul>
<li><strong>本质</strong>：数据库操作的基本单元，由一组逻辑相关的读写操作组成，确保数据一致性和可靠性。</li>
<li>核心特性（ACID）
<ul>
<li><strong>原子性（Atomicity）</strong>：事务中的操作要么全部执行，要么全部回滚（全或无）。</li>
<li><strong>一致性（Consistency）</strong>：事务执行前后，数据库约束（如主键、外键、CHECK 条件）保持成立。</li>
<li><strong>隔离性（Isolation）</strong>：事务执行时不受其他事务干扰，如同独占数据库。</li>
<li><strong>持久性（Durability）</strong>：已提交事务的修改永久保存，即使系统崩溃也不丢失。</li>
</ul>
</li>
<li><strong>生命周期</strong>：以<code>BEGIN</code>开始，以<code>COMMIT</code>（提交）或<code>ABORT</code>（回滚）结束。</li>
</ul>
<p>（2）ACID 核心特性对比</p>
<p>| <strong>特性</strong> | <strong>核心作用</strong>                                 | <strong>实现方式</strong>                                        |<br>
| -------- | -------------------------------------------- | --------------------------------------------------- |<br>
| 原子性   | 确保事务操作的完整性，失败时回滚到初始状态。 | 日志记录（Redo/Undo 日志）、影子分页。              |<br>
| 一致性   | 维护数据库约束，确保业务规则不被破坏。       | 应用层约束定义、数据库内置约束（如 SQL 的 CHECK）。 |<br>
| 隔离性   | 控制并发事务的交互，避免数据不一致。         | 并发控制协议（锁机制、时间戳排序、乐观并发控制）。  |<br>
| 持久性   | 保证已提交数据的永久性。                     | 日志持久化（WAL，预写日志）、磁盘同步。             |</p>
<p>（3）核心问题与解决方案</p>
<p><strong>核心挑战</strong></p>
<ul>
<li><strong>并发执行导致的数据不一致</strong>：如脏读（读取未提交数据）、丢失更新（覆盖未提交修改）、不可重复读（多次读取结果不同）。</li>
<li><strong>事务失败后的状态恢复</strong>：部分执行的事务需回滚，已提交事务需持久化。</li>
</ul>
<p><strong>解决方案</strong></p>
<ul>
<li>并发控制协议
<ul>
<li><strong>悲观协议</strong>：提前预防冲突（如锁机制，事务提前获取所需锁）。</li>
<li><strong>乐观协议</strong>：假设冲突罕见，事后验证冲突并回滚（如版本控制）。</li>
</ul>
</li>
<li><strong>恢复机制</strong>：通过日志记录事务操作，崩溃后利用 Redo 日志重做已提交事务，Undo 日志回滚未完成事务。</li>
</ul>
<p>（4）并发控制关键策略</p>
<p><strong>冲突操作定义</strong></p>
<p>两个事务对同一数据的操作满足以下条件之一：</p>
<ol>
<li><strong>读 - 写（RW）**或**写 - 读（WR）</strong>：一个读，一个写。</li>
<li><strong>写 - 写（WW）</strong>：两个写操作。</li>
</ol>
<p><strong>可串行化调度</strong></p>
<ul>
<li><strong>定义</strong>：多个事务的并发执行结果等同于某一串行执行结果（顺序不影响最终一致性）。</li>
<li><strong>冲突可串行化</strong>：若两个冲突操作的顺序在调度中与某一串行调度一致，则该调度为冲突可串行化。</li>
<li>依赖图判断法
<ul>
<li>节点：事务（如 T1、T2）。</li>
<li>边：若事务 Ti 的操作先于 Tj 的冲突操作，添加边 Ti→Tj。</li>
<li>结论：若依赖图无环，则调度是冲突可串行化的。</li>
</ul>
</li>
</ul>
<p>（5）核心公式与算法</p>
<p><strong>依赖图构建规则</strong></p>
<ol>
<li>每个事务为一个节点。</li>
<li>若 Ti 的操作 Oi 与 Tj 的操作 Oj 冲突，且 Oi 在 Oj 前执行，则添加边 Ti→Tj。</li>
<li>若图中存在环（如 T1→T2→T1），则调度不可串行化。</li>
</ol>
<p><strong>问题 1：判断冲突可串行化</strong></p>
<p><strong>操作序列</strong>：</p>
<p>T1: R(A) W(A)</p>
<p>T2: R(A) W(B)</p>
<p>T3: R(B) W(C)</p>
<p><strong>知识点讲解</strong></p>
<p>冲突操作仅发生在不同事务对同一数据的读写 / 写读 / 写写操作。本例中：</p>
<ul>
<li>T1 与 T2 对 A 有 RW 冲突（T1 读 A，T2 读 A 不冲突，因均为读；T1 写 A 与 T2 读 A 为 RW 冲突）。</li>
<li>T2 写 B 与 T3 读 B 为 RW 冲突。</li>
</ul>
<p><strong>依赖图构建</strong></p>
<p>T1→T2（T1 写 A 先于 T2 读 A？不，T2 读 A 在 T1 写 A 之前？需明确操作顺序。假设顺序为 T1.R (A), T2.R (A), T1.W (A), T2.W (B), T3.R (B), T3.W (C)，则 T1 写 A 在</p>
<p>T2 读 A 之后，无冲突；T2 写 B 先于 T3 读 B，边 T2→T3。图无环，故冲突可串行化。</p>
<p><strong>答案解析</strong></p>
<p>需明确操作顺序：</p>
<p>若顺序为：</p>
<p>T1: R(A), W(A)</p>
<p>T2: R(A), W(B)</p>
<p>T3: R(B), W(C)</p>
<p>则：</p>
<ul>
<li>T1 与 T2 对 A 的操作：T1 写 A 在 T2 读 A 之后？若 T2 读 A 在 T1 写 A 之前，则无 RW 冲突（T2 读 A 时 A 未被 T1 修改）。</li>
<li>T2 写 B 与 T3 读 B 为 RW 冲突，边 T2→T3。依赖图无环，调度冲突可串行化。</li>
</ul>
<p><strong>问题 2：脏读场景分析</strong></p>
<p><strong>事务序列</strong>：</p>
<p>T1: BEGIN, W (A, 10), 未提交</p>
<p>T2: BEGIN, R (A)（读取 T1 未提交的 A=10）, ABORT</p>
<p><strong>知识点讲解</strong></p>
<p>脏读发生在事务读取另一个未提交事务修改的数据。若 T2 在 T1 提交前读取 A，且 T1 最终回滚，T2 读取的是无效数据。</p>
<p><strong>答案解析</strong></p>
<ul>
<li>T2 执行 R (A) 时，T1 未提交，属于脏读。</li>
<li>隔离性要求需避免此类场景（如通过读已提交隔离级别）。</li>
</ul>
<p><strong>总结对比</strong></p>
<p>| <strong>并发问题</strong> | <strong>现象</strong>                       | <strong>解决方案</strong>                       |<br>
| ------------ | ------------------------------ | ---------------------------------- |<br>
| 脏读         | 读取未提交数据                 | 提高隔离级别（如读已提交）、写锁。 |<br>
| 丢失更新     | 后提交事务覆盖前事务未提交修改 | 行级锁、乐观锁（版本号校验）。     |<br>
| 不可重复读   | 两次读取结果不同               | 快照隔离、范围锁。                 |<br>
| 幻读         | 新增数据导致范围查询结果变化   | 可串行化隔离级别、间隙锁。         |</p>
<p>通过 ACID 特性和并发控制协议，数据库确保了事务的正确性和可靠性，平衡了性能与一致性需求。</p>
<h2 id="5-15（最后一次课）12-concurrency-control并发控制">5.15（最后一次课）12.concurrency control并发控制</h2>
<p>1.并发控制（Concurrency Control）</p>
<p>（1）定义与特性</p>
<ul>
<li><strong>本质</strong>：确保数据库事务并发执行时结果与串行执行一致（可串行化），通过锁机制或时间戳排序避免冲突。</li>
<li><strong>核心目标</strong>：处理读写冲突（如脏读、不可重复读、幻读），保证数据一致性。</li>
<li>主要方法
<ul>
<li><strong>锁机制</strong>：通过共享锁（S-Lock）和排他锁（X-Lock）控制对数据对象的访问。</li>
<li><strong>时间戳排序（Timestamp Ordering）</strong>：为每个事务分配唯一时间戳，按时间戳顺序处理操作，冲突时回滚事务。</li>
</ul>
</li>
</ul>
<p>（2）与时间戳排序对比</p>
<p>| <strong>特性</strong>     | <strong>锁机制</strong>           | <strong>时间戳排序</strong>       |<br>
| ------------ | -------------------- | -------------------- |<br>
| <strong>冲突处理</strong> | 阻塞等待（可能死锁） | 回滚重启（无死锁）   |<br>
| <strong>执行顺序</strong> | 由锁获取顺序决定     | 由时间戳顺序决定     |<br>
| <strong>系统开销</strong> | 锁管理开销、死锁检测 | 时间戳维护、回滚开销 |<br>
| <strong>适用场景</strong> | 高竞争、长事务       | 低冲突、短事务       |</p>
<p>（3）核心问题与解决方案</p>
<p><strong>核心挑战</strong></p>
<ul>
<li><strong>问题 1</strong>：锁冲突导致死锁（多个事务循环等待锁）。</li>
<li><strong>问题 2</strong>：级联回滚（Cascading Abort）：一个事务回滚导致依赖它的事务也回滚。</li>
<li><strong>问题 3</strong>：读写顺序冲突（如写 - 读、读 - 写冲突）。</li>
</ul>
<p><strong>解决方案</strong></p>
<ul>
<li><strong>死锁处理</strong>：超时检测、等待图检测（Cycle Detection）或银行家算法（通过安全序列避免死锁）。</li>
<li><strong>避免级联回滚</strong>：使用严格两阶段锁（Strict 2PL），事务提交前不释放写锁。</li>
<li>冲突控制
<ul>
<li>锁机制：通过兼容性矩阵（如 S 锁与 X 锁互斥）控制并发访问。</li>
<li>时间戳排序：通过比较事务时间戳与数据对象的读写时间戳（R-TS/W-TS）决定是否允许操作。</li>
</ul>
</li>
</ul>
<p>（4）关键策略与协议</p>
<p><strong>1. 锁类型与兼容性矩阵</strong></p>
<ul>
<li>
<p><strong>S-Lock（共享锁）</strong>：允许多个事务同时读取同一数据项，但禁止其他事务写入。</p>
</li>
<li>
<p><strong>X-Lock（排他锁）</strong>：独占数据项，禁止其他事务读写。</p>
</li>
<li>
<p><strong>锁兼容性矩阵</strong>：</p>
</li>
</ul>
<p>| 现有锁 \ 请求锁                                              | S-Lock | X-Lock |<br>
| ------------------------------------------------------------ | ------ | ------ |<br>
| S-Lock                                                       | 允许   | 拒绝   |<br>
| X-Lock                                                       | 拒绝   | 拒绝   |<br>
| 例如，若事务 T1 持有 S 锁，事务 T2 可申请 S 锁（共享读），但 T2 申请 X 锁会被拒绝。 |        |        |</p>
<h2 id="assignment-3前置练习：B-树">assignment 3前置练习：B+树</h2>
<p><strong>B+ Tree</strong></p>
<p>Construct a B+ Tree where each node can hold a maximum of 4 keys.</p>
<p>To ensure consistency and avoid ambiguity during node splits, follow these rules:</p>
<ol>
<li>
<p>Splitting Rule: When a node overflows (i.e., receives a 5th key), split it into two child nodes such that the right node contains more keys than the left node (in case of an odd number).</p>
</li>
<li>
<p>Promotion Rule:</p>
</li>
</ol>
<p>Leaf Node Split: <strong>Copy</strong> (<strong>do not move</strong>) the leftmost key of the right node up to the parent. Leaf nodes store actual data, so the inserted key must remain in the leaf.</p>
<p>Internal Node Split: <strong>Move</strong> the leftmost key of the right node up to the parent. Internal nodes are used purely for navigation, so the promoted key is removed from the child node to reduce redundancy. (This splitting behavior differs from that presented in our lecture slides. However, both approaches are considered acceptable in practice. For the purposes of this exercise, please follow the rule described above)</p>
<ol start="3">
<li>Deletion Rule:</li>
</ol>
<p>When a key is deleted and a node underflows (i.e., it has fewer than the minimum number of keys), attempt to borrow a key from a sibling. If borrowing is not possible, merge the underflowed node with a sibling, and remove the separating key from the parent. This can cause cascading merges up the tree.</p>
<p>If, as a result of merging, the root node ends up with only one remaining child, the root is removed, and its single child becomes the new root, effectively reducing the height of the tree by one.</p>
<p>Task 1: Insert the following sequence of keys and draw the final tree</p>
<p>10, 20, 5, 6, 12, 30, 7, 17, 3, 25, 27, 4, 31</p>
<p>Task 2: Delete the following number of keys 3, 4, 25 Tip: You can use the visualisation tool to verify your answer</p>
<p>(https://www.cs.usfca.edu/~galles/visualization/BPlusTree.html)</p>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>组会学习记录</title>
    <url>/%E7%BB%84%E4%BC%9A%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<p>大二小白科研学习的亿天</p>
<h2 id="2025-4-20">2025/4/20</h2>
<h3 id="zps学长">zps学长</h3>
<p>ppt套了学校的模板</p>
<p>讲了transformer</p>
<p>patch：在训练深度学习模型时，通常不会将整个数据集一次性输入到模型中进行处理，而是将数据集分成若干个小的子集，这些子集就被称为 “batch”（批次）。例如，有一个包含 1000 张图片的数据集，若将其分成每个批次包含 100 张图片的小批次，那么就可以得到 10 个 “batch”。</p>
<p>backbound：“backbone” 指的是模型的核心主干部分，主要用于特征提取</p>
<ul>
<li><strong>特征提取</strong>：backbone 通常由多个卷积层、池化层和全连接层等组成，能够对输入数据进行逐步的特征提取和抽象。例如，在处理图像数据时，它可以从原始图像中提取出边缘、纹理、颜色等低级特征，并逐渐组合成更高级、更抽象的语义特征，如物体的部分、整体结构以及类别等信息。像 VGGNet 中的连续卷积层，通过不断地卷积操作来提取图像的不同层次特征。</li>
<li><strong>模型基础</strong>：它是构建整个深度学习模型的基础架构。许多复杂的深度学习模型都是在特定的 backbone 基础上进行修改、扩展或添加其他模块而形成的。例如，在目标检测模型 Faster R-CNN 中，使用了 ResNet 作为 backbone 来提取图像特征，然后在其基础上添加了区域提议网络（RPN）和检测头来实现目标检测功能。</li>
<li><strong>可迁移性</strong>：预训练好的 backbone 具有很强的可迁移性。由于在大规模数据集上进行了训练，学习到了通用的特征表示，因此可以将其应用于不同的但相关的任务中。比如，在图像分类任务中预训练好的 ResNet 模型，可以通过微调的方式应用于图像检索、目标跟踪等其他视觉任务中，往往能够在新任务上取得较好的性能，减少了在新任务上的训练数据量和训练时间。</li>
<li><strong>决定模型性能</strong>：backbone 的性能很大程度上影响着整个模型的性能。一个好的 backbone 能够提取到更有代表性、更具区分度的特征，从而为后续的任务特定模块提供更好的基础，有助于提高模型的准确性、泛化能力等性能指标。例如，ResNet 引入残差连接，解决了深层网络训练中的梯度消失和退化问题，使得训练极深的网络成为可能，从而提升了模型对复杂图像特征的提取能力，在各种图像任务中取得了显著的性能提升。</li>
</ul>
<h3 id="yzh学长">yzh学长</h3>
<p>AI4Science 即 “AI for Science”，是指将人工智能技术应用于科学研究领域，以加速科学发现和解决科学问题。</p>
<p>AI4Science 融合了机器学习、深度学习等人工智能方法与自然科学各个领域的知识和数据，旨在利用人工智能的强大能力，如数据挖掘、模型构建、预测分析等，来处理科学研究中的复杂问题。例如，在分子和材料计算方面开发快速准确的量子化学模拟工具，通过机器学习力场模型研究材料动力学和特性；在生命科学领域进行蛋白质 - 药物复合物结构和相互作用预测、生物分子的有效动力学模拟和蛋白质功能状态预测等。</p>
<p>它代表了科学发现的一个新方向，有望成为继经验观察、理论模型、数值计算和数据密集型科学发现之后的 “第五范式”。AI4Science 不仅促进了 AI 与各种科学学科的协同，还搭建了不同科学子领域间的桥梁，有助于解决超越单一学科的大规模挑战。</p>
<p>先验知识（比如ai4science领域）：；</p>
<ul>
<li><strong>领域科学理论</strong>：如物理学中的牛顿运动定律、麦克斯韦电动力学方程，化学中的量子化学理论，生物学中的进化论等。这些理论经过长期的科学研究和实践验证，是对自然现象的高度概括和总结，为 AI 模型提供了基本的原理和规律依据。例如，在 AI 模拟分子动力学时，量子化学理论中的薛定谔方程可作为先验知识，帮助模型理解分子和物质的行为。</li>
<li><strong>专家经验</strong>：领域专家在长期的研究和实践中积累了丰富的经验，这些经验可以转化为 AI 模型的先验知识。比如，材料科学家根据自己的实验经验，知道某些材料的特定属性与结构之间的关系，将这些经验融入 AI 模型，有助于模型更准确地预测材料的性能。</li>
<li><strong>历史数据</strong>：以往的科学实验、观测数据中蕴含着大量的信息。通过对这些数据的分析和总结，可以提取出有价值的先验知识。例如，在气候科学中，长期的气象观测数据可以帮助 AI 模型了解气候变化的规律和趋势。</li>
</ul>
<p>先验知识融入模型比较难，主要是因为存在知识表示、与模型架构融合、平衡调整及验证评估方面的挑战：</p>
<ul>
<li>知识表示与转化难题
<ul>
<li><strong>复杂知识形式化</strong>：先验知识形式多样，包括科学理论、专家经验、行业规则等。很多知识难以用精确的数学或逻辑语言表示。例如，医学领域中医生对疾病的综合判断经验，涉及模糊的症状描述、患者个体差异等，将这类知识转化为模型可理解的数值或符号形式非常困难。</li>
<li><strong>知识粒度把握</strong>：确定先验知识的合适表示粒度是个难题。粒度太粗，可能丢失关键细节；粒度太细，又会使知识表示过于复杂，增加模型负担。以地理信息系统中对地形地貌的知识表示为例，如果粒度太粗，可能无法准确体现地形对水流、气候等的影响；而粒度太细，数据量过大，会使模型训练和推理效率低下。</li>
</ul>
</li>
<li>模型架构与融合方式的挑战
<ul>
<li><strong>架构适配问题</strong>：不同的 AI 模型架构具有不同的特点和适用场景，将先验知识融入时需要考虑其与模型架构的兼容性。例如，循环神经网络（RNN）适用于处理序列数据，而将一些关于空间结构的先验知识融入 RNN 就比较困难，因为 RNN 的结构不太适合直接处理空间信息。</li>
<li><strong>融合方式选择</strong>：先验知识融入模型的方式有多种，如修改模型结构、调整损失函数、作为额外输入等，选择合适的融合方式并非易事。不同的融合方式对模型性能的影响差异较大，且缺乏通用的选择准则。例如，在图像识别模型中，将图像的一些先验知识（如物体的几何约束）通过修改模型结构来融入，可能会使模型变得复杂，难以训练；而通过调整损失函数来融入，又可能面临损失函数设计不合理，导致模型无法有效学习的问题。</li>
</ul>
</li>
<li>平衡与调整的复杂性
<ul>
<li><strong>知识与数据平衡</strong>：融入先验知识时，需要在知识和已有数据之间找到平衡。如果先验知识的权重过高，可能会使模型过于依赖先验知识，忽略数据中的实际信息，导致模型在新数据上的泛化能力下降；反之，如果先验知识的权重过低，又无法充分发挥先验知识的作用。例如，在预测股票价格时，先验的经济理论知识和历史股票数据都很重要，若过度依赖先验知识，可能无法准确捕捉市场的短期波动；而过度依赖历史数据，又可能忽略宏观经济环境等因素对股票价格的影响。</li>
<li><strong>动态调整困难</strong>：在模型训练和应用过程中，数据和问题场景可能会发生变化，这就需要动态调整先验知识的融入方式和权重。然而，实现动态调整非常困难，需要设计复杂的机制来监测数据和模型的变化，并相应地调整先验知识的作用。例如，在疾病诊断模型中，随着医学研究的进展和新的疾病特征出现，先验知识需要不断更新和调整，以保证模型的准确性和有效性，但这涉及到对知识的重新表示、融合方式的改变以及权重的重新分配等一系列复杂问题。</li>
</ul>
</li>
<li>验证与评估的不确定性
<ul>
<li><strong>评估指标局限</strong>：现有的模型评估指标如准确率、召回率等，可能无法全面准确地评估先验知识融入后的模型性能。因为这些指标主要关注模型对已知数据的拟合和预测能力，而对于先验知识是否正确融入、是否提高了模型的可解释性和泛化能力等方面的评估不够充分。例如，在一个基于 AI 的法律判决预测模型中，融入法律条文和法律原则等先验知识后，虽然模型的预测准确率可能有所提高，但对于模型是否真正理解了法律知识并基于此进行合理的判决预测，仅靠传统的评估指标难以判断。</li>
<li><strong>因果关系难以确定</strong>：融入先验知识后模型性能的提升，很难直接归因于先验知识的正确融入。可能存在其他因素影响模型性能，如数据的变化、模型参数的随机初始化等。要确定先验知识与模型性能提升之间的因果关系，需要进行严格的实验设计和分析，这在实际应用中往往具有很大的挑战性。例如，在一个基于深度学习的药物研发模型中，融入药物化学的先验知识后，模型发现了新的潜在药物分子，但很难确定这是由于先验知识的融入还是模型本身的学习能力导致的。</li>
</ul>
</li>
</ul>
<h2 id="cey学长">cey学长</h2>
<p>强化学习基础概念</p>
<p>Environment,agent,action,observation,reward,return,episode</p>
<blockquote>
<p>Environment：环境，智能体交互的外部系统</p>
<p>agent：智能体，做决策的主体</p>
<p>action：智能体可执行的动作集合</p>
<p>observation：智能体从环境中接收到的状态信息</p>
<p>reward：环境给予智能体的即时反馈信号</p>
<p>return：累积奖励，通常是折扣奖励和$R_t=\sum{y^i\times r_{t+i}}$</p>
<p>episode：一次完整的交互序列，从初始状态到终止状态</p>
</blockquote>
<p>强化学习的核心就是智能体与环境交互，并从中学习最优策略</p>
<p>agent通过观察环境状态，基于当前策略选择动作，环境接受动作后反馈新的状态和奖励</p>
<p>智能体的目标是最大化累积奖励，找到最优策略$\pi^*$</p>
<p>TD Learning时序差分学习</p>
<p>Q-learning</p>
<p>可以在ppt中插入滚动条代码块演示内容</p>
]]></content>
  </entry>
</search>
