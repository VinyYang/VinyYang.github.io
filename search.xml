<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法基础课(持续更新中)</title>
    <url>/algorithm_learning/</url>
    <content><![CDATA[<p>算法基础课</p>
<h2 id="基础算法（一）"><a href="#基础算法（一）" class="headerlink" title="基础算法（一）"></a>基础算法（一）</h2><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> i=l<span class="number">-1</span>,j=r<span class="number">+1</span>,x=q[l+r&gt;&gt;<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;j)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">do</span> i++;<span class="keyword">while</span>(q[i]&lt;x);</span><br><span class="line">        <span class="keyword">do</span> j--;<span class="keyword">while</span>(q[j]&gt;x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">quick_sort</span>(q,l,j);</span><br><span class="line">    <span class="built_in">quick_sort</span>(q,j<span class="number">+1</span>,r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> mid=l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">merge_sort</span>(q,l,mid),<span class="built_in">merge_sort</span>(q,mid<span class="number">+1</span>,r)；</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> k=<span class="number">0</span>,i=l,j=mid<span class="number">+1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid&amp;&amp;j&lt;=r)</span><br><span class="line">        <span class="keyword">if</span>(q[i]&lt;=q[j])tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">else</span> tmp[k++]=q[j++];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid)tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">while</span>(j&lt;=r)tmp[k++]=q[j++];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i=l,j=<span class="number">0</span>;i&lt;=r;i++,j++)q[i]=tmp[j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="二分"><a href="#二分" class="headerlink" title="二分"></a>二分</h3><h4 id="整数二分"><a href="#整数二分" class="headerlink" title="整数二分"></a>整数二分</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_1</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;    <span class="comment">// check()判断mid是否满足性质</span></span><br><span class="line">        <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_2</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) l = mid;</span><br><span class="line">        <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="浮点数二分"><a href="#浮点数二分" class="headerlink" title="浮点数二分"></a>浮点数二分</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">double</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">bsearch_3</span><span class="params">(<span class="type">double</span> l, <span class="type">double</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> eps = <span class="number">1e-6</span>;   <span class="comment">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class="line">    <span class="keyword">while</span> (r - l &gt; eps)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">double</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;</span><br><span class="line">        <span class="keyword">else</span> l = mid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="基础算法（二）"><a href="#基础算法（二）" class="headerlink" title="基础算法（二）"></a>基础算法（二）</h2><h3 id="高精度"><a href="#高精度" class="headerlink" title="高精度"></a>高精度</h3><h4 id="高精度加法"><a href="#高精度加法" class="headerlink" title="高精度加法"></a>高精度加法</h4><p>本质上是模拟人进行列竖式加法的过程，即$a_1$+…+$a_n$+t，其中当$a$的退一位加起来不超过10则$t&#x3D;0$否则$t&#x3D;1$，以此类推</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt;<span class="built_in">add</span>(vector&lt;<span class="type">int</span>&gt;&amp;A,vector&lt;<span class="type">int</span>&gt;&amp;B)</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;C;</span><br><span class="line">    <span class="type">int</span> t=<span class="number">0</span>;<span class="comment">//存储每一位相加后的数，同时完成是否需要进位的考量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>()||i&lt;B.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i&lt;A.<span class="built_in">size</span>())t+=A[i];</span><br><span class="line">        <span class="keyword">if</span>(i&lt;B.<span class="built_in">size</span>())t+=B[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>(t%<span class="number">10</span>);</span><br><span class="line">        t/=<span class="number">10</span>;</span><br><span class="line">	&#125;    </span><br><span class="line">    <span class="keyword">if</span>(t)C.<span class="built_in">push_back</span>(<span class="number">1</span>);<span class="comment">//最后一个t代表最高位，因此等for循环运行完后再用判断看看进位情况，是的话就在vector后面再加一个1</span></span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string a,b;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;A,B;</span><br><span class="line">    cin&gt;&gt;a&gt;&gt;b;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=a.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)A.<span class="built_in">push_back</span>(a[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=b.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)B.<span class="built_in">push_back</span>(b[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line"><span class="comment">//A、B元素进栈都是逆序进栈，比如a=&#x27;123456&#x27;,那A=[6.5,4,3,2,1],目的是方便进位多了1的时候整体数组挪动空间不大，否则要是最高位在前边ta进位后面数组所有数字都要往后挪</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">auto</span> C=<span class="built_in">add</span>(A,B);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=C.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)cout&lt;&lt;C[i];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="vector相关知识点"><a href="#vector相关知识点" class="headerlink" title="vector相关知识点"></a>vector相关知识点</h5><ul>
<li><p><strong>vector定义</strong> <code>vector</code> 是 C++ 标准模板库（STL）中的一个容器，它可以看作是一个动态数组。它能存储一系列相同类型的元素，并且可以根据需要自动调整大小。使用 <code>vector</code> 需要包含 <code>&lt;vector&gt;</code> 头文件。</p>
</li>
<li><p><strong>vector数组相关操作</strong>（最后面）增加用<code>.push_back(一个数)</code>，（最后面）删除<code>.pop_back(一个数)</code>，指定位置插入<code>.insert(向量,指定位置)</code>，指定位置删除<code>.erase(向量，指定位置)</code>，清空<code>.clear()</code>，查看大小<code>.size()</code></p>
</li>
<li><p><strong>vector元素的索引和数组一样，也是从 0 开始存储的</strong></p>
</li>
<li><p><strong>vector和普通数组区别</strong></p>
<p>1.大小灵活性</p>
<ul>
<li><strong>普通数组</strong>：大小在定义时就必须确定，且在程序运行过程中不能改变。例如 <code>int arr[10];</code> 定义了一个大小为 10 的整数数组，之后无法再改变其大小。</li>
<li><strong>vector</strong>：可以动态调整大小。可以使用 <code>push_back()</code> 方法在末尾添加元素，当空间不足时，<code>vector</code> 会自动分配更大的内存空间来存储元素。</li>
</ul>
<p>2.内存管理</p>
<ul>
<li><strong>普通数组</strong>：由程序员手动管理内存。如果数组在栈上分配，其生命周期受限于所在的代码块；如果在堆上分配（使用 <code>new</code>），则需要手动使用 <code>delete</code> 释放内存，否则会造成内存泄漏。</li>
<li><strong>vector</strong>：自动管理内存。当 <code>vector</code> 不再使用时，其占用的内存会自动释放，无需程序员手动干预。</li>
</ul>
<p>3.功能丰富度</p>
<ul>
<li><strong>普通数组</strong>：只提供基本的元素访问功能，操作相对有限。</li>
<li><strong>vector</strong>：提供了丰富的成员函数，如 <code>size()</code> 获取元素数量、<code>empty()</code> 判断是否为空、<code>clear()</code> 清空元素等，使用起来更加方便。</li>
</ul>
</li>
<li><p><strong>使用 vector 而不用数组的原因</strong></p>
<p>在大整数相加的场景中，输入的数字长度是不确定的。如果使用普通数组，需要预先定义一个足够大的数组来存储数字的每一位，但这样可能会浪费大量的内存空间。而 <code>vector</code> 可以根据输入数字的实际长度动态调整大小，避免了内存的浪费，并且其提供的 <code>push_back()</code> 方法可以方便地将数字的每一位添加到容器中，同时在处理进位时也更加方便。因此，使用 <code>vector</code> 能更好地适应这种动态长度的需求。</p>
</li>
</ul>
<h5 id="add函数运行示例"><a href="#add函数运行示例" class="headerlink" title="add函数运行示例"></a>add函数运行示例</h5><p>假设我们要计算两个四位数 <code>a = 1234</code> 和 <code>b = 5678</code> 的和。</p>
<p>在代码中，输入的数字以字符串形式存储，然后将其逆序存储到 <code>vector&lt;int&gt;</code> 中，这样做是为了方便处理进位。对于 <code>a = 1234</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>A = &#123;4, 3, 2, 1&#125;</code>；对于 <code>b = 5678</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>B = &#123;8, 7, 6, 5&#125;</code>。</p>
<p>具体步骤：</p>
<ul>
<li><p><input checked="" disabled="" type="checkbox"> 
初始化</p>
</li>
<li><p><code>vector&lt;int&gt; C;</code>：用于存储相加结果的向量，初始为空。</p>
</li>
<li><p><code>int t = 0;</code>：用于存储每一位相加后的结果以及进位信息，初始值为 0。</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
循环处理每一位</p>
</li>
</ul>
<p>循环条件为 <code>i &lt; A.size() || i &lt; B.size()</code>，这意味着只要 <code>A</code> 或 <code>B</code> 还有未处理的位，就会继续循环。</p>
<ul>
<li><p><input disabled="" type="checkbox"> 
第一次循环（<code>i = 0</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：因为 <code>i = 0</code> 小于 <code>A.size()</code>（4），所以 <code>t = t + A[0] = 0 + 4 = 4</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：因为 <code>i = 0</code> 小于 <code>B.size()</code>（4），所以 <code>t = t + B[0] = 4 + 8 = 12</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 2）添加到 <code>C</code> 中，此时 <code>C = &#123;2&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第二次循环（<code>i = 1</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[1] = 1 + 3 = 4</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[1] = 4 + 7 = 11</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 1）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第三次循环（<code>i = 2</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[2] = 1 + 2 = 3</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[2] = 3 + 6 = 9</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 9）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第四次循环（<code>i = 3</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[3] = 0 + 1 = 1</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[3] = 1 + 5 = 6</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 6）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9, 6&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
处理最后可能的进位</p>
</li>
</ul>
<p><code>if(t) C.push_back(1);</code></p>
<p>因为此时 <code>t = 0</code>，所以不需要添加额外的进位，<code>C</code> 仍然为 <code>&#123;2, 1, 9, 6&#125;</code>。</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> 返回结果</li>
</ul>
<p><code>return C;</code>：返回存储相加结果的向量 <code>C</code>。</p>
<p>在 <code>main</code> 函数中，将 <code>C</code> 逆序输出，得到最终结果 <code>6912</code>，即 <code>1234 + 5678 = 6912</code>。</p>
<h5 id="auto的用法"><a href="#auto的用法" class="headerlink" title="auto的用法"></a>auto的用法</h5><p><code>auto</code>是C++11的新特性，可以自动识别变量属性。比如<code>auto C=add(A,B);</code>就自动识别了add返回的类型，因此<code>auto C &lt;=&gt; vector&lt;int&gt;c</code></p>
<h5 id="a-i-0-的作用：将字符串每一位由ASCII码转为整数"><a href="#a-i-0-的作用：将字符串每一位由ASCII码转为整数" class="headerlink" title="a[i] - &#39;0&#39; 的作用：将字符串每一位由ASCII码转为整数"></a><code>a[i] - &#39;0&#39;</code> 的作用：将字符串每一位由ASCII码转为整数</h5><p>在 C++ 中，当你从输入读取一个数字字符串时，比如 <code>&quot;123&quot;</code>，字符串中的每个字符（如 <code>&#39;1&#39;</code>、<code>&#39;2&#39;</code>、<code>&#39;3&#39;</code>）实际上存储的是字符的 ASCII 码值，而不是对应的数值。字符 <code>&#39;0&#39;</code> 到 <code>&#39;9&#39;</code> 的 ASCII 码是连续的，<code>&#39;0&#39;</code> 的 ASCII 码值是 48，<code>&#39;1&#39;</code> 是 49，以此类推，<code>&#39;9&#39;</code> 是 57。</p>
<p><code>b[i] - &#39;0&#39;</code> 的作用就是将字符形式的数字转换为对应的整数值。例如，当 <code>b[i]</code> 为 <code>&#39;1&#39;</code> 时，<code>&#39;1&#39;</code> 的 ASCII 码值是 49，<code>&#39;0&#39;</code> 的 ASCII 码值是 48，那么 <code>&#39;1&#39; - &#39;0&#39;</code> 就等于 <code>49 - 48 = 1</code>，这样就把字符 <code>&#39;1&#39;</code> 转换为了整数 1。</p>
<p>如果没有 <code>b[i] - &#39;0&#39;</code> 这个操作，直接将字符存储到 <code>vector&lt;int&gt;</code> 中，那么存储的是字符的 ASCII 码值，而不是对应的数值，这会导致后续的计算出现错误。</p>
<h4 id="高精度减法"><a href="#高精度减法" class="headerlink" title="高精度减法"></a>高精度减法</h4><h4 id="高精度乘法"><a href="#高精度乘法" class="headerlink" title="高精度乘法"></a>高精度乘法</h4><h4 id="高精度除法"><a href="#高精度除法" class="headerlink" title="高精度除法"></a>高精度除法</h4>]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>ACWing</tag>
      </tags>
  </entry>
  <entry>
    <title>如何用hexo新建文件并上传/如何用hexo插入图片（next主题）</title>
    <url>/hexo_maintanance/</url>
    <content><![CDATA[<p>hexo（博主采用next主题）日常维护教程</p>
<h1 id="修改scaffolds-post-md（默认标题文件）"><a href="#修改scaffolds-post-md（默认标题文件）" class="headerlink" title="修改scaffolds&#x2F;post.md（默认标题文件）"></a>修改scaffolds&#x2F;post.md（默认标题文件）</h1><p>注意修改时不要把中文加进去，在此只起到注释作用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:([1,2]设置同时属于不同类别可以这样)</span><br><span class="line">categories:</span><br><span class="line">top:(表示置顶情况，不置顶不填即可数字大小代表置顶顺序，数字越大排序越前)</span><br></pre></td></tr></table></figure>



<h1 id="新建md文件"><a href="#新建md文件" class="headerlink" title="新建md文件"></a>新建md文件</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new/hexo n &quot;文件名&quot;</span><br></pre></td></tr></table></figure>

<h1 id="查看新建文件"><a href="#查看新建文件" class="headerlink" title="查看新建文件"></a>查看新建文件</h1><p>进入目录是source&#x2F;_posts</p>
<h1 id="完善标题对应信息，填写md"><a href="#完善标题对应信息，填写md" class="headerlink" title="完善标题对应信息，填写md"></a>完善标题对应信息，填写md</h1><p>也就是刚才上面那堆东西</p>
<h1 id="在blog目录下（因人而异）打开git-bash"><a href="#在blog目录下（因人而异）打开git-bash" class="headerlink" title="在blog目录下（因人而异）打开git bash"></a>在blog目录下（因人而异）打开git bash</h1><p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure>

<p>即可上传</p>
<p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>

<p>即可实时查看网页</p>
<h1 id="插入图片操作（图片和md文件最好均为英文名）"><a href="#插入图片操作（图片和md文件最好均为英文名）" class="headerlink" title="插入图片操作（图片和md文件最好均为英文名）"></a>插入图片操作（图片和md文件最好均为英文名）</h1><h2 id="下插件"><a href="#下插件" class="headerlink" title="下插件"></a>下插件</h2><p>见<a href="https://github.com/yiyungent/hexo-asset-img">yiyungent&#x2F;hexo-asset-img：🍰 Hexo 本地图片插件。|Hexo 本地图片插件：转换图片相对路径为asset_img</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-img --save</span><br><span class="line">或者</span><br><span class="line">npm install git://github.com/yiyungent/hexo-asset-img.git#main</span><br></pre></td></tr></table></figure>

<h2 id="修改host-config-yml"><a href="#修改host-config-yml" class="headerlink" title="修改host&#x2F;_config.yml"></a>修改host&#x2F;_config.yml</h2><p>permalink控制了永久域名的样式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.post_asset_folder: true</span><br><span class="line">2.permalink: :title/（我的自带了日期导致图片一直不行:year/:month/:date/:title/）</span><br></pre></td></tr></table></figure>

<h2 id="直接粘贴图片"><a href="#直接粘贴图片" class="headerlink" title="直接粘贴图片"></a>直接粘贴图片</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![1](./hexo_maintanance/1.png)</span><br></pre></td></tr></table></figure>

<p>上面是下面图片的路径。能看到下面的图片就能说明这个方法就是成功的。</p>
<p><img src="/./hexo_maintanance/1.png" alt="1"></p>
<h1 id="实现侧边栏标题全展开"><a href="#实现侧边栏标题全展开" class="headerlink" title="实现侧边栏标题全展开"></a>实现侧边栏标题全展开</h1><p>有些文件目录很长，不全展开不方便看。可以修改</p>
<p><code>blog\themes\next\source\css\_common\outline\sidebar\sidebar-toc.styl</code>文件</p>
<p>查找修改<code>.nav-child</code>对应代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">.<span class="property">nav</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (not hexo-<span class="title function_">config</span>(<span class="string">&#x27;toc.expand_all&#x27;</span>)) &#123;</span><br><span class="line">    .<span class="property">nav</span>-child &#123;</span><br><span class="line">      --<span class="attr">height</span>: auto;          <span class="comment">/* 取消高度限制 */</span></span><br><span class="line">      <span class="attr">height</span>: auto;            <span class="comment">/* 启用自动高度适应内容 */</span></span><br><span class="line">      <span class="attr">opacity</span>: <span class="number">1</span>;              <span class="comment">/* 取消透明度隐藏 */</span></span><br><span class="line">      <span class="attr">overflow</span>: visible;       <span class="comment">/* 允许内容溢出显示 */</span></span><br><span class="line">      transition-<span class="attr">property</span>: opacity;  <span class="comment">/* 仅保留透明度过渡 */</span></span><br><span class="line">      <span class="attr">visibility</span>: visible;     <span class="comment">/* 确保元素可见 */</span></span><br><span class="line">      <span class="attr">transition</span>: $transition-ease;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>原配置为：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">原配置--<span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">opacity</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">overflow</span>: hidden;</span><br><span class="line">transition-<span class="attr">property</span>: height, opacity, visibility;</span><br><span class="line"><span class="attr">transition</span>: $transition-ease;</span><br><span class="line"><span class="attr">visibility</span>: hidden;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>博客维护</category>
      </categories>
      <tags>
        <tag>博客维护</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕论文知识库</title>
    <url>/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/</url>
    <content><![CDATA[<p>手撕论文知识库</p>
<h2 id="深度学习项目代码目录全览及解析"><a href="#深度学习项目代码目录全览及解析" class="headerlink" title="深度学习项目代码目录全览及解析"></a>深度学习项目代码目录全览及解析</h2><blockquote>
<p>常见目录如下：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">project_name/</span><br><span class="line">├── data/                   # 数据集相关（原始/处理后的数据）</span><br><span class="line">├── dataloader/             # 数据加载与预处理模块（核心）</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── dataset.py          # 自定义Dataset类</span><br><span class="line">│   └── transforms.py       # 数据增强操作</span><br><span class="line">├── models/                 # 模型定义（核心）</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── backbone.py         # 主干网络</span><br><span class="line">│   └── layers.py           # 自定义网络层</span><br><span class="line">├── configs/                # 超参数配置文件（如YAML/JSON）</span><br><span class="line">├── utils/                  # 工具函数（如日志、评估指标）</span><br><span class="line">│   ├── data_utils.py</span><br><span class="line">│   ├── model_utils.py</span><br><span class="line">│   ├── visualization_utils.py</span><br><span class="line">│   └──...</span><br><span class="line">├── logs/                   # 记录训练和评估过程中的日志信息</span><br><span class="line">│   ├── training.log</span><br><span class="line">│   ├── validation.log</span><br><span class="line">│   └──...</span><br><span class="line">├── checkpoints/            # 训练保存的模型权重</span><br><span class="line">├── scripts/                # 运行脚本（训练/测试命令）</span><br><span class="line">├── requirements.txt        # 依赖库列表</span><br><span class="line">├── environment.yml         # Conda环境配置</span><br><span class="line">├── README.md               # 项目说明</span><br><span class="line">└── main.py                 # 主程序入口</span><br></pre></td></tr></table></figure>

<blockquote>
<ol>
<li><code>dataloader/</code></li>
</ol>
<p> <strong>作用</strong>：数据加载、预处理、增强（如论文项目中的 <code>with_colmap.py</code> 可能与多视图数据对齐相关）</p>
<p> <strong>典型内容</strong>：<code>Dataset</code> 类定义、数据增强函数、特征提取工具（如项目中的 <code>with_feature.py</code>）</p>
<ol start="2">
<li><strong><code>models/</code></strong></li>
</ol>
<p> <strong>作用</strong>：定义神经网络模型（如项目中的<code>nerf_models.py</code> 可实现NeRF的核心架构）。</p>
<p> <strong>典型内容</strong>：模型类继承<code>torch.nn.Module</code>，包含前向传播逻辑（如项目中的<code>depth_decoder.py</code>可用于深度估计解码）。</p>
<ol start="3">
<li><strong><code>utils/</code></strong></li>
</ol>
<p> <strong>作用</strong>：辅助工具（如项目中的 <code>pose_utils.py</code> 处理相机位姿，<code>training_utils.py</code> 封装训练逻辑）。</p>
<p> <strong>典型内容</strong>：评估指标计算、可视化工具、训练回调函数。</p>
<ol start="4">
<li><strong><code>third_party/</code></strong></li>
</ol>
<p> <strong>作用</strong>：第三方库或工具（如项目中的 <code>ATE</code> 可能用于轨迹评估，<code>pytorch_ssim</code> 实现结构相似性损失）。</p>
<ol start="5">
<li>其他关键要素</li>
</ol>
<p> <code>logs</code> 文件夹：记录训练和评估过程中的日志信息，如训练损失、验证损失、准确率等指标的变化情况，便于跟踪模型训练过程，分析模型的收敛性和性能表现。</p>
<p> <code>checkpoints</code> 文件夹：保存训练过程中的模型检查点，即模型在不同训练阶段的参数文件，用于在训练中断时恢复训练，或者用于选择在验证集上表现最好的模型进行测试和部署。</p>
<p> <code>requirement.txt</code>：列出项目所需的 Python 依赖库及其版本号，便于在新环境中快速安装项目所需的所有依赖。</p>
<p> <code>environment.yml</code>：Conda环境配置，确保依赖一致性。</p>
<p> <code>README.md</code>：项目说明、安装与使用指南（深度学习项目必备）。</p>
<p> <strong><code>main.py</code></strong>：项目的主程序入口，通常包含模型训练、评估和预测的主要逻辑，可通过命令行参数来控制程序的运行方式和参数设置。</p>
</blockquote>
<blockquote>
<p>示例如下：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ - nerf/  # 基于占用率的神经辐射场项目根目录</span><br><span class="line">├──.gitignore  # 指定Git不跟踪的文件或文件夹</span><br><span class="line">├── LICENSE  # 项目使用许可条款文件</span><br><span class="line">├── README.md  # 介绍项目背景、功能、使用方法等的说明文档</span><br><span class="line">├── environment.yml  # 定义项目运行所需软件环境</span><br><span class="line">├── local1.txt  # 用途不明，或为本地说明、配置等文件</span><br><span class="line">├── dataloader/  # 存放数据加载与预处理代码</span><br><span class="line">│   ├── any_folder.py  # 从任意文件夹结构加载数据</span><br><span class="line">│   ├── local_save.py  # 负责数据本地保存</span><br><span class="line">│   ├── with_colmap.py  # 从COLMAP处理后格式加载数据</span><br><span class="line">│   ├── with_feature.py  # 加载带特征的数据</span><br><span class="line">│   ├── with_feature_colmap.py  # 结合COLMAP与特征数据加载</span><br><span class="line">│   └── with_mask.py  # 加载带掩码的数据</span><br><span class="line">├── models/  # 存放深度学习模型定义与操作代码</span><br><span class="line">│   ├── depth_decoder.py  # 深度解码，用于深度估计</span><br><span class="line">│   ├── intrinsics.py  # 处理相机内参相关内容</span><br><span class="line">│   ├── layers.py  # 定义深度学习层结构</span><br><span class="line">│   ├── nerf_feature.py  # 处理NeRF特征相关逻辑</span><br><span class="line">│   ├── nerf_mask.py  # 处理NeRF模型中掩码相关内容</span><br><span class="line">│   ├── nerf_models.py  # 定义NeRF模型架构等核心内容</span><br><span class="line">│   └── poses.py  # 处理位姿相关操作</span><br><span class="line">├── utils/  # 包含辅助项目运行的工具函数</span><br><span class="line">│   ├── align_traj.py  # 实现轨迹对齐算法</span><br><span class="line">│   ├── comp_ate.py  # 计算绝对轨迹误差</span><br><span class="line">│   ├── comp_ray_dir.py  # 计算光线方向</span><br><span class="line">│   ├── lie_group_helper.py  # 提供李群相关辅助函数</span><br><span class="line">│   ├── pos_enc.py  # 实现位置编码</span><br><span class="line">│   ├── pose_utils.py  # 提供位姿相关实用工具函数</span><br><span class="line">│   ├── split_dataset.py  # 划分数据集为训练、验证、测试集</span><br><span class="line">│   ├── training_utils.py  # 提供模型训练辅助函数</span><br><span class="line">│   ├── vgg.py  # 与VGG神经网络相关操作</span><br><span class="line">│   ├── vis_cam_traj.py  # 可视化相机轨迹</span><br><span class="line">│   └── volume_op.py  # 操作三维体数据</span><br><span class="line">├── tasks/  # 存放训练、测试等具体任务代码</span><br><span class="line">│   └──...</span><br><span class="line">└── third_party/  # 存放第三方代码或库</span><br><span class="line">    ├── ATE/  # 与绝对轨迹误差计算相关</span><br><span class="line">    │   └── README.md  # 说明该部分功能与用法</span><br><span class="line">    └── pytorch_ssim/  # 计算结构相似性指数的库</span><br></pre></td></tr></table></figure>

<h2 id="深度学习-神经网络架构总览"><a href="#深度学习-神经网络架构总览" class="headerlink" title="深度学习&#x2F;神经网络架构总览"></a>深度学习&#x2F;神经网络架构总览</h2><h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><h3 id="什么是torch"><a href="#什么是torch" class="headerlink" title="什么是torch"></a>什么是torch</h3><p>Torch 是 PyTorch 深度学习框架的核心库，具备强大的功能与广泛的用途。它提供了丰富的张量操作，可在 CPU 或 GPU 上高效计算，能轻松处理各类数据；其自动求导机制极大简化了深度学习中梯度计算与反向传播的过程，让模型训练更为便捷。借助<code>torch.nn</code>模块可方便构建如 CNN、RNN 等复杂神经网络架构，<code>torch.optim</code>模块提供多种优化算法用于模型参数更新。此外，Torch 还支持预训练模型的使用与微调，结合可视化工具能助力监控训练过程，广泛应用于图像、自然语言处理、推荐系统等诸多领域。</p>
<h3 id="torch常用函数和功能"><a href="#torch常用函数和功能" class="headerlink" title="torch常用函数和功能"></a>torch常用函数和功能</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><h5 id="什么是张量"><a href="#什么是张量" class="headerlink" title="什么是张量"></a>什么是张量</h5><p>张量是多维数组的泛化表示，可理解为一个多维的数据容器，零维张量是标量，一维张量是向量，二维张量是矩阵，三维及以上则是更高阶的张量。在深度学习里，使用张量是因为它能够高效地表示和处理大量的数据，像图像可表示为三维张量（高度、宽度、通道数），视频可表示为四维张量（帧数、高度、宽度、通道数）。并且，深度学习框架（如 PyTorch）针对张量运算进行了高度优化，能利用 GPU 等硬件加速计算，张量还能自然地支持自动求导机制，方便进行模型训练时的梯度计算和参数更新。</p>
<p>张量是 PyTorch 中最基础的数据结构，类似于 NumPy 的多维数组，但它可以在 GPU 上进行加速计算，并且支持自动求导等深度学习所需的特性。</p>
<h5 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h5><p>维度（也称为轴）是指张量在某个方向上的延伸。可以将维度理解为数据组织的一个方向或一个层次，类似于在地理坐标系统中，经度和纬度分别代表了不同的方向，张量的每个维度也代表了数据的一个特定方向的排列。维度的数量被称为张量的阶（rank），零阶张量是标量（一个单独的数值），一阶张量是向量（一维数组），二阶张量是矩阵（二维数组），三阶及以上的张量则用于表示更复杂的数据结构。</p>
<p><strong>注：维度从0开始算起，比如对于二阶张量，维度0代表行，维度1代表列</strong></p>
<p><strong>另：维度排列遵循（$a_n$, $a_{n-1}$, …, $a_1$）的形式，数字越前代表越高维的堆叠。比如<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起</strong></p>
<p><strong>1.零阶张量（标量）</strong></p>
<p>零阶张量只有一个数值，它没有方向的概念，维度数量为 0。例如这里的 <code>scalar</code> 就是一个零阶张量，它代表一个单一的数值，不涉及方向或多个元素的排列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scalar = torch.tensor(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标量的维度数量:&quot;</span>, scalar.dim()) </span><br></pre></td></tr></table></figure>

<p><strong>2.一阶张量（向量）</strong></p>
<p>一阶张量可以看作是一个向量，它有一个维度。这个维度代表了向量中元素的排列方向，向量的长度就是这个维度的大小。例如<code>vector</code> 是一个一阶张量，维度数量为 1，该维度的大小为 4，表示向量中有 4 个元素。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">vector = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量的维度数量:&quot;</span>, vector.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量在该维度的大小:&quot;</span>, vector.size(<span class="number">0</span>)) </span><br></pre></td></tr></table></figure>

<p><strong>3.二阶张量（矩阵）</strong></p>
<p>二阶张量是一个矩阵，有两个维度，通常称为行和列。第一个维度代表矩阵的行方向，第二个维度代表矩阵的列方向。例如<code>matrix</code> 是一个 2 行 3 列的矩阵，第一个维度的大小为 2 表示有 2 行，第二个维度的大小为 3 表示有 3 列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">matrix = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵的维度数量:&quot;</span>, matrix.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第一个维度（行）的大小:&quot;</span>, matrix.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第二个维度（列）的大小:&quot;</span>, matrix.size(<span class="number">1</span>)) </span><br></pre></td></tr></table></figure>

<p><strong>4.高阶张量（图像、视频等）</strong></p>
<p>对于三阶及以上的张量，维度的含义更加丰富，通常与具体的数据类型和应用场景相关。</p>
<p><strong>图像数据</strong>：在处理图像时，通常使用三阶张量。例如，一张彩色图像可以表示为一个形状为 <code>(高度, 宽度, 通道数)</code> 的三阶张量。这里的第一个维度代表图像的高度方向，第二个维度代表图像的宽度方向，第三个维度代表图像的通道（如 RGB 三个通道）。</p>
<p><code>image = torch.randn(224, 224, 3)</code> 这行代码能够随机生成一个形状为 <code>(224, 224, 3)</code> 的张量来模拟图像的三通道数值。</p>
<p>由于 <code>torch.randn()</code> 生成的是服从标准正态分布的随机数，这些数值可能为负数，也可能超出了常见图像像素值的范围（通常是 0 - 255 或者 0 - 1）。在实际的图像处理任务中，如果需要模拟真实图像，可能需要对这些随机值进行进一步的处理，例如通过归一化或裁剪操作将其限制在合适的范围内。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">image = torch.randn(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像张量的维度数量:&quot;</span>, image.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像高度维度的大小:&quot;</span>, image.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像宽度维度的大小:&quot;</span>, image.size(<span class="number">1</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像通道维度的大小:&quot;</span>, image.size(<span class="number">2</span>)) </span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250219032451598.png" alt="image-20250219032451598"></p>
<p><strong>视频数据</strong>：视频可以看作是一系列的图像帧，因此可以用四阶张量表示，形状通常为 <code>(帧数, 高度, 宽度, 通道数)</code>。第一个维度代表视频中的帧数，其余维度与图像张量的含义相同。</p>
<p>*<strong>5.通道</strong></p>
<p>通道指图像中特定类型信息的集合，图像可含一个或多个通道，各通道存储图像某方面特征数据。像单通道存亮度，RGB 三通道分别存红、绿、蓝颜色信息，四通道还多了透明度通道，以此组合完整呈现图像。</p>
<p>通道能实现颜色表示与混合，如 RGB 三通道通过不同数值组合呈现丰富色彩；可用于特征提取与分析，不同通道提供不同特征，助力图像分析和目标识别；还能用于图像合成与特效制作，借助透明度通道可控制图像透明效果实现合成。</p>
<p>在图片里，灰度图用单通道呈现黑白影像；彩色照片靠 RGB 三通道展示多彩画面；PNG 图片利用四通道含透明度信息实现图像融合。视频是连续的图片帧，同样利用通道来呈现色彩、进行特效处理，如影视中常见的抠图合成场景就借助了通道特性。</p>
<h5 id="张量相关函数"><a href="#张量相关函数" class="headerlink" title="张量相关函数"></a>张量相关函数</h5><p><strong>1.创建</strong></p>
<ul>
<li><p>创建张量<code>torch.tensor()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>当你有现有的数据存储在 Python 列表或 NumPy 数组中，并且需要将其输入到 PyTorch 模型进行计算时使用。例如，在加载数据集后，将数据转换为张量形式以便后续处理。</p>
<p>从数据存储的角度来看，<code>tensor</code> 存储了 Python 列表 <code>data</code> 中的元素 <code>[1, 2, 3]</code>。它将这些数据以一种高效的、适合计算机处理的方式组织起来，存储在内存中。在这个例子中，<code>tensor</code> 是一个一维张量，形状为 <code>(3,)</code>，这意味着它包含 3 个元素。</p>
<p>在数学运算方面，<code>tensor</code> 可以参与各种数学运算，如加法、乘法、矩阵乘法等。PyTorch 为张量提供了丰富的数学运算函数，这些运算可以在 CPU 或 GPU 上高效执行。</p>
<p>在深度学习的上下文中，<code>tensor</code> 是模型输入、输出以及参数的基本表示形式。例如，在一个简单的全连接神经网络中，输入数据会被转换为张量输入到网络中，网络的权重和偏置也是以张量的形式存储和更新的。在上述例子中，<code>tensor</code> 可以作为一个简单的输入数据示例，如果要构建一个神经网络处理这个输入，可能会进行如下操作（以下是一个简单示例）：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 3，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(<span class="number">0</span>)  <span class="comment"># 转换为适合输入模型的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型输出:&quot;</span>, output)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>创建全零张量<code>torch.zeros()</code></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">zeros_tensor = torch.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>常用于初始化某些变量，如在初始化神经网络的偏置项时，可使用全零张量。另外，在需要填充零值进行数据预处理或占位时也会用到。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>传入的参数 <code>(2, 3)</code>对应创建的 <code>zeros_tensor</code> 是一个 2 行 3 列的二维张量。如果使用 <code>torch.zeros((2, 3, 4))</code> 这样的代码，那么创建的就是一个三维张量，其中 <code>2</code> 表示最外层维度的大小（可以想象成有 2 个二维矩阵堆叠在一起），<code>3</code> 表示每个二维矩阵的行数，<code>4</code> 表示每个二维矩阵的列数。依此类推，对于更高维的张量，每个数字都代表对应维度上的大小。相当于高是2，行是3，列是4</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>如果是<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起，数字越前就代表越高维的堆叠。</p>
<ul>
<li><p>创建全一张量**<code>torch.ones()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">ones_tensor = torch.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>与 <code>torch.zeros()</code> 类似，可用于初始化特定变量。在一些归一化操作或需要特定初始值为 1 的场景中会使用。</p>
<ul>
<li><p>创建指定形状的随机张量，元素值在[0 , 1)之间**<code>torch.rand()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">random_tensor = torch.rand((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>在初始化神经网络的权重时，随机初始化是常见的做法，可使用 <code>torch.rand()</code> 生成初始权重张量。</p>
<p><strong>2.操作</strong></p>
<ul>
<li><p><strong><code>torch.cat()</code></strong>：用于在指定维度上拼接多个张量。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">c = torch.cat((a, b), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>当需要将多个张量合并为一个更大的张量时使用。例如，在处理多模态数据时，将不同模态的特征张量拼接在一起。</p>
<p><code>torch.cat((a, b), dim=1)</code> 表示在维度 1（列方向）上对张量 <code>a</code> 和 <code>b</code> 进行拼接。可以看到，拼接后的张量 <code>c</code> 是将 <code>a</code> 和 <code>b</code> 的列进行了合并，行数不变，列数变为原来两个张量列数之和。在处理多模态数据时，比如一个模态的数据特征用张量 <code>a</code> 表示，另一个模态的数据特征用张量 <code>b</code> 表示，通过这种拼接操作可以将不同模态的特征合并在一起，方便后续的处理。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>

<p>在二维张量的语境下，维度 0 代表行方向。<code>torch.cat((a, b), dim=0)</code> 会将张量 <code>b</code> 按行的顺序拼接到张量 <code>a</code> 的下方，拼接后的张量列数不变，行数为原来两个张量行数之和。在实际应用中，若 <code>a</code> 和 <code>b</code> 分别表示两组样本数据，在维度 0 上拼接就相当于将这两组样本合并成一组更大的样本集。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">在维度 <span class="number">0</span> 上拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.reshape()</code></strong>：改变张量的形状。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([1, 2, 3, 4])</span><br><span class="line">reshaped_x = torch.reshape(x, (2, 2))</span><br></pre></td></tr></table></figure>

<p>在神经网络中，不同层之间的数据形状可能需要进行调整，使用 <code>torch.reshape()</code> 可以方便地改变张量形状以满足层的输入要求。</p>
<p><code>torch.reshape(x, (2, 2))</code> 是将一维张量 <code>x</code> 重塑为二维张量 <code>reshaped_x</code>，形状为 <code>(2, 2)</code>。在神经网络中，不同层之间的数据形状可能不匹配，例如某一层的输出是一维向量，而后续层需要二维矩阵作为输入，这时就可以使用 <code>torch.reshape()</code> 来调整数据的形状，使其满足层的输入要求。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">重塑后的张量 reshaped_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.transpose()</code></strong>：交换张量的两个维度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[1, 2], [3, 4]])</span><br><span class="line">transposed_x = torch.transpose(x, 0, 1)</span><br></pre></td></tr></table></figure>

<p>在矩阵运算中，有时需要对矩阵进行转置操作。在图像处理中，可能需要调整图像张量的维度顺序。</p>
<p><code>torch.transpose(x, 0, 1)</code> 表示交换张量 <code>x</code> 的第 0 维和第 1 维。在这个二维矩阵的例子中，就是对矩阵进行了转置操作，原来的行变成了列，列变成了行。在矩阵运算中，矩阵转置是一个常见的操作，例如在计算矩阵乘法时可能需要对矩阵进行转置。在图像处理中，图像张量的维度顺序可能需要调整，比如将 <code>(高度, 宽度, 通道数)</code> 调整为 <code>(通道数, 高度, 宽度)</code> 以适应某些模型的输入要求，这时就可以使用 <code>torch.transpose()</code> 来实现。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">转置后的张量 transposed_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.squeeze()</code>：移除张量中所有维度为 1 的轴（或指定轴）。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]])  <span class="comment"># 形状 [1, 3, 1]</span></span><br><span class="line">y = torch.squeeze(x)                 <span class="comment"># 形状变为 [3]</span></span><br></pre></td></tr></table></figure>

<p>在张量操作中，某些操作（如池化、索引）可能产生冗余的维度为 1 的轴。例如：</p>
<ul>
<li>处理单通道图像时，通道维度可能为 1。</li>
<li>批量处理单个样本时，批量维度可能为 1。</li>
<li>某些神经网络层的输出可能保留不必要的单维度。</li>
</ul>
<p><code>torch.squeeze()</code> 默认移除所有大小为 1 的维度，也可指定 <code>dim</code> 参数移除特定轴（仅当该轴大小为 1 时生效）。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">输入张量 x：</span><br><span class="line">tensor([[[<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>],</span><br><span class="line">         [<span class="number">3</span>]]])</span><br><span class="line">输出张量 y：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">squeeze() 移除了所有大小为 <span class="number">1</span> 的维度（第 <span class="number">0</span> 维和第 <span class="number">2</span> 维），仅保留第 <span class="number">1</span> 维（大小为 <span class="number">3</span>）。</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h4><h5 id="前向传播（Forward-Propagation）"><a href="#前向传播（Forward-Propagation）" class="headerlink" title="前向传播（Forward Propagation）"></a>前向传播（Forward Propagation）</h5><ol>
<li>定义</li>
</ol>
<p>前向传播是深度学习模型处理输入数据以产生输出的过程。在这个过程中，输入数据从输入层开始，依次经过神经网络的各个隐藏层，每层都会对输入进行特定的数学变换（如加权求和后通过激活函数），最终到达输出层得到预测结果。可以将其看作是信息从输入向输出流动的过程，每一层根据前一层的输出计算本层的输出，逐步传递直至得到最终输出。</p>
<ol start="2">
<li>作用</li>
</ol>
<p>根据当前模型的参数（权重和偏置）对输入数据进行预测。通过一系列的线性和非线性变换，模型能够学习到输入数据中的特征模式，并将其映射到输出空间。例如，在图像分类任务中，前向传播可以将输入的图像转换为不同类别的概率分布，从而判断图像所属的类别。它为后续的反向传播提供了预测结果，是整个深度学习训练过程的基础步骤。</p>
<h5 id="反向传播（Backward-Propagation）"><a href="#反向传播（Backward-Propagation）" class="headerlink" title="反向传播（Backward Propagation）"></a>反向传播（Backward Propagation）</h5><ol>
<li>定义</li>
</ol>
<p>反向传播是深度学习中用于计算损失函数（Loss Function）关于模型参数（权重和偏置）的梯度的算法。它基于链式法则，从输出层开始，将损失函数的误差沿着计算图反向传播到输入层，依次计算每一层参数的梯度。简单来说，反向传播是在已知前向传播得到的预测结果和真实标签的情况下，计算如何调整模型参数可以减小损失的过程。</p>
<ol start="2">
<li>作用</li>
</ol>
<p>为模型参数的更新提供依据。在深度学习中，通常使用优化算法（如随机梯度下降，Stochastic Gradient Descent，SGD）来更新模型的参数，而这些优化算法需要知道损失函数关于参数的梯度。反向传播通过高效地计算这些梯度，使得模型能够根据预测误差自动调整参数，从而不断优化模型的性能。通过多次迭代前向传播和反向传播，模型可以逐渐学习到输入数据和输出标签之间的映射关系，提高预测的准确性。</p>
<h5 id="计算图与梯度追踪（可略过）"><a href="#计算图与梯度追踪（可略过）" class="headerlink" title="计算图与梯度追踪（可略过）"></a>计算图与梯度追踪（可略过）</h5><ol>
<li><code>requires_grad</code>设置张量是否需要进行梯度追踪</li>
</ol>
<p>在深度学习模型训练过程中，需要计算损失函数关于模型参数的梯度，以便使用优化算法（如随机梯度下降）更新参数。通过将模型参数张量的 <code>requires_grad</code> 设置为 <code>True</code>，可以利用自动求导机制自动计算梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出 4.0</span></span><br></pre></td></tr></table></figure>

<p><code>requires_grad</code> 是 PyTorch 张量的一个属性，当将其设置为 <code>True</code> 时，PyTorch 会开始追踪该张量的所有操作，构建计算图。计算图是一个有向无环图，它记录了从输入张量到输出张量的所有操作路径。</p>
<p>在上述示例中，我们创建了一个张量 <code>x</code> 并将 <code>requires_grad</code> 设置为 <code>True</code>，然后定义了一个函数 <code>y = x ** 2</code>。PyTorch 会自动记录这个操作，构建相应的计算图。</p>
<p>调用 <code>y.backward()</code> 方法时，PyTorch 会根据链式法则沿着计算图反向传播，计算出 <code>y</code> 关于 <code>x</code> 的梯度，并将梯度存储在 <code>x.grad</code> 属性中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x 的梯度: tensor([4.])</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>grad_fn</code> 属性与反向传播链</li>
</ol>
<p><strong><code>grad_fn</code></strong>：PyTorch 张量的属性，指向创建该张量的函数对象，用于记录操作历史以支持反向传播求梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个需要梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义操作 y = x^2</span></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="comment"># 定义操作 z = y * 3</span></span><br><span class="line">z = y * <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个张量的 grad_fn</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y 的 grad_fn:&quot;</span>, y.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;z 的 grad_fn:&quot;</span>, z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># 打印 x 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x 的梯度:&quot;</span>, x.grad)</span><br></pre></td></tr></table></figure>

<p>在深度学习模型训练时，需要计算损失函数关于模型参数的梯度。由于模型中存在大量复杂的运算，通过 <code>grad_fn</code> 记录的操作历史，PyTorch 可以构建反向传播链，从而准确计算出梯度，为参数更新提供依据。</p>
<p>在上述代码中，首先创建了一个开启梯度追踪的张量 <code>x</code>。当执行 $y &#x3D; x^2$操作时，PyTorch 会创建一个表示平方操作的函数对象，<code>y.grad_fn</code> 就会指向这个函数对象，以此记录下 <code>y</code> 是由 <code>x</code> 经过平方操作得到的。接着执行 $z &#x3D; 3y$，同样地，<code>z.grad_fn</code> 会指向表示乘法操作的函数对象，记录 <code>z</code> 的计算来源。</p>
<p>当调用 <code>z.backward()</code> 时，PyTorch 会从 <code>z</code> 开始，依据 <code>z.grad_fn</code> 找到创建 <code>z</code> 的操作，然后通过这个操作回溯到 <code>y</code>，再根据 <code>y.grad_fn</code> 回溯到 <code>x</code>。在这个回溯过程中，按照链式法则逐步计算出 <code>z</code> 关于 <code>x</code> 的梯度，并将其存储在 <code>x.grad</code> 中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y 的 grad_fn: &lt;PowBackward0 object at 0x...&gt;</span><br><span class="line">z 的 grad_fn: &lt;MulBackward0 object at 0x...&gt;</span><br><span class="line">x 的梯度: tensor([12.])</span><br></pre></td></tr></table></figure>

<p><code>&lt;PowBackward0 object at 0x...&gt;</code> 是 PyTorch 中用于表示反向传播过程中特定操作的梯度计算函数对象的字符串表示形式。</p>
<p><strong>反向传播函数对象</strong>：在 PyTorch 的自动求导机制里，对张量进行各种运算（如加法、乘法、幂运算等）时，PyTorch 会构建一个计算图来记录这些操作的顺序和依赖关系。每个操作在计算图中都对应一个前向传播函数（用于计算输出结果）和一个反向传播函数（用于计算梯度）。</p>
<p><code>PowBackward0</code> 的意义：<code>PowBackward0</code> 表示的是幂运算的反向传播函数。执行 $y &#x3D; x^2$这样的幂运算时，<code>y</code> 的 <code>grad_fn</code> 属性就会指向一个 <code>PowBackward0</code> 对象，这个对象负责在反向传播过程中计算关于输入张量 <code>x</code> 的梯度。<strong><code>at 0x...</code></strong>：<code>at 0x...</code> 后面跟着的是该对象在内存中的地址。这个地址是系统为该对象分配的唯一标识符，用于在内存中定位该对象。</p>
<h5 id="梯度控制（可略过）"><a href="#梯度控制（可略过）" class="headerlink" title="梯度控制（可略过）"></a>梯度控制（可略过）</h5><p>1.<code>torch.no_grad()</code> 上下文管理器</p>
<p>用于临时禁止 PyTorch 的梯度计算功能，在其作用域内创建或操作的张量不会进行梯度追踪。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个需要梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 torch.no_grad() 上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y 是否进行梯度追踪:&quot;</span>, y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在上下文管理器外进行操作</span></span><br><span class="line">z = x ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;z 是否进行梯度追踪:&quot;</span>, z.requires_grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y 是否进行梯度追踪: False</span><br><span class="line">z 是否进行梯度追踪: True（z在上下文管理器外进行操作）</span><br></pre></td></tr></table></figure>

<p>2.<code>detach()</code> 分离计算图  </p>
<p><strong><code>detach()</code></strong>：用于将张量从当前计算图中分离，返回一个和原张量数据相同但不记录梯度信息、不参与反向传播的新张量。</p>
<ul>
<li>在模型评估阶段，只需得到预测结果，无需计算梯度，用 <code>detach()</code> 可节省内存和计算资源。</li>
<li>在多模型联合训练时，若某个模型输出用于另一模型但不影响自身梯度计算，可使用 <code>detach()</code> 分离。（核心还是节省内存和计算资源）</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建需梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line"><span class="comment"># 分离计算图</span></span><br><span class="line">y_detached = y.detach()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原张量 y 是否追踪梯度:&quot;</span>, y.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分离后的张量 y_detached 是否追踪梯度:&quot;</span>, y_detached.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    y_detached.backward()</span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;错误信息: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>上述代码里，<code>x</code> 开启梯度追踪，<code>y = 2 * x</code> 会记录在计算图中。调用 <code>y.detach()</code> 后得到 <code>y_detached</code>，它和 <code>y</code> 数据相同，但不追踪梯度。尝试对 <code>y_detached</code> 反向传播会报错，因为它已和计算图分离，无梯度信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原张量 y 是否追踪梯度: True</span><br><span class="line">分离后的张量 y_detached 是否追踪梯度: False</span><br><span class="line">错误信息: element 0 of tensors does not require grad and does not have a grad_fn</span><br></pre></td></tr></table></figure>

<p>3.<code>zero_grad()</code> (梯度清零) </p>
<p>在深度学习模型的训练过程中，通常会按批次（batch）输入数据进行训练。每次反向传播计算得到的梯度会累加到模型参数的 <code>.grad</code> 属性中。如果不进行梯度清零，那么下一次计算的梯度会与之前的梯度累加，导致梯度计算错误。因此，在每个批次训练开始前，需要调用 <code>zero_grad()</code> 方法将梯度清零，以确保每个批次的梯度计算是独立的。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">target = torch.randn(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(input_tensor)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播计算梯度</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;反向传播后第一个参数的梯度:&quot;</span>, model.weight.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度清零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度清零后第一个参数的梯度:&quot;</span>, model.weight.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">反向传播后第一个参数的梯度: tensor([[ 0.1234, -0.5678, ...]])</span><br><span class="line">梯度清零后第一个参数的梯度: tensor([[ 0., 0., ...]])</span><br></pre></td></tr></table></figure>

<h4 id="神经网络层"><a href="#神经网络层" class="headerlink" title="神经网络层"></a>神经网络层</h4><h5 id="核心基类-nn-Module"><a href="#核心基类-nn-Module" class="headerlink" title="核心基类 nn.Module"></a>核心基类 <code>nn.Module</code></h5><h6 id="什么是核心基类-nn-Module？"><a href="#什么是核心基类-nn-Module？" class="headerlink" title="什么是核心基类 nn.Module？"></a>什么是核心基类 <code>nn.Module</code>？</h6><p><strong><code>nn.Module</code></strong>：PyTorch 中所有神经网络模块的基类，用于构建自定义的神经网络模型，封装了模型的结构和参数，方便进行前向传播、参数管理和模型保存等操作。</p>
<p>在深度学习中，我们需要构建各种各样的神经网络模型，如卷积神经网络（CNN）、循环神经网络（RNN）等。<code>nn.Module</code> 提供了一个统一的框架，使得我们可以方便地定义和管理这些模型。无论是简单的全连接网络还是复杂的深度网络，都可以通过继承 <code>nn.Module</code> 来构建。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个简单的神经网络模型，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个全连接层，输入维度为 10，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 定义前向传播过程</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型结构：&quot;</span>, model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状：&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状：&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>__init__</code> 方法</strong>：在自定义的模型类中，<code>__init__</code> 方法用于初始化模型的各个层。通过调用 <code>super(SimpleNet, self).__init__()</code> 确保父类 <code>nn.Module</code> 的初始化被正确执行。然后定义了一个全连接层 <code>self.fc</code>。</li>
<li><strong><code>forward</code> 方法</strong>：<code>forward</code> 方法定义了模型的前向传播过程，即输入数据如何经过各个层得到输出。在这个例子中，输入数据 <code>x</code> 经过全连接层 <code>self.fc</code> 得到输出。</li>
<li>模型实例化和前向传播：创建 <code>SimpleNet</code> 的实例 <code>model</code> 后，将输入张量 <code>input_tensor</code> 传递给 <code>model</code> 就相当于调用了 <code>forward</code> 方法进行前向传播，得到输出 <code>output</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">模型结构： SimpleNet(</span><br><span class="line">  (fc): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">输入张量形状： torch.Size([1, 10])</span><br><span class="line">输出张量形状： torch.Size([1, 1])</span><br></pre></td></tr></table></figure>

<h6 id="自定义模型继承方法"><a href="#自定义模型继承方法" class="headerlink" title="自定义模型继承方法"></a>自定义模型继承方法</h6><ol>
<li>参数管理<code>parameters()</code>，<code>named_parameters()</code></li>
</ol>
<p><strong><code>parameters()</code></strong>：是 <code>nn.Module</code> 类的一个方法，它返回一个包含模型所有可学习参数的迭代器。通过遍历这个迭代器，能依次获取到模型中的各个参数张量，但不会提供参数的名称信息。</p>
<p><strong><code>named_parameters()</code></strong>：同样是 <code>nn.Module</code> 类的方法，它返回一个迭代器，该迭代器会生成模型可学习参数的名称和对应参数张量的元组。借助这个方法，我们不仅能获取参数张量，还能明确每个参数对应的名称，这在模型参数管理和调试时非常有用。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的神经网络模型，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个全连接层，输入维度为 10，输出维度为 5</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义另一个全连接层，输入维度为 5，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 parameters() 方法获取模型参数的迭代器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用 parameters() 获取参数：&quot;</span>)</span><br><span class="line">params = model.parameters()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数形状: <span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 named_parameters() 方法获取模型带名称的参数迭代器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用 named_parameters() 获取参数：&quot;</span>)</span><br><span class="line">named_params = model.named_parameters()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> named_params:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数名称: <span class="subst">&#123;name&#125;</span>, 参数形状: <span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong><code>parameters()</code></strong>：主要用于优化器初始化。在训练模型时，优化器（如 <code>torch.optim.SGD</code>、<code>torch.optim.Adam</code> 等）需要知道模型的可学习参数，以便对这些参数进行梯度更新。由于优化器只关心参数张量本身，不需要参数名称，所以使用 <code>parameters()</code> 即可。</p>
<p><strong><code>named_parameters()</code></strong>：在模型调试、参数分组优化或模型参数的选择性加载时非常有用。例如，你可能只想更新模型中某些特定层的参数，通过参数名称可以方便地筛选出这些参数；或者在加载预训练模型时，根据参数名称选择性地加载部分参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用 parameters() 获取参数：</span><br><span class="line">参数形状: torch.Size([5, 10])</span><br><span class="line">参数形状: torch.Size([5])</span><br><span class="line">参数形状: torch.Size([1, 5])</span><br><span class="line">参数形状: torch.Size([1])</span><br><span class="line"></span><br><span class="line">使用 named_parameters() 获取参数：</span><br><span class="line">参数名称: fc1.weight, 参数形状: torch.Size([5, 10])</span><br><span class="line">参数名称: fc1.bias, 参数形状: torch.Size([5])</span><br><span class="line">参数名称: fc2.weight, 参数形状: torch.Size([1, 5])</span><br><span class="line">参数名称: fc2.bias, 参数形状: torch.Size([1])</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>train()</code> 与 <code>eval()</code> 模式切换</li>
</ol>
<p><code>train()</code> 和 <code>eval()</code> 是 <code>nn.Module</code> 类中的方法，用于切换模型的训练和评估模式。<code>train()</code> 方法将模型设置为训练模式，<code>eval()</code> 方法将模型设置为评估模式，不同模式下部分层（如 <code>Dropout</code>、<code>BatchNorm</code>）会有不同的行为。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个包含 Dropout 层的简单神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置为训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式下 Dropout 是否启用:&quot;</span>, model.dropout.training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置为评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式下 Dropout 是否启用:&quot;</span>, model.dropout.training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output_eval = model(input_tensor)</span><br><span class="line">model.train()</span><br><span class="line">output_train = model(input_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式输出:&quot;</span>, output_eval)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式输出:&quot;</span>, output_train)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>训练模式（<code>train()</code>）</strong>：在模型训练阶段使用，此时模型中的 <code>Dropout</code> 层会按照设定的概率随机丢弃部分神经元，<code>BatchNorm</code> 层会根据当前批次的数据更新统计信息（如均值和方差），有助于提高模型的泛化能力。</li>
<li><strong>评估模式（<code>eval()</code>）</strong>：在模型评估、测试或者推理阶段使用。在评估模式下，<code>Dropout</code> 层不再丢弃神经元，<code>BatchNorm</code> 层使用训练阶段统计得到的均值和方差进行归一化操作，保证评估结果的稳定性和一致性。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练模式下 Dropout 是否启用: True</span><br><span class="line">评估模式下 Dropout 是否启用: False</span><br><span class="line">评估模式输出: tensor([[...]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">训练模式输出: tensor([[...]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>这里具体的输出数值会因随机生成的输入数据和模型初始化不同而有所变化，但可以看到在不同模式下 <code>Dropout</code> 层的行为不同，导致输出结果也可能不同。</p>
<h5 id="网络层-Layers"><a href="#网络层-Layers" class="headerlink" title="网络层 (Layers)"></a>网络层 (Layers)</h5><h6 id="基础层"><a href="#基础层" class="headerlink" title="基础层"></a>基础层</h6><ol>
<li><code>nn.Linear</code>全连接层</li>
</ol>
<p><strong><code>nn.Linear</code></strong>：PyTorch 中用于创建全连接层（也称为线性层）的类，它对输入数据进行线性变换，即执行矩阵乘法和加法操作，可用于构建各种神经网络模型。</p>
<p><strong>神经网络基础构建</strong>：全连接层是神经网络中最基本的组成部分之一，常用于多层感知机（MLP）的构建，可处理各种类型的数据，如图像、文本等特征向量。</p>
<p><strong>特征映射</strong>：可以将输入数据从一个特征空间映射到另一个特征空间，有助于模型学习数据中的复杂模式和特征表示。</p>
<hr>
<p>为什么是全连接层？</p>
<p>1.连接方式：在全连接层中，每一个输入神经元都与每一个输出神经元相连接，这种连接是 “全” 的，即完全连接。对于 </p>
<p><code>nn.Linear(in_features, out_features)</code> ，输入的 <code>in_features</code> 个神经元和输出的 <code>out_features</code> 个神经元之间存在着完整的连接关系。</p>
<p>2.数学运算：假设输入向量$X$维度为$n$（即 <code>in_features</code>），输出向量$Y$维度为$m$（即 <code>out_features</code>），全连接层通过权重矩阵$W$（形状为$m × n$）和偏置向量$b$（形状为$m$）进行线性变换$Y&#x3D;WX+b$。这里的权重矩阵$W$描述了输入神经元和输出神经元之间所有可能的连接强度，每一个输入元素都会影响到每一个输出元素的计算结果，这种全面的连接关系是 “全连接” 概念的核心体现。</p>
<p>3.网络结构对比：在神经网络中，除了全连接层，还有<strong>其他类型的层</strong>，比如卷积层、池化层等。卷积层中，神经元只与输入数据的局部区域相连接，而<strong>不是像全连接层那样与所有输入神经元连接</strong>；池化层则主要进行下采样操作，不存在像全连接层这样全面的神经元连接模式。因此，为了突出这种所有输入和输出神经元之间都有连接的特殊结构，将其称为全连接层，以便和其他类型的层进行区分。</p>
<hr>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个全连接层，输入维度为 10，输出维度为 5</span></span><br><span class="line">linear_layer = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据，假设有 1 个样本，每个样本有 10 个特征</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = linear_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全连接层的权重形状:&quot;</span>, linear_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全连接层的偏置形状:&quot;</span>, linear_layer.bias.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量的形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量的形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>初始化参数</strong>：<code>nn.Linear(in_features, out_features)</code> 中，<code>in_features</code> 表示输入特征的数量，<code>out_features</code> 表示输出特征的数量。在上述代码中，<code>in_features = 10</code>，<code>out_features = 5</code>，意味着输入的每个样本有 10 个特征，经过全连接层后输出的每个样本有 5 个特征。</li>
<li><strong>线性变换</strong>：全连接层的计算过程可以表示为$Y&#x3D;XW^T+b$ ，其中$X$是输入向量，$W$是权重矩阵，形状为 <code>(out_features, in_features)</code>，$b$是偏置向量，形状为 <code>(out_features,)</code>，$Y$是输出向量。</li>
<li><strong>前向传播</strong>：将输入张量 <code>input_tensor</code> 传递给 <code>linear_layer</code> 时，会自动执行上述线性变换，得到输出张量 <code>output</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">全连接层的权重形状: torch.Size([5, 10])</span><br><span class="line">全连接层的偏置形状: torch.Size([5])</span><br><span class="line">输入张量的形状: torch.Size([1, 10])</span><br><span class="line">输出张量的形状: torch.Size([1, 5])</span><br></pre></td></tr></table></figure>

<ul>
<li><input disabled="" type="checkbox"> 权重形状的确定:</li>
</ul>
<p>为了使矩阵乘法$WX$能够得到维度为$m$的输出向量，权重矩阵$W$的形状必须是$m × n$。这是因为在矩阵乘法中，两个矩阵能够相乘的条件是前一个矩阵的列数等于后一个矩阵的行数，并且相乘结果矩阵的行数等于前一个矩阵的行数，列数等于后一个矩阵的列数。即如果$W$是$m × n$矩阵，$X$是$n × 1$向量，那么$WX$的结果就是一个$m × 1$向量，符合输出向量$Y$的维度要求。</p>
<p>在代码示例中，输入特征数量 <code>in_features = 10</code>，输出特征数量 <code>out_features = 5</code>，所以权重矩阵 的形状就是 <code>(5, 10)</code>，即 <code>torch.Size([5, 10])</code>。</p>
<ul>
<li><input disabled="" type="checkbox"> 偏置形状的确定</li>
</ul>
<p>偏置向量$b$的作用是在经过矩阵乘法得到的结果上进行偏移。由于输出向量$Y$的维度是$m$，为了能够对$WX$的每一个元素都加上一个偏移量，偏置向量$b$的维度也必须是$m$，即$b∈\R^m$。</p>
<p>在代码示例中，输出特征数量 <code>out_features = 5</code>，所以偏置向量 的形状就是 <code>(5,)</code>，即 <code>torch.Size([5])</code>。</p>
<ol start="2">
<li><code>nn.Bilinear</code>双线性层</li>
</ol>
<p><strong><code>nn.Bilinear</code></strong>：PyTorch 中的一个类，用于创建双线性层。双线性层对两个输入进行双线性变换，能捕捉两个输入之间的交互信息，是一种比普通线性层更复杂的变换形式。</p>
<p><strong>关系建模</strong>：在需要捕捉两个不同特征之间交互关系的任务中非常有用。例如，在推荐系统中，可以用双线性层来建模用户特征和物品特征之间的交互，以预测用户对物品的偏好；在自然语言处理中，可用于处理两个不同句子或不同语义表示之间的关系。</p>
<p><strong>融合多模态信息</strong>：当处理多模态数据（如图像和文本）时，双线性层可以帮助融合不同模态之间的信息，挖掘它们之间的潜在关联。</p>
<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250222212028325.png" alt="image-20250222212028325"></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个双线性层</span></span><br><span class="line"><span class="comment"># 第一个输入维度为 10，第二个输入维度为 20，输出维度为 5</span></span><br><span class="line">bilinear_layer = nn.Bilinear(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟两个输入张量</span></span><br><span class="line"><span class="comment"># 第一个输入：假设有 1 个样本，每个样本有 10 个特征</span></span><br><span class="line">input1 = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 第二个输入：假设有 1 个样本，每个样本有 20 个特征</span></span><br><span class="line">input2 = torch.randn(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = bilinear_layer(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;双线性层的权重形状:&quot;</span>, bilinear_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;双线性层的偏置形状:&quot;</span>, bilinear_layer.bias.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个输入张量的形状:&quot;</span>, input1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第二个输入张量的形状:&quot;</span>, input2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量的形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>权重张量的形状符合 <code>(out_features, in1_features, in2_features)</code>，偏置向量的长度等于输出特征的数量，两个输入张量经过双线性层后，输出张量的特征数量变为 <code>out_features</code> 设定的值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">双线性层的权重形状: torch.Size([5, 10, 20])</span><br><span class="line">双线性层的偏置形状: torch.Size([5])</span><br><span class="line">第一个输入张量的形状: torch.Size([1, 10])</span><br><span class="line">第二个输入张量的形状: torch.Size([1, 20])</span><br><span class="line">输出张量的形状: torch.Size([1, 5])</span><br></pre></td></tr></table></figure>

<h6 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h6><hr>
<p>1.什么是卷积层？</p>
<p>卷积层本质上是通过可学习的卷积核（滤波器）在输入数据上进行滑动并执行卷积操作，以提取输入数据中的局部特征模式，同时利用参数共享减少模型参数数量。</p>
<p>卷积操作指的是卷积核（一个小的矩阵）在输入数据（如图像矩阵）上按一定步长滑动，每滑动到一个位置，就将卷积核与该位置对应的输入局部区域元素对应相乘后求和，得到输出特征图的一个值，不断滑动直至覆盖整个输入数据，最终生成完整的输出特征图。</p>
<p>假设输入是一个 4×4 的单通道图像矩阵，使用一个 2×2 的卷积核进行卷积操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入图像矩阵：</span><br><span class="line">[[1 2 3 4]</span><br><span class="line"> [5 6 7 8]</span><br><span class="line"> [9 10 11 12]</span><br><span class="line"> [13 14 15 16]]</span><br><span class="line"></span><br><span class="line">卷积核：</span><br><span class="line">[[1 2]</span><br><span class="line"> [3 4]]</span><br><span class="line"> </span><br><span class="line">第一步：卷积核位于输入图像的左上角，覆盖的局部区域是：</span><br><span class="line">[[1 2]</span><br><span class="line"> [5 6]]</span><br><span class="line">将卷积核与该局部区域对应元素相乘再求和：</span><br><span class="line">(1x1+2x2)+(3x5+4x6)=44</span><br><span class="line">这个 44 就是输出特征图左上角的值。</span><br><span class="line"></span><br><span class="line">第二步：卷积核向右滑动一个步长（假设步长为 1），覆盖的局部区域变为：</span><br><span class="line">[[2 3]</span><br><span class="line"> [6 7]]</span><br><span class="line">同样进行对应元素相乘再求和的操作：</span><br><span class="line">(1x2+2x3)+(3x6+4x7)=54</span><br><span class="line">这是输出特征图中左上角右侧位置的值。</span><br><span class="line"></span><br><span class="line">后续步骤：卷积核继续向右、向下滑动，每次都重复上述相乘求和的操作，直到遍历完整个输入图像矩阵，最终得到一个 3×3 的输出特征图（因为 4×4 的输入矩阵使用 2×2 卷积核，步长为 1 时会得到 3×3 的输出）。</span><br></pre></td></tr></table></figure>

<p>2.卷积层有什么好处？</p>
<p><strong>减少参数数量</strong></p>
<p>卷积层使用参数共享机制，即一个卷积核在整个输入数据上滑动使用，相比于全连接层每个输出神经元都与所有输入相连，大大减少了需要学习的参数数量，降低计算量和存储需求，也减少了过拟合风险。</p>
<p><strong>提取局部特征</strong></p>
<p>卷积核在输入数据的局部区域进行操作，能够有效捕捉数据中的局部特征，如在图像中可检测边缘、纹理等。这些局部特征在不同位置可能具有相似性，卷积层可以很好地学习和利用这种特性。</p>
<p><strong>保留空间结构</strong></p>
<p>卷积操作基于局部连接，能保留输入数据的空间结构信息，这对于处理具有空间结构的数据（如图像、音频）非常重要，有助于模型理解数据中元素之间的相对位置关系。</p>
<p><strong>可构建深层网络</strong></p>
<p>多个卷积层可以堆叠形成深层卷积神经网络，随着网络深度增加，能学习到从简单到复杂、从底层到高层的多层次特征，提升模型在各种任务（如图像分类、目标检测）中的性能。</p>
<hr>
<ol>
<li><code>nn.Conv1d</code> 1D卷积：处理时序数据（如音频、文本）</li>
</ol>
<p><code>nn.Conv1d</code> 是 PyTorch 中用于进行一维卷积操作的模块，主要用于处理时序数据，像音频信号、文本序列等。一维卷积在这些数据上沿着一个维度（通常是时间维度）进行卷积操作，能有效提取数据中的局部特征模式。</p>
<p><strong>音频处理</strong>：可以用于音频特征提取、语音识别等任务，通过一维卷积提取音频信号中的时域特征。</p>
<p><strong>文本处理</strong>：在自然语言处理中，将文本序列看作一维数据，一维卷积可以捕捉文本中的局部语义信息，常用于文本分类、情感分析等任务。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line"><span class="comment"># 输入数据形状：(批量大小, 输入通道数, 序列长度)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一维卷积层</span></span><br><span class="line"><span class="comment"># in_channels: 输入通道数</span></span><br><span class="line"><span class="comment"># out_channels: 输出通道数</span></span><br><span class="line"><span class="comment"># kernel_size: 卷积核大小</span></span><br><span class="line">conv1d_layer = nn.Conv1d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行卷积操作</span></span><br><span class="line">output = conv1d_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p><strong>输入数据</strong></p>
<p>通常是三维张量，形状为 <code>(batch_size, in_channels, sequence_length)</code>。其中 <code>batch_size</code> 表示一次处理的样本数量；<code>in_channels</code> 是输入数据的通道数，例如在音频处理中，单声道音频 <code>in_channels</code> 为 1，立体声音频 <code>in_channels</code> 为 2；<code>sequence_length</code> 是序列的长度，对于音频数据就是音频信号的采样点数，对于文本数据就是文本序列的长度。</p>
<p><strong>卷积层参数</strong></p>
<p><code>in_channels</code>：输入数据的通道数，必须与输入张量的第二维大小一致。</p>
<p><code>out_channels</code>：输出数据的通道数，即卷积层使用的卷积核数量。每个卷积核会提取一种特定的特征，因此不同的卷积核会输出不同的特征图。</p>
<p><code>kernel_size</code>：卷积核的大小，表示在序列维度上卷积核覆盖的元素个数。例如 <code>kernel_size=3</code> 表示卷积核在序列上每次覆盖 3 个元素。</p>
<p><strong>卷积操作过程</strong></p>
<p>卷积核在输入数据的序列维度上滑动，每次覆盖 <code>kernel_size</code> 个元素，并在每个通道上进行卷积操作，然后将各通道的结果相加（如果有多个输入通道），最后加上偏置项得到输出的一个值。卷积核不断滑动，最终得到输出特征图。</p>
<p><strong>输出数据</strong></p>
<p>输出数据同样是三维张量，形状为 <code>(batch_size, out_channels, new_sequence_length)</code>。其中 <code>batch_size</code> 与输入相同；<code>out_channels</code> 是卷积层定义的输出通道数；<code>new_sequence_length</code> 由输入序列长度、卷积核大小、步长（<code>stride</code>，默认为 1）和填充（<code>padding</code>，默认为 0）等因素决定，计算公式为：</p>
<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250222215408498.png" alt="image-20250222215408498"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([16, 3, 100])</span><br><span class="line">输出张量形状: torch.Size([16, 6, 98])</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p><code>nn.Conv2d</code> 2D卷积：处理图像数据</p>
</li>
<li><p><code>nn.Conv3d</code> 3D卷积：处理视频&#x2F;体数据</p>
</li>
<li><p><code>nn.ConvTranspose1d/2d/3d</code> 转置卷积：反卷积（上采样）</p>
</li>
<li><p><code>nn.Conv1d/2d/3d</code> （设置 <code>dilation</code>）空洞卷积：扩大感受野</p>
</li>
<li><p><code>nn.Conv2d + 分组操作</code> 可分离卷积：轻量化卷积</p>
</li>
</ol>
<h6 id="循环神经网络层"><a href="#循环神经网络层" class="headerlink" title="循环神经网络层"></a>循环神经网络层</h6><p>1.<code>nn.RNN</code> RNN：基础循环网络</p>
<p>2.<code>nn.LSTM</code> LSTM：长短期记忆网络</p>
<p>3.<code>nn.GRU</code> GRU：门控循环单元</p>
<h6 id="Transformer-相关层"><a href="#Transformer-相关层" class="headerlink" title="Transformer 相关层"></a>Transformer 相关层</h6><ol>
<li><p>什么是Transformer?</p>
</li>
<li><p><code>nn.Transformer</code> Transformer：完整 Transformer 模型</p>
</li>
<li><p><code>nn.TransformerEncoder</code> Transformer Encoder： 编码器堆叠</p>
</li>
<li><p><code>nn.TransformerDecoder</code> Transformer Decoder：解码器堆叠</p>
</li>
<li><p><code>nn.MultiheadAttention</code> 多头注意力：自注意力机制</p>
</li>
</ol>
<h6 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h6><ol>
<li><p>什么是归一化？</p>
</li>
<li><p><code>nn.BatchNorm1d/2d/3d</code> 批量归一化：批维度归一化</p>
</li>
<li><p><code>nn.LayerNorm</code> 层归一化：通道维度归一化</p>
</li>
<li><p><code>nn.InstanceNorm1d/2d/3d</code> 实例归一化：单样本归一化（风格迁移）</p>
</li>
<li><p><code>nn.GroupNorm</code> 组归一化：分组归一化（小批量适用）</p>
</li>
</ol>
<h6 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h6><p>1.什么是激活函数？</p>
<p>2.<code>nn.ReLU</code> ReLU：$max(0, x)$</p>
<p>3.<code>nn.LeakyReLU</code> LeakyReLU：$max(αx, x)$</p>
<p>4.<code>nn.Sigmoid</code> Sigmoid：$\frac{1}{1 + e^{-x}}$</p>
<p>5.<code>nn.Tanh</code> Tanh：$\frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
<p>6.<code>nn.GELU</code> GELU：高斯误差线性单元（Transformer 常用）</p>
<p>7.<code>nn.Softmax</code> Softmax：概率归一化</p>
<h6 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h6><p>1.什么是池化？</p>
<p>2.<code>nn.MaxPool1d/2d/3d</code> 最大池化：取局部最大值</p>
<p>3.<code>nn.AvgPool1d/2d/3d</code> 平均池化：取局部平均值</p>
<p>4.<code>nn.AdaptiveMaxPool1d/2d/3d</code> 自适应池化：动态调整输出尺寸</p>
<p>5.<code>nn.FractionalMaxPool2d</code> 分数池化：随机分数步长池化</p>
<h6 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a>Dropout 层</h6><ol>
<li><p>什么是Dropout？</p>
</li>
<li><p><code>nn.Dropout</code> 标准Dropout： 随机置零神经元</p>
</li>
<li><p><code>nn.Dropout1d/2d/3d</code> 空间Dropout：按通道&#x2F;空间置零</p>
</li>
</ol>
<h6 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h6><ol>
<li><p>什么是嵌入？</p>
</li>
<li><p><code>nn.Embedding</code> 词嵌入：nn.Embedding</p>
</li>
<li><p><code>nn.Embedding</code> 稀疏嵌入：高效处理变长序列</p>
</li>
</ol>
<h6 id="稀疏层"><a href="#稀疏层" class="headerlink" title="稀疏层"></a>稀疏层</h6><ol>
<li><p>什么是稀疏？</p>
</li>
<li><p><code>nn.Linear（输入为稀疏张量）</code> 稀疏全连接：稀疏矩阵乘法</p>
</li>
</ol>
<h6 id="视觉专用层"><a href="#视觉专用层" class="headerlink" title="视觉专用层"></a>视觉专用层</h6><ol>
<li><p><code>nn.PixelShuffle</code> 像素重排：子像素卷积（超分辨率）</p>
</li>
<li><p><code>nn.Unfold</code> 像素展开：滑动窗口提取局部块</p>
</li>
<li><p><code>nn.Fold</code> 像素折叠：逆操作于 <code>Unfold</code></p>
</li>
</ol>
<h5 id="模型容器"><a href="#模型容器" class="headerlink" title="模型容器"></a>模型容器</h5><ol>
<li><p>什么是容器？</p>
</li>
<li><p><code>nn.Sequential</code> 层字典</p>
</li>
<li><p><code>nn.ModuleList</code> 动态层列表</p>
</li>
<li><p><code>nn.ModuleDict</code>层字典</p>
</li>
</ol>
<h4 id="优化器和损失函数"><a href="#优化器和损失函数" class="headerlink" title="优化器和损失函数"></a>优化器和损失函数</h4><h5 id="优化器（Optimizers）"><a href="#优化器（Optimizers）" class="headerlink" title="优化器（Optimizers）"></a>优化器（Optimizers）</h5><h6 id="什么是优化器？"><a href="#什么是优化器？" class="headerlink" title="什么是优化器？"></a>什么是优化器？</h6><h6 id="经典优化器"><a href="#经典优化器" class="headerlink" title="经典优化器"></a>经典优化器</h6><ol>
<li><p><code>torch.optim.SGD</code>（含动量）</p>
</li>
<li><p><code>torch.optim.Adam</code>, <code>AdamW</code>, <code>RMSprop</code></p>
</li>
</ol>
<h6 id="学习率调度"><a href="#学习率调度" class="headerlink" title="学习率调度"></a>学习率调度</h6><ol>
<li><p>什么是学习率调度？</p>
</li>
<li><p>相关优化器<code>lr_scheduler.StepLR</code>, <code>CosineAnnealingLR</code>, <code>OneCycleLR</code></p>
</li>
</ol>
<h6 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h6><ol>
<li><p>什么是梯度裁剪？</p>
</li>
<li><p><code>nn.utils.clip_grad_norm_</code></p>
</li>
</ol>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><h6 id="什么是损失函数？"><a href="#什么是损失函数？" class="headerlink" title="什么是损失函数？"></a>什么是损失函数？</h6><h6 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h6><p><code>nn.CrossEntropyLoss</code>，<code>nn.BCEWithLogitsLoss</code></p>
<h6 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h6><p><code>nn.MSELoss</code>, <code>nn.L1Loss</code>, <code>nn.HuberLoss</code></p>
<h6 id="生成任务"><a href="#生成任务" class="headerlink" title="生成任务"></a>生成任务</h6><ol>
<li><p>生成什么？</p>
</li>
<li><p>相关损失函数<code>nn.BCELoss</code>, <code>nn.KLDivLoss</code>, 对抗损失（如 WGAN-GP）</p>
</li>
</ol>
<h4 id="预训练模型（torchvision-models）与迁移学习"><a href="#预训练模型（torchvision-models）与迁移学习" class="headerlink" title="预训练模型（torchvision.models）与迁移学习"></a>预训练模型（torchvision.models）与迁移学习</h4><h5 id="计算机视觉模型"><a href="#计算机视觉模型" class="headerlink" title="计算机视觉模型"></a>计算机视觉模型</h5><h6 id="经典-CNN-架构（通过-torchvision-models）"><a href="#经典-CNN-架构（通过-torchvision-models）" class="headerlink" title="经典 CNN 架构（通过 torchvision.models）"></a>经典 CNN 架构（通过 <code>torchvision.models</code>）</h6><p>什么是CNN</p>
<ol>
<li><p>ResNet</p>
</li>
<li><p>VGG</p>
</li>
<li><p>EfficientNet</p>
</li>
</ol>
<h6 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h6><p>什么是Transformer</p>
<ol>
<li><p>Vision Transformer （ViT）</p>
</li>
<li><p>Swin Transformer</p>
</li>
</ol>
<h5 id="自然语言处理模型"><a href="#自然语言处理模型" class="headerlink" title="自然语言处理模型"></a>自然语言处理模型</h5><h6 id="预训练语言模型（通过-transformers-库）"><a href="#预训练语言模型（通过-transformers-库）" class="headerlink" title="预训练语言模型（通过 transformers 库）"></a>预训练语言模型（通过 <code>transformers</code> 库）</h6><ol>
<li><p>BERT</p>
</li>
<li><p>GPT</p>
</li>
</ol>
<h5 id="迁移学习策略"><a href="#迁移学习策略" class="headerlink" title="迁移学习策略"></a>迁移学习策略</h5><p>什么是迁移学习策略</p>
<h6 id="特征提取（冻结部分层）"><a href="#特征提取（冻结部分层）" class="headerlink" title="特征提取（冻结部分层）"></a>特征提取（冻结部分层）</h6><h6 id="微调（Fine-tuning）"><a href="#微调（Fine-tuning）" class="headerlink" title="微调（Fine-tuning）"></a>微调（Fine-tuning）</h6><h6 id="使用预训练特征（如-CLIP）"><a href="#使用预训练特征（如-CLIP）" class="headerlink" title="使用预训练特征（如 CLIP）"></a>使用预训练特征（如 CLIP）</h6><h4 id="数据管道-Data-Pipeline"><a href="#数据管道-Data-Pipeline" class="headerlink" title="数据管道 (Data Pipeline)"></a>数据管道 (Data Pipeline)</h4><h5 id="数据集类-Dataset"><a href="#数据集类-Dataset" class="headerlink" title="数据集类 Dataset"></a>数据集类 <code>Dataset</code></h5><p>自定义数据集实现</p>
<h5 id="数据加载器-DataLoader"><a href="#数据加载器-DataLoader" class="headerlink" title="数据加载器 DataLoader"></a>数据加载器 <code>DataLoader</code></h5><ol>
<li><p>批量加载</p>
</li>
<li><p>多进程加速 (<code>num_workers</code>)</p>
</li>
</ol>
<h5 id="数据增强-torchvision-transforms"><a href="#数据增强-torchvision-transforms" class="headerlink" title="数据增强&#96;&#96;torchvision.transforms&#96;"></a>数据增强&#96;&#96;torchvision.transforms&#96;</h5><h4 id="模型部署与性能优化"><a href="#模型部署与性能优化" class="headerlink" title="模型部署与性能优化"></a>模型部署与性能优化</h4><h5 id="TorchScript-模型导出"><a href="#TorchScript-模型导出" class="headerlink" title="TorchScript 模型导出"></a>TorchScript 模型导出</h5><h5 id="ONNX-格式转换"><a href="#ONNX-格式转换" class="headerlink" title="ONNX 格式转换"></a>ONNX 格式转换</h5><h5 id="混合精度训练-torch-cuda-amp"><a href="#混合精度训练-torch-cuda-amp" class="headerlink" title="混合精度训练 (torch.cuda.amp)"></a>混合精度训练 (<code>torch.cuda.amp</code>)</h5><h2 id="OS操作系统接口模块"><a href="#OS操作系统接口模块" class="headerlink" title="OS操作系统接口模块"></a>OS操作系统接口模块</h2><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>np.stack</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研知识积累</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕OCC-NeRF:Occlusion-Free Scene Recovery via Neural Radiance Fields</title>
    <url>/%E6%89%8B%E6%92%95nerfmm/</url>
    <content><![CDATA[<p>链接：<a href="https://freebutuselesssoul.github.io/occnerf/">OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields</a></p>
<h2 id="文件夹目录"><a href="#文件夹目录" class="headerlink" title="文件夹目录"></a>文件夹目录</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ-nerf/</span><br><span class="line">├── .gitignore</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── environment.yml</span><br><span class="line">├── local1.txt</span><br><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save.py</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br><span class="line">├── models/</span><br><span class="line">│   ├── depth_decoder.py</span><br><span class="line">│   ├── intrinsics.py</span><br><span class="line">│   ├── layers.py</span><br><span class="line">│   ├── nerf_feature.py</span><br><span class="line">│   ├── nerf_mask.py</span><br><span class="line">│   ├── nerf_models.py</span><br><span class="line">│   └── poses.py</span><br><span class="line">├── utils/</span><br><span class="line">│   ├── align_traj.py</span><br><span class="line">│   ├── comp_ate.py</span><br><span class="line">│   ├── comp_ray_dir.py</span><br><span class="line">│   ├── lie_group_helper.py</span><br><span class="line">│   ├── pos_enc.py</span><br><span class="line">│   ├── pose_utils.py</span><br><span class="line">│   ├── split_dataset.py</span><br><span class="line">│   ├── training_utils.py</span><br><span class="line">│   ├── vgg.py</span><br><span class="line">│   ├── vis_cam_traj.py</span><br><span class="line">│   └── volume_op.py</span><br><span class="line">├── tasks/</span><br><span class="line">│   └── ...</span><br><span class="line">└── third_party/</span><br><span class="line">    ├── ATE/</span><br><span class="line">    │   └── README.md</span><br><span class="line">    └── pytorch_ssim/</span><br></pre></td></tr></table></figure>

<h2 id="DEBUG-代码"><a href="#DEBUG-代码" class="headerlink" title="DEBUG 代码"></a>DEBUG 代码</h2><h3 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save.py</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br></pre></td></tr></table></figure>

<h4 id="any-folder-py"><a href="#any-folder-py" class="headerlink" title="any_folder.py"></a>any_folder.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os                                       <span class="comment"># 操作系统接口模块</span></span><br><span class="line"><span class="keyword">import</span> torch                                    <span class="comment"># PyTorch 深度学习框架</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                              <span class="comment"># 科学计算库</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm                           <span class="comment"># 进度条显示模块</span></span><br><span class="line"><span class="keyword">import</span> imageio                                  <span class="comment"># 图像 IO 处理库</span></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs  <span class="comment"># 自定义图像缩放函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 获取并排序目录下所有文件名</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:                                 <span class="comment"># 从 start 开始按间隔 skip 取图</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 取 start 到 end 区间按间隔 skip 取图</span></span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:                           <span class="comment"># 是否打乱图像顺序</span></span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):          <span class="comment"># 检查请求数量是否超出范围</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;图像请求数<span class="subst">&#123;num_img_to_load&#125;</span>超过可用数<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>&#x27;</span>)</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:                   <span class="comment"># 加载全部可用图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;加载全部<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张图像&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 截取指定数量的图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;从<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张中加载<span class="subst">&#123;num_img_to_load&#125;</span>张&#x27;</span>)</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line">    </span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]  <span class="comment"># 构建完整文件路径</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)                         <span class="comment"># 计算实际加载数量</span></span><br><span class="line">    </span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:                                     <span class="comment"># 实际加载图像数据</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]        <span class="comment"># 读取 RGB 三通道图像</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        img_list = np.stack(img_list)                <span class="comment"># 堆叠为 4D 数组</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 转换为浮点张量并归一化</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]  <span class="comment"># 获取图像尺寸</span></span><br><span class="line">    <span class="keyword">else</span>:                                            <span class="comment"># 仅获取图像尺寸</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])</span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;                                         <span class="comment"># 返回结构化数据</span></span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,        <span class="comment"># 图像张量 (N, H, W, 3)</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># 图像文件名数组</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,        <span class="comment"># 总图像数</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,                  <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,                  <span class="comment"># 图像宽度</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, </span></span><br><span class="line"><span class="params">                 start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):  <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir                  <span class="comment"># 数据根目录</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name              <span class="comment"># 场景名称</span></span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio                <span class="comment"># 分辨率缩放比例</span></span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load    <span class="comment"># 最大加载数量</span></span><br><span class="line">        <span class="variable language_">self</span>.start = start                        <span class="comment"># 起始索引</span></span><br><span class="line">        <span class="variable language_">self</span>.end = end                            <span class="comment"># 结束索引</span></span><br><span class="line">        <span class="variable language_">self</span>.skip = skip                          <span class="comment"># 采样间隔</span></span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted            <span class="comment"># 是否保持顺序</span></span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img                  <span class="comment"># 是否实际加载图像</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)  <span class="comment"># 构建图像目录路径</span></span><br><span class="line">        </span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load,  <span class="comment"># 加载图像数据</span></span><br><span class="line">                              <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                              <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]             <span class="comment"># 图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]   <span class="comment"># 文件名列表</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]         <span class="comment"># 图像总数</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]               <span class="comment"># 原始高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]               <span class="comment"># 原始宽度</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span>                            <span class="comment"># 近裁剪面(NDC 坐标系)</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span>                             <span class="comment"># 远裁剪面(NDC 坐标系)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:                     <span class="comment"># 计算实际使用分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:                          <span class="comment"># 执行图像缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span>                   <span class="comment"># 数据根目录配置示例</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span>                <span class="comment"># 场景路径配置示例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span>                               <span class="comment"># 缩放比例配置</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span>                           <span class="comment"># 加载全部图像</span></span><br><span class="line">    start, end, skip = <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>                    <span class="comment"># 采样参数初始化</span></span><br><span class="line">    load_sorted, load_img = <span class="literal">True</span>, <span class="literal">True</span>             <span class="comment"># 加载配置参数</span></span><br><span class="line">    </span><br><span class="line">    scene = DataLoaderAnyFolder(                   <span class="comment"># 创建数据加载实例</span></span><br><span class="line">        base_dir=base_dir,</span><br><span class="line">        scene_name=scene_name,</span><br><span class="line">        res_ratio=resize_ratio,</span><br><span class="line">        num_img_to_load=num_img_to_load,</span><br><span class="line">        start=start,</span><br><span class="line">        end=end,</span><br><span class="line">        skip=skip,</span><br><span class="line">        load_sorted=load_sorted,</span><br><span class="line">        load_img=load_img)</span><br></pre></td></tr></table></figure>

<h4 id="local-save-py"><a href="#local-save-py" class="headerlink" title="local_save.py"></a>local_save.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vgg19</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, requires_grad=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 加载预训练的 VGG19 模型的特征提取部分</span></span><br><span class="line">        <span class="variable language_">self</span>.vgg_pretrained_features = models.vgg19(pretrained=<span class="literal">True</span>).features</span><br><span class="line">        <span class="comment"># 如果不需要计算梯度，则将模型参数的 requires_grad 属性设置为 False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> requires_grad:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">                param.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 初始化特征图的形状为 None</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_shape = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, indices=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 记录输入特征图的形状</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_shape = X.shape</span><br><span class="line">        <span class="comment"># 如果没有指定索引，则默认使用 [7, 25]</span></span><br><span class="line">        <span class="keyword">if</span> indices <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            indices = [<span class="number">7</span>,<span class="number">25</span>]</span><br><span class="line">        <span class="comment"># 存储提取的特征图</span></span><br><span class="line">        out = []</span><br><span class="line">        <span class="comment"># 遍历到最后一个索引位置</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(indices[-<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 通过 VGG19 的第 i 层进行特征提取</span></span><br><span class="line">            X = <span class="variable language_">self</span>.vgg_pretrained_features[i](X)</span><br><span class="line">            <span class="comment"># 如果当前层的索引加 1 在指定的索引列表中</span></span><br><span class="line">            <span class="keyword">if</span> (i+<span class="number">1</span>) <span class="keyword">in</span> indices:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.feature_shape <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="comment"># 如果特征图形状为 None，则记录当前特征图的形状</span></span><br><span class="line">                    <span class="variable language_">self</span>.feature_shape = X.shape</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 对特征图进行双线性插值，使其尺寸与输入特征图的尺寸一致</span></span><br><span class="line">                    X = F.interpolate(X,<span class="variable language_">self</span>.feature_shape[-<span class="number">2</span>:],mode=<span class="string">&#x27;bilinear&#x27;</span>,align_corners=<span class="literal">True</span>)</span><br><span class="line">                <span class="comment"># 将处理后的特征图添加到输出列表中</span></span><br><span class="line">                out.append(X)</span><br><span class="line">        <span class="comment"># 将所有提取的特征图在通道维度上拼接起来</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像的文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir))) <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 时间域下采样：根据 start、end 和 skip 参数选择图像</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:</span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查要加载的图像数量是否超过可用图像数量</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个图像文件名</span></span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建图像文件的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储加载的图像</span></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载进度</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            <span class="comment"># 读取图像并截取前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>] <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            <span class="comment"># 将图像添加到列表中</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list) <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span> <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>]) <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储加载图像的相关信息</span></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list, <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names, <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">    self.c2ws: (N_imgs, 4, 4) torch.float32</span></span><br><span class="line"><span class="string">    self.imgs (N_imgs, H, W, 4) torch.float32</span></span><br><span class="line"><span class="string">    self.ray_dir_cam (H, W, 3) torch.float32</span></span><br><span class="line"><span class="string">    self.H scalar</span></span><br><span class="line"><span class="string">    self.W scalar</span></span><br><span class="line"><span class="string">    self.N_imgs scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start, end, skip, load_sorted, load_img=<span class="literal">True</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 False：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用 load_imgs 函数加载图像</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                               <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        <span class="comment"># 加载的图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>] <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 图像的文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>] <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 初始化 VGG19 编码器</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = Vgg19()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W) <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="comment"># 存储图像的特征</span></span><br><span class="line">            <span class="variable language_">self</span>.features = []</span><br><span class="line">            <span class="comment"># 使用 tqdm 显示处理进度</span></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(<span class="variable language_">self</span>.imgs):</span><br><span class="line">                <span class="comment"># 对图像进行通道维度的调整，并通过编码器提取特征</span></span><br><span class="line">                <span class="variable language_">self</span>.features.append(<span class="variable language_">self</span>.encoder(img.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)[<span class="literal">None</span>,...]))</span><br><span class="line">            <span class="comment"># 将所有图像的特征在批次维度上拼接起来</span></span><br><span class="line">            <span class="variable language_">self</span>.features = torch.cat(<span class="variable language_">self</span>.features,<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 特征图的尺寸</span></span><br><span class="line">            <span class="variable language_">self</span>.feature_size = (<span class="variable language_">self</span>.features.shape[-<span class="number">2</span>],<span class="variable language_">self</span>.features.shape[-<span class="number">1</span>]) <span class="comment"># (H,W)</span></span><br><span class="line">            <span class="comment"># 打印特征图的形状</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="variable language_">self</span>.features.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 DataLoaderAnyFolder 类</span></span><br><span class="line">    scene = DataLoaderAnyFolder(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img)</span><br></pre></td></tr></table></figure>



<h4 id="with-colmap-py"><a href="#with-colmap-py" class="headerlink" title="with_colmap.py"></a>with_colmap.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可以用来处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 可以在 CPU 或 GPU 上进行高效的数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可以进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 可以在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_imgs</span>(<span class="params">imgs, new_h, new_w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param imgs:    (N, H, W, 3)            torch.float32 RGB</span></span><br><span class="line"><span class="string">    :param new_h:   int/torch int</span></span><br><span class="line"><span class="string">    :param new_w:   int/torch int</span></span><br><span class="line"><span class="string">    :return:        (N, new_H, new_W, 3)    torch.float32 RGB</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将图像张量从 (N, H, W, 3) 转换为 (N, 3, H, W) 以适应 F.interpolate 函数的输入要求</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (N, 3, H, W)</span></span><br><span class="line">    <span class="comment"># 使用双线性插值方法将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    imgs = F.interpolate(imgs, size=(new_h, new_w), mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># (N, 3, new_H, new_W)</span></span><br><span class="line">    <span class="comment"># 将图像张量从 (N, 3, new_H, new_W) 转换回 (N, new_H, new_W, 3)</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># (N, new_H, new_W, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs  <span class="comment"># (N, new_H, new_W, 3) torch.float32 RGB</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, img_ids, new_h, new_w</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line">    <span class="comment"># 根据给定的图像索引筛选出需要的图像文件名</span></span><br><span class="line">    img_names = img_names[img_ids]  <span class="comment"># image name for this split</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">        <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">        img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">        img_list.append(img)</span><br><span class="line">    <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">    img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">    img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">    <span class="comment"># 调用 resize_imgs 函数将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    img_list = resize_imgs(img_list, new_h, new_w)</span><br><span class="line">    <span class="keyword">return</span> img_list, img_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Read the poses_bounds.npy file produced by LLFF imgs2poses.py.</span></span><br><span class="line"><span class="string">    This function is modified from https://github.com/kwea123/nerf_pl.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))  <span class="comment"># (N_images, 17)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># (N_images, 3, 5)</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># (N_images, 2)</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;down right back&quot; 改为 &quot;right up back&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws -&gt; centred c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># (N_images, 3, 4), (4, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 获取最近深度值</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 计算缩放因子，将最近深度调整到稍大于 1.0 的位置</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  <span class="comment"># 0.75 is the default parameter</span></span><br><span class="line">        <span class="comment"># 对深度边界进行缩放</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        <span class="comment"># 对相机位姿的平移部分进行缩放</span></span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># (N, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) np</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) np</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) np</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderWithCOLMAP</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, data_type, res_ratio, num_img_to_load, skip, use_ndc, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir:</span></span><br><span class="line"><span class="string">        :param scene_name:</span></span><br><span class="line"><span class="string">        :param data_type:   &#x27;train&#x27; or &#x27;val&#x27;.</span></span><br><span class="line"><span class="string">        :param res_ratio:   int [1, 2, 4] etc to resize images to a lower resolution.</span></span><br><span class="line"><span class="string">        :param num_img_to_load/skip: control frame loading in temporal domain.</span></span><br><span class="line"><span class="string">        :param use_ndc      True/False, just centre the poses and scale them.</span></span><br><span class="line"><span class="string">        :param load_img:    True/False. If set to false: only count number of images, get H and W,</span></span><br><span class="line"><span class="string">                            but do not load imgs. Useful when vis poses or debug etc.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.data_type = data_type</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建场景目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = os.path.join(<span class="variable language_">self</span>.scene_dir, <span class="string">&#x27;images&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.scene_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = meta[<span class="string">&#x27;c2ws&#x27;</span>]  <span class="comment"># (N, 4, 4) all camera pose</span></span><br><span class="line">        <span class="comment"># 提取图像高度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.H = meta[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取图像宽度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.W = meta[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像高度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像宽度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对焦距进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 加载图像并调整到指定的高度和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.img_names = load_imgs(<span class="variable language_">self</span>.img_dir, np.arange(num_img_to_load), <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[:num_img_to_load]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = <span class="variable language_">self</span>.c2ws.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成相机坐标系下的光线方向</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = comp_ray_dir_cam(<span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.focal)  <span class="comment"># (H, W, 3) torch.float32</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相机位姿从 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.from_numpy(<span class="variable language_">self</span>.c2ws).<span class="built_in">float</span>()  <span class="comment"># (N, 4, 4) torch.float32</span></span><br><span class="line">        <span class="comment"># 将光线方向张量转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = <span class="variable language_">self</span>.ray_dir_cam.<span class="built_in">float</span>()  <span class="comment"># (H, W, 3) torch.float32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern&#x27;</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 注意：需要将 /your/data/path 替换为实际的数据路径，</span></span><br><span class="line">    <span class="comment"># 这里创建了一个 DataLoaderWithCOLMAP 类的实例，用于加载指定场景的数据</span></span><br><span class="line">    scene = DataLoaderWithCOLMAP(base_dir=<span class="string">&#x27;/your/data/path&#x27;</span>,</span><br><span class="line">                                 scene_name=scene_name,</span><br><span class="line">                                 data_type=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">                                 res_ratio=<span class="number">8</span>,</span><br><span class="line">                                 num_img_to_load=-<span class="number">1</span>,</span><br><span class="line">                                 skip=<span class="number">1</span>,</span><br><span class="line">                                 use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>



<h4 id="with-feature-colmap-py"><a href="#with-feature-colmap-py" class="headerlink" title="with_feature_colmap.py"></a>with_feature_colmap.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="comment"># 从 dataloader.with_colmap 模块导入 resize_imgs 函数，</span></span><br><span class="line"><span class="comment"># 该函数用于调整图像的尺寸。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="comment"># torchvision 是 PyTorch 中用于计算机视觉任务的库，</span></span><br><span class="line"><span class="comment"># models 子模块提供了预训练的深度学习模型。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 例如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.vgg <span class="keyword">import</span> Vgg19</span><br><span class="line"><span class="comment"># 从 utils.vgg 模块导入 Vgg19 类，可能用于特征提取。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在时间域上对帧进行下采样</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:</span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载下采样后的图像</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])  <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    result = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Read the poses_bounds.npy file produced by LLFF imgs2poses.py.</span></span><br><span class="line"><span class="string">    This function is modified from https://github.com/kwea123/nerf_pl.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;../poses_bounds.npy&#x27;</span>))  <span class="comment"># (N_images, 17)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># (N_images, 3, 5)</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># (N_images, 2)</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;down right back&quot; 改为 &quot;right up back&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws -&gt; centred c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># (N_images, 3, 4), (4, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 修正尺度，使最近的深度略大于 1.0</span></span><br><span class="line">        <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 0.75 是默认参数</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  </span><br><span class="line">        <span class="comment"># 最近的深度约为 1/0.75 = 1.33</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># (N, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) np</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) np</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) np</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataloader_feature_n_colmap</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start=<span class="number">0</span>, end=-<span class="number">1</span>, skip=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 load_sorted=<span class="literal">True</span>, load_img=<span class="literal">True</span>, use_ndc=<span class="literal">True</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 false：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息并转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.Tensor(meta[<span class="string">&#x27;c2ws&#x27;</span>])  <span class="comment"># (N, 4, 4) all camera pose</span></span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line">        <span class="comment"># 根据 start、end 和 skip 参数对相机位姿进行筛选</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.end == -<span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[<span class="variable language_">self</span>.start::<span class="variable language_">self</span>.skip]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[<span class="variable language_">self</span>.start:<span class="variable language_">self</span>.end:<span class="variable language_">self</span>.skip]</span><br><span class="line">        <span class="comment"># 加载图像数据</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                                <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        <span class="comment"># 提取加载的图像数据</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 提取图像文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 初始化 Vgg19 编码器并将其移动到指定设备</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = Vgg19().to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC）</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 计算调整后的图像高度</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 计算调整后的图像宽度</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        <span class="comment"># 调整焦距</span></span><br><span class="line">        <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率并将其移动到指定设备</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W).to(device)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="variable language_">self</span>.features = []</span><br><span class="line">            <span class="comment"># 使用 tqdm 显示处理进度</span></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(<span class="variable language_">self</span>.imgs):</span><br><span class="line">                <span class="comment"># 对图像进行通道维度的调整，并通过编码器提取特征</span></span><br><span class="line">                <span class="variable language_">self</span>.features.append(<span class="variable language_">self</span>.encoder(img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)[<span class="literal">None</span>, ...]))</span><br><span class="line">            <span class="comment"># 这里注释掉了特征拼接的代码，可根据需要取消注释</span></span><br><span class="line">            <span class="comment"># self.features = torch.cat(self.features, 0)</span></span><br><span class="line">            <span class="comment"># print(self.features.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否使用归一化设备坐标（NDC）</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 Dataloader_feature_n_colmap 类</span></span><br><span class="line">    scene = Dataloader_feature_n_colmap(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img,</span><br><span class="line">                                use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>



<h4 id="with-feature-py"><a href="#with-feature-py" class="headerlink" title="with_feature.py"></a>with_feature.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 例如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_imgs</span>(<span class="params">imgs, new_h, new_w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param imgs:    (N, H, W, 3)            torch.float32 格式的 RGB 图像</span></span><br><span class="line"><span class="string">    :param new_h:   整数或 torch 整数类型，表示新的图像高度</span></span><br><span class="line"><span class="string">    :param new_w:   整数或 torch 整数类型，表示新的图像宽度</span></span><br><span class="line"><span class="string">    :return:        (N, new_H, new_W, 3)    torch.float32 格式的 RGB 图像</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将图像张量的维度从 (N, H, W, 3) 调整为 (N, 3, H, W)，以适配 F.interpolate 函数的输入要求</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 变为 (N, 3, H, W)</span></span><br><span class="line">    <span class="comment"># 使用双线性插值方法将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    imgs = F.interpolate(imgs, size=(new_h, new_w), mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># 变为 (N, 3, new_H, new_W)</span></span><br><span class="line">    <span class="comment"># 将图像张量的维度从 (N, 3, new_H, new_W) 调整回 (N, new_H, new_W, 3)</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 变为 (N, new_H, new_W, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs  <span class="comment"># 返回 (N, new_H, new_W, 3) 格式的 torch.float32 类型 RGB 图像</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, img_ids, new_h, new_w</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 得到所有图像文件名</span></span><br><span class="line">    <span class="comment"># 根据给定的图像索引筛选出本次需要的图像文件名</span></span><br><span class="line">    img_names = img_names[img_ids]  <span class="comment"># 得到本次分割所需的图像文件名</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">        <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">        img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># 得到 (H, W, 3) 格式的 np.uint8 类型图像</span></span><br><span class="line">        img_list.append(img)</span><br><span class="line">    <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">    img_list = np.stack(img_list)  <span class="comment"># 变为 (N, H, W, 3) 格式</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">    img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 变为 (N, H, W, 3) 格式的 torch.float32 类型</span></span><br><span class="line">    <span class="comment"># 调用 resize_imgs 函数将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    img_list = resize_imgs(img_list, new_h, new_w)</span><br><span class="line">    <span class="keyword">return</span> img_list, img_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取由 LLFF 的 imgs2poses.py 生成的 poses_bounds.npy 文件。</span></span><br><span class="line"><span class="string">    此函数改编自 https://github.com/kwea123/nerf_pl。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))  <span class="comment"># 得到 (N_images, 17) 格式的数组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># 变为 (N_images, 3, 5) 格式</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># 变为 (N_images, 2) 格式</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;下 右 后&quot; 改为 &quot;右 上 后&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws 得到中心化后的 c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># 分别得到 (N_images, 3, 4) 和 (4, 4) 格式的数组</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 获取最近深度值</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 计算缩放因子，使最近深度调整到稍大于 1.0 的位置</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  <span class="comment"># 0.75 是默认参数</span></span><br><span class="line">        <span class="comment"># 现在最近深度约为 1/0.75 = 1.33</span></span><br><span class="line">        <span class="comment"># 对深度边界进行缩放</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        <span class="comment"># 对相机位姿的平移部分进行缩放</span></span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># 变为 (N, 4, 4) 格式</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) 格式的 numpy 数组</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) 格式的 numpy 数组</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># 标量，图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># 标量，图像宽度</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># 标量，焦距</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) 格式的 numpy 数组</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderWithCOLMAP</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    最有用的字段：</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4) 格式的 torch.float32 类型张量，表示相机位姿</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4) 格式的 torch.float32 类型张量，表示图像</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3) 格式的 torch.float32 类型张量，表示相机坐标系下的光线方向</span></span><br><span class="line"><span class="string">        self.H              标量，图像高度</span></span><br><span class="line"><span class="string">        self.W              标量，图像宽度</span></span><br><span class="line"><span class="string">        self.N_imgs         标量，图像数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, data_type, res_ratio, num_img_to_load, skip, use_ndc, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param data_type: 数据类型，&#x27;train&#x27; 或 &#x27;val&#x27;。</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param num_img_to_load/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param use_ndc: 布尔值，是否对相机位姿进行中心化和缩放。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 False：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.data_type = data_type</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建场景目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = os.path.join(<span class="variable language_">self</span>.scene_dir, <span class="string">&#x27;images&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.scene_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = meta[<span class="string">&#x27;c2ws&#x27;</span>]  <span class="comment"># (N, 4, 4) 格式的 numpy 数组，表示所有相机位姿</span></span><br><span class="line">        <span class="comment"># 提取图像高度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.H = meta[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取图像宽度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.W = meta[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像高度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像宽度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对焦距进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 加载图像并调整到指定的高度和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.img_names = load_imgs(<span class="variable language_">self</span>.img_dir, np.arange(num_img_to_load), <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[:num_img_to_load]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = <span class="variable language_">self</span>.c2ws.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成相机坐标系下的光线方向</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = comp_ray_dir_cam(<span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.focal)  <span class="comment"># (H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相机位姿从 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.from_numpy(<span class="variable language_">self</span>.c2ws).<span class="built_in">float</span>()  <span class="comment"># (N, 4, 4) 格式的 torch.float32 类型张量</span></span><br><span class="line">        <span class="comment"># 将光线方向张量转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = <span class="variable language_">self</span>.ray_dir_cam.<span class="built_in">float</span>()  <span class="comment"># (H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern&#x27;</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 注意：需要将 /your/data/path 替换为实际的数据路径，</span></span><br><span class="line">    <span class="comment"># 这里创建了一个 DataLoaderWithCOLMAP 类的实例，用于加载指定场景的数据</span></span><br><span class="line">    scene = DataLoaderWithCOLMAP(base_dir=<span class="string">&#x27;/your/data/path&#x27;</span>,</span><br><span class="line">                                 scene_name=scene_name,</span><br><span class="line">                                 data_type=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">                                 res_ratio=<span class="number">8</span>,</span><br><span class="line">                                 num_img_to_load=-<span class="number">1</span>,</span><br><span class="line">                                 skip=<span class="number">1</span>,</span><br><span class="line">                                 use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>



<h4 id="with-mask-py"><a href="#with-mask-py" class="headerlink" title="with_mask.py"></a>with_mask.py</h4><p>他们的mask其实是掩码文件，有没有可能只基于掩码文件去做呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="comment"># 从 dataloader.with_colmap 模块导入 resize_imgs 函数，</span></span><br><span class="line"><span class="comment"># 该函数用于调整图像的尺寸。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, mask_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在时间域上对帧进行下采样</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(os.listdir(mask_dir)) == <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="comment"># 若 end 为 -1 且掩码目录和图像目录文件数量相同，则按 skip 间隔选取</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 取 end 和掩码目录文件数量的最小值，避免越界</span></span><br><span class="line">        end = <span class="built_in">min</span>(end, <span class="built_in">len</span>(os.listdir(mask_dir)))</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载下采样后的图像</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 构建每个掩码图像的完整路径，假设掩码图像为 png 格式，且文件名和图像文件名对应</span></span><br><span class="line">    mask_paths = [os.path.join(mask_dir, n[:-<span class="number">4</span>]+<span class="string">&#x27;.png&#x27;</span>) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    img_list, mask_list = [], []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(img_paths)):</span><br><span class="line">            <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">            <span class="comment"># 读取对应的掩码图像，只取第一个通道</span></span><br><span class="line">            img = imageio.imread(mask_paths[i])[:, :, [<span class="number">0</span>]]  <span class="comment"># (H, W, 1)</span></span><br><span class="line">            mask_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将掩码列表转换为 numpy 数组</span></span><br><span class="line">        mask_list = np.stack(mask_list)</span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        mask_list = torch.from_numpy(mask_list).<span class="built_in">float</span>() / <span class="number">255</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])  <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;masks&#x27;</span>: mask_list,  <span class="comment"># 掩码图像张量</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 false：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建掩码目录的完整路径，假设掩码目录在图像目录的上一级的 mask 文件夹下</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_dir = os.path.join(<span class="variable language_">self</span>.imgs_dir, <span class="string">&#x27;../mask/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用 load_imgs 函数加载图像和掩码数据</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.mask_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                               <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取加载的图像数据</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 提取图像文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 提取掩码数据</span></span><br><span class="line">        <span class="variable language_">self</span>.masks = image_data[<span class="string">&#x27;masks&#x27;</span>]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC），设置近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC），设置远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 计算调整后的图像高度</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 计算调整后的图像宽度</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="comment"># 调整掩码图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.masks = resize_imgs(<span class="variable language_">self</span>.masks, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 DataLoaderAnyFolder 类</span></span><br><span class="line">    scene = DataLoaderAnyFolder(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img)</span><br></pre></td></tr></table></figure>

<h3 id="models"><a href="#models" class="headerlink" title="models"></a>models</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── models/  # 模型文件夹</span><br><span class="line">│   ├── depth_decoder.py  # 深度解码器脚本文件</span><br><span class="line">│   ├── intrinsics.py  # 内参相关脚本文件</span><br><span class="line">│   ├── layers.py  # 层相关脚本文件</span><br><span class="line">│   ├── nerf_feature.py  # NeRF特征相关脚本文件</span><br><span class="line">│   ├── nerf_mask.py  # NeRF掩码相关脚本文件</span><br><span class="line">│   ├── nerf_models.py  # NeRF模型相关脚本文件</span><br><span class="line">│   └── poses.py  # 位姿相关脚本文件</span><br></pre></td></tr></table></figure>

<h4 id="depth-decoder-py"><a href="#depth-decoder-py" class="headerlink" title="depth_decoder.py"></a>depth_decoder.py</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 版权所有 Niantic 2019。专利申请中。保留所有权利。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本软件遵循 Monodepth2 许可证的条款，</span></span><br><span class="line"><span class="comment"># 该许可证仅允许非商业用途，完整条款可在 LICENSE 文件中获取。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> models.layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积层，输入通道数为 in_planes，输出通道数为 out_planes，卷积核大小为 kernel_size</span></span><br><span class="line"><span class="comment"># 如果 instancenorm 为 True，则使用实例归一化；否则使用批量归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv</span>(<span class="params">in_planes, out_planes, kernel_size, instancenorm=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> instancenorm:</span><br><span class="line">        <span class="comment"># 构建一个包含卷积层、实例归一化层和 LeakyReLU 激活函数的序列</span></span><br><span class="line">        m = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层，使用指定的输入和输出通道数、卷积核大小，步长为 1，填充为 (kernel_size - 1) // 2，无偏置</span></span><br><span class="line">            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,</span><br><span class="line">                      stride=<span class="number">1</span>, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            <span class="comment"># 实例归一化层</span></span><br><span class="line">            nn.InstanceNorm2d(out_planes),</span><br><span class="line">            <span class="comment"># LeakyReLU 激活函数，负斜率为 0.1，原地操作</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 构建一个包含卷积层、批量归一化层和 LeakyReLU 激活函数的序列</span></span><br><span class="line">        m = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层，使用指定的输入和输出通道数、卷积核大小，步长为 1，填充为 (kernel_size - 1) // 2，无偏置</span></span><br><span class="line">            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,</span><br><span class="line">                      stride=<span class="number">1</span>, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            <span class="comment"># 批量归一化层</span></span><br><span class="line">            nn.BatchNorm2d(out_planes),</span><br><span class="line">            <span class="comment"># LeakyReLU 激活函数，负斜率为 0.1，原地操作</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度解码器类，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DepthDecoder</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 将元组转换为字符串，用于作为字典的键</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tuple_to_str</span>(<span class="params">self, key_tuple</span>):</span><br><span class="line">        key_str = <span class="string">&#x27;-&#x27;</span>.join(<span class="built_in">str</span>(key_tuple))</span><br><span class="line">        <span class="keyword">return</span> key_str</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_ch_enc, embedder, embedder_out_dim,</span></span><br><span class="line"><span class="params">                 use_alpha=<span class="literal">False</span>, scales=<span class="built_in">range</span>(<span class="params"><span class="number">4</span></span>), num_output_channels=<span class="number">4</span>,</span></span><br><span class="line"><span class="params">                 use_skips=<span class="literal">True</span>, sigma_dropout_rate=<span class="number">0.0</span>, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(DepthDecoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_output_channels = num_output_channels</span><br><span class="line">        <span class="comment"># 是否使用跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.use_skips = use_skips</span><br><span class="line">        <span class="comment"># 上采样模式</span></span><br><span class="line">        <span class="variable language_">self</span>.upsample_mode = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">        <span class="comment"># 要处理的尺度</span></span><br><span class="line">        <span class="variable language_">self</span>.scales = scales</span><br><span class="line">        <span class="comment"># 是否使用 alpha</span></span><br><span class="line">        <span class="variable language_">self</span>.use_alpha = use_alpha</span><br><span class="line">        <span class="comment"># sigma 的丢弃率</span></span><br><span class="line">        <span class="variable language_">self</span>.sigma_dropout_rate = sigma_dropout_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 嵌入器</span></span><br><span class="line">        <span class="variable language_">self</span>.embedder = embedder</span><br><span class="line">        <span class="comment"># 嵌入器的输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.E = embedder_out_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器最后一层的输出通道数</span></span><br><span class="line">        final_enc_out_channels = num_ch_enc[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 最大池化层，用于下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.downsample = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最近邻上采样层，用于上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.upsample = nn.UpsamplingNearest2d(scale_factor=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第一个下采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_down1 = conv(final_enc_out_channels, <span class="number">512</span>, <span class="number">1</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个下采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_down2 = conv(<span class="number">512</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第一个上采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_up1 = conv(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个上采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_up2 = conv(<span class="number">256</span>, final_enc_out_channels, <span class="number">1</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器各层的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_enc = num_ch_enc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;num_ch_enc=&quot;</span>, num_ch_enc)</span><br><span class="line">        <span class="comment"># 将编码器各层的通道数加上嵌入器的输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_enc = [x + <span class="variable language_">self</span>.E <span class="keyword">for</span> x <span class="keyword">in</span> <span class="variable language_">self</span>.num_ch_enc]</span><br><span class="line">        <span class="comment"># 解码器各层的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_dec = np.array([<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>])</span><br><span class="line">        <span class="comment"># self.num_ch_enc = np.array([64, 64, 128, 256, 512])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器的卷积层，使用 nn.ModuleDict 存储</span></span><br><span class="line">        <span class="variable language_">self</span>.convs = nn.ModuleDict()</span><br><span class="line">        <span class="comment"># 从 4 到 0 遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 上卷积层 0</span></span><br><span class="line">            <span class="comment"># 如果 i 为 4，则输入通道数为编码器最后一层的通道数；否则为解码器上一层的通道数</span></span><br><span class="line">            num_ch_in = <span class="variable language_">self</span>.num_ch_enc[-<span class="number">1</span>] <span class="keyword">if</span> i == <span class="number">4</span> <span class="keyword">else</span> <span class="variable language_">self</span>.num_ch_dec[i + <span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 输出通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_out = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 创建卷积块并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">0</span>))] = ConvBlock(num_ch_in, num_ch_out)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;upconv_&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="number">0</span>), num_ch_in, num_ch_out)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 上卷积层 1</span></span><br><span class="line">            <span class="comment"># 输入通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_in = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 如果使用跳跃连接且 i 大于 0，则输入通道数加上编码器上一层的通道数</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_skips <span class="keyword">and</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                num_ch_in += <span class="variable language_">self</span>.num_ch_enc[i - <span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 输出通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_out = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 创建卷积块并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">1</span>))] = ConvBlock(num_ch_in, num_ch_out)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;upconv_&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="number">1</span>), num_ch_in, num_ch_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历要处理的尺度</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.scales:</span><br><span class="line">            <span class="comment"># 创建一个 3x3 的卷积层并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;dispconv&quot;</span>, s))] = Conv3x3(<span class="variable language_">self</span>.num_ch_dec[s], <span class="variable language_">self</span>.num_output_channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sigmoid 激活函数，用于将输出映射到 [0, 1] 范围</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_features, disparity</span>):</span><br><span class="line">        <span class="comment"># 获取输入视差的批次大小和序列长度</span></span><br><span class="line">        B, S = disparity.size()</span><br><span class="line">        <span class="comment"># 对输入视差进行嵌入操作，然后增加两个维度</span></span><br><span class="line">        disparity = <span class="variable language_">self</span>.embedder(disparity.reshape(B * S, <span class="number">1</span>)).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line">tensor_1d 是一个一维张量，直接使用 torch.tensor 创建，其形状为 (<span class="number">3</span>,)。</span><br><span class="line">tensor_2d 是通过对 tensor_1d 使用 unsqueeze(<span class="number">1</span>) 增加一个维度得到的，形状为 (<span class="number">3</span>, <span class="number">1</span>)。</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为 (3,) 的一维张量</span></span><br><span class="line">tensor_1d = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tensor_1d 是否定义:&quot;</span>, <span class="string">&#x27;tensor_1d&#x27;</span> <span class="keyword">in</span> <span class="built_in">locals</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tensor_1d 的值:&quot;</span>, tensor_1d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制一维张量</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(tensor_1d)), tensor_1d, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Shape (3,) Tensor&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为 (3, 1) 的二维张量</span></span><br><span class="line">tensor_2d = tensor_1d.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制二维张量</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(tensor_2d)), tensor_2d.flatten(), width=<span class="number">0.5</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Shape (3, 1) Tensor&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line">        <span class="comment"># 扩展编码器的输出以增加感受野</span></span><br><span class="line">        <span class="comment"># 获取编码器最后一层的输出</span></span><br><span class="line">        encoder_out = input_features[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 对编码器输出进行下采样，然后通过第一个下采样卷积层</span></span><br><span class="line">        conv_down1 = <span class="variable language_">self</span>.conv_down1(<span class="variable language_">self</span>.downsample(encoder_out))</span><br><span class="line">        <span class="comment"># 对第一个下采样卷积层的输出进行下采样，然后通过第二个下采样卷积层</span></span><br><span class="line">        conv_down2 = <span class="variable language_">self</span>.conv_down2(<span class="variable language_">self</span>.downsample(conv_down1))</span><br><span class="line">        <span class="comment"># 对第二个下采样卷积层的输出进行上采样，然后通过第一个上采样卷积层</span></span><br><span class="line">        conv_up1 = <span class="variable language_">self</span>.conv_up1(<span class="variable language_">self</span>.upsample(conv_down2))</span><br><span class="line">        <span class="comment"># 对第一个上采样卷积层的输出进行上采样，然后通过第二个上采样卷积层</span></span><br><span class="line">        conv_up2 = <span class="variable language_">self</span>.conv_up2(<span class="variable language_">self</span>.upsample(conv_up1))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复 / 重塑特征</span></span><br><span class="line">        <span class="comment"># 获取第二个上采样卷积层输出的通道数、高度和宽度</span></span><br><span class="line">        _, C_feat, H_feat, W_feat = conv_up2.size()</span><br><span class="line">        <span class="comment"># 对第二个上采样卷积层的输出进行扩展和重塑</span></span><br><span class="line">        feat_tmp = conv_up2.unsqueeze(<span class="number">1</span>).expand(B, S, C_feat, H_feat, W_feat) \</span><br><span class="line">            .contiguous().view(B * S, C_feat, H_feat, W_feat)</span><br><span class="line">        <span class="comment"># 对视差进行重复操作以匹配特征图的大小</span></span><br><span class="line">        disparity_BsCHW = disparity.repeat(<span class="number">1</span>, <span class="number">1</span>, H_feat, W_feat)</span><br><span class="line">        <span class="comment"># 将扩展后的特征和视差拼接在一起</span></span><br><span class="line">        conv_up2 = torch.cat((feat_tmp, disparity_BsCHW), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复 / 重塑输入特征</span></span><br><span class="line">        <span class="keyword">for</span> i, feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(input_features):</span><br><span class="line">            <span class="comment"># 获取输入特征的通道数、高度和宽度</span></span><br><span class="line">            _, C_feat, H_feat, W_feat = feat.size()</span><br><span class="line">            <span class="comment"># 对输入特征进行扩展和重塑</span></span><br><span class="line">            feat_tmp = feat.unsqueeze(<span class="number">1</span>).expand(B, S, C_feat, H_feat, W_feat) \</span><br><span class="line">                .contiguous().view(B * S, C_feat, H_feat, W_feat)</span><br><span class="line">            <span class="comment"># 对视差进行重复操作以匹配特征图的大小</span></span><br><span class="line">            disparity_BsCHW = disparity.repeat(<span class="number">1</span>, <span class="number">1</span>, H_feat, W_feat)</span><br><span class="line">            <span class="comment"># 将扩展后的特征和视差拼接在一起</span></span><br><span class="line">            input_features[i] = torch.cat((feat_tmp, disparity_BsCHW), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        <span class="comment"># 存储输出结果的字典</span></span><br><span class="line">        outputs = &#123;&#125;</span><br><span class="line">        <span class="comment"># 初始输入为扩展后的第二个上采样卷积层的输出</span></span><br><span class="line">        x = conv_up2</span><br><span class="line">        <span class="comment"># 从 4 到 0 遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 通过上卷积层 0</span></span><br><span class="line">            x = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">0</span>))](x)</span><br><span class="line">            <span class="comment"># 进行上采样</span></span><br><span class="line">            x = [upsample(x)]</span><br><span class="line">            <span class="comment"># 如果使用跳跃连接且 i 大于 0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_skips <span class="keyword">and</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 将编码器上一层的特征添加到列表中</span></span><br><span class="line">                x += [input_features[i - <span class="number">1</span>]]</span><br><span class="line">            <span class="comment"># 将列表中的特征拼接在一起</span></span><br><span class="line">            x = torch.cat(x, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 通过上卷积层 1</span></span><br><span class="line">            x = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">1</span>))](x)</span><br><span class="line">            <span class="comment"># 如果当前尺度在要处理的尺度列表中</span></span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.scales:</span><br><span class="line">                <span class="comment"># 通过视差卷积层得到输出</span></span><br><span class="line">                output = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;dispconv&quot;</span>, i))](x)</span><br><span class="line">                <span class="comment"># 获取输出的高度和宽度</span></span><br><span class="line">                H_mpi, W_mpi = output.size(<span class="number">2</span>), output.size(<span class="number">3</span>)</span><br><span class="line">                <span class="comment"># 调整输出的维度</span></span><br><span class="line">                mpi = output.view(B, S, <span class="number">4</span>, H_mpi, W_mpi)</span><br><span class="line">                <span class="comment"># 对 RGB 通道应用 Sigmoid 激活函数</span></span><br><span class="line">                mpi_rgb = <span class="variable language_">self</span>.sigmoid(mpi[:, :, <span class="number">0</span>:<span class="number">3</span>, :, :])</span><br><span class="line">                <span class="comment"># 如果不使用 alpha，则取绝对值并加上一个小的常数；否则应用 Sigmoid 激活函数</span></span><br><span class="line">                mpi_sigma = torch.<span class="built_in">abs</span>(mpi[:, :, <span class="number">3</span>:, :, :]) + <span class="number">1e-4</span> \</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.use_alpha \</span><br><span class="line">                        <span class="keyword">else</span> <span class="variable language_">self</span>.sigmoid(mpi[:, :, <span class="number">3</span>:, :, :])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果 sigma 丢弃率大于 0 且处于训练模式</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.sigma_dropout_rate &gt; <span class="number">0.0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.training:</span><br><span class="line">                    <span class="comment"># 对 sigma 通道应用 2D Dropout</span></span><br><span class="line">                    mpi_sigma = F.dropout2d(mpi_sigma, p=<span class="variable language_">self</span>.sigma_dropout_rate)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将 RGB 和 sigma 通道拼接在一起，并存储到输出字典中</span></span><br><span class="line">                outputs[(<span class="string">&quot;disp&quot;</span>, i)] = torch.cat((mpi_rgb, mpi_sigma), dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<h4 id="intrinsics-py"><a href="#intrinsics-py" class="headerlink" title="intrinsics.py"></a>intrinsics.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个用于学习焦距的神经网络模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearnFocal</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, H, W, req_grad, fx_only, order=<span class="number">2</span>, init_focal=<span class="literal">None</span>, learn_distortion=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 调用父类 nn.Module 的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(LearnFocal, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.H = H</span><br><span class="line">        <span class="comment"># 图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="comment"># 一个布尔值，如果为 True，只输出 [fx, fx]；如果为 False，输出 [fx, fy]</span></span><br><span class="line">        <span class="variable language_">self</span>.fx_only = fx_only  </span><br><span class="line">        <span class="comment"># 焦距初始化的阶数，检查我们的补充部分有相关说明</span></span><br><span class="line">        <span class="variable language_">self</span>.order = order  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 畸变相关</span></span><br><span class="line">        <span class="comment"># 是否学习畸变参数</span></span><br><span class="line">        <span class="variable language_">self</span>.learn_distortion = learn_distortion</span><br><span class="line">        <span class="keyword">if</span> learn_distortion:</span><br><span class="line">            <span class="comment"># 第一个畸变系数，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">            <span class="variable language_">self</span>.k1 = nn.Parameter(torch.tensor(<span class="number">0.0</span>, dtype=torch.float32), requires_grad=req_grad)</span><br><span class="line">            <span class="comment"># 第二个畸变系数，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">            <span class="variable language_">self</span>.k2 = nn.Parameter(torch.tensor(<span class="number">0.0</span>, dtype=torch.float32), requires_grad=req_grad)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.fx_only:</span><br><span class="line">            <span class="keyword">if</span> init_focal <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fx 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * W = fx 计算系数 a，即 a**2 = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(W)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.order == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a * W = fx 计算系数 a，即 a = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(init_focal / <span class="built_in">float</span>(W), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;焦距初始化阶数需要为 1 或 2。退出&#x27;</span>)</span><br><span class="line">                    exit()</span><br><span class="line">                <span class="comment"># 将计算得到的系数作为 fx，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(coe_x, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> init_focal <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fx 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fy 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fy = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * W = fx 计算 x 方向的系数 a，即 a**2 = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(W)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * H = fy 计算 y 方向的系数 a，即 a**2 = fy / H</span></span><br><span class="line">                    coe_y = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(H)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.order == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a * W = fx 计算 x 方向的系数 a，即 a = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(init_focal / <span class="built_in">float</span>(W), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                    <span class="comment"># 根据公式 a * H = fy 计算 y 方向的系数 a，即 a = fy / H</span></span><br><span class="line">                    coe_y = torch.tensor(init_focal / <span class="built_in">float</span>(H), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;焦距初始化阶数需要为 1 或 2。退出&#x27;</span>)</span><br><span class="line">                    exit()</span><br><span class="line">                <span class="comment"># 将计算得到的 x 方向系数作为 fx，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(coe_x, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">                <span class="comment"># 将计算得到的 y 方向系数作为 fy，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fy = nn.Parameter(coe_y, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, i=<span class="literal">None</span></span>):  <span class="comment"># 参数 i=None 只是为了支持多 GPU 训练</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.fx_only:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy，因为 fx_only 为 True，所以 fy 等于 fx</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx ** <span class="number">2</span> * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fx ** <span class="number">2</span> * <span class="variable language_">self</span>.W])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy，因为 fx_only 为 True，所以 fy 等于 fx</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx**<span class="number">2</span> * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fy**<span class="number">2</span> * <span class="variable language_">self</span>.H])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fy * <span class="variable language_">self</span>.H])</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.learn_distortion:</span><br><span class="line">            <span class="comment"># 如果要学习畸变参数，返回焦距和畸变系数</span></span><br><span class="line">            <span class="keyword">return</span> fxfy, <span class="variable language_">self</span>.k1, <span class="variable language_">self</span>.k2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则只返回焦距</span></span><br><span class="line">            <span class="keyword">return</span> fxfy</span><br></pre></td></tr></table></figure>

<h4 id="layers-py"><a href="#layers-py" class="headerlink" title="layers.py"></a>layers.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 版权所有 Niantic 2019。专利申请中。保留所有权利。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本软件遵循 Monodepth2 许可证的条款，</span></span><br><span class="line"><span class="comment"># 该许可证仅允许非商业用途，完整条款可在 LICENSE 文件中获取。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络的 sigmoid 输出转换为深度预测</span></span><br><span class="line"><span class="comment"># 此转换公式在论文的“额外考虑”部分给出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">disp_to_depth</span>(<span class="params">disp, min_depth, max_depth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将网络的 sigmoid 输出转换为深度预测</span></span><br><span class="line"><span class="string">    该转换公式在论文的“额外考虑”部分给出。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 最小视差，为最大深度的倒数</span></span><br><span class="line">    min_disp = <span class="number">1</span> / max_depth</span><br><span class="line">    <span class="comment"># 最大视差，为最小深度的倒数</span></span><br><span class="line">    max_disp = <span class="number">1</span> / min_depth</span><br><span class="line">    <span class="comment"># 缩放后的视差</span></span><br><span class="line">    scaled_disp = min_disp + (max_disp - min_disp) * disp</span><br><span class="line">    <span class="comment"># 深度值，为缩放后视差的倒数</span></span><br><span class="line">    depth = <span class="number">1</span> / scaled_disp</span><br><span class="line">    <span class="keyword">return</span> scaled_disp, depth</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络输出的 (轴角, 平移) 转换为 4x4 矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transformation_from_parameters</span>(<span class="params">axisangle, translation, invert=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将网络的 (轴角, 平移) 输出转换为 4x4 矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从轴角表示转换为旋转矩阵</span></span><br><span class="line">    R = rot_from_axisangle(axisangle)</span><br><span class="line">    <span class="comment"># 克隆平移向量</span></span><br><span class="line">    t = translation.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        <span class="comment"># 如果需要反转，对旋转矩阵进行转置</span></span><br><span class="line">        R = R.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 平移向量取负</span></span><br><span class="line">        t *= -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line">    T = get_translation_matrix(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        <span class="comment"># 如果需要反转，先旋转再平移</span></span><br><span class="line">        M = torch.matmul(R, T)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 正常情况下，先平移再旋转</span></span><br><span class="line">        M = torch.matmul(T, R)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_translation_matrix</span>(<span class="params">translation_vector</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化一个全零的 4x4 矩阵，形状为 (batch_size, 4, 4)</span></span><br><span class="line">    T = torch.zeros(translation_vector.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>).to(device=translation_vector.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将平移向量调整为 (batch_size, 3, 1) 的形状</span></span><br><span class="line">    t = translation_vector.contiguous().view(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置矩阵的对角元素为 1</span></span><br><span class="line">    T[:, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">1</span>, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">2</span>, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将平移向量添加到矩阵的最后一列</span></span><br><span class="line">    T[:, :<span class="number">3</span>, <span class="number">3</span>, <span class="literal">None</span>] = t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将轴角旋转表示转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="comment"># （改编自 https://github.com/Wallacoloo/printipi）</span></span><br><span class="line"><span class="comment"># 输入 &#x27;vec&#x27; 必须是 Bx1x3 的形状</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rot_from_axisangle</span>(<span class="params">vec</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将轴角旋转表示转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="string">    （改编自 https://github.com/Wallacoloo/printipi）</span></span><br><span class="line"><span class="string">    输入 &#x27;vec&#x27; 必须是 Bx1x3 的形状</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算轴角的模长</span></span><br><span class="line">    angle = torch.norm(vec, <span class="number">2</span>, <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 计算单位轴向量</span></span><br><span class="line">    axis = vec / (angle + <span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算角度的余弦值</span></span><br><span class="line">    ca = torch.cos(angle)</span><br><span class="line">    <span class="comment"># 计算角度的正弦值</span></span><br><span class="line">    sa = torch.sin(angle)</span><br><span class="line">    <span class="comment"># 计算 1 - cos(angle)</span></span><br><span class="line">    C = <span class="number">1</span> - ca</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取轴向量的 x 分量，并增加一个维度</span></span><br><span class="line">    x = axis[..., <span class="number">0</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取轴向量的 y 分量，并增加一个维度</span></span><br><span class="line">    y = axis[..., <span class="number">1</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取轴向量的 z 分量，并增加一个维度</span></span><br><span class="line">    z = axis[..., <span class="number">2</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 x * sin(angle)</span></span><br><span class="line">    xs = x * sa</span><br><span class="line">    <span class="comment"># 计算 y * sin(angle)</span></span><br><span class="line">    ys = y * sa</span><br><span class="line">    <span class="comment"># 计算 z * sin(angle)</span></span><br><span class="line">    zs = z * sa</span><br><span class="line">    <span class="comment"># 计算 x * (1 - cos(angle))</span></span><br><span class="line">    xC = x * C</span><br><span class="line">    <span class="comment"># 计算 y * (1 - cos(angle))</span></span><br><span class="line">    yC = y * C</span><br><span class="line">    <span class="comment"># 计算 z * (1 - cos(angle))</span></span><br><span class="line">    zC = z * C</span><br><span class="line">    <span class="comment"># 计算 x * y * (1 - cos(angle))</span></span><br><span class="line">    xyC = x * yC</span><br><span class="line">    <span class="comment"># 计算 y * z * (1 - cos(angle))</span></span><br><span class="line">    yzC = y * zC</span><br><span class="line">    <span class="comment"># 计算 z * x * (1 - cos(angle))</span></span><br><span class="line">    zxC = z * xC</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化一个全零的 4x4 旋转矩阵，形状为 (batch_size, 4, 4)</span></span><br><span class="line">    rot = torch.zeros((vec.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>)).to(device=vec.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置旋转矩阵的元素</span></span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">0</span>] = torch.squeeze(x * xC + ca)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">1</span>] = torch.squeeze(xyC - zs)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">2</span>] = torch.squeeze(zxC + ys)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">0</span>] = torch.squeeze(xyC + zs)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">1</span>] = torch.squeeze(y * yC + ca)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">2</span>] = torch.squeeze(yzC - xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">0</span>] = torch.squeeze(zxC - ys)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">1</span>] = torch.squeeze(yzC + xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">2</span>] = torch.squeeze(z * zC + ca)</span><br><span class="line">    rot[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积块，包含卷积层、批量归一化层和 ELU 激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;执行卷积后接 ELU 的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3x3 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = Conv3x3(in_channels, out_channels)</span><br><span class="line">        <span class="comment"># ELU 激活函数，原地操作</span></span><br><span class="line">        <span class="variable language_">self</span>.nonlin = nn.ELU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 批量归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.bn = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 通过卷积层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        <span class="comment"># 通过批量归一化层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.bn(out)</span><br><span class="line">        <span class="comment"># 通过 ELU 激活函数</span></span><br><span class="line">        out = <span class="variable language_">self</span>.nonlin(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个 3x3 卷积层，包含填充操作</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对输入进行填充和卷积的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, use_refl=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv3x3, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_refl:</span><br><span class="line">            <span class="comment"># 使用反射填充</span></span><br><span class="line">            <span class="variable language_">self</span>.pad = nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用零填充</span></span><br><span class="line">            <span class="variable language_">self</span>.pad = nn.ZeroPad2d(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(<span class="built_in">int</span>(in_channels), <span class="built_in">int</span>(out_channels), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 进行填充操作</span></span><br><span class="line">        out = <span class="variable language_">self</span>.pad(x)</span><br><span class="line">        <span class="comment"># 进行卷积操作</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将深度图像转换为点云的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BackprojectDepth</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将深度图像转换为点云的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width</span>):</span><br><span class="line">        <span class="built_in">super</span>(BackprojectDepth, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 批量大小</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="variable language_">self</span>.height = height</span><br><span class="line">        <span class="comment"># 图像宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.width = width</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成二维网格坐标</span></span><br><span class="line">        meshgrid = np.meshgrid(<span class="built_in">range</span>(<span class="variable language_">self</span>.width), <span class="built_in">range</span>(<span class="variable language_">self</span>.height), indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line">        <span class="comment"># 将网格坐标堆叠在一起，并转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.id_coords = np.stack(meshgrid, axis=<span class="number">0</span>).astype(np.float32)</span><br><span class="line">        <span class="comment"># 将网格坐标转换为 PyTorch 张量，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.id_coords = nn.Parameter(torch.from_numpy(<span class="variable language_">self</span>.id_coords),</span><br><span class="line">                                      requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一个全为 1 的张量，形状为 (batch_size, 1, height * width)，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.ones = nn.Parameter(torch.ones(<span class="variable language_">self</span>.batch_size, <span class="number">1</span>, <span class="variable language_">self</span>.height * <span class="variable language_">self</span>.width),</span><br><span class="line">                                 requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调整网格坐标的形状，并重复 batch_size 次</span></span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = torch.unsqueeze(torch.stack(</span><br><span class="line">            [<span class="variable language_">self</span>.id_coords[<span class="number">0</span>].view(-<span class="number">1</span>), <span class="variable language_">self</span>.id_coords[<span class="number">1</span>].view(-<span class="number">1</span>)], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = <span class="variable language_">self</span>.pix_coords.repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将网格坐标和全 1 张量拼接在一起，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = nn.Parameter(torch.cat([<span class="variable language_">self</span>.pix_coords, <span class="variable language_">self</span>.ones], <span class="number">1</span>),</span><br><span class="line">                                       requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, depth, inv_K</span>):</span><br><span class="line">        <span class="comment"># 将逆相机内参矩阵与像素坐标相乘</span></span><br><span class="line">        cam_points = torch.matmul(inv_K[:, :<span class="number">3</span>, :<span class="number">3</span>], <span class="variable language_">self</span>.pix_coords)</span><br><span class="line">        <span class="comment"># 将深度值与相机坐标相乘</span></span><br><span class="line">        cam_points = depth.view(<span class="variable language_">self</span>.batch_size, <span class="number">1</span>, -<span class="number">1</span>) * cam_points</span><br><span class="line">        <span class="comment"># 将相机坐标和全 1 张量拼接在一起</span></span><br><span class="line">        cam_points = torch.cat([cam_points, <span class="variable language_">self</span>.ones], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cam_points</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 3D 点投影到具有内参 K 和位置 T 的相机中的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Project3D</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将 3D 点投影到具有内参 K 和位置 T 的相机中的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width, eps=<span class="number">1e-7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Project3D, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 批量大小</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="variable language_">self</span>.height = height</span><br><span class="line">        <span class="comment"># 图像宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.width = width</span><br><span class="line">        <span class="comment"># 防止除零的小常数</span></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, points, K, T</span>):</span><br><span class="line">        <span class="comment"># 计算投影矩阵 P</span></span><br><span class="line">        P = torch.matmul(K, T)[:, :<span class="number">3</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将投影矩阵 P 与 3D 点相乘</span></span><br><span class="line">        cam_points = torch.matmul(P, points)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算像素坐标</span></span><br><span class="line">        pix_coords = cam_points[:, :<span class="number">2</span>, :] / (cam_points[:, <span class="number">2</span>, :].unsqueeze(<span class="number">1</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line">        <span class="comment"># 调整像素坐标的形状</span></span><br><span class="line">        pix_coords = pix_coords.view(<span class="variable language_">self</span>.batch_size, <span class="number">2</span>, <span class="variable language_">self</span>.height, <span class="variable language_">self</span>.width)</span><br><span class="line">        <span class="comment"># 交换维度</span></span><br><span class="line">        pix_coords = pix_coords.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 归一化像素坐标</span></span><br><span class="line">        pix_coords[..., <span class="number">0</span>] /= <span class="variable language_">self</span>.width - <span class="number">1</span></span><br><span class="line">        pix_coords[..., <span class="number">1</span>] /= <span class="variable language_">self</span>.height - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将像素坐标映射到 [-1, 1] 范围</span></span><br><span class="line">        pix_coords = (pix_coords - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> pix_coords</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入张量上采样 2 倍</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upsample</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将输入张量上采样 2 倍</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> F.interpolate(x, scale_factor=<span class="number">2</span>, mode=<span class="string">&quot;nearest&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算视差图像的平滑损失</span></span><br><span class="line"><span class="comment"># 彩色图像用于边缘感知平滑</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_smooth_loss</span>(<span class="params">disp, img</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算视差图像的平滑损失</span></span><br><span class="line"><span class="string">    彩色图像用于边缘感知平滑</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算视差在 x 方向的梯度</span></span><br><span class="line">    grad_disp_x = torch.<span class="built_in">abs</span>(disp[:, :, :, :-<span class="number">1</span>] - disp[:, :, :, <span class="number">1</span>:])</span><br><span class="line">    <span class="comment"># 计算视差在 y 方向的梯度</span></span><br><span class="line">    grad_disp_y = torch.<span class="built_in">abs</span>(disp[:, :, :-<span class="number">1</span>, :] - disp[:, :, <span class="number">1</span>:, :])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算图像在 x 方向的平均梯度</span></span><br><span class="line">    grad_img_x = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :, :-<span class="number">1</span>] - img[:, :, :, <span class="number">1</span>:]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 计算图像在 y 方向的平均梯度</span></span><br><span class="line">    grad_img_y = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :-<span class="number">1</span>, :] - img[:, :, <span class="number">1</span>:, :]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据图像梯度对视差梯度进行加权</span></span><br><span class="line">    grad_disp_x *= torch.exp(-grad_img_x)</span><br><span class="line">    grad_disp_y *= torch.exp(-grad_img_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回视差梯度的平均值</span></span><br><span class="line">    <span class="keyword">return</span> grad_disp_x.mean() + grad_disp_y.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算一对图像之间 SSIM 损失的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SSIM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算一对图像之间 SSIM 损失的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SSIM, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算均值</span></span><br><span class="line">        <span class="variable language_">self</span>.mu_x_pool   = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mu_y_pool   = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算方差</span></span><br><span class="line">        <span class="variable language_">self</span>.sig_x_pool  = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sig_y_pool  = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算协方差</span></span><br><span class="line">        <span class="variable language_">self</span>.sig_xy_pool = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反射填充层</span></span><br><span class="line">        <span class="variable language_">self</span>.refl = nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 常数 C1</span></span><br><span class="line">        <span class="variable language_">self</span>.C1 = <span class="number">0.01</span> ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 常数 C2</span></span><br><span class="line">        <span class="variable language_">self</span>.C2 = <span class="number">0.03</span> ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 对输入图像进行反射填充</span></span><br><span class="line">        x = <span class="variable language_">self</span>.refl(x)</span><br><span class="line">        y = <span class="variable language_">self</span>.refl(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算图像 x 的均值</span></span><br><span class="line">        mu_x = <span class="variable language_">self</span>.mu_x_pool(x)</span><br><span class="line">        <span class="comment"># 计算图像 y 的均值</span></span><br><span class="line">        mu_y = <span class="variable language_">self</span>.mu_y_pool(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算图像 x 的方差</span></span><br><span class="line">        sigma_x  = <span class="variable language_">self</span>.sig_x_pool(x ** <span class="number">2</span>) - mu_x ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 计算图像 y 的方差</span></span><br><span class="line">        sigma_y  = <span class="variable language_">self</span>.sig_y_pool(y ** <span class="number">2</span>) - mu_y ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 计算图像 x 和 y 的协方差</span></span><br><span class="line">        sigma_xy = <span class="variable language_">self</span>.sig_xy_pool(x * y) - mu_x * mu_y</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 SSIM 分子</span></span><br><span class="line">        SSIM_n = (<span class="number">2</span> * mu_x * mu_y + <span class="variable language_">self</span>.C1) * (<span class="number">2</span> * sigma_xy + <span class="variable language_">self</span>.C2)</span><br><span class="line">        <span class="comment"># 计算 SSIM 分母</span></span><br><span class="line">        SSIM_d = (mu_x ** <span class="number">2</span> + mu_y ** <span class="number">2</span> + <span class="variable language_">self</span>.C1) * (sigma_x + sigma_y + <span class="variable language_">self</span>.C2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 SSIM 损失，并将结果限制在 [0, 1] 范围内</span></span><br><span class="line">        <span class="keyword">return</span> torch.clamp((<span class="number">1</span> - SSIM_n / SSIM_d) / <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测深度和真实深度之间的误差指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_depth_errors</span>(<span class="params">gt, pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测深度和真实深度之间的误差指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算预测深度和真实深度的比值的最大值</span></span><br><span class="line">    thresh = torch.<span class="built_in">max</span>((gt / pred), (pred / gt))</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25 的比例</span></span><br><span class="line">    a1 = (thresh &lt; <span class="number">1.25</span>     ).<span class="built_in">float</span>().mean()</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25^2 的比例</span></span><br><span class="line">    a2 = (thresh &lt; <span class="number">1.25</span> ** <span class="number">2</span>).<span class="built_in">float</span>().mean()</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25^3 的比例</span></span><br><span class="line">    a3 = (thresh &lt; <span class="number">1.25</span> ** <span class="number">3</span>).<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算均方误差</span></span><br><span class="line">    rmse = (gt - pred) ** <span class="number">2</span></span><br><span class="line">    rmse = torch.sqrt(rmse.mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算对数均方误差</span></span><br><span class="line">    rmse_log = (torch.log(gt) - torch.log(pred)) ** <span class="number">2</span></span><br><span class="line">    rmse_log = torch.sqrt(rmse_log.mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算绝对相对误差</span></span><br><span class="line">    abs_rel = torch.mean(torch.<span class="built_in">abs</span>(gt - pred) / gt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平方相对误差</span></span><br><span class="line">    sq_rel = torch.mean((gt - pred) ** <span class="number">2</span> / gt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> abs_rel, sq_rel, rmse, rmse_log, a</span><br></pre></td></tr></table></figure>

<h4 id="nerf-feature-py"><a href="#nerf-feature-py" class="headerlink" title="nerf_feature.py"></a>nerf_feature.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个名为 NerfWFeatures 的神经网络模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NerfWFeatures</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D:           标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类 nn.Module 的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络块，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络块，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims + <span class="number">32</span>, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义用于计算密度的全连接层，最后使用 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(</span><br><span class="line">            nn.Linear(D, <span class="number">1</span>), nn.Softplus()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义用于提取特征的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Sequential(</span><br><span class="line">            nn.Linear(D, D)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义用于处理特征和方向信息以生成中间特征的层</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义用于从中间特征生成 RGB 颜色的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc, cost_volume</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络块处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果、原始编码位置和代价体进行拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc, cost_volume], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, D+pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络块处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将提取的特征和编码后的方向进行拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, D+dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 rgb_layers 层处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># (H, W, N_sample, D/2)</span></span><br><span class="line">        <span class="comment"># 生成 RGB 颜色</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 颜色和密度进行拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br></pre></td></tr></table></figure>

<h4 id="nerf-mask-py"><a href="#nerf-mask-py" class="headerlink" title="nerf_mask.py"></a>nerf_mask.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个名为 OfficialNerf 的神经网络模块，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OfficialNerf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D: 标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(OfficialNerf, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络序列，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络序列，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义掩码网络，包含两个线性层，中间有 ReLU 激活函数，最后有 Sigmoid 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_net = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, <span class="number">1</span>), nn.Sigmoid())</span><br><span class="line">        <span class="comment"># 定义密度预测网络，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(nn.Linear(D, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 定义特征提取的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Linear(D, D)</span><br><span class="line">        <span class="comment"># 定义 RGB 处理的神经网络序列，包含一个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义 RGB 预测的神经网络序列，包含一个线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络序列处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果和原始编码位置在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过掩码网络得到掩码概率</span></span><br><span class="line">        mask_prob = <span class="variable language_">self</span>.mask_net(x)</span><br><span class="line">        <span class="comment"># 通过密度预测网络得到密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过特征提取线性层得到特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将特征和编码后的方向在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 处理神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D / 2)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 预测神经网络序列得到 RGB 值</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 值、密度和掩码概率在第3维拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density, mask_prob], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br></pre></td></tr></table></figure>

<h4 id="nerf-models-py"><a href="#nerf-models-py" class="headerlink" title="nerf_models.py"></a>nerf_models.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 OfficialNerf 类，继承自 nn.Module，用于实现官方的 NeRF 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OfficialNerf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D: 标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(OfficialNerf, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络序列，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络序列，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义密度预测网络，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(nn.Linear(D, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 定义特征提取的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Linear(D, D)</span><br><span class="line">        <span class="comment"># 定义 RGB 处理的神经网络序列，包含一个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义 RGB 预测的神经网络序列，包含一个线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络序列处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果和原始编码位置在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过密度预测网络得到密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过特征提取线性层得到特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将特征和编码后的方向在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 处理神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D / 2)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 预测神经网络序列得到 RGB 值</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 值和密度在第 3 维拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 fullNeRF 类，继承自 nn.Module，用于实现完整的 NeRF 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">fullNeRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels_xyz, in_channels_dir, W, D=<span class="number">8</span>, skips=[<span class="number">4</span>]</span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 存储网络的深度</span></span><br><span class="line">        <span class="variable language_">self</span>.D = D</span><br><span class="line">        <span class="comment"># 存储隐藏层的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="comment"># 存储跳跃连接的位置</span></span><br><span class="line">        <span class="variable language_">self</span>.skips = skips</span><br><span class="line">        <span class="comment"># 存储输入位置编码的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.in_channels_xyz = in_channels_xyz</span><br><span class="line">        <span class="comment"># 存储输入方向编码的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.in_channels_dir = in_channels_dir</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义位置编码层</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(D):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 第一层，输入维度为输入位置编码的通道数，输出维度为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(in_channels_xyz, W)</span><br><span class="line">            <span class="keyword">elif</span> i <span class="keyword">in</span> skips:</span><br><span class="line">                <span class="comment"># 跳跃连接层，输入维度为隐藏层宽度加上输入位置编码的通道数，输出维度为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(W + in_channels_xyz, W)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 普通层，输入和输出维度均为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(W, W)</span><br><span class="line">            <span class="comment"># 为每个层添加 ReLU 激活函数</span></span><br><span class="line">            layer = nn.Sequential(layer, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">            <span class="comment"># 将层添加到模型中</span></span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;xyz_encoding_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>, layer)</span><br><span class="line">        <span class="comment"># 定义位置编码的最终线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.xyz_encoding_final = nn.Linear(W, W)</span><br><span class="line">        <span class="comment"># 定义方向编码层</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_encoding = nn.Sequential(</span><br><span class="line">            nn.Linear(W + in_channels_dir, W), nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(W, W // <span class="number">2</span>), nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义静态输出层</span></span><br><span class="line">        <span class="comment"># 静态密度预测层，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.static_sigma = nn.Sequential(nn.Linear(W, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 静态 RGB 预测层，包含两个线性层，中间有 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.static_rgb = nn.Sequential(nn.Linear(W // <span class="number">2</span>, W // <span class="number">2</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Linear(W // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_xyz, input_dir_a</span>):</span><br><span class="line">        <span class="comment"># 存储输入的位置编码</span></span><br><span class="line">        xyz_ = input_xyz</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.D):</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.skips:</span><br><span class="line">                <span class="comment"># 如果是跳跃连接位置，将输入的位置编码和当前处理结果拼接</span></span><br><span class="line">                xyz_ = torch.cat([input_xyz, xyz_], -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 通过相应的位置编码层处理</span></span><br><span class="line">            xyz_ = <span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;xyz_encoding_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>)(xyz_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过静态密度预测层得到静态密度</span></span><br><span class="line">        static_sigma = <span class="variable language_">self</span>.static_sigma(xyz_)  <span class="comment"># 输出形状为 (B, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过位置编码的最终线性层得到最终位置编码</span></span><br><span class="line">        xyz_encoding_final = <span class="variable language_">self</span>.xyz_encoding_final(xyz_)</span><br><span class="line">        <span class="comment"># 将最终位置编码和输入的方向编码拼接</span></span><br><span class="line">        dir_encoding_input = torch.cat([xyz_encoding_final, input_dir_a], -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 通过方向编码层处理拼接后的结果</span></span><br><span class="line">        dir_encoding = <span class="variable language_">self</span>.dir_encoding(dir_encoding_input)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过静态 RGB 预测层得到静态 RGB 值</span></span><br><span class="line">        static_rgb = <span class="variable language_">self</span>.static_rgb(dir_encoding)</span><br><span class="line">        <span class="comment"># 将静态 RGB 值和静态密度拼接</span></span><br><span class="line">        static = torch.cat([static_rgb, static_sigma], -<span class="number">1</span>)  <span class="comment"># 输出形状为 (B, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> static</span><br></pre></td></tr></table></figure>

<h4 id="poses-py"><a href="#poses-py" class="headerlink" title="poses.py"></a>poses.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> make_c2w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LearnPose 类，继承自 nn.Module，用于学习相机位姿</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearnPose</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_cams, learn_R, learn_t, init_c2w=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param num_cams: 相机的数量</span></span><br><span class="line"><span class="string">        :param learn_R: 是否学习旋转部分，布尔值</span></span><br><span class="line"><span class="string">        :param learn_t: 是否学习平移部分，布尔值</span></span><br><span class="line"><span class="string">        :param init_c2w: (N, 4, 4) 的 torch 张量，表示初始的相机到世界的变换矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(LearnPose, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 存储相机的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.num_cams = num_cams</span><br><span class="line">        <span class="comment"># 初始化初始相机到世界的变换矩阵为 None</span></span><br><span class="line">        <span class="variable language_">self</span>.init_c2w = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> init_c2w <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果提供了初始变换矩阵，将其作为不可训练的参数存储</span></span><br><span class="line">            <span class="variable language_">self</span>.init_c2w = nn.Parameter(init_c2w, requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义旋转参数，初始化为全零，是否可训练由 learn_R 决定</span></span><br><span class="line">        <span class="variable language_">self</span>.r = nn.Parameter(torch.zeros(size=(num_cams, <span class="number">3</span>), dtype=torch.float32), requires_grad=learn_R)  <span class="comment"># (N, 3)</span></span><br><span class="line">        <span class="comment"># 定义平移参数，初始化为全零，是否可训练由 learn_t 决定</span></span><br><span class="line">        <span class="variable language_">self</span>.t = nn.Parameter(torch.zeros(size=(num_cams, <span class="number">3</span>), dtype=torch.float32), requires_grad=learn_t)  <span class="comment"># (N, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, cam_id</span>):</span><br><span class="line">        <span class="comment"># 根据相机 ID 提取对应的旋转参数，形状为 (3, )，表示轴角</span></span><br><span class="line">        r = <span class="variable language_">self</span>.r[cam_id]  <span class="comment"># (3, ) 轴角</span></span><br><span class="line">        <span class="comment"># 根据相机 ID 提取对应的平移参数，形状为 (3, )</span></span><br><span class="line">        t = <span class="variable language_">self</span>.t[cam_id]  <span class="comment"># (3, )</span></span><br><span class="line">        <span class="comment"># 使用 make_c2w 函数将轴角和平移参数转换为相机到世界的变换矩阵，形状为 (4, 4)</span></span><br><span class="line">        c2w = make_c2w(r, t)  <span class="comment"># (4, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果提供了初始变换矩阵，学习初始位姿和目标位姿之间的增量位姿</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.init_c2w <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 将当前计算得到的变换矩阵与初始变换矩阵相乘</span></span><br><span class="line">            c2w = c2w @ <span class="variable language_">self</span>.init_c2w[cam_id]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> c2w</span><br></pre></td></tr></table></figure>



<h2 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── utils/  # 工具文件夹</span><br><span class="line">│   ├── align_traj.py  # 轨迹对齐脚本文件，用于对不同轨迹数据进行对齐操作</span><br><span class="line">│   ├── comp_ate.py  # 计算绝对轨迹误差（Absolute Trajectory Error, ATE）的脚本文件</span><br><span class="line">│   ├── comp_ray_dir.py  # 计算光线方向的脚本文件，常用于计算机视觉和三维重建中的光线追踪等场景</span><br><span class="line">│   ├── lie_group_helper.py  # 李群相关辅助函数的脚本文件，李群在机器人运动学、计算机视觉中的位姿表示等方面有应用</span><br><span class="line">│   ├── pos_enc.py  # 位置编码脚本文件，在深度学习模型（如NeRF）中用于对位置信息进行编码</span><br><span class="line">│   ├── pose_utils.py  # 位姿处理工具脚本文件，包含处理相机位姿或物体位姿的相关函数</span><br><span class="line">│   ├── split_dataset.py  # 数据集划分脚本文件，用于将数据集划分为训练集、验证集和测试集等</span><br><span class="line">│   ├── training_utils.py  # 训练辅助工具脚本文件，包含训练模型时常用的工具函数，如学习率调整、损失函数计算等</span><br><span class="line">│   ├── vgg.py  # VGG网络相关脚本文件，可能包含VGG模型的定义、加载预训练权重等操作</span><br><span class="line">│   ├── vis_cam_traj.py  # 可视化相机轨迹的脚本文件，用于将相机在三维空间中的运动轨迹进行可视化展示</span><br><span class="line">│   └── volume_op.py  # 体操作脚本文件，在三维重建、体渲染等场景中对三维体数据进行操作</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
</search>
