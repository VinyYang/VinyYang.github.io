<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法基础课(持续更新中)</title>
    <url>/algorithm_learning/</url>
    <content><![CDATA[<p>算法基础课</p>
<h2 id="基础算法（一）"><a href="#基础算法（一）" class="headerlink" title="基础算法（一）"></a>基础算法（一）</h2><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> i=l<span class="number">-1</span>,j=r<span class="number">+1</span>,x=q[l+r&gt;&gt;<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;j)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">do</span> i++;<span class="keyword">while</span>(q[i]&lt;x);</span><br><span class="line">        <span class="keyword">do</span> j--;<span class="keyword">while</span>(q[j]&gt;x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">quick_sort</span>(q,l,j);</span><br><span class="line">    <span class="built_in">quick_sort</span>(q,j<span class="number">+1</span>,r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> mid=l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">merge_sort</span>(q,l,mid),<span class="built_in">merge_sort</span>(q,mid<span class="number">+1</span>,r)；</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> k=<span class="number">0</span>,i=l,j=mid<span class="number">+1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid&amp;&amp;j&lt;=r)</span><br><span class="line">        <span class="keyword">if</span>(q[i]&lt;=q[j])tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">else</span> tmp[k++]=q[j++];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid)tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">while</span>(j&lt;=r)tmp[k++]=q[j++];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i=l,j=<span class="number">0</span>;i&lt;=r;i++,j++)q[i]=tmp[j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="二分"><a href="#二分" class="headerlink" title="二分"></a>二分</h3><h4 id="整数二分"><a href="#整数二分" class="headerlink" title="整数二分"></a>整数二分</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_1</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;    <span class="comment">// check()判断mid是否满足性质</span></span><br><span class="line">        <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_2</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) l = mid;</span><br><span class="line">        <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="浮点数二分"><a href="#浮点数二分" class="headerlink" title="浮点数二分"></a>浮点数二分</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">double</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">bsearch_3</span><span class="params">(<span class="type">double</span> l, <span class="type">double</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> eps = <span class="number">1e-6</span>;   <span class="comment">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class="line">    <span class="keyword">while</span> (r - l &gt; eps)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">double</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;</span><br><span class="line">        <span class="keyword">else</span> l = mid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="基础算法（二）"><a href="#基础算法（二）" class="headerlink" title="基础算法（二）"></a>基础算法（二）</h2><h3 id="高精度"><a href="#高精度" class="headerlink" title="高精度"></a>高精度</h3><h4 id="高精度加法"><a href="#高精度加法" class="headerlink" title="高精度加法"></a>高精度加法</h4><p>本质上是模拟人进行列竖式加法的过程，即$a_1$+…+$a_n$+t，其中当$a$的退一位加起来不超过10则$t&#x3D;0$否则$t&#x3D;1$，以此类推</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt;<span class="built_in">add</span>(vector&lt;<span class="type">int</span>&gt;&amp;A,vector&lt;<span class="type">int</span>&gt;&amp;B)</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;C;</span><br><span class="line">    <span class="type">int</span> t=<span class="number">0</span>;<span class="comment">//存储每一位相加后的数，同时完成是否需要进位的考量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>()||i&lt;B.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i&lt;A.<span class="built_in">size</span>())t+=A[i];</span><br><span class="line">        <span class="keyword">if</span>(i&lt;B.<span class="built_in">size</span>())t+=B[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>(t%<span class="number">10</span>);</span><br><span class="line">        t/=<span class="number">10</span>;</span><br><span class="line">	&#125;    </span><br><span class="line">    <span class="keyword">if</span>(t)C.<span class="built_in">push_back</span>(<span class="number">1</span>);<span class="comment">//最后一个t代表最高位，因此等for循环运行完后再用判断看看进位情况，是的话就在vector后面再加一个1</span></span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string a,b;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;A,B;</span><br><span class="line">    cin&gt;&gt;a&gt;&gt;b;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=a.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)A.<span class="built_in">push_back</span>(a[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=b.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)B.<span class="built_in">push_back</span>(b[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line"><span class="comment">//A、B元素进栈都是逆序进栈，比如a=&#x27;123456&#x27;,那A=[6.5,4,3,2,1],目的是方便进位多了1的时候整体数组挪动空间不大，否则要是最高位在前边ta进位后面数组所有数字都要往后挪</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">auto</span> C=<span class="built_in">add</span>(A,B);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=C.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)cout&lt;&lt;C[i];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="vector相关知识点"><a href="#vector相关知识点" class="headerlink" title="vector相关知识点"></a>vector相关知识点</h5><ul>
<li><p><strong>vector定义</strong> <code>vector</code> 是 C++ 标准模板库（STL）中的一个容器，它可以看作是一个动态数组。它能存储一系列相同类型的元素，并且可以根据需要自动调整大小。使用 <code>vector</code> 需要包含 <code>&lt;vector&gt;</code> 头文件。</p>
</li>
<li><p><strong>vector数组相关操作</strong>（最后面）增加用<code>.push_back(一个数)</code>，（最后面）删除<code>.pop_back(一个数)</code>，指定位置插入<code>.insert(向量,指定位置)</code>，指定位置删除<code>.erase(向量，指定位置)</code>，清空<code>.clear()</code>，查看大小<code>.size()</code></p>
</li>
<li><p><strong>vector元素的索引和数组一样，也是从 0 开始存储的</strong></p>
</li>
<li><p><strong>vector和普通数组区别</strong></p>
<p>1.大小灵活性</p>
<ul>
<li><strong>普通数组</strong>：大小在定义时就必须确定，且在程序运行过程中不能改变。例如 <code>int arr[10];</code> 定义了一个大小为 10 的整数数组，之后无法再改变其大小。</li>
<li><strong>vector</strong>：可以动态调整大小。可以使用 <code>push_back()</code> 方法在末尾添加元素，当空间不足时，<code>vector</code> 会自动分配更大的内存空间来存储元素。</li>
</ul>
<p>2.内存管理</p>
<ul>
<li><strong>普通数组</strong>：由程序员手动管理内存。如果数组在栈上分配，其生命周期受限于所在的代码块；如果在堆上分配（使用 <code>new</code>），则需要手动使用 <code>delete</code> 释放内存，否则会造成内存泄漏。</li>
<li><strong>vector</strong>：自动管理内存。当 <code>vector</code> 不再使用时，其占用的内存会自动释放，无需程序员手动干预。</li>
</ul>
<p>3.功能丰富度</p>
<ul>
<li><strong>普通数组</strong>：只提供基本的元素访问功能，操作相对有限。</li>
<li><strong>vector</strong>：提供了丰富的成员函数，如 <code>size()</code> 获取元素数量、<code>empty()</code> 判断是否为空、<code>clear()</code> 清空元素等，使用起来更加方便。</li>
</ul>
</li>
<li><p><strong>使用 vector 而不用数组的原因</strong></p>
<p>在大整数相加的场景中，输入的数字长度是不确定的。如果使用普通数组，需要预先定义一个足够大的数组来存储数字的每一位，但这样可能会浪费大量的内存空间。而 <code>vector</code> 可以根据输入数字的实际长度动态调整大小，避免了内存的浪费，并且其提供的 <code>push_back()</code> 方法可以方便地将数字的每一位添加到容器中，同时在处理进位时也更加方便。因此，使用 <code>vector</code> 能更好地适应这种动态长度的需求。</p>
</li>
</ul>
<h5 id="add函数运行示例"><a href="#add函数运行示例" class="headerlink" title="add函数运行示例"></a>add函数运行示例</h5><p>假设我们要计算两个四位数 <code>a = 1234</code> 和 <code>b = 5678</code> 的和。</p>
<p>在代码中，输入的数字以字符串形式存储，然后将其逆序存储到 <code>vector&lt;int&gt;</code> 中，这样做是为了方便处理进位。对于 <code>a = 1234</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>A = &#123;4, 3, 2, 1&#125;</code>；对于 <code>b = 5678</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>B = &#123;8, 7, 6, 5&#125;</code>。</p>
<p>具体步骤：</p>
<ul>
<li><p><input checked="" disabled="" type="checkbox"> 
初始化</p>
</li>
<li><p><code>vector&lt;int&gt; C;</code>：用于存储相加结果的向量，初始为空。</p>
</li>
<li><p><code>int t = 0;</code>：用于存储每一位相加后的结果以及进位信息，初始值为 0。</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
循环处理每一位</p>
</li>
</ul>
<p>循环条件为 <code>i &lt; A.size() || i &lt; B.size()</code>，这意味着只要 <code>A</code> 或 <code>B</code> 还有未处理的位，就会继续循环。</p>
<ul>
<li><p><input disabled="" type="checkbox"> 
第一次循环（<code>i = 0</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：因为 <code>i = 0</code> 小于 <code>A.size()</code>（4），所以 <code>t = t + A[0] = 0 + 4 = 4</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：因为 <code>i = 0</code> 小于 <code>B.size()</code>（4），所以 <code>t = t + B[0] = 4 + 8 = 12</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 2）添加到 <code>C</code> 中，此时 <code>C = &#123;2&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第二次循环（<code>i = 1</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[1] = 1 + 3 = 4</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[1] = 4 + 7 = 11</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 1）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第三次循环（<code>i = 2</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[2] = 1 + 2 = 3</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[2] = 3 + 6 = 9</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 9）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第四次循环（<code>i = 3</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[3] = 0 + 1 = 1</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[3] = 1 + 5 = 6</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 6）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9, 6&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
处理最后可能的进位</p>
</li>
</ul>
<p><code>if(t) C.push_back(1);</code></p>
<p>因为此时 <code>t = 0</code>，所以不需要添加额外的进位，<code>C</code> 仍然为 <code>&#123;2, 1, 9, 6&#125;</code>。</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> 返回结果</li>
</ul>
<p><code>return C;</code>：返回存储相加结果的向量 <code>C</code>。</p>
<p>在 <code>main</code> 函数中，将 <code>C</code> 逆序输出，得到最终结果 <code>6912</code>，即 <code>1234 + 5678 = 6912</code>。</p>
<h5 id="auto的用法"><a href="#auto的用法" class="headerlink" title="auto的用法"></a>auto的用法</h5><p><code>auto</code>是C++11的新特性，可以自动识别变量属性。比如<code>auto C=add(A,B);</code>就自动识别了add返回的类型，因此<code>auto C &lt;=&gt; vector&lt;int&gt;c</code></p>
<h5 id="a-i-0-的作用：将字符串每一位由ASCII码转为整数"><a href="#a-i-0-的作用：将字符串每一位由ASCII码转为整数" class="headerlink" title="a[i] - &#39;0&#39; 的作用：将字符串每一位由ASCII码转为整数"></a><code>a[i] - &#39;0&#39;</code> 的作用：将字符串每一位由ASCII码转为整数</h5><p>在 C++ 中，当你从输入读取一个数字字符串时，比如 <code>&quot;123&quot;</code>，字符串中的每个字符（如 <code>&#39;1&#39;</code>、<code>&#39;2&#39;</code>、<code>&#39;3&#39;</code>）实际上存储的是字符的 ASCII 码值，而不是对应的数值。字符 <code>&#39;0&#39;</code> 到 <code>&#39;9&#39;</code> 的 ASCII 码是连续的，<code>&#39;0&#39;</code> 的 ASCII 码值是 48，<code>&#39;1&#39;</code> 是 49，以此类推，<code>&#39;9&#39;</code> 是 57。</p>
<p><code>b[i] - &#39;0&#39;</code> 的作用就是将字符形式的数字转换为对应的整数值。例如，当 <code>b[i]</code> 为 <code>&#39;1&#39;</code> 时，<code>&#39;1&#39;</code> 的 ASCII 码值是 49，<code>&#39;0&#39;</code> 的 ASCII 码值是 48，那么 <code>&#39;1&#39; - &#39;0&#39;</code> 就等于 <code>49 - 48 = 1</code>，这样就把字符 <code>&#39;1&#39;</code> 转换为了整数 1。</p>
<p>如果没有 <code>b[i] - &#39;0&#39;</code> 这个操作，直接将字符存储到 <code>vector&lt;int&gt;</code> 中，那么存储的是字符的 ASCII 码值，而不是对应的数值，这会导致后续的计算出现错误。</p>
<h4 id="高精度减法"><a href="#高精度减法" class="headerlink" title="高精度减法"></a>高精度减法</h4><h4 id="高精度乘法"><a href="#高精度乘法" class="headerlink" title="高精度乘法"></a>高精度乘法</h4><h4 id="高精度除法"><a href="#高精度除法" class="headerlink" title="高精度除法"></a>高精度除法</h4>]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>ACWing</tag>
      </tags>
  </entry>
  <entry>
    <title>如何用hexo新建文件并上传/如何用hexo插入图片（next主题）</title>
    <url>/hexo_maintanance/</url>
    <content><![CDATA[<p>hexo（博主采用next主题）日常维护教程</p>
<h1 id="修改scaffolds-post-md（默认标题文件）"><a href="#修改scaffolds-post-md（默认标题文件）" class="headerlink" title="修改scaffolds&#x2F;post.md（默认标题文件）"></a>修改scaffolds&#x2F;post.md（默认标题文件）</h1><p>注意修改时不要把中文加进去，在此只起到注释作用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:([1,2]设置同时属于不同类别可以这样)</span><br><span class="line">categories:</span><br><span class="line">top:(表示置顶情况，不置顶不填即可数字大小代表置顶顺序，数字越大排序越前)</span><br></pre></td></tr></table></figure>



<h1 id="新建md文件"><a href="#新建md文件" class="headerlink" title="新建md文件"></a>新建md文件</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new/hexo n &quot;文件名&quot;</span><br></pre></td></tr></table></figure>

<h1 id="查看新建文件"><a href="#查看新建文件" class="headerlink" title="查看新建文件"></a>查看新建文件</h1><p>进入目录是source&#x2F;_posts</p>
<h1 id="完善标题对应信息，填写md"><a href="#完善标题对应信息，填写md" class="headerlink" title="完善标题对应信息，填写md"></a>完善标题对应信息，填写md</h1><p>也就是刚才上面那堆东西</p>
<h1 id="在blog目录下（因人而异）打开git-bash"><a href="#在blog目录下（因人而异）打开git-bash" class="headerlink" title="在blog目录下（因人而异）打开git bash"></a>在blog目录下（因人而异）打开git bash</h1><p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure>

<p>即可上传</p>
<p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>

<p>即可实时查看网页</p>
<h1 id="插入图片操作（图片和md文件最好均为英文名）"><a href="#插入图片操作（图片和md文件最好均为英文名）" class="headerlink" title="插入图片操作（图片和md文件最好均为英文名）"></a>插入图片操作（图片和md文件最好均为英文名）</h1><h2 id="下插件"><a href="#下插件" class="headerlink" title="下插件"></a>下插件</h2><p>见<a href="https://github.com/yiyungent/hexo-asset-img">yiyungent&#x2F;hexo-asset-img：🍰 Hexo 本地图片插件。|Hexo 本地图片插件：转换图片相对路径为asset_img</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-img --save</span><br><span class="line">或者</span><br><span class="line">npm install git://github.com/yiyungent/hexo-asset-img.git#main</span><br></pre></td></tr></table></figure>

<h2 id="修改host-config-yml"><a href="#修改host-config-yml" class="headerlink" title="修改host&#x2F;_config.yml"></a>修改host&#x2F;_config.yml</h2><p>permalink控制了永久域名的样式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.post_asset_folder: true</span><br><span class="line">2.permalink: :title/（我的自带了日期导致图片一直不行:year/:month/:date/:title/）</span><br></pre></td></tr></table></figure>

<h2 id="直接粘贴图片"><a href="#直接粘贴图片" class="headerlink" title="直接粘贴图片"></a>直接粘贴图片</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![1](./hexo_maintanance/1.png)</span><br></pre></td></tr></table></figure>

<p>上面是下面图片的路径。能看到下面的图片就能说明这个方法就是成功的。</p>
<p><img src="/./hexo_maintanance/1.png" alt="1"></p>
<h1 id="实现侧边栏标题全展开"><a href="#实现侧边栏标题全展开" class="headerlink" title="实现侧边栏标题全展开"></a>实现侧边栏标题全展开</h1><p>有些文件目录很长，不全展开不方便看。可以修改</p>
<p><code>blog\themes\next\source\css\_common\outline\sidebar\sidebar-toc.styl</code>文件</p>
<p>查找修改<code>.nav-child</code>对应代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">.<span class="property">nav</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (not hexo-<span class="title function_">config</span>(<span class="string">&#x27;toc.expand_all&#x27;</span>)) &#123;</span><br><span class="line">    .<span class="property">nav</span>-child &#123;</span><br><span class="line">      --<span class="attr">height</span>: auto;          <span class="comment">/* 取消高度限制 */</span></span><br><span class="line">      <span class="attr">height</span>: auto;            <span class="comment">/* 启用自动高度适应内容 */</span></span><br><span class="line">      <span class="attr">opacity</span>: <span class="number">1</span>;              <span class="comment">/* 取消透明度隐藏 */</span></span><br><span class="line">      <span class="attr">overflow</span>: visible;       <span class="comment">/* 允许内容溢出显示 */</span></span><br><span class="line">      transition-<span class="attr">property</span>: opacity;  <span class="comment">/* 仅保留透明度过渡 */</span></span><br><span class="line">      <span class="attr">visibility</span>: visible;     <span class="comment">/* 确保元素可见 */</span></span><br><span class="line">      <span class="attr">transition</span>: $transition-ease;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>原配置为：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">原配置--<span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">opacity</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">overflow</span>: hidden;</span><br><span class="line">transition-<span class="attr">property</span>: height, opacity, visibility;</span><br><span class="line"><span class="attr">transition</span>: $transition-ease;</span><br><span class="line"><span class="attr">visibility</span>: hidden;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>博客维护</category>
      </categories>
      <tags>
        <tag>博客维护</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕OCC-NeRF:Occlusion-Free Scene Recovery via Neural Radiance Fields</title>
    <url>/%E6%89%8B%E6%92%95nerfmm/</url>
    <content><![CDATA[<p>链接：<a href="https://freebutuselesssoul.github.io/occnerf/">OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields</a></p>
<h2 id="文件夹目录"><a href="#文件夹目录" class="headerlink" title="文件夹目录"></a>文件夹目录</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ-nerf/</span><br><span class="line">├── .gitignore</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── environment.yml</span><br><span class="line">├── local1.txt</span><br><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save.py</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br><span class="line">├── models/</span><br><span class="line">│   ├── depth_decoder.py</span><br><span class="line">│   ├── intrinsics.py</span><br><span class="line">│   ├── layers.py</span><br><span class="line">│   ├── nerf_feature.py</span><br><span class="line">│   ├── nerf_mask.py</span><br><span class="line">│   ├── nerf_models.py</span><br><span class="line">│   └── poses.py</span><br><span class="line">├── utils/</span><br><span class="line">│   ├── align_traj.py</span><br><span class="line">│   ├── comp_ate.py</span><br><span class="line">│   ├── comp_ray_dir.py</span><br><span class="line">│   ├── lie_group_helper.py</span><br><span class="line">│   ├── pos_enc.py</span><br><span class="line">│   ├── pose_utils.py</span><br><span class="line">│   ├── split_dataset.py</span><br><span class="line">│   ├── training_utils.py</span><br><span class="line">│   ├── vgg.py</span><br><span class="line">│   ├── vis_cam_traj.py</span><br><span class="line">│   └── volume_op.py</span><br><span class="line">├── tasks/</span><br><span class="line">│   └── ...</span><br><span class="line">└── third_party/</span><br><span class="line">    ├── ATE/</span><br><span class="line">    │   └── README.md</span><br><span class="line">    └── pytorch_ssim/</span><br></pre></td></tr></table></figure>

<h2 id="DEBUG-代码"><a href="#DEBUG-代码" class="headerlink" title="DEBUG 代码"></a>DEBUG 代码</h2><h3 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save.py</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br></pre></td></tr></table></figure>

<h4 id="any-folder-py"><a href="#any-folder-py" class="headerlink" title="any_folder.py"></a>any_folder.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os                                       <span class="comment"># 操作系统接口模块</span></span><br><span class="line"><span class="keyword">import</span> torch                                    <span class="comment"># PyTorch 深度学习框架</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                              <span class="comment"># 科学计算库</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm                           <span class="comment"># 进度条显示模块</span></span><br><span class="line"><span class="keyword">import</span> imageio                                  <span class="comment"># 图像 IO 处理库</span></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs  <span class="comment"># 自定义图像缩放函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 获取并排序目录下所有文件名</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:                                 <span class="comment"># 从 start 开始按间隔 skip 取图</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 取 start 到 end 区间按间隔 skip 取图</span></span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:                           <span class="comment"># 是否打乱图像顺序</span></span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):          <span class="comment"># 检查请求数量是否超出范围</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;图像请求数<span class="subst">&#123;num_img_to_load&#125;</span>超过可用数<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>&#x27;</span>)</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:                   <span class="comment"># 加载全部可用图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;加载全部<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张图像&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 截取指定数量的图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;从<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张中加载<span class="subst">&#123;num_img_to_load&#125;</span>张&#x27;</span>)</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line">    </span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]  <span class="comment"># 构建完整文件路径</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)                         <span class="comment"># 计算实际加载数量</span></span><br><span class="line">    </span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:                                     <span class="comment"># 实际加载图像数据</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]        <span class="comment"># 读取 RGB 三通道图像</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        img_list = np.stack(img_list)                <span class="comment"># 堆叠为 4D 数组</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 转换为浮点张量并归一化</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]  <span class="comment"># 获取图像尺寸</span></span><br><span class="line">    <span class="keyword">else</span>:                                            <span class="comment"># 仅获取图像尺寸</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])</span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;                                         <span class="comment"># 返回结构化数据</span></span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,        <span class="comment"># 图像张量 (N, H, W, 3)</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># 图像文件名数组</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,        <span class="comment"># 总图像数</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,                  <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,                  <span class="comment"># 图像宽度</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, </span></span><br><span class="line"><span class="params">                 start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):  <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir                  <span class="comment"># 数据根目录</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name              <span class="comment"># 场景名称</span></span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio                <span class="comment"># 分辨率缩放比例</span></span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load    <span class="comment"># 最大加载数量</span></span><br><span class="line">        <span class="variable language_">self</span>.start = start                        <span class="comment"># 起始索引</span></span><br><span class="line">        <span class="variable language_">self</span>.end = end                            <span class="comment"># 结束索引</span></span><br><span class="line">        <span class="variable language_">self</span>.skip = skip                          <span class="comment"># 采样间隔</span></span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted            <span class="comment"># 是否保持顺序</span></span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img                  <span class="comment"># 是否实际加载图像</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)  <span class="comment"># 构建图像目录路径</span></span><br><span class="line">        </span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load,  <span class="comment"># 加载图像数据</span></span><br><span class="line">                              <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                              <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]             <span class="comment"># 图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]   <span class="comment"># 文件名列表</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]         <span class="comment"># 图像总数</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]               <span class="comment"># 原始高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]               <span class="comment"># 原始宽度</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span>                            <span class="comment"># 近裁剪面(NDC 坐标系)</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span>                             <span class="comment"># 远裁剪面(NDC 坐标系)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:                     <span class="comment"># 计算实际使用分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:                          <span class="comment"># 执行图像缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span>                   <span class="comment"># 数据根目录配置示例</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span>                <span class="comment"># 场景路径配置示例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span>                               <span class="comment"># 缩放比例配置</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span>                           <span class="comment"># 加载全部图像</span></span><br><span class="line">    start, end, skip = <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>                    <span class="comment"># 采样参数初始化</span></span><br><span class="line">    load_sorted, load_img = <span class="literal">True</span>, <span class="literal">True</span>             <span class="comment"># 加载配置参数</span></span><br><span class="line">    </span><br><span class="line">    scene = DataLoaderAnyFolder(                   <span class="comment"># 创建数据加载实例</span></span><br><span class="line">        base_dir=base_dir,</span><br><span class="line">        scene_name=scene_name,</span><br><span class="line">        res_ratio=resize_ratio,</span><br><span class="line">        num_img_to_load=num_img_to_load,</span><br><span class="line">        start=start,</span><br><span class="line">        end=end,</span><br><span class="line">        skip=skip,</span><br><span class="line">        load_sorted=load_sorted,</span><br><span class="line">        load_img=load_img)</span><br></pre></td></tr></table></figure>

<h4 id="local-save-py"><a href="#local-save-py" class="headerlink" title="local_save.py"></a>local_save.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vgg19</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, requires_grad=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 加载预训练的 VGG19 模型的特征提取部分</span></span><br><span class="line">        <span class="variable language_">self</span>.vgg_pretrained_features = models.vgg19(pretrained=<span class="literal">True</span>).features</span><br><span class="line">        <span class="comment"># 如果不需要计算梯度，则将模型参数的 requires_grad 属性设置为 False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> requires_grad:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> <span class="variable language_">self</span>.parameters():</span><br><span class="line">                param.requires_grad = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 初始化特征图的形状为 None</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_shape = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, indices=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 记录输入特征图的形状</span></span><br><span class="line">        <span class="variable language_">self</span>.feature_shape = X.shape</span><br><span class="line">        <span class="comment"># 如果没有指定索引，则默认使用 [7, 25]</span></span><br><span class="line">        <span class="keyword">if</span> indices <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            indices = [<span class="number">7</span>,<span class="number">25</span>]</span><br><span class="line">        <span class="comment"># 存储提取的特征图</span></span><br><span class="line">        out = []</span><br><span class="line">        <span class="comment"># 遍历到最后一个索引位置</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(indices[-<span class="number">1</span>]):</span><br><span class="line">            <span class="comment"># 通过 VGG19 的第 i 层进行特征提取</span></span><br><span class="line">            X = <span class="variable language_">self</span>.vgg_pretrained_features[i](X)</span><br><span class="line">            <span class="comment"># 如果当前层的索引加 1 在指定的索引列表中</span></span><br><span class="line">            <span class="keyword">if</span> (i+<span class="number">1</span>) <span class="keyword">in</span> indices:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.feature_shape <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="comment"># 如果特征图形状为 None，则记录当前特征图的形状</span></span><br><span class="line">                    <span class="variable language_">self</span>.feature_shape = X.shape</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment"># 对特征图进行双线性插值，使其尺寸与输入特征图的尺寸一致</span></span><br><span class="line">                    X = F.interpolate(X,<span class="variable language_">self</span>.feature_shape[-<span class="number">2</span>:],mode=<span class="string">&#x27;bilinear&#x27;</span>,align_corners=<span class="literal">True</span>)</span><br><span class="line">                <span class="comment"># 将处理后的特征图添加到输出列表中</span></span><br><span class="line">                out.append(X)</span><br><span class="line">        <span class="comment"># 将所有提取的特征图在通道维度上拼接起来</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat(out,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像的文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir))) <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 时间域下采样：根据 start、end 和 skip 参数选择图像</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:</span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查要加载的图像数量是否超过可用图像数量</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个图像文件名</span></span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建图像文件的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储加载的图像</span></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载进度</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            <span class="comment"># 读取图像并截取前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>] <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            <span class="comment"># 将图像添加到列表中</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list) <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span> <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>]) <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储加载图像的相关信息</span></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list, <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names, <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">    self.c2ws: (N_imgs, 4, 4) torch.float32</span></span><br><span class="line"><span class="string">    self.imgs (N_imgs, H, W, 4) torch.float32</span></span><br><span class="line"><span class="string">    self.ray_dir_cam (H, W, 3) torch.float32</span></span><br><span class="line"><span class="string">    self.H scalar</span></span><br><span class="line"><span class="string">    self.W scalar</span></span><br><span class="line"><span class="string">    self.N_imgs scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start, end, skip, load_sorted, load_img=<span class="literal">True</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 False：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用 load_imgs 函数加载图像</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                               <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        <span class="comment"># 加载的图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>] <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 图像的文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>] <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 初始化 VGG19 编码器</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = Vgg19()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W) <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="comment"># 存储图像的特征</span></span><br><span class="line">            <span class="variable language_">self</span>.features = []</span><br><span class="line">            <span class="comment"># 使用 tqdm 显示处理进度</span></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(<span class="variable language_">self</span>.imgs):</span><br><span class="line">                <span class="comment"># 对图像进行通道维度的调整，并通过编码器提取特征</span></span><br><span class="line">                <span class="variable language_">self</span>.features.append(<span class="variable language_">self</span>.encoder(img.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)[<span class="literal">None</span>,...]))</span><br><span class="line">            <span class="comment"># 将所有图像的特征在批次维度上拼接起来</span></span><br><span class="line">            <span class="variable language_">self</span>.features = torch.cat(<span class="variable language_">self</span>.features,<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 特征图的尺寸</span></span><br><span class="line">            <span class="variable language_">self</span>.feature_size = (<span class="variable language_">self</span>.features.shape[-<span class="number">2</span>],<span class="variable language_">self</span>.features.shape[-<span class="number">1</span>]) <span class="comment"># (H,W)</span></span><br><span class="line">            <span class="comment"># 打印特征图的形状</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="variable language_">self</span>.features.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 DataLoaderAnyFolder 类</span></span><br><span class="line">    scene = DataLoaderAnyFolder(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img)</span><br></pre></td></tr></table></figure>



<h4 id="with-colmap-py"><a href="#with-colmap-py" class="headerlink" title="with_colmap.py"></a>with_colmap.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可以用来处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 可以在 CPU 或 GPU 上进行高效的数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可以进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 可以在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_imgs</span>(<span class="params">imgs, new_h, new_w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param imgs:    (N, H, W, 3)            torch.float32 RGB</span></span><br><span class="line"><span class="string">    :param new_h:   int/torch int</span></span><br><span class="line"><span class="string">    :param new_w:   int/torch int</span></span><br><span class="line"><span class="string">    :return:        (N, new_H, new_W, 3)    torch.float32 RGB</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将图像张量从 (N, H, W, 3) 转换为 (N, 3, H, W) 以适应 F.interpolate 函数的输入要求</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (N, 3, H, W)</span></span><br><span class="line">    <span class="comment"># 使用双线性插值方法将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    imgs = F.interpolate(imgs, size=(new_h, new_w), mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># (N, 3, new_H, new_W)</span></span><br><span class="line">    <span class="comment"># 将图像张量从 (N, 3, new_H, new_W) 转换回 (N, new_H, new_W, 3)</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># (N, new_H, new_W, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs  <span class="comment"># (N, new_H, new_W, 3) torch.float32 RGB</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, img_ids, new_h, new_w</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line">    <span class="comment"># 根据给定的图像索引筛选出需要的图像文件名</span></span><br><span class="line">    img_names = img_names[img_ids]  <span class="comment"># image name for this split</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">        <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">        img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">        img_list.append(img)</span><br><span class="line">    <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">    img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">    img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">    <span class="comment"># 调用 resize_imgs 函数将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    img_list = resize_imgs(img_list, new_h, new_w)</span><br><span class="line">    <span class="keyword">return</span> img_list, img_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Read the poses_bounds.npy file produced by LLFF imgs2poses.py.</span></span><br><span class="line"><span class="string">    This function is modified from https://github.com/kwea123/nerf_pl.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))  <span class="comment"># (N_images, 17)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># (N_images, 3, 5)</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># (N_images, 2)</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;down right back&quot; 改为 &quot;right up back&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws -&gt; centred c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># (N_images, 3, 4), (4, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 获取最近深度值</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 计算缩放因子，将最近深度调整到稍大于 1.0 的位置</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  <span class="comment"># 0.75 is the default parameter</span></span><br><span class="line">        <span class="comment"># 对深度边界进行缩放</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        <span class="comment"># 对相机位姿的平移部分进行缩放</span></span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># (N, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) np</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) np</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) np</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderWithCOLMAP</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, data_type, res_ratio, num_img_to_load, skip, use_ndc, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir:</span></span><br><span class="line"><span class="string">        :param scene_name:</span></span><br><span class="line"><span class="string">        :param data_type:   &#x27;train&#x27; or &#x27;val&#x27;.</span></span><br><span class="line"><span class="string">        :param res_ratio:   int [1, 2, 4] etc to resize images to a lower resolution.</span></span><br><span class="line"><span class="string">        :param num_img_to_load/skip: control frame loading in temporal domain.</span></span><br><span class="line"><span class="string">        :param use_ndc      True/False, just centre the poses and scale them.</span></span><br><span class="line"><span class="string">        :param load_img:    True/False. If set to false: only count number of images, get H and W,</span></span><br><span class="line"><span class="string">                            but do not load imgs. Useful when vis poses or debug etc.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.data_type = data_type</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建场景目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = os.path.join(<span class="variable language_">self</span>.scene_dir, <span class="string">&#x27;images&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.scene_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = meta[<span class="string">&#x27;c2ws&#x27;</span>]  <span class="comment"># (N, 4, 4) all camera pose</span></span><br><span class="line">        <span class="comment"># 提取图像高度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.H = meta[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取图像宽度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.W = meta[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像高度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像宽度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对焦距进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 加载图像并调整到指定的高度和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.img_names = load_imgs(<span class="variable language_">self</span>.img_dir, np.arange(num_img_to_load), <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[:num_img_to_load]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = <span class="variable language_">self</span>.c2ws.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成相机坐标系下的光线方向</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = comp_ray_dir_cam(<span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.focal)  <span class="comment"># (H, W, 3) torch.float32</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相机位姿从 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.from_numpy(<span class="variable language_">self</span>.c2ws).<span class="built_in">float</span>()  <span class="comment"># (N, 4, 4) torch.float32</span></span><br><span class="line">        <span class="comment"># 将光线方向张量转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = <span class="variable language_">self</span>.ray_dir_cam.<span class="built_in">float</span>()  <span class="comment"># (H, W, 3) torch.float32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern&#x27;</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 注意：需要将 /your/data/path 替换为实际的数据路径，</span></span><br><span class="line">    <span class="comment"># 这里创建了一个 DataLoaderWithCOLMAP 类的实例，用于加载指定场景的数据</span></span><br><span class="line">    scene = DataLoaderWithCOLMAP(base_dir=<span class="string">&#x27;/your/data/path&#x27;</span>,</span><br><span class="line">                                 scene_name=scene_name,</span><br><span class="line">                                 data_type=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">                                 res_ratio=<span class="number">8</span>,</span><br><span class="line">                                 num_img_to_load=-<span class="number">1</span>,</span><br><span class="line">                                 skip=<span class="number">1</span>,</span><br><span class="line">                                 use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>



<h4 id="with-feature-colmap-py"><a href="#with-feature-colmap-py" class="headerlink" title="with_feature_colmap.py"></a>with_feature_colmap.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="comment"># 从 dataloader.with_colmap 模块导入 resize_imgs 函数，</span></span><br><span class="line"><span class="comment"># 该函数用于调整图像的尺寸。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="comment"># torchvision 是 PyTorch 中用于计算机视觉任务的库，</span></span><br><span class="line"><span class="comment"># models 子模块提供了预训练的深度学习模型。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 例如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.vgg <span class="keyword">import</span> Vgg19</span><br><span class="line"><span class="comment"># 从 utils.vgg 模块导入 Vgg19 类，可能用于特征提取。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在时间域上对帧进行下采样</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:</span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载下采样后的图像</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])  <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    result = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Read the poses_bounds.npy file produced by LLFF imgs2poses.py.</span></span><br><span class="line"><span class="string">    This function is modified from https://github.com/kwea123/nerf_pl.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;../poses_bounds.npy&#x27;</span>))  <span class="comment"># (N_images, 17)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># (N_images, 3, 5)</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># (N_images, 2)</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;down right back&quot; 改为 &quot;right up back&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws -&gt; centred c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># (N_images, 3, 4), (4, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 修正尺度，使最近的深度略大于 1.0</span></span><br><span class="line">        <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 0.75 是默认参数</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  </span><br><span class="line">        <span class="comment"># 最近的深度约为 1/0.75 = 1.33</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># (N, 4, 4)</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) np</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) np</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># scalar</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) np</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataloader_feature_n_colmap</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start=<span class="number">0</span>, end=-<span class="number">1</span>, skip=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 load_sorted=<span class="literal">True</span>, load_img=<span class="literal">True</span>, use_ndc=<span class="literal">True</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 false：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息并转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.Tensor(meta[<span class="string">&#x27;c2ws&#x27;</span>])  <span class="comment"># (N, 4, 4) all camera pose</span></span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line">        <span class="comment"># 根据 start、end 和 skip 参数对相机位姿进行筛选</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.end == -<span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[<span class="variable language_">self</span>.start::<span class="variable language_">self</span>.skip]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[<span class="variable language_">self</span>.start:<span class="variable language_">self</span>.end:<span class="variable language_">self</span>.skip]</span><br><span class="line">        <span class="comment"># 加载图像数据</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                                <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        <span class="comment"># 提取加载的图像数据</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 提取图像文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 初始化 Vgg19 编码器并将其移动到指定设备</span></span><br><span class="line">        <span class="variable language_">self</span>.encoder = Vgg19().to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC）</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 计算调整后的图像高度</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 计算调整后的图像宽度</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        <span class="comment"># 调整焦距</span></span><br><span class="line">        <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率并将其移动到指定设备</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W).to(device)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="variable language_">self</span>.features = []</span><br><span class="line">            <span class="comment"># 使用 tqdm 显示处理进度</span></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(<span class="variable language_">self</span>.imgs):</span><br><span class="line">                <span class="comment"># 对图像进行通道维度的调整，并通过编码器提取特征</span></span><br><span class="line">                <span class="variable language_">self</span>.features.append(<span class="variable language_">self</span>.encoder(img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)[<span class="literal">None</span>, ...]))</span><br><span class="line">            <span class="comment"># 这里注释掉了特征拼接的代码，可根据需要取消注释</span></span><br><span class="line">            <span class="comment"># self.features = torch.cat(self.features, 0)</span></span><br><span class="line">            <span class="comment"># print(self.features.shape)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否使用归一化设备坐标（NDC）</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 Dataloader_feature_n_colmap 类</span></span><br><span class="line">    scene = Dataloader_feature_n_colmap(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img,</span><br><span class="line">                                use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>



<h4 id="with-feature-py"><a href="#with-feature-py" class="headerlink" title="with_feature.py"></a>with_feature.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># torch.nn.functional 提供了许多神经网络中常用的函数，</span></span><br><span class="line"><span class="comment"># 例如激活函数、损失函数、卷积、池化等操作，</span></span><br><span class="line"><span class="comment"># 这些函数是无状态的，通常用于自定义神经网络层中的具体运算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.comp_ray_dir <span class="keyword">import</span> comp_ray_dir_cam</span><br><span class="line"><span class="comment"># 从 utils 包中的 comp_ray_dir 模块导入 comp_ray_dir_cam 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于计算相机坐标系下的光线方向。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.pose_utils <span class="keyword">import</span> center_poses</span><br><span class="line"><span class="comment"># 从 utils 包中的 pose_utils 模块导入 center_poses 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于对相机位姿进行中心化处理。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> convert3x4_4x4</span><br><span class="line"><span class="comment"># 从 utils 包中的 lie_group_helper 模块导入 convert3x4_4x4 函数，</span></span><br><span class="line"><span class="comment"># 推测该函数用于将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize_imgs</span>(<span class="params">imgs, new_h, new_w</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param imgs:    (N, H, W, 3)            torch.float32 格式的 RGB 图像</span></span><br><span class="line"><span class="string">    :param new_h:   整数或 torch 整数类型，表示新的图像高度</span></span><br><span class="line"><span class="string">    :param new_w:   整数或 torch 整数类型，表示新的图像宽度</span></span><br><span class="line"><span class="string">    :return:        (N, new_H, new_W, 3)    torch.float32 格式的 RGB 图像</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将图像张量的维度从 (N, H, W, 3) 调整为 (N, 3, H, W)，以适配 F.interpolate 函数的输入要求</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 变为 (N, 3, H, W)</span></span><br><span class="line">    <span class="comment"># 使用双线性插值方法将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    imgs = F.interpolate(imgs, size=(new_h, new_w), mode=<span class="string">&#x27;bilinear&#x27;</span>)  <span class="comment"># 变为 (N, 3, new_H, new_W)</span></span><br><span class="line">    <span class="comment"># 将图像张量的维度从 (N, 3, new_H, new_W) 调整回 (N, new_H, new_W, 3)</span></span><br><span class="line">    imgs = imgs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 变为 (N, new_H, new_W, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> imgs  <span class="comment"># 返回 (N, new_H, new_W, 3) 格式的 torch.float32 类型 RGB 图像</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, img_ids, new_h, new_w</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 得到所有图像文件名</span></span><br><span class="line">    <span class="comment"># 根据给定的图像索引筛选出本次需要的图像文件名</span></span><br><span class="line">    img_names = img_names[img_ids]  <span class="comment"># 得到本次分割所需的图像文件名</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line"></span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">        <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">        img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># 得到 (H, W, 3) 格式的 np.uint8 类型图像</span></span><br><span class="line">        img_list.append(img)</span><br><span class="line">    <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">    img_list = np.stack(img_list)  <span class="comment"># 变为 (N, H, W, 3) 格式</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">    img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 变为 (N, H, W, 3) 格式的 torch.float32 类型</span></span><br><span class="line">    <span class="comment"># 调用 resize_imgs 函数将图像调整到指定的新高度和新宽度</span></span><br><span class="line">    img_list = resize_imgs(img_list, new_h, new_w)</span><br><span class="line">    <span class="keyword">return</span> img_list, img_names</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_meta</span>(<span class="params">in_dir, use_ndc</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取由 LLFF 的 imgs2poses.py 生成的 poses_bounds.npy 文件。</span></span><br><span class="line"><span class="string">    此函数改编自 https://github.com/kwea123/nerf_pl。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 加载 poses_bounds.npy 文件，该文件包含相机位姿和深度边界信息</span></span><br><span class="line">    poses_bounds = np.load(os.path.join(in_dir, <span class="string">&#x27;poses_bounds.npy&#x27;</span>))  <span class="comment"># 得到 (N_images, 17) 格式的数组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取相机位姿信息，将其重塑为 (N_images, 3, 5) 的形状</span></span><br><span class="line">    c2ws = poses_bounds[:, :<span class="number">15</span>].reshape(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>)  <span class="comment"># 变为 (N_images, 3, 5) 格式</span></span><br><span class="line">    <span class="comment"># 提取深度边界信息</span></span><br><span class="line">    bounds = poses_bounds[:, -<span class="number">2</span>:]  <span class="comment"># 变为 (N_images, 2) 格式</span></span><br><span class="line">    <span class="comment"># 提取图像高度、宽度和焦距信息</span></span><br><span class="line">    H, W, focal = c2ws[<span class="number">0</span>, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 修正相机位姿的旋转部分，将旋转形式从 &quot;下 右 后&quot; 改为 &quot;右 上 后&quot;</span></span><br><span class="line">    <span class="comment"># 参考 https://github.com/bmild/nerf/issues/34</span></span><br><span class="line">    c2ws = np.concatenate([c2ws[..., <span class="number">1</span>:<span class="number">2</span>], -c2ws[..., :<span class="number">1</span>], c2ws[..., <span class="number">2</span>:<span class="number">4</span>]], -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对相机位姿进行中心化处理，返回中心化后的相机位姿和平均位姿</span></span><br><span class="line">    <span class="comment"># pose_avg @ c2ws 得到中心化后的 c2ws</span></span><br><span class="line">    c2ws, pose_avg = center_poses(c2ws)  <span class="comment"># 分别得到 (N_images, 3, 4) 和 (4, 4) 格式的数组</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> use_ndc:</span><br><span class="line">        <span class="comment"># 获取最近深度值</span></span><br><span class="line">        near_original = bounds.<span class="built_in">min</span>()</span><br><span class="line">        <span class="comment"># 计算缩放因子，使最近深度调整到稍大于 1.0 的位置</span></span><br><span class="line">        scale_factor = near_original * <span class="number">0.75</span>  <span class="comment"># 0.75 是默认参数</span></span><br><span class="line">        <span class="comment"># 现在最近深度约为 1/0.75 = 1.33</span></span><br><span class="line">        <span class="comment"># 对深度边界进行缩放</span></span><br><span class="line">        bounds /= scale_factor</span><br><span class="line">        <span class="comment"># 对相机位姿的平移部分进行缩放</span></span><br><span class="line">        c2ws[..., <span class="number">3</span>] /= scale_factor</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 3x4 的相机位姿矩阵转换为 4x4 的齐次矩阵形式</span></span><br><span class="line">    c2ws = convert3x4_4x4(c2ws)  <span class="comment"># 变为 (N, 4, 4) 格式</span></span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;c2ws&#x27;</span>: c2ws,       <span class="comment"># (N, 4, 4) 格式的 numpy 数组</span></span><br><span class="line">        <span class="string">&#x27;bounds&#x27;</span>: bounds,   <span class="comment"># (N_images, 2) 格式的 numpy 数组</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: <span class="built_in">int</span>(H),        <span class="comment"># 标量，图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: <span class="built_in">int</span>(W),        <span class="comment"># 标量，图像宽度</span></span><br><span class="line">        <span class="string">&#x27;focal&#x27;</span>: focal,     <span class="comment"># 标量，焦距</span></span><br><span class="line">        <span class="string">&#x27;pose_avg&#x27;</span>: pose_avg,  <span class="comment"># (4, 4) 格式的 numpy 数组</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderWithCOLMAP</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    最有用的字段：</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4) 格式的 torch.float32 类型张量，表示相机位姿</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4) 格式的 torch.float32 类型张量，表示图像</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3) 格式的 torch.float32 类型张量，表示相机坐标系下的光线方向</span></span><br><span class="line"><span class="string">        self.H              标量，图像高度</span></span><br><span class="line"><span class="string">        self.W              标量，图像宽度</span></span><br><span class="line"><span class="string">        self.N_imgs         标量，图像数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, data_type, res_ratio, num_img_to_load, skip, use_ndc, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param data_type: 数据类型，&#x27;train&#x27; 或 &#x27;val&#x27;。</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param num_img_to_load/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param use_ndc: 布尔值，是否对相机位姿进行中心化和缩放。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 False：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.data_type = data_type</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.use_ndc = use_ndc</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建场景目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.img_dir = os.path.join(<span class="variable language_">self</span>.scene_dir, <span class="string">&#x27;images&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取所有的元信息，包括相机位姿、深度边界、图像尺寸和焦距等</span></span><br><span class="line">        meta = read_meta(<span class="variable language_">self</span>.scene_dir, <span class="variable language_">self</span>.use_ndc)</span><br><span class="line">        <span class="comment"># 提取相机位姿信息</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = meta[<span class="string">&#x27;c2ws&#x27;</span>]  <span class="comment"># (N, 4, 4) 格式的 numpy 数组，表示所有相机位姿</span></span><br><span class="line">        <span class="comment"># 提取图像高度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.H = meta[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取图像宽度信息</span></span><br><span class="line">        <span class="variable language_">self</span>.W = meta[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line">        <span class="comment"># 提取焦距信息</span></span><br><span class="line">        <span class="variable language_">self</span>.focal = <span class="built_in">float</span>(meta[<span class="string">&#x27;focal&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像高度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对图像宽度进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 如果需要调整图像分辨率，对焦距进行相应的缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.focal /= <span class="variable language_">self</span>.res_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 加载图像并调整到指定的高度和宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.img_names = load_imgs(<span class="variable language_">self</span>.img_dir, np.arange(num_img_to_load), <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line">        <span class="comment"># 截取前 num_img_to_load 个相机位姿</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = <span class="variable language_">self</span>.c2ws[:num_img_to_load]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = <span class="variable language_">self</span>.c2ws.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成相机坐标系下的光线方向</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = comp_ray_dir_cam(<span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.focal)  <span class="comment"># (H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将相机位姿从 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line">        <span class="variable language_">self</span>.c2ws = torch.from_numpy(<span class="variable language_">self</span>.c2ws).<span class="built_in">float</span>()  <span class="comment"># (N, 4, 4) 格式的 torch.float32 类型张量</span></span><br><span class="line">        <span class="comment"># 将光线方向张量转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.ray_dir_cam = <span class="variable language_">self</span>.ray_dir_cam.<span class="built_in">float</span>()  <span class="comment"># (H, W, 3) 格式的 torch.float32 类型张量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern&#x27;</span></span><br><span class="line">    use_ndc = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 注意：需要将 /your/data/path 替换为实际的数据路径，</span></span><br><span class="line">    <span class="comment"># 这里创建了一个 DataLoaderWithCOLMAP 类的实例，用于加载指定场景的数据</span></span><br><span class="line">    scene = DataLoaderWithCOLMAP(base_dir=<span class="string">&#x27;/your/data/path&#x27;</span>,</span><br><span class="line">                                 scene_name=scene_name,</span><br><span class="line">                                 data_type=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">                                 res_ratio=<span class="number">8</span>,</span><br><span class="line">                                 num_img_to_load=-<span class="number">1</span>,</span><br><span class="line">                                 skip=<span class="number">1</span>,</span><br><span class="line">                                 use_ndc=use_ndc)</span><br></pre></td></tr></table></figure>



<h4 id="with-mask-py"><a href="#with-mask-py" class="headerlink" title="with_mask.py"></a>with_mask.py</h4><p>他们的mask其实是掩码文件，有没有可能只基于掩码文件去做呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># os 模块提供了与操作系统进行交互的功能，</span></span><br><span class="line"><span class="comment"># 可用于处理文件和目录路径、创建/删除目录、获取环境变量等，</span></span><br><span class="line"><span class="comment"># 在本代码里主要用于构建文件和目录的路径。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># torch 是 PyTorch 的核心库，提供了张量（Tensor）数据结构，</span></span><br><span class="line"><span class="comment"># 支持自动求导机制，用于构建和训练深度学习模型，</span></span><br><span class="line"><span class="comment"># 能够在 CPU 或 GPU 上高效地进行数值计算。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># numpy 是 Python 中用于科学计算的基础库，</span></span><br><span class="line"><span class="comment"># 提供了高效的多维数组对象和各种数学函数，</span></span><br><span class="line"><span class="comment"># 可进行数组操作、线性代数运算、随机数生成等，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于处理图像数据和数组操作。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="comment"># tqdm 是一个快速、可扩展的进度条工具，</span></span><br><span class="line"><span class="comment"># 能在循环中显示进度条，方便用户了解代码执行的进度。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="comment"># imageio 是一个用于读取和写入多种图像文件格式的库，</span></span><br><span class="line"><span class="comment"># 在代码中主要用于读取图像文件。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs</span><br><span class="line"><span class="comment"># 从 dataloader.with_colmap 模块导入 resize_imgs 函数，</span></span><br><span class="line"><span class="comment"># 该函数用于调整图像的尺寸。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, mask_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    <span class="comment"># 获取图像目录下所有图像文件名，并按字母顺序排序</span></span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># all image names</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在时间域上对帧进行下采样</span></span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(os.listdir(mask_dir)) == <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="comment"># 若 end 为 -1 且掩码目录和图像目录文件数量相同，则按 skip 间隔选取</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 取 end 和掩码目录文件数量的最小值，避免越界</span></span><br><span class="line">        end = <span class="built_in">min</span>(end, <span class="built_in">len</span>(os.listdir(mask_dir)))</span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不按顺序加载图像，则对图像文件名进行随机打乱</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:</span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载下采样后的图像</span></span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Asked for &#123;0:6d&#125; images but only &#123;1:6d&#125; available. Exit.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading all available &#123;0:6d&#125; images&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(img_names)))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading &#123;0:6d&#125; images out of &#123;1:6d&#125; images.&#x27;</span>.<span class="built_in">format</span>(num_img_to_load, <span class="built_in">len</span>(img_names)))</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建每个图像的完整路径</span></span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 构建每个掩码图像的完整路径，假设掩码图像为 png 格式，且文件名和图像文件名对应</span></span><br><span class="line">    mask_paths = [os.path.join(mask_dir, n[:-<span class="number">4</span>]+<span class="string">&#x27;.png&#x27;</span>) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]</span><br><span class="line">    <span class="comment"># 图像的数量</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)</span><br><span class="line"></span><br><span class="line">    img_list, mask_list = [], []</span><br><span class="line">    <span class="keyword">if</span> load_img:</span><br><span class="line">        <span class="comment"># 使用 tqdm 显示加载图像的进度</span></span><br><span class="line">        <span class="keyword">for</span> i, p <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(img_paths)):</span><br><span class="line">            <span class="comment"># 读取图像并只保留前三个通道（RGB）</span></span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]  <span class="comment"># (H, W, 3) np.uint8</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">            <span class="comment"># 读取对应的掩码图像，只取第一个通道</span></span><br><span class="line">            img = imageio.imread(mask_paths[i])[:, :, [<span class="number">0</span>]]  <span class="comment"># (H, W, 1)</span></span><br><span class="line">            mask_list.append(img)</span><br><span class="line">        <span class="comment"># 将图像列表转换为 numpy 数组</span></span><br><span class="line">        img_list = np.stack(img_list)  <span class="comment"># (N, H, W, 3)</span></span><br><span class="line">        <span class="comment"># 将掩码列表转换为 numpy 数组</span></span><br><span class="line">        mask_list = np.stack(mask_list)</span><br><span class="line">        <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将像素值归一化到 [0, 1] 范围</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        mask_list = torch.from_numpy(mask_list).<span class="built_in">float</span>() / <span class="number">255</span></span><br><span class="line">        <span class="comment"># 获取图像的高度和宽度</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不加载图像，则读取第一张图像以获取图像的高度和宽度</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])  <span class="comment"># load one image to get H, W</span></span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="string">&#x27;masks&#x27;</span>: mask_list,  <span class="comment"># 掩码图像张量</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,</span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,</span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Most useful fields:</span></span><br><span class="line"><span class="string">        self.c2ws:          (N_imgs, 4, 4)      torch.float32</span></span><br><span class="line"><span class="string">        self.imgs           (N_imgs, H, W, 4)   torch.float32</span></span><br><span class="line"><span class="string">        self.ray_dir_cam    (H, W, 3)           torch.float32</span></span><br><span class="line"><span class="string">        self.H              scalar</span></span><br><span class="line"><span class="string">        self.W              scalar</span></span><br><span class="line"><span class="string">        self.N_imgs         scalar</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param base_dir: 数据的基础目录</span></span><br><span class="line"><span class="string">        :param scene_name: 场景的名称</span></span><br><span class="line"><span class="string">        :param res_ratio: 整数，如 [1, 2, 4] 等，用于将图像调整为较低的分辨率。</span></span><br><span class="line"><span class="string">        :param start/end/skip: 用于在时间域上控制帧的加载。</span></span><br><span class="line"><span class="string">        :param load_sorted: 布尔值，是否按顺序加载图像。</span></span><br><span class="line"><span class="string">        :param load_img: 布尔值。如果设置为 false：仅统计图像数量、获取图像的高度和宽度，</span></span><br><span class="line"><span class="string">                         但不加载图像。在可视化位姿或调试等情况下很有用。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir</span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name</span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio</span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load</span><br><span class="line">        <span class="variable language_">self</span>.start = start</span><br><span class="line">        <span class="variable language_">self</span>.end = end</span><br><span class="line">        <span class="variable language_">self</span>.skip = skip</span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted</span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建图像目录的完整路径</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)</span><br><span class="line">        <span class="comment"># 构建掩码目录的完整路径，假设掩码目录在图像目录的上一级的 mask 文件夹下</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_dir = os.path.join(<span class="variable language_">self</span>.imgs_dir, <span class="string">&#x27;../mask/&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调用 load_imgs 函数加载图像和掩码数据</span></span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.mask_dir, <span class="variable language_">self</span>.num_img_to_load, <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                               <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取加载的图像数据</span></span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">        <span class="comment"># 提取图像文件名</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]  <span class="comment"># (N, )</span></span><br><span class="line">        <span class="comment"># 提取掩码数据</span></span><br><span class="line">        <span class="variable language_">self</span>.masks = image_data[<span class="string">&#x27;masks&#x27;</span>]</span><br><span class="line">        <span class="comment"># 图像的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]</span><br><span class="line">        <span class="comment"># 原始图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC），设置近裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 始终使用归一化设备坐标（NDC），设置远裁剪平面距离</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果需要调整图像分辨率</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="comment"># 计算调整后的图像高度</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="comment"># 计算调整后的图像宽度</span></span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:</span><br><span class="line">            <span class="comment"># 调整图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)  <span class="comment"># (N, H, W, 3) torch.float32</span></span><br><span class="line">            <span class="comment"># 调整掩码图像的分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.masks = resize_imgs(<span class="variable language_">self</span>.masks, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 数据的基础目录，需要替换为实际的路径</span></span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span></span><br><span class="line">    <span class="comment"># 场景的名称</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span></span><br><span class="line">    <span class="comment"># 图像的缩放比例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span></span><br><span class="line">    <span class="comment"># 要加载的图像数量，-1 表示加载所有图像</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 开始加载图像的索引</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 结束加载图像的索引，-1 表示加载到最后</span></span><br><span class="line">    end = -<span class="number">1</span></span><br><span class="line">    <span class="comment"># 加载图像的间隔</span></span><br><span class="line">    skip = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 是否按顺序加载图像</span></span><br><span class="line">    load_sorted = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 是否加载图像</span></span><br><span class="line">    load_img = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化 DataLoaderAnyFolder 类</span></span><br><span class="line">    scene = DataLoaderAnyFolder(base_dir=base_dir,</span><br><span class="line">                                scene_name=scene_name,</span><br><span class="line">                                res_ratio=resize_ratio,</span><br><span class="line">                                num_img_to_load=num_img_to_load,</span><br><span class="line">                                start=start,</span><br><span class="line">                                end=end,</span><br><span class="line">                                skip=skip,</span><br><span class="line">                                load_sorted=load_sorted,</span><br><span class="line">                                load_img=load_img)</span><br></pre></td></tr></table></figure>

<h3 id="models"><a href="#models" class="headerlink" title="models"></a>models</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── models/  # 模型文件夹</span><br><span class="line">│   ├── depth_decoder.py  # 深度解码器脚本文件</span><br><span class="line">│   ├── intrinsics.py  # 内参相关脚本文件</span><br><span class="line">│   ├── layers.py  # 层相关脚本文件</span><br><span class="line">│   ├── nerf_feature.py  # NeRF特征相关脚本文件</span><br><span class="line">│   ├── nerf_mask.py  # NeRF掩码相关脚本文件</span><br><span class="line">│   ├── nerf_models.py  # NeRF模型相关脚本文件</span><br><span class="line">│   └── poses.py  # 位姿相关脚本文件</span><br></pre></td></tr></table></figure>

<h4 id="depth-decoder-py"><a href="#depth-decoder-py" class="headerlink" title="depth_decoder.py"></a>depth_decoder.py</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 版权所有 Niantic 2019。专利申请中。保留所有权利。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本软件遵循 Monodepth2 许可证的条款，</span></span><br><span class="line"><span class="comment"># 该许可证仅允许非商业用途，完整条款可在 LICENSE 文件中获取。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> models.layers <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积层，输入通道数为 in_planes，输出通道数为 out_planes，卷积核大小为 kernel_size</span></span><br><span class="line"><span class="comment"># 如果 instancenorm 为 True，则使用实例归一化；否则使用批量归一化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv</span>(<span class="params">in_planes, out_planes, kernel_size, instancenorm=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> instancenorm:</span><br><span class="line">        <span class="comment"># 构建一个包含卷积层、实例归一化层和 LeakyReLU 激活函数的序列</span></span><br><span class="line">        m = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层，使用指定的输入和输出通道数、卷积核大小，步长为 1，填充为 (kernel_size - 1) // 2，无偏置</span></span><br><span class="line">            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,</span><br><span class="line">                      stride=<span class="number">1</span>, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            <span class="comment"># 实例归一化层</span></span><br><span class="line">            nn.InstanceNorm2d(out_planes),</span><br><span class="line">            <span class="comment"># LeakyReLU 激活函数，负斜率为 0.1，原地操作</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 构建一个包含卷积层、批量归一化层和 LeakyReLU 激活函数的序列</span></span><br><span class="line">        m = nn.Sequential(</span><br><span class="line">            <span class="comment"># 卷积层，使用指定的输入和输出通道数、卷积核大小，步长为 1，填充为 (kernel_size - 1) // 2，无偏置</span></span><br><span class="line">            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size,</span><br><span class="line">                      stride=<span class="number">1</span>, padding=(kernel_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            <span class="comment"># 批量归一化层</span></span><br><span class="line">            nn.BatchNorm2d(out_planes),</span><br><span class="line">            <span class="comment"># LeakyReLU 激活函数，负斜率为 0.1，原地操作</span></span><br><span class="line">            nn.LeakyReLU(<span class="number">0.1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度解码器类，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DepthDecoder</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 将元组转换为字符串，用于作为字典的键</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tuple_to_str</span>(<span class="params">self, key_tuple</span>):</span><br><span class="line">        key_str = <span class="string">&#x27;-&#x27;</span>.join(<span class="built_in">str</span>(key_tuple))</span><br><span class="line">        <span class="keyword">return</span> key_str</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_ch_enc, embedder, embedder_out_dim,</span></span><br><span class="line"><span class="params">                 use_alpha=<span class="literal">False</span>, scales=<span class="built_in">range</span>(<span class="params"><span class="number">4</span></span>), num_output_channels=<span class="number">4</span>,</span></span><br><span class="line"><span class="params">                 use_skips=<span class="literal">True</span>, sigma_dropout_rate=<span class="number">0.0</span>, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(DepthDecoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_output_channels = num_output_channels</span><br><span class="line">        <span class="comment"># 是否使用跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.use_skips = use_skips</span><br><span class="line">        <span class="comment"># 上采样模式</span></span><br><span class="line">        <span class="variable language_">self</span>.upsample_mode = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">        <span class="comment"># 要处理的尺度</span></span><br><span class="line">        <span class="variable language_">self</span>.scales = scales</span><br><span class="line">        <span class="comment"># 是否使用 alpha</span></span><br><span class="line">        <span class="variable language_">self</span>.use_alpha = use_alpha</span><br><span class="line">        <span class="comment"># sigma 的丢弃率</span></span><br><span class="line">        <span class="variable language_">self</span>.sigma_dropout_rate = sigma_dropout_rate</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 嵌入器</span></span><br><span class="line">        <span class="variable language_">self</span>.embedder = embedder</span><br><span class="line">        <span class="comment"># 嵌入器的输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.E = embedder_out_dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器最后一层的输出通道数</span></span><br><span class="line">        final_enc_out_channels = num_ch_enc[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 最大池化层，用于下采样</span></span><br><span class="line">        <span class="variable language_">self</span>.downsample = nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最近邻上采样层，用于上采样</span></span><br><span class="line">        <span class="variable language_">self</span>.upsample = nn.UpsamplingNearest2d(scale_factor=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 第一个下采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_down1 = conv(final_enc_out_channels, <span class="number">512</span>, <span class="number">1</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个下采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_down2 = conv(<span class="number">512</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第一个上采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_up1 = conv(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 第二个上采样卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv_up2 = conv(<span class="number">256</span>, final_enc_out_channels, <span class="number">1</span>, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码器各层的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_enc = num_ch_enc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;num_ch_enc=&quot;</span>, num_ch_enc)</span><br><span class="line">        <span class="comment"># 将编码器各层的通道数加上嵌入器的输出维度</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_enc = [x + <span class="variable language_">self</span>.E <span class="keyword">for</span> x <span class="keyword">in</span> <span class="variable language_">self</span>.num_ch_enc]</span><br><span class="line">        <span class="comment"># 解码器各层的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.num_ch_dec = np.array([<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>])</span><br><span class="line">        <span class="comment"># self.num_ch_enc = np.array([64, 64, 128, 256, 512])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器的卷积层，使用 nn.ModuleDict 存储</span></span><br><span class="line">        <span class="variable language_">self</span>.convs = nn.ModuleDict()</span><br><span class="line">        <span class="comment"># 从 4 到 0 遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 上卷积层 0</span></span><br><span class="line">            <span class="comment"># 如果 i 为 4，则输入通道数为编码器最后一层的通道数；否则为解码器上一层的通道数</span></span><br><span class="line">            num_ch_in = <span class="variable language_">self</span>.num_ch_enc[-<span class="number">1</span>] <span class="keyword">if</span> i == <span class="number">4</span> <span class="keyword">else</span> <span class="variable language_">self</span>.num_ch_dec[i + <span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 输出通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_out = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 创建卷积块并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">0</span>))] = ConvBlock(num_ch_in, num_ch_out)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;upconv_&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="number">0</span>), num_ch_in, num_ch_out)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 上卷积层 1</span></span><br><span class="line">            <span class="comment"># 输入通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_in = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 如果使用跳跃连接且 i 大于 0，则输入通道数加上编码器上一层的通道数</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_skips <span class="keyword">and</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                num_ch_in += <span class="variable language_">self</span>.num_ch_enc[i - <span class="number">1</span>]</span><br><span class="line">            <span class="comment"># 输出通道数为解码器当前层的通道数</span></span><br><span class="line">            num_ch_out = <span class="variable language_">self</span>.num_ch_dec[i]</span><br><span class="line">            <span class="comment"># 创建卷积块并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">1</span>))] = ConvBlock(num_ch_in, num_ch_out)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;upconv_&#123;&#125;_&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i, <span class="number">1</span>), num_ch_in, num_ch_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历要处理的尺度</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="variable language_">self</span>.scales:</span><br><span class="line">            <span class="comment"># 创建一个 3x3 的卷积层并添加到 convs 字典中</span></span><br><span class="line">            <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;dispconv&quot;</span>, s))] = Conv3x3(<span class="variable language_">self</span>.num_ch_dec[s], <span class="variable language_">self</span>.num_output_channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Sigmoid 激活函数，用于将输出映射到 [0, 1] 范围</span></span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_features, disparity</span>):</span><br><span class="line">        <span class="comment"># 获取输入视差的批次大小和序列长度</span></span><br><span class="line">        B, S = disparity.size()</span><br><span class="line">        <span class="comment"># 对输入视差进行嵌入操作，然后增加两个维度</span></span><br><span class="line">        disparity = <span class="variable language_">self</span>.embedder(disparity.reshape(B * S, <span class="number">1</span>)).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line">tensor_1d 是一个一维张量，直接使用 torch.tensor 创建，其形状为 (<span class="number">3</span>,)。</span><br><span class="line">tensor_2d 是通过对 tensor_1d 使用 unsqueeze(<span class="number">1</span>) 增加一个维度得到的，形状为 (<span class="number">3</span>, <span class="number">1</span>)。</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为 (3,) 的一维张量</span></span><br><span class="line">tensor_1d = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tensor_1d 是否定义:&quot;</span>, <span class="string">&#x27;tensor_1d&#x27;</span> <span class="keyword">in</span> <span class="built_in">locals</span>())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;tensor_1d 的值:&quot;</span>, tensor_1d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制一维张量</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(tensor_1d)), tensor_1d, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Shape (3,) Tensor&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为 (3, 1) 的二维张量</span></span><br><span class="line">tensor_2d = tensor_1d.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制二维张量</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.bar(<span class="built_in">range</span>(<span class="built_in">len</span>(tensor_2d)), tensor_2d.flatten(), width=<span class="number">0.5</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Shape (3, 1) Tensor&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line">---------------------------------------------------------------------------------------------------------------------</span><br><span class="line">        <span class="comment"># 扩展编码器的输出以增加感受野</span></span><br><span class="line">        <span class="comment"># 获取编码器最后一层的输出</span></span><br><span class="line">        encoder_out = input_features[-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 对编码器输出进行下采样，然后通过第一个下采样卷积层</span></span><br><span class="line">        conv_down1 = <span class="variable language_">self</span>.conv_down1(<span class="variable language_">self</span>.downsample(encoder_out))</span><br><span class="line">        <span class="comment"># 对第一个下采样卷积层的输出进行下采样，然后通过第二个下采样卷积层</span></span><br><span class="line">        conv_down2 = <span class="variable language_">self</span>.conv_down2(<span class="variable language_">self</span>.downsample(conv_down1))</span><br><span class="line">        <span class="comment"># 对第二个下采样卷积层的输出进行上采样，然后通过第一个上采样卷积层</span></span><br><span class="line">        conv_up1 = <span class="variable language_">self</span>.conv_up1(<span class="variable language_">self</span>.upsample(conv_down2))</span><br><span class="line">        <span class="comment"># 对第一个上采样卷积层的输出进行上采样，然后通过第二个上采样卷积层</span></span><br><span class="line">        conv_up2 = <span class="variable language_">self</span>.conv_up2(<span class="variable language_">self</span>.upsample(conv_up1))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复 / 重塑特征</span></span><br><span class="line">        <span class="comment"># 获取第二个上采样卷积层输出的通道数、高度和宽度</span></span><br><span class="line">        _, C_feat, H_feat, W_feat = conv_up2.size()</span><br><span class="line">        <span class="comment"># 对第二个上采样卷积层的输出进行扩展和重塑</span></span><br><span class="line">        feat_tmp = conv_up2.unsqueeze(<span class="number">1</span>).expand(B, S, C_feat, H_feat, W_feat) \</span><br><span class="line">            .contiguous().view(B * S, C_feat, H_feat, W_feat)</span><br><span class="line">        <span class="comment"># 对视差进行重复操作以匹配特征图的大小</span></span><br><span class="line">        disparity_BsCHW = disparity.repeat(<span class="number">1</span>, <span class="number">1</span>, H_feat, W_feat)</span><br><span class="line">        <span class="comment"># 将扩展后的特征和视差拼接在一起</span></span><br><span class="line">        conv_up2 = torch.cat((feat_tmp, disparity_BsCHW), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重复 / 重塑输入特征</span></span><br><span class="line">        <span class="keyword">for</span> i, feat <span class="keyword">in</span> <span class="built_in">enumerate</span>(input_features):</span><br><span class="line">            <span class="comment"># 获取输入特征的通道数、高度和宽度</span></span><br><span class="line">            _, C_feat, H_feat, W_feat = feat.size()</span><br><span class="line">            <span class="comment"># 对输入特征进行扩展和重塑</span></span><br><span class="line">            feat_tmp = feat.unsqueeze(<span class="number">1</span>).expand(B, S, C_feat, H_feat, W_feat) \</span><br><span class="line">                .contiguous().view(B * S, C_feat, H_feat, W_feat)</span><br><span class="line">            <span class="comment"># 对视差进行重复操作以匹配特征图的大小</span></span><br><span class="line">            disparity_BsCHW = disparity.repeat(<span class="number">1</span>, <span class="number">1</span>, H_feat, W_feat)</span><br><span class="line">            <span class="comment"># 将扩展后的特征和视差拼接在一起</span></span><br><span class="line">            input_features[i] = torch.cat((feat_tmp, disparity_BsCHW), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码器部分</span></span><br><span class="line">        <span class="comment"># 存储输出结果的字典</span></span><br><span class="line">        outputs = &#123;&#125;</span><br><span class="line">        <span class="comment"># 初始输入为扩展后的第二个上采样卷积层的输出</span></span><br><span class="line">        x = conv_up2</span><br><span class="line">        <span class="comment"># 从 4 到 0 遍历</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 通过上卷积层 0</span></span><br><span class="line">            x = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">0</span>))](x)</span><br><span class="line">            <span class="comment"># 进行上采样</span></span><br><span class="line">            x = [upsample(x)]</span><br><span class="line">            <span class="comment"># 如果使用跳跃连接且 i 大于 0</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.use_skips <span class="keyword">and</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 将编码器上一层的特征添加到列表中</span></span><br><span class="line">                x += [input_features[i - <span class="number">1</span>]]</span><br><span class="line">            <span class="comment"># 将列表中的特征拼接在一起</span></span><br><span class="line">            x = torch.cat(x, <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 通过上卷积层 1</span></span><br><span class="line">            x = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;upconv&quot;</span>, i, <span class="number">1</span>))](x)</span><br><span class="line">            <span class="comment"># 如果当前尺度在要处理的尺度列表中</span></span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.scales:</span><br><span class="line">                <span class="comment"># 通过视差卷积层得到输出</span></span><br><span class="line">                output = <span class="variable language_">self</span>.convs[<span class="variable language_">self</span>.tuple_to_str((<span class="string">&quot;dispconv&quot;</span>, i))](x)</span><br><span class="line">                <span class="comment"># 获取输出的高度和宽度</span></span><br><span class="line">                H_mpi, W_mpi = output.size(<span class="number">2</span>), output.size(<span class="number">3</span>)</span><br><span class="line">                <span class="comment"># 调整输出的维度</span></span><br><span class="line">                mpi = output.view(B, S, <span class="number">4</span>, H_mpi, W_mpi)</span><br><span class="line">                <span class="comment"># 对 RGB 通道应用 Sigmoid 激活函数</span></span><br><span class="line">                mpi_rgb = <span class="variable language_">self</span>.sigmoid(mpi[:, :, <span class="number">0</span>:<span class="number">3</span>, :, :])</span><br><span class="line">                <span class="comment"># 如果不使用 alpha，则取绝对值并加上一个小的常数；否则应用 Sigmoid 激活函数</span></span><br><span class="line">                mpi_sigma = torch.<span class="built_in">abs</span>(mpi[:, :, <span class="number">3</span>:, :, :]) + <span class="number">1e-4</span> \</span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.use_alpha \</span><br><span class="line">                        <span class="keyword">else</span> <span class="variable language_">self</span>.sigmoid(mpi[:, :, <span class="number">3</span>:, :, :])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 如果 sigma 丢弃率大于 0 且处于训练模式</span></span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.sigma_dropout_rate &gt; <span class="number">0.0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.training:</span><br><span class="line">                    <span class="comment"># 对 sigma 通道应用 2D Dropout</span></span><br><span class="line">                    mpi_sigma = F.dropout2d(mpi_sigma, p=<span class="variable language_">self</span>.sigma_dropout_rate)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将 RGB 和 sigma 通道拼接在一起，并存储到输出字典中</span></span><br><span class="line">                outputs[(<span class="string">&quot;disp&quot;</span>, i)] = torch.cat((mpi_rgb, mpi_sigma), dim=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<h4 id="intrinsics-py"><a href="#intrinsics-py" class="headerlink" title="intrinsics.py"></a>intrinsics.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个用于学习焦距的神经网络模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearnFocal</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, H, W, req_grad, fx_only, order=<span class="number">2</span>, init_focal=<span class="literal">None</span>, learn_distortion=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 调用父类 nn.Module 的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(LearnFocal, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 图像的高度</span></span><br><span class="line">        <span class="variable language_">self</span>.H = H</span><br><span class="line">        <span class="comment"># 图像的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="comment"># 一个布尔值，如果为 True，只输出 [fx, fx]；如果为 False，输出 [fx, fy]</span></span><br><span class="line">        <span class="variable language_">self</span>.fx_only = fx_only  </span><br><span class="line">        <span class="comment"># 焦距初始化的阶数，检查我们的补充部分有相关说明</span></span><br><span class="line">        <span class="variable language_">self</span>.order = order  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 畸变相关</span></span><br><span class="line">        <span class="comment"># 是否学习畸变参数</span></span><br><span class="line">        <span class="variable language_">self</span>.learn_distortion = learn_distortion</span><br><span class="line">        <span class="keyword">if</span> learn_distortion:</span><br><span class="line">            <span class="comment"># 第一个畸变系数，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">            <span class="variable language_">self</span>.k1 = nn.Parameter(torch.tensor(<span class="number">0.0</span>, dtype=torch.float32), requires_grad=req_grad)</span><br><span class="line">            <span class="comment"># 第二个畸变系数，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">            <span class="variable language_">self</span>.k2 = nn.Parameter(torch.tensor(<span class="number">0.0</span>, dtype=torch.float32), requires_grad=req_grad)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.fx_only:</span><br><span class="line">            <span class="keyword">if</span> init_focal <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fx 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * W = fx 计算系数 a，即 a**2 = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(W)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.order == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a * W = fx 计算系数 a，即 a = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(init_focal / <span class="built_in">float</span>(W), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;焦距初始化阶数需要为 1 或 2。退出&#x27;</span>)</span><br><span class="line">                    exit()</span><br><span class="line">                <span class="comment"># 将计算得到的系数作为 fx，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(coe_x, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> init_focal <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fx 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">                <span class="comment"># 如果没有提供初始焦距，将 fy 初始化为 1.0，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fy = nn.Parameter(torch.tensor(<span class="number">1.0</span>, dtype=torch.float32), requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * W = fx 计算 x 方向的系数 a，即 a**2 = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(W)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                    <span class="comment"># 根据公式 a**2 * H = fy 计算 y 方向的系数 a，即 a**2 = fy / H</span></span><br><span class="line">                    coe_y = torch.tensor(np.sqrt(init_focal / <span class="built_in">float</span>(H)), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">elif</span> <span class="variable language_">self</span>.order == <span class="number">1</span>:</span><br><span class="line">                    <span class="comment"># 根据公式 a * W = fx 计算 x 方向的系数 a，即 a = fx / W</span></span><br><span class="line">                    coe_x = torch.tensor(init_focal / <span class="built_in">float</span>(W), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                    <span class="comment"># 根据公式 a * H = fy 计算 y 方向的系数 a，即 a = fy / H</span></span><br><span class="line">                    coe_y = torch.tensor(init_focal / <span class="built_in">float</span>(H), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;焦距初始化阶数需要为 1 或 2。退出&#x27;</span>)</span><br><span class="line">                    exit()</span><br><span class="line">                <span class="comment"># 将计算得到的 x 方向系数作为 fx，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fx = nn.Parameter(coe_x, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line">                <span class="comment"># 将计算得到的 y 方向系数作为 fy，可根据 req_grad 设置是否需要梯度</span></span><br><span class="line">                <span class="variable language_">self</span>.fy = nn.Parameter(coe_y, requires_grad=req_grad)  <span class="comment"># (1, )</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, i=<span class="literal">None</span></span>):  <span class="comment"># 参数 i=None 只是为了支持多 GPU 训练</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.fx_only:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy，因为 fx_only 为 True，所以 fy 等于 fx</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx ** <span class="number">2</span> * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fx ** <span class="number">2</span> * <span class="variable language_">self</span>.W])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy，因为 fx_only 为 True，所以 fy 等于 fx</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.order == <span class="number">2</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx**<span class="number">2</span> * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fy**<span class="number">2</span> * <span class="variable language_">self</span>.H])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 根据公式计算 fx 和 fy</span></span><br><span class="line">                fxfy = torch.stack([<span class="variable language_">self</span>.fx * <span class="variable language_">self</span>.W, <span class="variable language_">self</span>.fy * <span class="variable language_">self</span>.H])</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.learn_distortion:</span><br><span class="line">            <span class="comment"># 如果要学习畸变参数，返回焦距和畸变系数</span></span><br><span class="line">            <span class="keyword">return</span> fxfy, <span class="variable language_">self</span>.k1, <span class="variable language_">self</span>.k2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则只返回焦距</span></span><br><span class="line">            <span class="keyword">return</span> fxfy</span><br></pre></td></tr></table></figure>

<h4 id="layers-py"><a href="#layers-py" class="headerlink" title="layers.py"></a>layers.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 版权所有 Niantic 2019。专利申请中。保留所有权利。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 本软件遵循 Monodepth2 许可证的条款，</span></span><br><span class="line"><span class="comment"># 该许可证仅允许非商业用途，完整条款可在 LICENSE 文件中获取。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络的 sigmoid 输出转换为深度预测</span></span><br><span class="line"><span class="comment"># 此转换公式在论文的“额外考虑”部分给出</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">disp_to_depth</span>(<span class="params">disp, min_depth, max_depth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将网络的 sigmoid 输出转换为深度预测</span></span><br><span class="line"><span class="string">    该转换公式在论文的“额外考虑”部分给出。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 最小视差，为最大深度的倒数</span></span><br><span class="line">    min_disp = <span class="number">1</span> / max_depth</span><br><span class="line">    <span class="comment"># 最大视差，为最小深度的倒数</span></span><br><span class="line">    max_disp = <span class="number">1</span> / min_depth</span><br><span class="line">    <span class="comment"># 缩放后的视差</span></span><br><span class="line">    scaled_disp = min_disp + (max_disp - min_disp) * disp</span><br><span class="line">    <span class="comment"># 深度值，为缩放后视差的倒数</span></span><br><span class="line">    depth = <span class="number">1</span> / scaled_disp</span><br><span class="line">    <span class="keyword">return</span> scaled_disp, depth</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将网络输出的 (轴角, 平移) 转换为 4x4 矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transformation_from_parameters</span>(<span class="params">axisangle, translation, invert=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将网络的 (轴角, 平移) 输出转换为 4x4 矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 从轴角表示转换为旋转矩阵</span></span><br><span class="line">    R = rot_from_axisangle(axisangle)</span><br><span class="line">    <span class="comment"># 克隆平移向量</span></span><br><span class="line">    t = translation.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        <span class="comment"># 如果需要反转，对旋转矩阵进行转置</span></span><br><span class="line">        R = R.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 平移向量取负</span></span><br><span class="line">        t *= -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line">    T = get_translation_matrix(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        <span class="comment"># 如果需要反转，先旋转再平移</span></span><br><span class="line">        M = torch.matmul(R, T)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 正常情况下，先平移再旋转</span></span><br><span class="line">        M = torch.matmul(T, R)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_translation_matrix</span>(<span class="params">translation_vector</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将平移向量转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化一个全零的 4x4 矩阵，形状为 (batch_size, 4, 4)</span></span><br><span class="line">    T = torch.zeros(translation_vector.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>).to(device=translation_vector.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将平移向量调整为 (batch_size, 3, 1) 的形状</span></span><br><span class="line">    t = translation_vector.contiguous().view(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置矩阵的对角元素为 1</span></span><br><span class="line">    T[:, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">1</span>, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">2</span>, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将平移向量添加到矩阵的最后一列</span></span><br><span class="line">    T[:, :<span class="number">3</span>, <span class="number">3</span>, <span class="literal">None</span>] = t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将轴角旋转表示转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="comment"># （改编自 https://github.com/Wallacoloo/printipi）</span></span><br><span class="line"><span class="comment"># 输入 &#x27;vec&#x27; 必须是 Bx1x3 的形状</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rot_from_axisangle</span>(<span class="params">vec</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将轴角旋转表示转换为 4x4 变换矩阵</span></span><br><span class="line"><span class="string">    （改编自 https://github.com/Wallacoloo/printipi）</span></span><br><span class="line"><span class="string">    输入 &#x27;vec&#x27; 必须是 Bx1x3 的形状</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算轴角的模长</span></span><br><span class="line">    angle = torch.norm(vec, <span class="number">2</span>, <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 计算单位轴向量</span></span><br><span class="line">    axis = vec / (angle + <span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算角度的余弦值</span></span><br><span class="line">    ca = torch.cos(angle)</span><br><span class="line">    <span class="comment"># 计算角度的正弦值</span></span><br><span class="line">    sa = torch.sin(angle)</span><br><span class="line">    <span class="comment"># 计算 1 - cos(angle)</span></span><br><span class="line">    C = <span class="number">1</span> - ca</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取轴向量的 x 分量，并增加一个维度</span></span><br><span class="line">    x = axis[..., <span class="number">0</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取轴向量的 y 分量，并增加一个维度</span></span><br><span class="line">    y = axis[..., <span class="number">1</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 提取轴向量的 z 分量，并增加一个维度</span></span><br><span class="line">    z = axis[..., <span class="number">2</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 x * sin(angle)</span></span><br><span class="line">    xs = x * sa</span><br><span class="line">    <span class="comment"># 计算 y * sin(angle)</span></span><br><span class="line">    ys = y * sa</span><br><span class="line">    <span class="comment"># 计算 z * sin(angle)</span></span><br><span class="line">    zs = z * sa</span><br><span class="line">    <span class="comment"># 计算 x * (1 - cos(angle))</span></span><br><span class="line">    xC = x * C</span><br><span class="line">    <span class="comment"># 计算 y * (1 - cos(angle))</span></span><br><span class="line">    yC = y * C</span><br><span class="line">    <span class="comment"># 计算 z * (1 - cos(angle))</span></span><br><span class="line">    zC = z * C</span><br><span class="line">    <span class="comment"># 计算 x * y * (1 - cos(angle))</span></span><br><span class="line">    xyC = x * yC</span><br><span class="line">    <span class="comment"># 计算 y * z * (1 - cos(angle))</span></span><br><span class="line">    yzC = y * zC</span><br><span class="line">    <span class="comment"># 计算 z * x * (1 - cos(angle))</span></span><br><span class="line">    zxC = z * xC</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化一个全零的 4x4 旋转矩阵，形状为 (batch_size, 4, 4)</span></span><br><span class="line">    rot = torch.zeros((vec.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>)).to(device=vec.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置旋转矩阵的元素</span></span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">0</span>] = torch.squeeze(x * xC + ca)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">1</span>] = torch.squeeze(xyC - zs)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">2</span>] = torch.squeeze(zxC + ys)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">0</span>] = torch.squeeze(xyC + zs)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">1</span>] = torch.squeeze(y * yC + ca)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">2</span>] = torch.squeeze(yzC - xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">0</span>] = torch.squeeze(zxC - ys)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">1</span>] = torch.squeeze(yzC + xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">2</span>] = torch.squeeze(z * zC + ca)</span><br><span class="line">    rot[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个卷积块，包含卷积层、批量归一化层和 ELU 激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConvBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;执行卷积后接 ELU 的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ConvBlock, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3x3 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = Conv3x3(in_channels, out_channels)</span><br><span class="line">        <span class="comment"># ELU 激活函数，原地操作</span></span><br><span class="line">        <span class="variable language_">self</span>.nonlin = nn.ELU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 批量归一化层</span></span><br><span class="line">        <span class="variable language_">self</span>.bn = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 通过卷积层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv(x)</span><br><span class="line">        <span class="comment"># 通过批量归一化层</span></span><br><span class="line">        out = <span class="variable language_">self</span>.bn(out)</span><br><span class="line">        <span class="comment"># 通过 ELU 激活函数</span></span><br><span class="line">        out = <span class="variable language_">self</span>.nonlin(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个 3x3 卷积层，包含填充操作</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv3x3</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对输入进行填充和卷积的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, use_refl=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv3x3, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_refl:</span><br><span class="line">            <span class="comment"># 使用反射填充</span></span><br><span class="line">            <span class="variable language_">self</span>.pad = nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用零填充</span></span><br><span class="line">            <span class="variable language_">self</span>.pad = nn.ZeroPad2d(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 卷积层</span></span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(<span class="built_in">int</span>(in_channels), <span class="built_in">int</span>(out_channels), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 进行填充操作</span></span><br><span class="line">        out = <span class="variable language_">self</span>.pad(x)</span><br><span class="line">        <span class="comment"># 进行卷积操作</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将深度图像转换为点云的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BackprojectDepth</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将深度图像转换为点云的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width</span>):</span><br><span class="line">        <span class="built_in">super</span>(BackprojectDepth, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 批量大小</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="variable language_">self</span>.height = height</span><br><span class="line">        <span class="comment"># 图像宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.width = width</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成二维网格坐标</span></span><br><span class="line">        meshgrid = np.meshgrid(<span class="built_in">range</span>(<span class="variable language_">self</span>.width), <span class="built_in">range</span>(<span class="variable language_">self</span>.height), indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line">        <span class="comment"># 将网格坐标堆叠在一起，并转换为 float32 类型</span></span><br><span class="line">        <span class="variable language_">self</span>.id_coords = np.stack(meshgrid, axis=<span class="number">0</span>).astype(np.float32)</span><br><span class="line">        <span class="comment"># 将网格坐标转换为 PyTorch 张量，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.id_coords = nn.Parameter(torch.from_numpy(<span class="variable language_">self</span>.id_coords),</span><br><span class="line">                                      requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一个全为 1 的张量，形状为 (batch_size, 1, height * width)，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.ones = nn.Parameter(torch.ones(<span class="variable language_">self</span>.batch_size, <span class="number">1</span>, <span class="variable language_">self</span>.height * <span class="variable language_">self</span>.width),</span><br><span class="line">                                 requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 调整网格坐标的形状，并重复 batch_size 次</span></span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = torch.unsqueeze(torch.stack(</span><br><span class="line">            [<span class="variable language_">self</span>.id_coords[<span class="number">0</span>].view(-<span class="number">1</span>), <span class="variable language_">self</span>.id_coords[<span class="number">1</span>].view(-<span class="number">1</span>)], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = <span class="variable language_">self</span>.pix_coords.repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将网格坐标和全 1 张量拼接在一起，并设置为不需要梯度</span></span><br><span class="line">        <span class="variable language_">self</span>.pix_coords = nn.Parameter(torch.cat([<span class="variable language_">self</span>.pix_coords, <span class="variable language_">self</span>.ones], <span class="number">1</span>),</span><br><span class="line">                                       requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, depth, inv_K</span>):</span><br><span class="line">        <span class="comment"># 将逆相机内参矩阵与像素坐标相乘</span></span><br><span class="line">        cam_points = torch.matmul(inv_K[:, :<span class="number">3</span>, :<span class="number">3</span>], <span class="variable language_">self</span>.pix_coords)</span><br><span class="line">        <span class="comment"># 将深度值与相机坐标相乘</span></span><br><span class="line">        cam_points = depth.view(<span class="variable language_">self</span>.batch_size, <span class="number">1</span>, -<span class="number">1</span>) * cam_points</span><br><span class="line">        <span class="comment"># 将相机坐标和全 1 张量拼接在一起</span></span><br><span class="line">        cam_points = torch.cat([cam_points, <span class="variable language_">self</span>.ones], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cam_points</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 3D 点投影到具有内参 K 和位置 T 的相机中的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Project3D</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将 3D 点投影到具有内参 K 和位置 T 的相机中的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width, eps=<span class="number">1e-7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Project3D, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 批量大小</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="variable language_">self</span>.height = height</span><br><span class="line">        <span class="comment"># 图像宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.width = width</span><br><span class="line">        <span class="comment"># 防止除零的小常数</span></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, points, K, T</span>):</span><br><span class="line">        <span class="comment"># 计算投影矩阵 P</span></span><br><span class="line">        P = torch.matmul(K, T)[:, :<span class="number">3</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将投影矩阵 P 与 3D 点相乘</span></span><br><span class="line">        cam_points = torch.matmul(P, points)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算像素坐标</span></span><br><span class="line">        pix_coords = cam_points[:, :<span class="number">2</span>, :] / (cam_points[:, <span class="number">2</span>, :].unsqueeze(<span class="number">1</span>) + <span class="variable language_">self</span>.eps)</span><br><span class="line">        <span class="comment"># 调整像素坐标的形状</span></span><br><span class="line">        pix_coords = pix_coords.view(<span class="variable language_">self</span>.batch_size, <span class="number">2</span>, <span class="variable language_">self</span>.height, <span class="variable language_">self</span>.width)</span><br><span class="line">        <span class="comment"># 交换维度</span></span><br><span class="line">        pix_coords = pix_coords.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 归一化像素坐标</span></span><br><span class="line">        pix_coords[..., <span class="number">0</span>] /= <span class="variable language_">self</span>.width - <span class="number">1</span></span><br><span class="line">        pix_coords[..., <span class="number">1</span>] /= <span class="variable language_">self</span>.height - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将像素坐标映射到 [-1, 1] 范围</span></span><br><span class="line">        pix_coords = (pix_coords - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> pix_coords</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将输入张量上采样 2 倍</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upsample</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将输入张量上采样 2 倍</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> F.interpolate(x, scale_factor=<span class="number">2</span>, mode=<span class="string">&quot;nearest&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算视差图像的平滑损失</span></span><br><span class="line"><span class="comment"># 彩色图像用于边缘感知平滑</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_smooth_loss</span>(<span class="params">disp, img</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算视差图像的平滑损失</span></span><br><span class="line"><span class="string">    彩色图像用于边缘感知平滑</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算视差在 x 方向的梯度</span></span><br><span class="line">    grad_disp_x = torch.<span class="built_in">abs</span>(disp[:, :, :, :-<span class="number">1</span>] - disp[:, :, :, <span class="number">1</span>:])</span><br><span class="line">    <span class="comment"># 计算视差在 y 方向的梯度</span></span><br><span class="line">    grad_disp_y = torch.<span class="built_in">abs</span>(disp[:, :, :-<span class="number">1</span>, :] - disp[:, :, <span class="number">1</span>:, :])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算图像在 x 方向的平均梯度</span></span><br><span class="line">    grad_img_x = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :, :-<span class="number">1</span>] - img[:, :, :, <span class="number">1</span>:]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 计算图像在 y 方向的平均梯度</span></span><br><span class="line">    grad_img_y = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :-<span class="number">1</span>, :] - img[:, :, <span class="number">1</span>:, :]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据图像梯度对视差梯度进行加权</span></span><br><span class="line">    grad_disp_x *= torch.exp(-grad_img_x)</span><br><span class="line">    grad_disp_y *= torch.exp(-grad_img_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回视差梯度的平均值</span></span><br><span class="line">    <span class="keyword">return</span> grad_disp_x.mean() + grad_disp_y.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算一对图像之间 SSIM 损失的层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SSIM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算一对图像之间 SSIM 损失的层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SSIM, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算均值</span></span><br><span class="line">        <span class="variable language_">self</span>.mu_x_pool   = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.mu_y_pool   = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算方差</span></span><br><span class="line">        <span class="variable language_">self</span>.sig_x_pool  = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sig_y_pool  = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 3x3 平均池化层，用于计算协方差</span></span><br><span class="line">        <span class="variable language_">self</span>.sig_xy_pool = nn.AvgPool2d(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反射填充层</span></span><br><span class="line">        <span class="variable language_">self</span>.refl = nn.ReflectionPad2d(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 常数 C1</span></span><br><span class="line">        <span class="variable language_">self</span>.C1 = <span class="number">0.01</span> ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 常数 C2</span></span><br><span class="line">        <span class="variable language_">self</span>.C2 = <span class="number">0.03</span> ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 对输入图像进行反射填充</span></span><br><span class="line">        x = <span class="variable language_">self</span>.refl(x)</span><br><span class="line">        y = <span class="variable language_">self</span>.refl(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算图像 x 的均值</span></span><br><span class="line">        mu_x = <span class="variable language_">self</span>.mu_x_pool(x)</span><br><span class="line">        <span class="comment"># 计算图像 y 的均值</span></span><br><span class="line">        mu_y = <span class="variable language_">self</span>.mu_y_pool(y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算图像 x 的方差</span></span><br><span class="line">        sigma_x  = <span class="variable language_">self</span>.sig_x_pool(x ** <span class="number">2</span>) - mu_x ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 计算图像 y 的方差</span></span><br><span class="line">        sigma_y  = <span class="variable language_">self</span>.sig_y_pool(y ** <span class="number">2</span>) - mu_y ** <span class="number">2</span></span><br><span class="line">        <span class="comment"># 计算图像 x 和 y 的协方差</span></span><br><span class="line">        sigma_xy = <span class="variable language_">self</span>.sig_xy_pool(x * y) - mu_x * mu_y</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 SSIM 分子</span></span><br><span class="line">        SSIM_n = (<span class="number">2</span> * mu_x * mu_y + <span class="variable language_">self</span>.C1) * (<span class="number">2</span> * sigma_xy + <span class="variable language_">self</span>.C2)</span><br><span class="line">        <span class="comment"># 计算 SSIM 分母</span></span><br><span class="line">        SSIM_d = (mu_x ** <span class="number">2</span> + mu_y ** <span class="number">2</span> + <span class="variable language_">self</span>.C1) * (sigma_x + sigma_y + <span class="variable language_">self</span>.C2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 SSIM 损失，并将结果限制在 [0, 1] 范围内</span></span><br><span class="line">        <span class="keyword">return</span> torch.clamp((<span class="number">1</span> - SSIM_n / SSIM_d) / <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算预测深度和真实深度之间的误差指标</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_depth_errors</span>(<span class="params">gt, pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测深度和真实深度之间的误差指标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算预测深度和真实深度的比值的最大值</span></span><br><span class="line">    thresh = torch.<span class="built_in">max</span>((gt / pred), (pred / gt))</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25 的比例</span></span><br><span class="line">    a1 = (thresh &lt; <span class="number">1.25</span>     ).<span class="built_in">float</span>().mean()</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25^2 的比例</span></span><br><span class="line">    a2 = (thresh &lt; <span class="number">1.25</span> ** <span class="number">2</span>).<span class="built_in">float</span>().mean()</span><br><span class="line">    <span class="comment"># 计算阈值小于 1.25^3 的比例</span></span><br><span class="line">    a3 = (thresh &lt; <span class="number">1.25</span> ** <span class="number">3</span>).<span class="built_in">float</span>().mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算均方误差</span></span><br><span class="line">    rmse = (gt - pred) ** <span class="number">2</span></span><br><span class="line">    rmse = torch.sqrt(rmse.mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算对数均方误差</span></span><br><span class="line">    rmse_log = (torch.log(gt) - torch.log(pred)) ** <span class="number">2</span></span><br><span class="line">    rmse_log = torch.sqrt(rmse_log.mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算绝对相对误差</span></span><br><span class="line">    abs_rel = torch.mean(torch.<span class="built_in">abs</span>(gt - pred) / gt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平方相对误差</span></span><br><span class="line">    sq_rel = torch.mean((gt - pred) ** <span class="number">2</span> / gt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> abs_rel, sq_rel, rmse, rmse_log, a</span><br></pre></td></tr></table></figure>

<h4 id="nerf-feature-py"><a href="#nerf-feature-py" class="headerlink" title="nerf_feature.py"></a>nerf_feature.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个名为 NerfWFeatures 的神经网络模块</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NerfWFeatures</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D:           标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类 nn.Module 的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络块，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络块，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims + <span class="number">32</span>, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义用于计算密度的全连接层，最后使用 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(</span><br><span class="line">            nn.Linear(D, <span class="number">1</span>), nn.Softplus()</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义用于提取特征的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Sequential(</span><br><span class="line">            nn.Linear(D, D)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义用于处理特征和方向信息以生成中间特征的层</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义用于从中间特征生成 RGB 颜色的全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc, cost_volume</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络块处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果、原始编码位置和代价体进行拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc, cost_volume], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, D+pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络块处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将提取的特征和编码后的方向进行拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, D+dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 rgb_layers 层处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># (H, W, N_sample, D/2)</span></span><br><span class="line">        <span class="comment"># 生成 RGB 颜色</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 颜色和密度进行拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density], dim=<span class="number">3</span>)  <span class="comment"># (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br></pre></td></tr></table></figure>

<h4 id="nerf-mask-py"><a href="#nerf-mask-py" class="headerlink" title="nerf_mask.py"></a>nerf_mask.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个名为 OfficialNerf 的神经网络模块，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OfficialNerf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D: 标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(OfficialNerf, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络序列，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络序列，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 定义掩码网络，包含两个线性层，中间有 ReLU 激活函数，最后有 Sigmoid 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.mask_net = nn.Sequential(nn.Linear(D, D), nn.ReLU(), nn.Linear(D, <span class="number">1</span>), nn.Sigmoid())</span><br><span class="line">        <span class="comment"># 定义密度预测网络，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(nn.Linear(D, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 定义特征提取的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Linear(D, D)</span><br><span class="line">        <span class="comment"># 定义 RGB 处理的神经网络序列，包含一个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义 RGB 预测的神经网络序列，包含一个线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络序列处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果和原始编码位置在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过掩码网络得到掩码概率</span></span><br><span class="line">        mask_prob = <span class="variable language_">self</span>.mask_net(x)</span><br><span class="line">        <span class="comment"># 通过密度预测网络得到密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过特征提取线性层得到特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将特征和编码后的方向在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 处理神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D / 2)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 预测神经网络序列得到 RGB 值</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 值、密度和掩码概率在第3维拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density, mask_prob], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br></pre></td></tr></table></figure>

<h4 id="nerf-models-py"><a href="#nerf-models-py" class="headerlink" title="nerf_models.py"></a>nerf_models.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.parallel</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 OfficialNerf 类，继承自 nn.Module，用于实现官方的 NeRF 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OfficialNerf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, pos_in_dims, dir_in_dims, D</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_in_dims: 标量，编码后位置的通道数</span></span><br><span class="line"><span class="string">        :param dir_in_dims: 标量，编码后方向的通道数</span></span><br><span class="line"><span class="string">        :param D: 标量，隐藏层的维度数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(OfficialNerf, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 存储编码后位置的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_in_dims = pos_in_dims</span><br><span class="line">        <span class="comment"># 存储编码后方向的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_in_dims = dir_in_dims</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第一层神经网络序列，包含四个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.layers0 = nn.Sequential(</span><br><span class="line">            nn.Linear(pos_in_dims, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义第二层神经网络序列，包含四个线性层和 ReLU 激活函数，有一个跳跃连接</span></span><br><span class="line">        <span class="variable language_">self</span>.layers1 = nn.Sequential(</span><br><span class="line">            nn.Linear(D + pos_in_dims, D), nn.ReLU(),  <span class="comment"># 跳跃连接</span></span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">            nn.Linear(D, D), nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义密度预测网络，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_density = nn.Sequential(nn.Linear(D, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 定义特征提取的线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_feature = nn.Linear(D, D)</span><br><span class="line">        <span class="comment"># 定义 RGB 处理的神经网络序列，包含一个线性层和 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.rgb_layers = nn.Sequential(nn.Linear(D + dir_in_dims, D // <span class="number">2</span>), nn.ReLU())</span><br><span class="line">        <span class="comment"># 定义 RGB 预测的神经网络序列，包含一个线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.fc_rgb = nn.Sequential(nn.Linear(D // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以下代码被注释掉，原本用于初始化偏置</span></span><br><span class="line">        <span class="comment"># self.fc_density[0].bias.data = torch.tensor([0.1]).float()</span></span><br><span class="line">        <span class="comment"># self.fc_rgb[0].bias.data = torch.tensor([0.02, 0.02, 0.02]).float()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pos_enc, dir_enc</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param pos_enc: (H, W, N_sample, pos_in_dims) 编码后的位置</span></span><br><span class="line"><span class="string">        :param dir_enc: (H, W, N_sample, dir_in_dims) 编码后的方向</span></span><br><span class="line"><span class="string">        :return: rgb_density (H, W, N_sample, 4)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 通过第一层神经网络序列处理编码后的位置</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers0(pos_enc)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将处理后的结果和原始编码位置在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([x, pos_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + pos_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过第二层神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layers1(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过密度预测网络得到密度</span></span><br><span class="line">        density = <span class="variable language_">self</span>.fc_density(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过特征提取线性层得到特征</span></span><br><span class="line">        feat = <span class="variable language_">self</span>.fc_feature(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D)</span></span><br><span class="line">        <span class="comment"># 将特征和编码后的方向在第 3 维拼接</span></span><br><span class="line">        x = torch.cat([feat, dir_enc], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, D + dir_in_dims)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 处理神经网络序列处理拼接后的结果</span></span><br><span class="line">        x = <span class="variable language_">self</span>.rgb_layers(x)  <span class="comment"># 输出形状为 (H, W, N_sample, D / 2)</span></span><br><span class="line">        <span class="comment"># 通过 RGB 预测神经网络序列得到 RGB 值</span></span><br><span class="line">        rgb = <span class="variable language_">self</span>.fc_rgb(x)  <span class="comment"># 输出形状为 (H, W, N_sample, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 RGB 值和密度在第 3 维拼接</span></span><br><span class="line">        rgb_den = torch.cat([rgb, density], dim=<span class="number">3</span>)  <span class="comment"># 输出形状为 (H, W, N_sample, 4)</span></span><br><span class="line">        <span class="keyword">return</span> rgb_den</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 fullNeRF 类，继承自 nn.Module，用于实现完整的 NeRF 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">fullNeRF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels_xyz, in_channels_dir, W, D=<span class="number">8</span>, skips=[<span class="number">4</span>]</span>):</span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 存储网络的深度</span></span><br><span class="line">        <span class="variable language_">self</span>.D = D</span><br><span class="line">        <span class="comment"># 存储隐藏层的宽度</span></span><br><span class="line">        <span class="variable language_">self</span>.W = W</span><br><span class="line">        <span class="comment"># 存储跳跃连接的位置</span></span><br><span class="line">        <span class="variable language_">self</span>.skips = skips</span><br><span class="line">        <span class="comment"># 存储输入位置编码的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.in_channels_xyz = in_channels_xyz</span><br><span class="line">        <span class="comment"># 存储输入方向编码的通道数</span></span><br><span class="line">        <span class="variable language_">self</span>.in_channels_dir = in_channels_dir</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义位置编码层</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(D):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 第一层，输入维度为输入位置编码的通道数，输出维度为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(in_channels_xyz, W)</span><br><span class="line">            <span class="keyword">elif</span> i <span class="keyword">in</span> skips:</span><br><span class="line">                <span class="comment"># 跳跃连接层，输入维度为隐藏层宽度加上输入位置编码的通道数，输出维度为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(W + in_channels_xyz, W)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 普通层，输入和输出维度均为隐藏层宽度</span></span><br><span class="line">                layer = nn.Linear(W, W)</span><br><span class="line">            <span class="comment"># 为每个层添加 ReLU 激活函数</span></span><br><span class="line">            layer = nn.Sequential(layer, nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">            <span class="comment"># 将层添加到模型中</span></span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;xyz_encoding_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>, layer)</span><br><span class="line">        <span class="comment"># 定义位置编码的最终线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.xyz_encoding_final = nn.Linear(W, W)</span><br><span class="line">        <span class="comment"># 定义方向编码层</span></span><br><span class="line">        <span class="variable language_">self</span>.dir_encoding = nn.Sequential(</span><br><span class="line">            nn.Linear(W + in_channels_dir, W), nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(W, W // <span class="number">2</span>), nn.ReLU(<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义静态输出层</span></span><br><span class="line">        <span class="comment"># 静态密度预测层，包含一个线性层和 Softplus 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.static_sigma = nn.Sequential(nn.Linear(W, <span class="number">1</span>), nn.Softplus())</span><br><span class="line">        <span class="comment"># 静态 RGB 预测层，包含两个线性层，中间有 ReLU 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.static_rgb = nn.Sequential(nn.Linear(W // <span class="number">2</span>, W // <span class="number">2</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                        nn.Linear(W // <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_xyz, input_dir_a</span>):</span><br><span class="line">        <span class="comment"># 存储输入的位置编码</span></span><br><span class="line">        xyz_ = input_xyz</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.D):</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.skips:</span><br><span class="line">                <span class="comment"># 如果是跳跃连接位置，将输入的位置编码和当前处理结果拼接</span></span><br><span class="line">                xyz_ = torch.cat([input_xyz, xyz_], -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 通过相应的位置编码层处理</span></span><br><span class="line">            xyz_ = <span class="built_in">getattr</span>(<span class="variable language_">self</span>, <span class="string">f&quot;xyz_encoding_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>&quot;</span>)(xyz_)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过静态密度预测层得到静态密度</span></span><br><span class="line">        static_sigma = <span class="variable language_">self</span>.static_sigma(xyz_)  <span class="comment"># 输出形状为 (B, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过位置编码的最终线性层得到最终位置编码</span></span><br><span class="line">        xyz_encoding_final = <span class="variable language_">self</span>.xyz_encoding_final(xyz_)</span><br><span class="line">        <span class="comment"># 将最终位置编码和输入的方向编码拼接</span></span><br><span class="line">        dir_encoding_input = torch.cat([xyz_encoding_final, input_dir_a], -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 通过方向编码层处理拼接后的结果</span></span><br><span class="line">        dir_encoding = <span class="variable language_">self</span>.dir_encoding(dir_encoding_input)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过静态 RGB 预测层得到静态 RGB 值</span></span><br><span class="line">        static_rgb = <span class="variable language_">self</span>.static_rgb(dir_encoding)</span><br><span class="line">        <span class="comment"># 将静态 RGB 值和静态密度拼接</span></span><br><span class="line">        static = torch.cat([static_rgb, static_sigma], -<span class="number">1</span>)  <span class="comment"># 输出形状为 (B, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> static</span><br></pre></td></tr></table></figure>

<h4 id="poses-py"><a href="#poses-py" class="headerlink" title="poses.py"></a>poses.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> utils.lie_group_helper <span class="keyword">import</span> make_c2w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 LearnPose 类，继承自 nn.Module，用于学习相机位姿</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LearnPose</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_cams, learn_R, learn_t, init_c2w=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param num_cams: 相机的数量</span></span><br><span class="line"><span class="string">        :param learn_R: 是否学习旋转部分，布尔值</span></span><br><span class="line"><span class="string">        :param learn_t: 是否学习平移部分，布尔值</span></span><br><span class="line"><span class="string">        :param init_c2w: (N, 4, 4) 的 torch 张量，表示初始的相机到世界的变换矩阵</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 调用父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(LearnPose, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 存储相机的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.num_cams = num_cams</span><br><span class="line">        <span class="comment"># 初始化初始相机到世界的变换矩阵为 None</span></span><br><span class="line">        <span class="variable language_">self</span>.init_c2w = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> init_c2w <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 如果提供了初始变换矩阵，将其作为不可训练的参数存储</span></span><br><span class="line">            <span class="variable language_">self</span>.init_c2w = nn.Parameter(init_c2w, requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义旋转参数，初始化为全零，是否可训练由 learn_R 决定</span></span><br><span class="line">        <span class="variable language_">self</span>.r = nn.Parameter(torch.zeros(size=(num_cams, <span class="number">3</span>), dtype=torch.float32), requires_grad=learn_R)  <span class="comment"># (N, 3)</span></span><br><span class="line">        <span class="comment"># 定义平移参数，初始化为全零，是否可训练由 learn_t 决定</span></span><br><span class="line">        <span class="variable language_">self</span>.t = nn.Parameter(torch.zeros(size=(num_cams, <span class="number">3</span>), dtype=torch.float32), requires_grad=learn_t)  <span class="comment"># (N, 3)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, cam_id</span>):</span><br><span class="line">        <span class="comment"># 根据相机 ID 提取对应的旋转参数，形状为 (3, )，表示轴角</span></span><br><span class="line">        r = <span class="variable language_">self</span>.r[cam_id]  <span class="comment"># (3, ) 轴角</span></span><br><span class="line">        <span class="comment"># 根据相机 ID 提取对应的平移参数，形状为 (3, )</span></span><br><span class="line">        t = <span class="variable language_">self</span>.t[cam_id]  <span class="comment"># (3, )</span></span><br><span class="line">        <span class="comment"># 使用 make_c2w 函数将轴角和平移参数转换为相机到世界的变换矩阵，形状为 (4, 4)</span></span><br><span class="line">        c2w = make_c2w(r, t)  <span class="comment"># (4, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果提供了初始变换矩阵，学习初始位姿和目标位姿之间的增量位姿</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.init_c2w <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 将当前计算得到的变换矩阵与初始变换矩阵相乘</span></span><br><span class="line">            c2w = c2w @ <span class="variable language_">self</span>.init_c2w[cam_id]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> c2w</span><br></pre></td></tr></table></figure>



<h2 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── utils/  # 工具文件夹</span><br><span class="line">│   ├── align_traj.py  # 轨迹对齐脚本文件，用于对不同轨迹数据进行对齐操作</span><br><span class="line">│   ├── comp_ate.py  # 计算绝对轨迹误差（Absolute Trajectory Error, ATE）的脚本文件</span><br><span class="line">│   ├── comp_ray_dir.py  # 计算光线方向的脚本文件，常用于计算机视觉和三维重建中的光线追踪等场景</span><br><span class="line">│   ├── lie_group_helper.py  # 李群相关辅助函数的脚本文件，李群在机器人运动学、计算机视觉中的位姿表示等方面有应用</span><br><span class="line">│   ├── pos_enc.py  # 位置编码脚本文件，在深度学习模型（如NeRF）中用于对位置信息进行编码</span><br><span class="line">│   ├── pose_utils.py  # 位姿处理工具脚本文件，包含处理相机位姿或物体位姿的相关函数</span><br><span class="line">│   ├── split_dataset.py  # 数据集划分脚本文件，用于将数据集划分为训练集、验证集和测试集等</span><br><span class="line">│   ├── training_utils.py  # 训练辅助工具脚本文件，包含训练模型时常用的工具函数，如学习率调整、损失函数计算等</span><br><span class="line">│   ├── vgg.py  # VGG网络相关脚本文件，可能包含VGG模型的定义、加载预训练权重等操作</span><br><span class="line">│   ├── vis_cam_traj.py  # 可视化相机轨迹的脚本文件，用于将相机在三维空间中的运动轨迹进行可视化展示</span><br><span class="line">│   └── volume_op.py  # 体操作脚本文件，在三维重建、体渲染等场景中对三维体数据进行操作</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕论文知识库</title>
    <url>/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/</url>
    <content><![CDATA[<p>手撕论文知识库</p>
<h2 id="深度学习项目代码目录全览及解析"><a href="#深度学习项目代码目录全览及解析" class="headerlink" title="深度学习项目代码目录全览及解析"></a>深度学习项目代码目录全览及解析</h2><blockquote>
<p>常见目录如下：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">project_name/</span><br><span class="line">├── data/                   # 数据集相关（原始/处理后的数据）</span><br><span class="line">├── dataloader/             # 数据加载与预处理模块（核心）</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── dataset.py          # 自定义Dataset类</span><br><span class="line">│   └── transforms.py       # 数据增强操作</span><br><span class="line">├── models/                 # 模型定义（核心）</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── backbone.py         # 主干网络</span><br><span class="line">│   └── layers.py           # 自定义网络层</span><br><span class="line">├── configs/                # 超参数配置文件（如YAML/JSON）</span><br><span class="line">├── utils/                  # 工具函数（如日志、评估指标）</span><br><span class="line">│   ├── data_utils.py</span><br><span class="line">│   ├── model_utils.py</span><br><span class="line">│   ├── visualization_utils.py</span><br><span class="line">│   └──...</span><br><span class="line">├── logs/                   # 记录训练和评估过程中的日志信息</span><br><span class="line">│   ├── training.log</span><br><span class="line">│   ├── validation.log</span><br><span class="line">│   └──...</span><br><span class="line">├── checkpoints/            # 训练保存的模型权重</span><br><span class="line">├── scripts/                # 运行脚本（训练/测试命令）</span><br><span class="line">├── requirements.txt        # 依赖库列表</span><br><span class="line">├── environment.yml         # Conda环境配置</span><br><span class="line">├── README.md               # 项目说明</span><br><span class="line">└── main.py                 # 主程序入口</span><br></pre></td></tr></table></figure>

<blockquote>
<ol>
<li><code>dataloader/</code></li>
</ol>
<p> <strong>作用</strong>：数据加载、预处理、增强（如论文项目中的 <code>with_colmap.py</code> 可能与多视图数据对齐相关）</p>
<p> <strong>典型内容</strong>：<code>Dataset</code> 类定义、数据增强函数、特征提取工具（如项目中的 <code>with_feature.py</code>）</p>
<ol start="2">
<li><strong><code>models/</code></strong></li>
</ol>
<p> <strong>作用</strong>：定义神经网络模型（如项目中的<code>nerf_models.py</code> 可实现NeRF的核心架构）。</p>
<p> <strong>典型内容</strong>：模型类继承<code>torch.nn.Module</code>，包含前向传播逻辑（如项目中的<code>depth_decoder.py</code>可用于深度估计解码）。</p>
<ol start="3">
<li><strong><code>utils/</code></strong></li>
</ol>
<p> <strong>作用</strong>：辅助工具（如项目中的 <code>pose_utils.py</code> 处理相机位姿，<code>training_utils.py</code> 封装训练逻辑）。</p>
<p> <strong>典型内容</strong>：评估指标计算、可视化工具、训练回调函数。</p>
<ol start="4">
<li><strong><code>third_party/</code></strong></li>
</ol>
<p> <strong>作用</strong>：第三方库或工具（如项目中的 <code>ATE</code> 可能用于轨迹评估，<code>pytorch_ssim</code> 实现结构相似性损失）。</p>
<ol start="5">
<li>其他关键要素</li>
</ol>
<p> <code>logs</code> 文件夹：记录训练和评估过程中的日志信息，如训练损失、验证损失、准确率等指标的变化情况，便于跟踪模型训练过程，分析模型的收敛性和性能表现。</p>
<p> <code>checkpoints</code> 文件夹：保存训练过程中的模型检查点，即模型在不同训练阶段的参数文件，用于在训练中断时恢复训练，或者用于选择在验证集上表现最好的模型进行测试和部署。</p>
<p> <code>requirement.txt</code>：列出项目所需的 Python 依赖库及其版本号，便于在新环境中快速安装项目所需的所有依赖。</p>
<p> <code>environment.yml</code>：Conda环境配置，确保依赖一致性。</p>
<p> <code>README.md</code>：项目说明、安装与使用指南（深度学习项目必备）。</p>
<p> <strong><code>main.py</code></strong>：项目的主程序入口，通常包含模型训练、评估和预测的主要逻辑，可通过命令行参数来控制程序的运行方式和参数设置。</p>
</blockquote>
<blockquote>
<p>示例如下：</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ - nerf/  # 基于占用率的神经辐射场项目根目录</span><br><span class="line">├──.gitignore  # 指定Git不跟踪的文件或文件夹</span><br><span class="line">├── LICENSE  # 项目使用许可条款文件</span><br><span class="line">├── README.md  # 介绍项目背景、功能、使用方法等的说明文档</span><br><span class="line">├── environment.yml  # 定义项目运行所需软件环境</span><br><span class="line">├── local1.txt  # 用途不明，或为本地说明、配置等文件</span><br><span class="line">├── dataloader/  # 存放数据加载与预处理代码</span><br><span class="line">│   ├── any_folder.py  # 从任意文件夹结构加载数据</span><br><span class="line">│   ├── local_save.py  # 负责数据本地保存</span><br><span class="line">│   ├── with_colmap.py  # 从COLMAP处理后格式加载数据</span><br><span class="line">│   ├── with_feature.py  # 加载带特征的数据</span><br><span class="line">│   ├── with_feature_colmap.py  # 结合COLMAP与特征数据加载</span><br><span class="line">│   └── with_mask.py  # 加载带掩码的数据</span><br><span class="line">├── models/  # 存放深度学习模型定义与操作代码</span><br><span class="line">│   ├── depth_decoder.py  # 深度解码，用于深度估计</span><br><span class="line">│   ├── intrinsics.py  # 处理相机内参相关内容</span><br><span class="line">│   ├── layers.py  # 定义深度学习层结构</span><br><span class="line">│   ├── nerf_feature.py  # 处理NeRF特征相关逻辑</span><br><span class="line">│   ├── nerf_mask.py  # 处理NeRF模型中掩码相关内容</span><br><span class="line">│   ├── nerf_models.py  # 定义NeRF模型架构等核心内容</span><br><span class="line">│   └── poses.py  # 处理位姿相关操作</span><br><span class="line">├── utils/  # 包含辅助项目运行的工具函数</span><br><span class="line">│   ├── align_traj.py  # 实现轨迹对齐算法</span><br><span class="line">│   ├── comp_ate.py  # 计算绝对轨迹误差</span><br><span class="line">│   ├── comp_ray_dir.py  # 计算光线方向</span><br><span class="line">│   ├── lie_group_helper.py  # 提供李群相关辅助函数</span><br><span class="line">│   ├── pos_enc.py  # 实现位置编码</span><br><span class="line">│   ├── pose_utils.py  # 提供位姿相关实用工具函数</span><br><span class="line">│   ├── split_dataset.py  # 划分数据集为训练、验证、测试集</span><br><span class="line">│   ├── training_utils.py  # 提供模型训练辅助函数</span><br><span class="line">│   ├── vgg.py  # 与VGG神经网络相关操作</span><br><span class="line">│   ├── vis_cam_traj.py  # 可视化相机轨迹</span><br><span class="line">│   └── volume_op.py  # 操作三维体数据</span><br><span class="line">├── tasks/  # 存放训练、测试等具体任务代码</span><br><span class="line">│   └──...</span><br><span class="line">└── third_party/  # 存放第三方代码或库</span><br><span class="line">    ├── ATE/  # 与绝对轨迹误差计算相关</span><br><span class="line">    │   └── README.md  # 说明该部分功能与用法</span><br><span class="line">    └── pytorch_ssim/  # 计算结构相似性指数的库</span><br></pre></td></tr></table></figure>

<h2 id="深度学习-神经网络架构总览"><a href="#深度学习-神经网络架构总览" class="headerlink" title="深度学习&#x2F;神经网络架构总览"></a>深度学习&#x2F;神经网络架构总览</h2><h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><h3 id="什么是torch"><a href="#什么是torch" class="headerlink" title="什么是torch"></a>什么是torch</h3><p>Torch 是 PyTorch 深度学习框架的核心库，具备强大的功能与广泛的用途。它提供了丰富的张量操作，可在 CPU 或 GPU 上高效计算，能轻松处理各类数据；其自动求导机制极大简化了深度学习中梯度计算与反向传播的过程，让模型训练更为便捷。借助<code>torch.nn</code>模块可方便构建如 CNN、RNN 等复杂神经网络架构，<code>torch.optim</code>模块提供多种优化算法用于模型参数更新。此外，Torch 还支持预训练模型的使用与微调，结合可视化工具能助力监控训练过程，广泛应用于图像、自然语言处理、推荐系统等诸多领域。</p>
<h3 id="torch常用函数和功能"><a href="#torch常用函数和功能" class="headerlink" title="torch常用函数和功能"></a>torch常用函数和功能</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><h5 id="什么是张量"><a href="#什么是张量" class="headerlink" title="什么是张量"></a>什么是张量</h5><p>张量是多维数组的泛化表示，可理解为一个多维的数据容器，零维张量是标量，一维张量是向量，二维张量是矩阵，三维及以上则是更高阶的张量。在深度学习里，使用张量是因为它能够高效地表示和处理大量的数据，像图像可表示为三维张量（高度、宽度、通道数），视频可表示为四维张量（帧数、高度、宽度、通道数）。并且，深度学习框架（如 PyTorch）针对张量运算进行了高度优化，能利用 GPU 等硬件加速计算，张量还能自然地支持自动求导机制，方便进行模型训练时的梯度计算和参数更新。</p>
<p>张量是 PyTorch 中最基础的数据结构，类似于 NumPy 的多维数组，但它可以在 GPU 上进行加速计算，并且支持自动求导等深度学习所需的特性。</p>
<h5 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h5><p>维度（也称为轴）是指张量在某个方向上的延伸。可以将维度理解为数据组织的一个方向或一个层次，类似于在地理坐标系统中，经度和纬度分别代表了不同的方向，张量的每个维度也代表了数据的一个特定方向的排列。维度的数量被称为张量的阶（rank），零阶张量是标量（一个单独的数值），一阶张量是向量（一维数组），二阶张量是矩阵（二维数组），三阶及以上的张量则用于表示更复杂的数据结构。</p>
<p><strong>注：维度从0开始算起，比如对于二阶张量，维度0代表行，维度1代表列</strong></p>
<p><strong>另：维度排列遵循（$a_n$, $a_{n-1}$, …, $a_1$）的形式，数字越前代表越高维的堆叠。比如<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起</strong></p>
<p><strong>1.零阶张量（标量）</strong></p>
<p>零阶张量只有一个数值，它没有方向的概念，维度数量为 0。例如这里的 <code>scalar</code> 就是一个零阶张量，它代表一个单一的数值，不涉及方向或多个元素的排列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scalar = torch.tensor(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标量的维度数量:&quot;</span>, scalar.dim()) </span><br></pre></td></tr></table></figure>

<p><strong>2.一阶张量（向量）</strong></p>
<p>一阶张量可以看作是一个向量，它有一个维度。这个维度代表了向量中元素的排列方向，向量的长度就是这个维度的大小。例如<code>vector</code> 是一个一阶张量，维度数量为 1，该维度的大小为 4，表示向量中有 4 个元素。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">vector = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量的维度数量:&quot;</span>, vector.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量在该维度的大小:&quot;</span>, vector.size(<span class="number">0</span>)) </span><br></pre></td></tr></table></figure>

<p><strong>3.二阶张量（矩阵）</strong></p>
<p>二阶张量是一个矩阵，有两个维度，通常称为行和列。第一个维度代表矩阵的行方向，第二个维度代表矩阵的列方向。例如<code>matrix</code> 是一个 2 行 3 列的矩阵，第一个维度的大小为 2 表示有 2 行，第二个维度的大小为 3 表示有 3 列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">matrix = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵的维度数量:&quot;</span>, matrix.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第一个维度（行）的大小:&quot;</span>, matrix.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第二个维度（列）的大小:&quot;</span>, matrix.size(<span class="number">1</span>)) </span><br></pre></td></tr></table></figure>

<p><strong>4.高阶张量（图像、视频等）</strong></p>
<p>对于三阶及以上的张量，维度的含义更加丰富，通常与具体的数据类型和应用场景相关。</p>
<p><strong>图像数据</strong>：在处理图像时，通常使用三阶张量。例如，一张彩色图像可以表示为一个形状为 <code>(高度, 宽度, 通道数)</code> 的三阶张量。这里的第一个维度代表图像的高度方向，第二个维度代表图像的宽度方向，第三个维度代表图像的通道（如 RGB 三个通道）。</p>
<p><code>image = torch.randn(224, 224, 3)</code> 这行代码能够随机生成一个形状为 <code>(224, 224, 3)</code> 的张量来模拟图像的三通道数值。</p>
<p>由于 <code>torch.randn()</code> 生成的是服从标准正态分布的随机数，这些数值可能为负数，也可能超出了常见图像像素值的范围（通常是 0 - 255 或者 0 - 1）。在实际的图像处理任务中，如果需要模拟真实图像，可能需要对这些随机值进行进一步的处理，例如通过归一化或裁剪操作将其限制在合适的范围内。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">image = torch.randn(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像张量的维度数量:&quot;</span>, image.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像高度维度的大小:&quot;</span>, image.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像宽度维度的大小:&quot;</span>, image.size(<span class="number">1</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像通道维度的大小:&quot;</span>, image.size(<span class="number">2</span>)) </span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250219032451598.png" alt="image-20250219032451598"></p>
<p><strong>视频数据</strong>：视频可以看作是一系列的图像帧，因此可以用四阶张量表示，形状通常为 <code>(帧数, 高度, 宽度, 通道数)</code>。第一个维度代表视频中的帧数，其余维度与图像张量的含义相同。</p>
<p>*<strong>5.通道</strong></p>
<p>通道指图像中特定类型信息的集合，图像可含一个或多个通道，各通道存储图像某方面特征数据。像单通道存亮度，RGB 三通道分别存红、绿、蓝颜色信息，四通道还多了透明度通道，以此组合完整呈现图像。</p>
<p>通道能实现颜色表示与混合，如 RGB 三通道通过不同数值组合呈现丰富色彩；可用于特征提取与分析，不同通道提供不同特征，助力图像分析和目标识别；还能用于图像合成与特效制作，借助透明度通道可控制图像透明效果实现合成。</p>
<p>在图片里，灰度图用单通道呈现黑白影像；彩色照片靠 RGB 三通道展示多彩画面；PNG 图片利用四通道含透明度信息实现图像融合。视频是连续的图片帧，同样利用通道来呈现色彩、进行特效处理，如影视中常见的抠图合成场景就借助了通道特性。</p>
<h5 id="张量相关函数"><a href="#张量相关函数" class="headerlink" title="张量相关函数"></a>张量相关函数</h5><p><strong>1.创建</strong></p>
<ul>
<li><p>创建张量<code>torch.tensor()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>当你有现有的数据存储在 Python 列表或 NumPy 数组中，并且需要将其输入到 PyTorch 模型进行计算时使用。例如，在加载数据集后，将数据转换为张量形式以便后续处理。</p>
<p>从数据存储的角度来看，<code>tensor</code> 存储了 Python 列表 <code>data</code> 中的元素 <code>[1, 2, 3]</code>。它将这些数据以一种高效的、适合计算机处理的方式组织起来，存储在内存中。在这个例子中，<code>tensor</code> 是一个一维张量，形状为 <code>(3,)</code>，这意味着它包含 3 个元素。</p>
<p>在数学运算方面，<code>tensor</code> 可以参与各种数学运算，如加法、乘法、矩阵乘法等。PyTorch 为张量提供了丰富的数学运算函数，这些运算可以在 CPU 或 GPU 上高效执行。</p>
<p>在深度学习的上下文中，<code>tensor</code> 是模型输入、输出以及参数的基本表示形式。例如，在一个简单的全连接神经网络中，输入数据会被转换为张量输入到网络中，网络的权重和偏置也是以张量的形式存储和更新的。在上述例子中，<code>tensor</code> 可以作为一个简单的输入数据示例，如果要构建一个神经网络处理这个输入，可能会进行如下操作（以下是一个简单示例）：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 3，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(<span class="number">0</span>)  <span class="comment"># 转换为适合输入模型的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型输出:&quot;</span>, output)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>创建全零张量<code>torch.zeros()</code></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">zeros_tensor = torch.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>常用于初始化某些变量，如在初始化神经网络的偏置项时，可使用全零张量。另外，在需要填充零值进行数据预处理或占位时也会用到。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>传入的参数 <code>(2, 3)</code>对应创建的 <code>zeros_tensor</code> 是一个 2 行 3 列的二维张量。如果使用 <code>torch.zeros((2, 3, 4))</code> 这样的代码，那么创建的就是一个三维张量，其中 <code>2</code> 表示最外层维度的大小（可以想象成有 2 个二维矩阵堆叠在一起），<code>3</code> 表示每个二维矩阵的行数，<code>4</code> 表示每个二维矩阵的列数。依此类推，对于更高维的张量，每个数字都代表对应维度上的大小。相当于高是2，行是3，列是4</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>如果是<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起，数字越前就代表越高维的堆叠。</p>
<ul>
<li><p>创建全一张量**<code>torch.ones()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">ones_tensor = torch.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>与 <code>torch.zeros()</code> 类似，可用于初始化特定变量。在一些归一化操作或需要特定初始值为 1 的场景中会使用。</p>
<ul>
<li><p>创建指定形状的随机张量，元素值在[0 , 1)之间**<code>torch.rand()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">random_tensor = torch.rand((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>在初始化神经网络的权重时，随机初始化是常见的做法，可使用 <code>torch.rand()</code> 生成初始权重张量。</p>
<p><strong>2.操作</strong></p>
<ul>
<li><p><strong><code>torch.cat()</code></strong>：用于在指定维度上拼接多个张量。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">c = torch.cat((a, b), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>当需要将多个张量合并为一个更大的张量时使用。例如，在处理多模态数据时，将不同模态的特征张量拼接在一起。</p>
<p><code>torch.cat((a, b), dim=1)</code> 表示在维度 1（列方向）上对张量 <code>a</code> 和 <code>b</code> 进行拼接。可以看到，拼接后的张量 <code>c</code> 是将 <code>a</code> 和 <code>b</code> 的列进行了合并，行数不变，列数变为原来两个张量列数之和。在处理多模态数据时，比如一个模态的数据特征用张量 <code>a</code> 表示，另一个模态的数据特征用张量 <code>b</code> 表示，通过这种拼接操作可以将不同模态的特征合并在一起，方便后续的处理。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>

<p>在二维张量的语境下，维度 0 代表行方向。<code>torch.cat((a, b), dim=0)</code> 会将张量 <code>b</code> 按行的顺序拼接到张量 <code>a</code> 的下方，拼接后的张量列数不变，行数为原来两个张量行数之和。在实际应用中，若 <code>a</code> 和 <code>b</code> 分别表示两组样本数据，在维度 0 上拼接就相当于将这两组样本合并成一组更大的样本集。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">在维度 <span class="number">0</span> 上拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.reshape()</code></strong>：改变张量的形状。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([1, 2, 3, 4])</span><br><span class="line">reshaped_x = torch.reshape(x, (2, 2))</span><br></pre></td></tr></table></figure>

<p>在神经网络中，不同层之间的数据形状可能需要进行调整，使用 <code>torch.reshape()</code> 可以方便地改变张量形状以满足层的输入要求。</p>
<p><code>torch.reshape(x, (2, 2))</code> 是将一维张量 <code>x</code> 重塑为二维张量 <code>reshaped_x</code>，形状为 <code>(2, 2)</code>。在神经网络中，不同层之间的数据形状可能不匹配，例如某一层的输出是一维向量，而后续层需要二维矩阵作为输入，这时就可以使用 <code>torch.reshape()</code> 来调整数据的形状，使其满足层的输入要求。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">重塑后的张量 reshaped_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.transpose()</code></strong>：交换张量的两个维度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[1, 2], [3, 4]])</span><br><span class="line">transposed_x = torch.transpose(x, 0, 1)</span><br></pre></td></tr></table></figure>

<p>在矩阵运算中，有时需要对矩阵进行转置操作。在图像处理中，可能需要调整图像张量的维度顺序。</p>
<p><code>torch.transpose(x, 0, 1)</code> 表示交换张量 <code>x</code> 的第 0 维和第 1 维。在这个二维矩阵的例子中，就是对矩阵进行了转置操作，原来的行变成了列，列变成了行。在矩阵运算中，矩阵转置是一个常见的操作，例如在计算矩阵乘法时可能需要对矩阵进行转置。在图像处理中，图像张量的维度顺序可能需要调整，比如将 <code>(高度, 宽度, 通道数)</code> 调整为 <code>(通道数, 高度, 宽度)</code> 以适应某些模型的输入要求，这时就可以使用 <code>torch.transpose()</code> 来实现。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">转置后的张量 transposed_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.squeeze()</code>：移除张量中所有维度为 1 的轴（或指定轴）。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]])  <span class="comment"># 形状 [1, 3, 1]</span></span><br><span class="line">y = torch.squeeze(x)                 <span class="comment"># 形状变为 [3]</span></span><br></pre></td></tr></table></figure>

<p>在张量操作中，某些操作（如池化、索引）可能产生冗余的维度为 1 的轴。例如：</p>
<ul>
<li>处理单通道图像时，通道维度可能为 1。</li>
<li>批量处理单个样本时，批量维度可能为 1。</li>
<li>某些神经网络层的输出可能保留不必要的单维度。</li>
</ul>
<p><code>torch.squeeze()</code> 默认移除所有大小为 1 的维度，也可指定 <code>dim</code> 参数移除特定轴（仅当该轴大小为 1 时生效）。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">输入张量 x：</span><br><span class="line">tensor([[[<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>],</span><br><span class="line">         [<span class="number">3</span>]]])</span><br><span class="line">输出张量 y：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">squeeze() 移除了所有大小为 <span class="number">1</span> 的维度（第 <span class="number">0</span> 维和第 <span class="number">2</span> 维），仅保留第 <span class="number">1</span> 维（大小为 <span class="number">3</span>）。</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h4><h5 id="前向传播（Forward-Propagation）"><a href="#前向传播（Forward-Propagation）" class="headerlink" title="前向传播（Forward Propagation）"></a>前向传播（Forward Propagation）</h5><ol>
<li>定义</li>
</ol>
<p>前向传播是深度学习模型处理输入数据以产生输出的过程。在这个过程中，输入数据从输入层开始，依次经过神经网络的各个隐藏层，每层都会对输入进行特定的数学变换（如加权求和后通过激活函数），最终到达输出层得到预测结果。可以将其看作是信息从输入向输出流动的过程，每一层根据前一层的输出计算本层的输出，逐步传递直至得到最终输出。</p>
<ol start="2">
<li>作用</li>
</ol>
<p>根据当前模型的参数（权重和偏置）对输入数据进行预测。通过一系列的线性和非线性变换，模型能够学习到输入数据中的特征模式，并将其映射到输出空间。例如，在图像分类任务中，前向传播可以将输入的图像转换为不同类别的概率分布，从而判断图像所属的类别。它为后续的反向传播提供了预测结果，是整个深度学习训练过程的基础步骤。</p>
<h5 id="反向传播（Backward-Propagation）"><a href="#反向传播（Backward-Propagation）" class="headerlink" title="反向传播（Backward Propagation）"></a>反向传播（Backward Propagation）</h5><ol>
<li>定义</li>
</ol>
<p>反向传播是深度学习中用于计算损失函数（Loss Function）关于模型参数（权重和偏置）的梯度的算法。它基于链式法则，从输出层开始，将损失函数的误差沿着计算图反向传播到输入层，依次计算每一层参数的梯度。简单来说，反向传播是在已知前向传播得到的预测结果和真实标签的情况下，计算如何调整模型参数可以减小损失的过程。</p>
<ol start="2">
<li>作用</li>
</ol>
<p>为模型参数的更新提供依据。在深度学习中，通常使用优化算法（如随机梯度下降，Stochastic Gradient Descent，SGD）来更新模型的参数，而这些优化算法需要知道损失函数关于参数的梯度。反向传播通过高效地计算这些梯度，使得模型能够根据预测误差自动调整参数，从而不断优化模型的性能。通过多次迭代前向传播和反向传播，模型可以逐渐学习到输入数据和输出标签之间的映射关系，提高预测的准确性。</p>
<h5 id="计算图与梯度追踪（可略过）"><a href="#计算图与梯度追踪（可略过）" class="headerlink" title="计算图与梯度追踪（可略过）"></a>计算图与梯度追踪（可略过）</h5><ol>
<li><code>requires_grad</code>设置张量是否需要进行梯度追踪</li>
</ol>
<p>在深度学习模型训练过程中，需要计算损失函数关于模型参数的梯度，以便使用优化算法（如随机梯度下降）更新参数。通过将模型参数张量的 <code>requires_grad</code> 设置为 <code>True</code>，可以利用自动求导机制自动计算梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出 4.0</span></span><br></pre></td></tr></table></figure>

<p><code>requires_grad</code> 是 PyTorch 张量的一个属性，当将其设置为 <code>True</code> 时，PyTorch 会开始追踪该张量的所有操作，构建计算图。计算图是一个有向无环图，它记录了从输入张量到输出张量的所有操作路径。</p>
<p>在上述示例中，我们创建了一个张量 <code>x</code> 并将 <code>requires_grad</code> 设置为 <code>True</code>，然后定义了一个函数 <code>y = x ** 2</code>。PyTorch 会自动记录这个操作，构建相应的计算图。</p>
<p>调用 <code>y.backward()</code> 方法时，PyTorch 会根据链式法则沿着计算图反向传播，计算出 <code>y</code> 关于 <code>x</code> 的梯度，并将梯度存储在 <code>x.grad</code> 属性中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x 的梯度: tensor([4.])</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>grad_fn</code> 属性与反向传播链</li>
</ol>
<p><strong><code>grad_fn</code></strong>：PyTorch 张量的属性，指向创建该张量的函数对象，用于记录操作历史以支持反向传播求梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个需要梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义操作 y = x^2</span></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="comment"># 定义操作 z = y * 3</span></span><br><span class="line">z = y * <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印每个张量的 grad_fn</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y 的 grad_fn:&quot;</span>, y.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;z 的 grad_fn:&quot;</span>, z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># 打印 x 的梯度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x 的梯度:&quot;</span>, x.grad)</span><br></pre></td></tr></table></figure>

<p>在深度学习模型训练时，需要计算损失函数关于模型参数的梯度。由于模型中存在大量复杂的运算，通过 <code>grad_fn</code> 记录的操作历史，PyTorch 可以构建反向传播链，从而准确计算出梯度，为参数更新提供依据。</p>
<p>在上述代码中，首先创建了一个开启梯度追踪的张量 <code>x</code>。当执行 $y &#x3D; x^2$操作时，PyTorch 会创建一个表示平方操作的函数对象，<code>y.grad_fn</code> 就会指向这个函数对象，以此记录下 <code>y</code> 是由 <code>x</code> 经过平方操作得到的。接着执行 $z &#x3D; 3y$，同样地，<code>z.grad_fn</code> 会指向表示乘法操作的函数对象，记录 <code>z</code> 的计算来源。</p>
<p>当调用 <code>z.backward()</code> 时，PyTorch 会从 <code>z</code> 开始，依据 <code>z.grad_fn</code> 找到创建 <code>z</code> 的操作，然后通过这个操作回溯到 <code>y</code>，再根据 <code>y.grad_fn</code> 回溯到 <code>x</code>。在这个回溯过程中，按照链式法则逐步计算出 <code>z</code> 关于 <code>x</code> 的梯度，并将其存储在 <code>x.grad</code> 中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y 的 grad_fn: &lt;PowBackward0 object at 0x...&gt;</span><br><span class="line">z 的 grad_fn: &lt;MulBackward0 object at 0x...&gt;</span><br><span class="line">x 的梯度: tensor([12.])</span><br></pre></td></tr></table></figure>

<p><code>&lt;PowBackward0 object at 0x...&gt;</code> 是 PyTorch 中用于表示反向传播过程中特定操作的梯度计算函数对象的字符串表示形式。</p>
<p><strong>反向传播函数对象</strong>：在 PyTorch 的自动求导机制里，对张量进行各种运算（如加法、乘法、幂运算等）时，PyTorch 会构建一个计算图来记录这些操作的顺序和依赖关系。每个操作在计算图中都对应一个前向传播函数（用于计算输出结果）和一个反向传播函数（用于计算梯度）。</p>
<p><code>PowBackward0</code> 的意义：<code>PowBackward0</code> 表示的是幂运算的反向传播函数。执行 $y &#x3D; x^2$这样的幂运算时，<code>y</code> 的 <code>grad_fn</code> 属性就会指向一个 <code>PowBackward0</code> 对象，这个对象负责在反向传播过程中计算关于输入张量 <code>x</code> 的梯度。<strong><code>at 0x...</code></strong>：<code>at 0x...</code> 后面跟着的是该对象在内存中的地址。这个地址是系统为该对象分配的唯一标识符，用于在内存中定位该对象。</p>
<h5 id="梯度控制（可略过）"><a href="#梯度控制（可略过）" class="headerlink" title="梯度控制（可略过）"></a>梯度控制（可略过）</h5><p>1.<code>torch.no_grad()</code> 上下文管理器</p>
<p>用于临时禁止 PyTorch 的梯度计算功能，在其作用域内创建或操作的张量不会进行梯度追踪。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个需要梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 torch.no_grad() 上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y = x ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;y 是否进行梯度追踪:&quot;</span>, y.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在上下文管理器外进行操作</span></span><br><span class="line">z = x ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;z 是否进行梯度追踪:&quot;</span>, z.requires_grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">y 是否进行梯度追踪: False</span><br><span class="line">z 是否进行梯度追踪: True（z在上下文管理器外进行操作）</span><br></pre></td></tr></table></figure>

<p>2.<code>detach()</code> 分离计算图  </p>
<p><strong><code>detach()</code></strong>：用于将张量从当前计算图中分离，返回一个和原张量数据相同但不记录梯度信息、不参与反向传播的新张量。</p>
<ul>
<li>在模型评估阶段，只需得到预测结果，无需计算梯度，用 <code>detach()</code> 可节省内存和计算资源。</li>
<li>在多模型联合训练时，若某个模型输出用于另一模型但不影响自身梯度计算，可使用 <code>detach()</code> 分离。（核心还是节省内存和计算资源）</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建需梯度追踪的张量</span></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line"><span class="comment"># 分离计算图</span></span><br><span class="line">y_detached = y.detach()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原张量 y 是否追踪梯度:&quot;</span>, y.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分离后的张量 y_detached 是否追踪梯度:&quot;</span>, y_detached.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    y_detached.backward()</span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;错误信息: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>上述代码里，<code>x</code> 开启梯度追踪，<code>y = 2 * x</code> 会记录在计算图中。调用 <code>y.detach()</code> 后得到 <code>y_detached</code>，它和 <code>y</code> 数据相同，但不追踪梯度。尝试对 <code>y_detached</code> 反向传播会报错，因为它已和计算图分离，无梯度信息。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">原张量 y 是否追踪梯度: True</span><br><span class="line">分离后的张量 y_detached 是否追踪梯度: False</span><br><span class="line">错误信息: element 0 of tensors does not require grad and does not have a grad_fn</span><br></pre></td></tr></table></figure>

<p>3.<code>zero_grad()</code> (梯度清零) </p>
<p>在深度学习模型的训练过程中，通常会按批次（batch）输入数据进行训练。每次反向传播计算得到的梯度会累加到模型参数的 <code>.grad</code> 属性中。如果不进行梯度清零，那么下一次计算的梯度会与之前的梯度累加，导致梯度计算错误。因此，在每个批次训练开始前，需要调用 <code>zero_grad()</code> 方法将梯度清零，以确保每个批次的梯度计算是独立的。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的线性模型</span></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">target = torch.randn(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">output = model(input_tensor)</span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播计算梯度</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;反向传播后第一个参数的梯度:&quot;</span>, model.weight.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度清零</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;梯度清零后第一个参数的梯度:&quot;</span>, model.weight.grad)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">反向传播后第一个参数的梯度: tensor([[ 0.1234, -0.5678, ...]])</span><br><span class="line">梯度清零后第一个参数的梯度: tensor([[ 0., 0., ...]])</span><br></pre></td></tr></table></figure>

<h4 id="神经网络层"><a href="#神经网络层" class="headerlink" title="神经网络层"></a>神经网络层</h4><h5 id="核心基类-nn-Module"><a href="#核心基类-nn-Module" class="headerlink" title="核心基类 nn.Module"></a>核心基类 <code>nn.Module</code></h5><h6 id="什么是核心基类-nn-Module？"><a href="#什么是核心基类-nn-Module？" class="headerlink" title="什么是核心基类 nn.Module？"></a>什么是核心基类 <code>nn.Module</code>？</h6><p><strong><code>nn.Module</code></strong>：PyTorch 中所有神经网络模块的基类，用于构建自定义的神经网络模型，封装了模型的结构和参数，方便进行前向传播、参数管理和模型保存等操作。</p>
<p>在深度学习中，我们需要构建各种各样的神经网络模型，如卷积神经网络（CNN）、循环神经网络（RNN）等。<code>nn.Module</code> 提供了一个统一的框架，使得我们可以方便地定义和管理这些模型。无论是简单的全连接网络还是复杂的深度网络，都可以通过继承 <code>nn.Module</code> 来构建。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个简单的神经网络模型，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个全连接层，输入维度为 10，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 定义前向传播过程</span></span><br><span class="line">        x = <span class="variable language_">self</span>.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型结构：&quot;</span>, model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状：&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状：&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>__init__</code> 方法</strong>：在自定义的模型类中，<code>__init__</code> 方法用于初始化模型的各个层。通过调用 <code>super(SimpleNet, self).__init__()</code> 确保父类 <code>nn.Module</code> 的初始化被正确执行。然后定义了一个全连接层 <code>self.fc</code>。</li>
<li><strong><code>forward</code> 方法</strong>：<code>forward</code> 方法定义了模型的前向传播过程，即输入数据如何经过各个层得到输出。在这个例子中，输入数据 <code>x</code> 经过全连接层 <code>self.fc</code> 得到输出。</li>
<li>模型实例化和前向传播：创建 <code>SimpleNet</code> 的实例 <code>model</code> 后，将输入张量 <code>input_tensor</code> 传递给 <code>model</code> 就相当于调用了 <code>forward</code> 方法进行前向传播，得到输出 <code>output</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">模型结构： SimpleNet(</span><br><span class="line">  (fc): Linear(in_features=10, out_features=1, bias=True)</span><br><span class="line">)</span><br><span class="line">输入张量形状： torch.Size([1, 10])</span><br><span class="line">输出张量形状： torch.Size([1, 1])</span><br></pre></td></tr></table></figure>

<h6 id="自定义模型继承方法"><a href="#自定义模型继承方法" class="headerlink" title="自定义模型继承方法"></a>自定义模型继承方法</h6><ol>
<li>参数管理<code>parameters()</code>，<code>named_parameters()</code></li>
</ol>
<p><strong><code>parameters()</code></strong>：是 <code>nn.Module</code> 类的一个方法，它返回一个包含模型所有可学习参数的迭代器。通过遍历这个迭代器，能依次获取到模型中的各个参数张量，但不会提供参数的名称信息。</p>
<p><strong><code>named_parameters()</code></strong>：同样是 <code>nn.Module</code> 类的方法，它返回一个迭代器，该迭代器会生成模型可学习参数的名称和对应参数张量的元组。借助这个方法，我们不仅能获取参数张量，还能明确每个参数对应的名称，这在模型参数管理和调试时非常有用。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的神经网络模型，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 定义一个全连接层，输入维度为 10，输出维度为 5</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 定义另一个全连接层，输入维度为 5，输出维度为 1</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 parameters() 方法获取模型参数的迭代器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用 parameters() 获取参数：&quot;</span>)</span><br><span class="line">params = model.parameters()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数形状: <span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 named_parameters() 方法获取模型带名称的参数迭代器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用 named_parameters() 获取参数：&quot;</span>)</span><br><span class="line">named_params = model.named_parameters()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> named_params:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;参数名称: <span class="subst">&#123;name&#125;</span>, 参数形状: <span class="subst">&#123;param.shape&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong><code>parameters()</code></strong>：主要用于优化器初始化。在训练模型时，优化器（如 <code>torch.optim.SGD</code>、<code>torch.optim.Adam</code> 等）需要知道模型的可学习参数，以便对这些参数进行梯度更新。由于优化器只关心参数张量本身，不需要参数名称，所以使用 <code>parameters()</code> 即可。</p>
<p><strong><code>named_parameters()</code></strong>：在模型调试、参数分组优化或模型参数的选择性加载时非常有用。例如，你可能只想更新模型中某些特定层的参数，通过参数名称可以方便地筛选出这些参数；或者在加载预训练模型时，根据参数名称选择性地加载部分参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用 parameters() 获取参数：</span><br><span class="line">参数形状: torch.Size([5, 10])</span><br><span class="line">参数形状: torch.Size([5])</span><br><span class="line">参数形状: torch.Size([1, 5])</span><br><span class="line">参数形状: torch.Size([1])</span><br><span class="line"></span><br><span class="line">使用 named_parameters() 获取参数：</span><br><span class="line">参数名称: fc1.weight, 参数形状: torch.Size([5, 10])</span><br><span class="line">参数名称: fc1.bias, 参数形状: torch.Size([5])</span><br><span class="line">参数名称: fc2.weight, 参数形状: torch.Size([1, 5])</span><br><span class="line">参数名称: fc2.bias, 参数形状: torch.Size([1])</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>train()</code> 与 <code>eval()</code> 模式切换</li>
</ol>
<p><code>train()</code> 和 <code>eval()</code> 是 <code>nn.Module</code> 类中的方法，用于切换模型的训练和评估模式。<code>train()</code> 方法将模型设置为训练模式，<code>eval()</code> 方法将模型设置为评估模式，不同模式下部分层（如 <code>Dropout</code>、<code>BatchNorm</code>）会有不同的行为。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个包含 Dropout 层的简单神经网络模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=<span class="number">0.5</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置为训练模式</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式下 Dropout 是否启用:&quot;</span>, model.dropout.training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置为评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式下 Dropout 是否启用:&quot;</span>, model.dropout.training)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output_eval = model(input_tensor)</span><br><span class="line">model.train()</span><br><span class="line">output_train = model(input_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;评估模式输出:&quot;</span>, output_eval)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练模式输出:&quot;</span>, output_train)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>训练模式（<code>train()</code>）</strong>：在模型训练阶段使用，此时模型中的 <code>Dropout</code> 层会按照设定的概率随机丢弃部分神经元，<code>BatchNorm</code> 层会根据当前批次的数据更新统计信息（如均值和方差），有助于提高模型的泛化能力。</li>
<li><strong>评估模式（<code>eval()</code>）</strong>：在模型评估、测试或者推理阶段使用。在评估模式下，<code>Dropout</code> 层不再丢弃神经元，<code>BatchNorm</code> 层使用训练阶段统计得到的均值和方差进行归一化操作，保证评估结果的稳定性和一致性。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练模式下 Dropout 是否启用: True</span><br><span class="line">评估模式下 Dropout 是否启用: False</span><br><span class="line">评估模式输出: tensor([[...]], grad_fn=&lt;AddmmBackward0&gt;)</span><br><span class="line">训练模式输出: tensor([[...]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>这里具体的输出数值会因随机生成的输入数据和模型初始化不同而有所变化，但可以看到在不同模式下 <code>Dropout</code> 层的行为不同，导致输出结果也可能不同。</p>
<h5 id="网络层-Layers"><a href="#网络层-Layers" class="headerlink" title="网络层 (Layers)"></a>网络层 (Layers)</h5><h6 id="基础层"><a href="#基础层" class="headerlink" title="基础层"></a>基础层</h6><ol>
<li><code>nn.Linear</code>全连接层</li>
</ol>
<p><strong><code>nn.Linear</code></strong>：PyTorch 中用于创建全连接层（也称为线性层）的类，它对输入数据进行线性变换，即执行矩阵乘法和加法操作，可用于构建各种神经网络模型。</p>
<p><strong>神经网络基础构建</strong>：全连接层是神经网络中最基本的组成部分之一，常用于多层感知机（MLP）的构建，可处理各种类型的数据，如图像、文本等特征向量。</p>
<p><strong>特征映射</strong>：可以将输入数据从一个特征空间映射到另一个特征空间，有助于模型学习数据中的复杂模式和特征表示。</p>
<hr>
<p>为什么是全连接层？</p>
<p>1.连接方式：在全连接层中，每一个输入神经元都与每一个输出神经元相连接，这种连接是 “全” 的，即完全连接。对于 </p>
<p><code>nn.Linear(in_features, out_features)</code> ，输入的 <code>in_features</code> 个神经元和输出的 <code>out_features</code> 个神经元之间存在着完整的连接关系。</p>
<p>2.数学运算：假设输入向量$X$维度为$n$（即 <code>in_features</code>），输出向量$Y$维度为$m$（即 <code>out_features</code>），全连接层通过权重矩阵$W$（形状为$m × n$）和偏置向量$b$（形状为$m$）进行线性变换$Y&#x3D;WX+b$。这里的权重矩阵$W$描述了输入神经元和输出神经元之间所有可能的连接强度，每一个输入元素都会影响到每一个输出元素的计算结果，这种全面的连接关系是 “全连接” 概念的核心体现。</p>
<p>3.网络结构对比：在神经网络中，除了全连接层，还有<strong>其他类型的层</strong>，比如卷积层、池化层等。卷积层中，神经元只与输入数据的局部区域相连接，而<strong>不是像全连接层那样与所有输入神经元连接</strong>；池化层则主要进行下采样操作，不存在像全连接层这样全面的神经元连接模式。因此，为了突出这种所有输入和输出神经元之间都有连接的特殊结构，将其称为全连接层，以便和其他类型的层进行区分。</p>
<hr>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个全连接层，输入维度为 10，输出维度为 5</span></span><br><span class="line">linear_layer = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入数据，假设有 1 个样本，每个样本有 10 个特征</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = linear_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全连接层的权重形状:&quot;</span>, linear_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;全连接层的偏置形状:&quot;</span>, linear_layer.bias.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量的形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量的形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>初始化参数</strong>：<code>nn.Linear(in_features, out_features)</code> 中，<code>in_features</code> 表示输入特征的数量，<code>out_features</code> 表示输出特征的数量。在上述代码中，<code>in_features = 10</code>，<code>out_features = 5</code>，意味着输入的每个样本有 10 个特征，经过全连接层后输出的每个样本有 5 个特征。</li>
<li><strong>线性变换</strong>：全连接层的计算过程可以表示为$Y&#x3D;XW^T+b$ ，其中$X$是输入向量，$W$是权重矩阵，形状为 <code>(out_features, in_features)</code>，$b$是偏置向量，形状为 <code>(out_features,)</code>，$Y$是输出向量。</li>
<li><strong>前向传播</strong>：将输入张量 <code>input_tensor</code> 传递给 <code>linear_layer</code> 时，会自动执行上述线性变换，得到输出张量 <code>output</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">全连接层的权重形状: torch.Size([5, 10])</span><br><span class="line">全连接层的偏置形状: torch.Size([5])</span><br><span class="line">输入张量的形状: torch.Size([1, 10])</span><br><span class="line">输出张量的形状: torch.Size([1, 5])</span><br></pre></td></tr></table></figure>

<ul>
<li><input disabled="" type="checkbox"> 权重形状的确定:</li>
</ul>
<p>为了使矩阵乘法$WX$能够得到维度为$m$的输出向量，权重矩阵$W$的形状必须是$m × n$。这是因为在矩阵乘法中，两个矩阵能够相乘的条件是前一个矩阵的列数等于后一个矩阵的行数，并且相乘结果矩阵的行数等于前一个矩阵的行数，列数等于后一个矩阵的列数。即如果$W$是$m × n$矩阵，$X$是$n × 1$向量，那么$WX$的结果就是一个$m × 1$向量，符合输出向量$Y$的维度要求。</p>
<p>在代码示例中，输入特征数量 <code>in_features = 10</code>，输出特征数量 <code>out_features = 5</code>，所以权重矩阵 的形状就是 <code>(5, 10)</code>，即 <code>torch.Size([5, 10])</code>。</p>
<ul>
<li><input disabled="" type="checkbox"> 偏置形状的确定</li>
</ul>
<p>偏置向量$b$的作用是在经过矩阵乘法得到的结果上进行偏移。由于输出向量$Y$的维度是$m$，为了能够对$WX$的每一个元素都加上一个偏移量，偏置向量$b$的维度也必须是$m$，即$b∈\R^m$。</p>
<p>在代码示例中，输出特征数量 <code>out_features = 5</code>，所以偏置向量 的形状就是 <code>(5,)</code>，即 <code>torch.Size([5])</code>。</p>
<ol start="2">
<li><code>nn.Bilinear</code>双线性层</li>
</ol>
<p><strong><code>nn.Bilinear</code></strong>：PyTorch 中的一个类，用于创建双线性层。双线性层对两个输入进行双线性变换，能捕捉两个输入之间的交互信息，是一种比普通线性层更复杂的变换形式。</p>
<p><strong>关系建模</strong>：在需要捕捉两个不同特征之间交互关系的任务中非常有用。例如，在推荐系统中，可以用双线性层来建模用户特征和物品特征之间的交互，以预测用户对物品的偏好；在自然语言处理中，可用于处理两个不同句子或不同语义表示之间的关系。</p>
<p><strong>融合多模态信息</strong>：当处理多模态数据（如图像和文本）时，双线性层可以帮助融合不同模态之间的信息，挖掘它们之间的潜在关联。</p>
<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250222212028325.png" alt="image-20250222212028325"></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个双线性层</span></span><br><span class="line"><span class="comment"># 第一个输入维度为 10，第二个输入维度为 20，输出维度为 5</span></span><br><span class="line">bilinear_layer = nn.Bilinear(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟两个输入张量</span></span><br><span class="line"><span class="comment"># 第一个输入：假设有 1 个样本，每个样本有 10 个特征</span></span><br><span class="line">input1 = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># 第二个输入：假设有 1 个样本，每个样本有 20 个特征</span></span><br><span class="line">input2 = torch.randn(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = bilinear_layer(input1, input2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;双线性层的权重形状:&quot;</span>, bilinear_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;双线性层的偏置形状:&quot;</span>, bilinear_layer.bias.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一个输入张量的形状:&quot;</span>, input1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第二个输入张量的形状:&quot;</span>, input2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量的形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>权重张量的形状符合 <code>(out_features, in1_features, in2_features)</code>，偏置向量的长度等于输出特征的数量，两个输入张量经过双线性层后，输出张量的特征数量变为 <code>out_features</code> 设定的值。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">双线性层的权重形状: torch.Size([5, 10, 20])</span><br><span class="line">双线性层的偏置形状: torch.Size([5])</span><br><span class="line">第一个输入张量的形状: torch.Size([1, 10])</span><br><span class="line">第二个输入张量的形状: torch.Size([1, 20])</span><br><span class="line">输出张量的形状: torch.Size([1, 5])</span><br></pre></td></tr></table></figure>

<h6 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h6><hr>
<p>1.什么是卷积层？</p>
<p>卷积层本质上是通过可学习的卷积核（滤波器）在输入数据上进行滑动并执行卷积操作，以提取输入数据中的局部特征模式，同时利用参数共享减少模型参数数量。</p>
<p>卷积操作指的是卷积核（一个小的矩阵）在输入数据（如图像矩阵）上按一定步长滑动，每滑动到一个位置，就将卷积核与该位置对应的输入局部区域元素对应相乘后求和，得到输出特征图的一个值，不断滑动直至覆盖整个输入数据，最终生成完整的输出特征图。</p>
<p>假设输入是一个 4×4 的单通道图像矩阵，使用一个 2×2 的卷积核进行卷积操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入图像矩阵：</span><br><span class="line">[[1 2 3 4]</span><br><span class="line"> [5 6 7 8]</span><br><span class="line"> [9 10 11 12]</span><br><span class="line"> [13 14 15 16]]</span><br><span class="line"></span><br><span class="line">卷积核：</span><br><span class="line">[[1 2]</span><br><span class="line"> [3 4]]</span><br><span class="line"> </span><br><span class="line">第一步：卷积核位于输入图像的左上角，覆盖的局部区域是：</span><br><span class="line">[[1 2]</span><br><span class="line"> [5 6]]</span><br><span class="line">将卷积核与该局部区域对应元素相乘再求和：</span><br><span class="line">(1x1+2x2)+(3x5+4x6)=44</span><br><span class="line">这个 44 就是输出特征图左上角的值。</span><br><span class="line"></span><br><span class="line">第二步：卷积核向右滑动一个步长（假设步长为 1），覆盖的局部区域变为：</span><br><span class="line">[[2 3]</span><br><span class="line"> [6 7]]</span><br><span class="line">同样进行对应元素相乘再求和的操作：</span><br><span class="line">(1x2+2x3)+(3x6+4x7)=54</span><br><span class="line">这是输出特征图中左上角右侧位置的值。</span><br><span class="line"></span><br><span class="line">后续步骤：卷积核继续向右、向下滑动，每次都重复上述相乘求和的操作，直到遍历完整个输入图像矩阵，最终得到一个 3×3 的输出特征图（因为 4×4 的输入矩阵使用 2×2 卷积核，步长为 1 时会得到 3×3 的输出）。</span><br></pre></td></tr></table></figure>

<p>2.卷积层有什么好处？</p>
<p><strong>减少参数数量</strong></p>
<p>卷积层使用参数共享机制，即一个卷积核在整个输入数据上滑动使用，相比于全连接层每个输出神经元都与所有输入相连，大大减少了需要学习的参数数量，降低计算量和存储需求，也减少了过拟合风险。</p>
<p><strong>提取局部特征</strong></p>
<p>卷积核在输入数据的局部区域进行操作，能够有效捕捉数据中的局部特征，如在图像中可检测边缘、纹理等。这些局部特征在不同位置可能具有相似性，卷积层可以很好地学习和利用这种特性。</p>
<p><strong>保留空间结构</strong></p>
<p>卷积操作基于局部连接，能保留输入数据的空间结构信息，这对于处理具有空间结构的数据（如图像、音频）非常重要，有助于模型理解数据中元素之间的相对位置关系。</p>
<p><strong>可构建深层网络</strong></p>
<p>多个卷积层可以堆叠形成深层卷积神经网络，随着网络深度增加，能学习到从简单到复杂、从底层到高层的多层次特征，提升模型在各种任务（如图像分类、目标检测）中的性能。</p>
<hr>
<ol>
<li><code>nn.Conv1d</code> 1D卷积：处理时序数据（如音频、文本）</li>
</ol>
<p><code>nn.Conv1d</code> 是 PyTorch 中用于进行一维卷积操作的模块，主要用于处理时序数据，像音频信号、文本序列等。一维卷积在这些数据上沿着一个维度（通常是时间维度）进行卷积操作，能有效提取数据中的局部特征模式。</p>
<p><strong>音频处理</strong>：可以用于音频特征提取、语音识别等任务，通过一维卷积提取音频信号中的时域特征。</p>
<p><strong>文本处理</strong>：在自然语言处理中，将文本序列看作一维数据，一维卷积可以捕捉文本中的局部语义信息，常用于文本分类、情感分析等任务。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入数据</span></span><br><span class="line"><span class="comment"># 输入数据形状：(批量大小, 输入通道数, 序列长度)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">16</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一维卷积层</span></span><br><span class="line"><span class="comment"># in_channels: 输入通道数</span></span><br><span class="line"><span class="comment"># out_channels: 输出通道数</span></span><br><span class="line"><span class="comment"># kernel_size: 卷积核大小</span></span><br><span class="line">conv1d_layer = nn.Conv1d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行卷积操作</span></span><br><span class="line">output = conv1d_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p><strong>输入数据</strong></p>
<p>通常是三维张量，形状为 <code>(batch_size, in_channels, sequence_length)</code>。其中 <code>batch_size</code> 表示一次处理的样本数量；<code>in_channels</code> 是输入数据的通道数，例如在音频处理中，单声道音频 <code>in_channels</code> 为 1，立体声音频 <code>in_channels</code> 为 2；<code>sequence_length</code> 是序列的长度，对于音频数据就是音频信号的采样点数，对于文本数据就是文本序列的长度。</p>
<p><strong>卷积层参数</strong></p>
<p><code>in_channels</code>：输入数据的通道数，必须与输入张量的第二维大小一致。</p>
<p><code>out_channels</code>：输出数据的通道数，即卷积层使用的卷积核数量。每个卷积核会提取一种特定的特征，因此不同的卷积核会输出不同的特征图。</p>
<p><code>kernel_size</code>：卷积核的大小，表示在序列维度上卷积核覆盖的元素个数。例如 <code>kernel_size=3</code> 表示卷积核在序列上每次覆盖 3 个元素。</p>
<p><strong>卷积操作过程</strong></p>
<p>卷积核在输入数据的序列维度上滑动，每次覆盖 <code>kernel_size</code> 个元素，并在每个通道上进行卷积操作，然后将各通道的结果相加（如果有多个输入通道），最后加上偏置项得到输出的一个值。卷积核不断滑动，最终得到输出特征图。</p>
<p><strong>输出数据</strong></p>
<p>输出数据同样是三维张量，形状为 <code>(batch_size, out_channels, new_sequence_length)</code>。其中 <code>batch_size</code> 与输入相同；<code>out_channels</code> 是卷积层定义的输出通道数；<code>new_sequence_length</code> 由输入序列长度、卷积核大小、步长（<code>stride</code>，默认为 1）和填充（<code>padding</code>，默认为 0）等因素决定，计算公式为：</p>
<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250222215408498.png" alt="image-20250222215408498"></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([16, 3, 100])</span><br><span class="line">输出张量形状: torch.Size([16, 6, 98])</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><code>nn.Conv2d</code> 2D卷积：处理图像数据</li>
</ol>
<p><code>nn.Conv2d</code>：PyTorch 中用于实现二维卷积操作的类，主要处理图像数据。通过可学习的卷积核在输入图像上滑动并进行卷积运算，提取图像局部特征，利用参数共享减少模型参数。</p>
<p><strong>图像分类</strong>：用于提取图像特征，结合后续层将图像映射到不同类别，如区分猫狗图像。</p>
<p><strong>目标检测</strong>：提取物体特征，配合检测算法确定图像中物体的位置和类别，像自动驾驶中检测车辆、行人。</p>
<p><strong>语义分割</strong>：对图像每个像素分类，将图像分割成不同语义区域，例如医学图像中分割肿瘤和正常组织。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据，仅保留第一张图片</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">in_channels = <span class="number">3</span></span><br><span class="line">height = <span class="number">4</span></span><br><span class="line">width = <span class="number">4</span></span><br><span class="line"><span class="comment"># 手动设定输入张量，方便后续手动计算验证</span></span><br><span class="line">input_tensor = torch.tensor([</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">         [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">         [<span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">         [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>]],</span><br><span class="line">        [[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">         [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>],</span><br><span class="line">         [<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>]],</span><br><span class="line">        [[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">         [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">         [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">         [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>]]</span><br><span class="line">    ]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义卷积层</span></span><br><span class="line">out_channels = <span class="number">2</span></span><br><span class="line">kernel_size = <span class="number">2</span></span><br><span class="line">stride = <span class="number">1</span></span><br><span class="line">padding = <span class="number">0</span></span><br><span class="line">conv2d_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,</span><br><span class="line">                         kernel_size=kernel_size, stride=stride, padding=padding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动设定卷积核权重，方便后续手动计算验证</span></span><br><span class="line"><span class="comment"># 这里假设的卷积核和前面理论部分一致</span></span><br><span class="line">conv2d_layer.weight.data = torch.tensor([</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>]],</span><br><span class="line">        [[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">0</span>]],</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">        [[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">        [[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">         [<span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">    ]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动设定偏置为 0，简化计算</span></span><br><span class="line">conv2d_layer.bias.data = torch.zeros(out_channels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行卷积操作</span></span><br><span class="line">output = conv2d_layer(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;卷积核形状:&quot;</span>, conv2d_layer.weight.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;第一张图片的输出:&quot;</span>, output[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动计算第一张图片经过第一个卷积核的左上角输出值</span></span><br><span class="line">first_kernel_channel1 = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line">first_kernel_channel2 = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]], dtype=torch.float32)</span><br><span class="line">first_kernel_channel3 = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个通道的局部卷积</span></span><br><span class="line">local_conv_channel1 = (input_tensor[<span class="number">0</span>, <span class="number">0</span>, :<span class="number">2</span>, :<span class="number">2</span>] * first_kernel_channel1).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 第二个通道的局部卷积</span></span><br><span class="line">local_conv_channel2 = (input_tensor[<span class="number">0</span>, <span class="number">1</span>, :<span class="number">2</span>, :<span class="number">2</span>] * first_kernel_channel2).<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 第三个通道的局部卷积</span></span><br><span class="line">local_conv_channel3 = (input_tensor[<span class="number">0</span>, <span class="number">2</span>, :<span class="number">2</span>, :<span class="number">2</span>] * first_kernel_channel3).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多通道结果相加</span></span><br><span class="line">manual_output = local_conv_channel1 + local_conv_channel2 + local_conv_channel3</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;手动计算第一张图片经过第一个卷积核左上角输出值:&quot;</span>, manual_output)</span><br></pre></td></tr></table></figure>

<p>上述代码模拟了处理彩色图像的过程。输入数据是一个四维张量，形状为 <code>(batch_size, in_channels, height, width)</code>。例如，<code>batch_size = 1</code> 表示一次处理 1 张图像；<code>in_channels = 3</code> 对应 RGB 三个通道；<code>height = 4</code> 和 <code>width = 4</code> 是图像的高度和宽度。输入数据可以表示为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入数据形状: (2, 3, 4, 4)</span><br><span class="line">第 1 张图像通道 1:</span><br><span class="line">[[1, 2, 3, 4],</span><br><span class="line"> [5, 6, 7, 8],</span><br><span class="line"> [9, 10, 11, 12],</span><br><span class="line"> [13, 14, 15, 16]]</span><br><span class="line"></span><br><span class="line">第 1 张图像通道 2:</span><br><span class="line">[[2, 3, 4, 5],</span><br><span class="line"> [6, 7, 8, 9],</span><br><span class="line"> [10, 11, 12, 13],</span><br><span class="line"> [14, 15, 16, 17]]</span><br><span class="line"></span><br><span class="line">第 1 张图像通道 3:</span><br><span class="line">[[3, 4, 5, 6],</span><br><span class="line"> [7, 8, 9, 10],</span><br><span class="line"> [11, 12, 13, 14],</span><br><span class="line"> [15, 16, 17, 18]]</span><br><span class="line"></span><br><span class="line">...（其他图片类似）</span><br></pre></td></tr></table></figure>

<p>定义一个二维卷积层，设置参数如下：</p>
<ul>
<li><code>in_channels = 3</code>，与输入图像的通道数一致。</li>
<li><code>out_channels = 2</code>，表示使用 2 个卷积核。</li>
<li><code>kernel_size = 2</code>，卷积核是 2x2 的正方形。</li>
<li><code>stride = 1</code>，卷积核每次滑动 1 个单位。</li>
<li><code>padding = 0</code>，不进行填充。</li>
</ul>
<p>每个卷积核也是一个四维张量，形状为 <code>(out_channels, in_channels, kernel_height, kernel_width)</code>，这里是 <code>(2, 3, 2, 2)</code>。假设两个卷积核分别为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积核 1:</span><br><span class="line">通道 1:</span><br><span class="line">[[1, 0],</span><br><span class="line"> [0, 1]]</span><br><span class="line"></span><br><span class="line">通道 2:</span><br><span class="line">[[0, 1],</span><br><span class="line"> [1, 0]]</span><br><span class="line"></span><br><span class="line">通道 3:</span><br><span class="line">[[1, 1],</span><br><span class="line"> [1, 1]]</span><br><span class="line"></span><br><span class="line">卷积核 2:</span><br><span class="line">通道 1:</span><br><span class="line">[[1, 1],</span><br><span class="line"> [1, 1]]</span><br><span class="line"></span><br><span class="line">通道 2:</span><br><span class="line">[[0, 0],</span><br><span class="line"> [0, 0]]</span><br><span class="line"></span><br><span class="line">通道 3:</span><br><span class="line">[[1, 0],</span><br><span class="line"> [0, 1]]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">卷积操作</span><br><span class="line">以第 1 张图像为例，对于卷积核 1：</span><br><span class="line">通道 1 的卷积：卷积核在通道 1 的输入图像上滑动，计算局部区域与卷积核对应元素相乘再求和。例如，在左上角区域：</span><br><span class="line">通道 2 和通道 3 同样操作：得到各自的局部卷积结果。</span><br><span class="line">多通道结果相加：将三个通道的局部卷积结果相加，得到该位置的最终输出值。</span><br><span class="line">滑动卷积核：卷积核在图像上按步长 1 滑动，重复上述操作，得到卷积核 1 对第 1 张图像的输出特征图。</span><br><span class="line">对于卷积核 2 也进行同样的操作，最终得到两个输出特征图，这两个特征图组成了第 1 张图像经过卷积层后的输出。对于第 2 张图像，也重复上述卷积操作。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([1, 3, 4, 4])</span><br><span class="line">卷积核形状: torch.Size([2, 3, 2, 2])</span><br><span class="line">输出张量形状: torch.Size([1, 2, 3, 3])</span><br><span class="line">第一张图片的输出:</span><br><span class="line">tensor([[[ 41.,  48.,  55.],</span><br><span class="line">         [ 65.,  72.,  79.],</span><br><span class="line">         [ 89.,  96., 103.]],</span><br><span class="line"></span><br><span class="line">        [[ 26.,  30.,  34.],</span><br><span class="line">         [ 46.,  50.,  54.],</span><br><span class="line">         [ 66.,  70.,  74.]]], grad_fn=&lt;SelectBackward0&gt;)</span><br><span class="line">手动计算第一张图片经过第一个卷积核左上角输出值: tensor(41.)</span><br></pre></td></tr></table></figure>

<hr>
<p>卷积核如何确定？</p>
<p>卷积核的确定需要综合考虑任务需求、数据特点、模型架构和计算资源等因素，并通过实验和调优不断优化，以达到最佳的性能。</p>
<hr>
<ol start="3">
<li><code>nn.Conv3d</code> 3D卷积：处理视频&#x2F;体数据</li>
</ol>
<p><code>nn.Conv3d</code> 是 PyTorch 中用于实现三维卷积操作的类，主要用于处理视频数据（包含时间维度和空间维度）或体数据（如医学影像中的三维体数据）。它通过可学习的三维卷积核在输入数据上进行滑动并执行卷积运算，提取数据中的三维特征，利用参数共享机制减少模型参数数量，降低计算复杂度。</p>
<p><strong>视频分类</strong>：在视频分类任务中，<code>nn.Conv3d</code> 用于提取视频的时空特征，通过多个卷积层和池化层的组合，将视频特征映射到不同的类别。例如，在识别视频中的动作类型（如跑步、跳舞、打球等）时，三维卷积可以捕捉视频中随时间变化的动作特征。</p>
<p><strong>医学影像分析</strong>：在医学影像分析中，如对 CT 或 MRI 扫描得到的三维体数据进行分析，<code>nn.Conv3d</code> 可以用于肿瘤检测、器官分割等任务。它能够提取体数据中的三维结构信息，帮助医生进行疾病诊断。</p>
<p><strong>自动驾驶场景</strong>：在自动驾驶中处理点云数据（可以看作三维空间中的数据）时，<code>nn.Conv3d</code> 可用于识别道路上的障碍物、车辆、行人等目标，通过分析点云数据的三维特征来辅助决策。</p>
<p><strong>卷积核与卷积操作</strong></p>
<p>三维卷积核是一个三维的矩阵，其元素是可学习的权重参数。在进行卷积操作时，三维卷积核会在输入的三维数据（如视频的多个帧组成的序列或者三维体数据）上按一定的步长进行滑动，每次覆盖一个与卷积核大小相同的三维局部区域。对于这个局部区域，将卷积核与该区域的元素对应相乘后求和，得到一个输出值，这个值就构成了输出特征体的一个元素。随着卷积核不断滑动，遍历整个输入三维数据，最终生成完整的输出特征体。</p>
<hr>
<p><code>nn.Conv1d/2d/3d</code>的共同原理与特征</p>
<p><strong>共同原理</strong></p>
<p>1.卷积操作核心</p>
<p>核心都是卷积运算。卷积核（也称为滤波器）是一组可学习的权重参数，在输入数据上按照一定规则滑动，每次覆盖一个局部区域，将卷积核与该局部区域的元素对应相乘后求和，得到一个输出值。这个过程不断重复，直到卷积核遍历完整个输入数据，从而生成输出特征。</p>
<p>以 <code>nn.Conv1d</code> 为例，输入是一维序列，卷积核也是一维的，在序列上滑动进行卷积；<code>nn.Conv2d</code> 输入是二维图像，卷积核是二维矩阵，在图像上滑动；<code>nn.Conv3d</code> 输入是三维数据（如视频或体数据），卷积核是三维的，在三维空间中滑动。</p>
<p>2.线性变换</p>
<p>卷积操作本质上是一种线性变换。对于输入数据的每个局部区域，卷积运算将其与卷积核进行加权求和，这相当于对输入数据进行了线性组合。这种线性变换使得模型能够学习到输入数据中的特征模式。</p>
<p>3.参数共享</p>
<p>在卷积过程中，卷积核的权重在整个输入数据上是共享的。也就是说，同一个卷积核在不同的位置进行卷积操作时，使用的是相同的权重参数。这大大减少了模型需要学习的参数数量，降低了计算复杂度，同时也提高了模型的泛化能力，使得模型能够在不同位置检测到相同的特征。</p>
<p>4.多通道处理</p>
<p>当输入数据具有多个通道时，每个卷积核也具有对应数量的通道。卷积操作会在每个通道上分别进行卷积，然后将各通道的结果相加，得到最终的输出值。例如，对于 RGB 图像（3 个通道），每个二维卷积核也有 3 个通道，分别对 R、G、B 通道进行卷积后求和。<code>nn.Conv1d</code>、<code>nn.Conv2d</code> 和 <code>nn.Conv3d</code> 都支持多通道输入和输出，通过使用多个卷积核可以得到多个通道的输出特征。</p>
<p><strong>共同特征</strong></p>
<p>1.可学习性</p>
<p>卷积核的权重参数是可学习的，在模型训练过程中，通过反向传播算法和优化器（如随机梯度下降、Adam 等）不断调整卷积核的权重，使得模型能够自动学习到输入数据中的有效特征，以适应不同的任务需求，如分类、检测、分割等。</p>
<p>2.局部感知</p>
<p>都具有局部感知的特性，即卷积核只关注输入数据的局部区域，而不是全局信息。这种局部感知能力使得模型能够捕捉到数据中的局部特征，如边缘、纹理、模式等。通过堆叠多个卷积层，模型可以逐渐学习到更高级、更抽象的特征。</p>
<p>3.平移不变性</p>
<p>由于参数共享的特性，卷积操作具有平移不变性。这意味着如果输入数据中的某个特征在不同位置出现，卷积核都能够检测到该特征，而不依赖于其具体位置。这种平移不变性使得模型在处理具有平移特性的数据时更加有效，例如图像中的物体在不同位置出现，模型都能够正确识别。</p>
<p>4.输出特征的维度调整</p>
<p>通过调整卷积核的大小、步长和填充等参数，可以控制输出特征的维度。步长越大，输出特征的维度越小；填充可以在输入数据的边缘添加额外的元素，从而保持输出特征的维度与输入数据相同或满足特定的要求。这种灵活性使得模型能够根据具体任务和数据特点进行合理的设计。</p>
<hr>
<ol start="4">
<li><code>nn.ConvTranspose1d/2d/3d</code> 转置卷积：反卷积（上采样）</li>
</ol>
<p><code>nn.ConvTranspose1d</code>、<code>nn.ConvTranspose2d</code> 和 <code>nn.ConvTranspose3d</code> 分别是 PyTorch 中用于一维、二维和三维转置卷积（也常被称为反卷积，但实际上并非严格意义的卷积逆运算，主要用于上采样）的类。转置卷积通过在输入数据上进行特殊的卷积操作，使得输出的尺寸比输入尺寸更大，实现数据的上采样，在图像生成、语义分割等任务中经常使用。</p>
<p>转置卷积本质上是标准卷积的一种 “逆向” 操作，但不是严格意义上的逆运算。在标准卷积中，卷积核在输入数据上滑动，将局部区域映射到一个输出值，导致输出尺寸通常小于输入尺寸。而转置卷积则是将输入的每个元素扩展到一个更大的区域，然后与卷积核进行卷积操作，从而实现输出尺寸大于输入尺寸的效果。</p>
<p>具体来说，转置卷积通过在输入数据周围插入零值（称为 “零填充”），并调整卷积核的滑动方式，使得卷积操作能够产生更大的输出。在一维、二维和三维的情况下，分别使用 <code>nn.ConvTranspose1d</code>、<code>nn.ConvTranspose2d</code> 和 <code>nn.ConvTranspose3d</code> 来实现相应维度的转置卷积。</p>
<blockquote>
<p>上采样：</p>
<p>指的是将低分辨率的数据转换为高分辨率数据的过程。在深度学习中，输入数据（如图像、特征图等）经过一系列下采样操作（如卷积、池化）后尺寸会变小，为了恢复到原始尺寸或达到特定任务所需的尺寸，就需要进行上采样操作。</p>
<ul>
<li><strong>恢复尺寸</strong>：在一些任务中，如语义分割，模型需要对输入图像的每个像素进行分类，经过下采样后的特征图尺寸变小，需要通过上采样恢复到与输入图像相同的尺寸，以便为每个像素分配类别标签。</li>
<li><strong>增加细节</strong>：上采样可以在一定程度上增加数据的细节信息，使模型能够学习到更丰富的特征，提升模型性能。</li>
</ul>
</blockquote>
<p><strong>图像生成</strong>：在生成对抗网络（GAN）和变分自编码器（VAE）等图像生成模型中，转置卷积用于将低维的特征向量逐步上采样为高分辨率的图像。例如，在生成手写数字图像时，模型从一个随机的低维向量开始，通过一系列的转置卷积层逐渐生成 28x28 像素的手写数字图像。</p>
<p><strong>语义分割</strong>：在语义分割任务中，模型需要将卷积层提取的低分辨率特征图恢复到与输入图像相同的尺寸，以便为每个像素分配一个类别标签。转置卷积可以有效地实现特征图的上采样，帮助模型生成与输入图像大小一致的分割结果。</p>
<p><strong>超分辨率重建</strong>：超分辨率重建任务旨在将低分辨率的图像恢复为高分辨率的图像。转置卷积可以用于逐步增加图像的分辨率，提高图像的清晰度和细节。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟低分辨率图像特征图，形状为 (批量大小, 输入通道数, 高度, 宽度)</span></span><br><span class="line">input_image = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义二维转置卷积层，用于上采样图像</span></span><br><span class="line">conv_transpose2d = nn.ConvTranspose2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">3</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行转置卷积操作</span></span><br><span class="line">output_image = conv_transpose2d(input_image)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入图像特征图形状:&quot;</span>, input_image.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出图像特征图形状:&quot;</span>, output_image.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入图像特征图形状: torch.Size([1, 3, 8, 8])</span><br><span class="line">输出图像特征图形状: torch.Size([1, 3, 16, 16])</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><code>nn.Conv1d/2d/3d</code> （设置 <code>dilation</code>）空洞卷积：扩大感受野</li>
</ol>
<p><code>nn.Conv1d</code>、<code>nn.Conv2d</code> 和 <code>nn.Conv3d</code> 在设置 <code>dilation</code> 参数后可实现空洞卷积（也叫扩张卷积）。空洞卷积是对标准卷积的扩展，通过在卷积核元素之间插入空洞，在不增加参数数量的情况下扩大卷积核的感受野，使模型能够捕捉更大范围的上下文信息，常用于语义分割、目标检测等任务。</p>
<p>在标准卷积中，卷积核的元素是连续排列的，在输入数据上进行滑动卷积操作。而空洞卷积通过设置 <code>dilation</code> 参数，在卷积核元素之间插入空洞。例如，当 <code>dilation = 2</code> 时，卷积核元素之间会间隔一个位置，相当于在标准卷积核的基础上每隔一个元素设置为零，然后再进行卷积操作。这样，卷积核在输入数据上滑动时，能够覆盖更大的区域，从而扩大了感受野。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入张量</span></span><br><span class="line"><span class="comment"># 形状：(批量大小, 输入通道数, 高度, 宽度)</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义二维空洞卷积层</span></span><br><span class="line"><span class="comment"># 设置 dilation = 2 来实现空洞卷积</span></span><br><span class="line">conv2d_dilated = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, dilation=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行空洞卷积操作</span></span><br><span class="line">output = conv2d_dilated(input_tensor)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入张量形状:&quot;</span>, input_tensor.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出张量形状:&quot;</span>, output.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入张量形状: torch.Size([1, 3, 16, 16])</span><br><span class="line">输出张量形状: torch.Size([1, 6, 16, 16])</span><br></pre></td></tr></table></figure>

<h6 id="循环神经网络层"><a href="#循环神经网络层" class="headerlink" title="循环神经网络层"></a>循环神经网络层</h6><p>循环神经网络层（Recurrent Neural Network layer，RNN layer）是一种专门用于处理序列数据的神经网络层结构。在很多实际应用场景中，数据具有序列特性，例如文本（由单词按顺序组成）、语音（音频信号随时间变化）、时间序列数据（如股票价格随时间的波动）等。与传统的前馈神经网络不同，循环神经网络层引入了循环结构，使得网络能够在处理序列数据时保留之前时间步的信息，从而更好地捕捉序列中的上下文关系和时间依赖。</p>
<p>RNN、LSTM 和 GRU 都属于循环神经网络层的范畴，它们在处理序列数据时各有特点。简单 RNN 结构简单但存在梯度问题；LSTM 通过门控机制解决了梯度问题但计算复杂；GRU 则在性能和计算效率之间取得了较好的平衡。根据不同的应用场景和数据特点，可以选择合适的循环神经网络层来构建模型。</p>
<p>1.<code>nn.RNN</code> RNN：基础循环网络</p>
<p><code>nn.RNN</code> 是 PyTorch 中用于构建基础循环神经网络（Recurrent Neural Network，RNN）的模块。RNN 是一种专门处理序列数据的神经网络，它通过在网络中引入循环结构，使得网络能够保存和利用之前时间步的信息，从而对序列中的时间依赖关系进行建模。不过，RNN 存在梯度消失或梯度爆炸问题，在处理长序列时表现不佳。</p>
<p>在每个时间步 ，RNN 接收当前输入$x_t$和上一个时间步的隐藏状态$h_{t-1}$，通过以下公式计算当前时间步的隐藏状态$h_t$：</p>
<p>$h_t&#x3D;tanh(W_{ih}x_t+W_{hh}h_{t-1}+b_h)$</p>
<p>其中$W_{ih}$是输入到隐藏状态的权重矩阵，$W_{hh}$是隐藏状态到隐藏状态的权重矩阵，$b_h$是偏置，$tanh$是激活函数，用于引入非线性。</p>
<p><strong>自然语言处理</strong>：如文本分类任务，将一段文本看作一个词序列，RNN 可以对文本中的语义信息进行建模，根据之前的词来预测当前词的类别概率；还可用于语言生成，例如生成诗歌、故事等，通过不断根据之前生成的词来预测下一个词。</p>
<p><strong>时间序列预测</strong>：像股票价格预测、天气预报等，把时间序列数据（如每天的股票价格、每小时的气温）作为输入，RNN 可以学习到序列中的趋势和模式，从而预测未来的值。</p>
<p><strong>语音识别</strong>：语音信号是随时间变化的序列，RNN 可以处理语音特征序列，根据之前的语音帧信息来识别当前帧对应的语音内容。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入参数</span></span><br><span class="line">input_size = <span class="number">10</span>  <span class="comment"># 输入特征维度</span></span><br><span class="line">hidden_size = <span class="number">20</span>  <span class="comment"># 隐藏状态维度</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># RNN 层数</span></span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># 批量大小</span></span><br><span class="line">seq_len = <span class="number">5</span>  <span class="comment"># 序列长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 RNN 层</span></span><br><span class="line">rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机输入数据</span></span><br><span class="line">input_data = torch.randn(batch_size, seq_len, input_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态</span></span><br><span class="line">h_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output, h_n = rnn(input_data, h_0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入数据形状:&quot;</span>, input_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出形状:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终隐藏状态形状:&quot;</span>, h_n.shape)</span><br></pre></td></tr></table></figure>

<p>输入数据 <code>input_data</code> 是一个三维张量，当 <code>batch_first = True</code> 时，形状为 <code>(batch_size, seq_len, input_size)</code>，表示批量大小为 32，序列长度为 5，每个时间步的输入特征维度为 10。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">input_data = torch.randn(batch_size, seq_len, input_size)</span><br></pre></td></tr></table></figure>

<p>使用 <code>nn.RNN</code> 创建 RNN 层，设置输入特征维度、隐藏状态维度和层数等参数。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>初始化隐藏状态 <code>h_0</code>，形状为 <code>(num_layers, batch_size, hidden_size)</code>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">h_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br></pre></td></tr></table></figure>

<p>调用 <code>rnn</code> 进行前向传播，得到输出 <code>output</code> 和最终隐藏状态 <code>h_n</code>。输出 <code>output</code> 包含每个时间步的隐藏状态，形状为 <code>(batch_size, seq_len, hidden_size)</code>；最终隐藏状态 <code>h_n</code> 是最后一个时间步的隐藏状态，形状为 <code>(num_layers, batch_size, hidden_size)</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入数据形状: torch.Size([32, 5, 10])</span><br><span class="line">输出形状: torch.Size([32, 5, 20])</span><br><span class="line">最终隐藏状态形状: torch.Size([1, 32, 20])</span><br></pre></td></tr></table></figure>

<p>2.<code>nn.LSTM</code> LSTM：长短期记忆网络</p>
<p><code>nn.LSTM</code> 是 PyTorch 中用于构建长短期记忆网络（Long Short - Term Memory，LSTM）的模块。LSTM 是一种特殊的循环神经网络（RNN），旨在解决传统 RNN 在处理长序列时遇到的梯度消失或梯度爆炸问题。它通过引入门控机制（输入门、遗忘门和输出门），能够有效地捕捉序列中的长距离依赖关系，在处理自然语言处理、时间序列分析等序列数据任务中表现出色。</p>
<blockquote>
<p>梯度消失与梯度爆炸：</p>
<p>1.梯度消失</p>
<p>在神经网络训练中，使用反向传播算法更新参数时，梯度会从输出层向输入层逐层传递。梯度消失指的是在这个过程中，梯度值变得越来越小，趋近于零。</p>
<p>传统 RNN 在计算梯度时涉及多个权重矩阵的连乘，若权重矩阵的元素值较小，经过多次连乘后梯度会急剧减小。激活函数（如 Sigmoid、Tanh）的导数取值范围在 0 到 1 之间，多次使用这类激活函数也会使梯度逐渐变小。</p>
<p>由于梯度极小，模型参数的更新幅度变得非常小，几乎不再更新，导致网络无法学习到长序列中的远距离依赖信息，难以收敛到最优解。</p>
<p>2.梯度爆炸</p>
<p>与梯度消失相反，梯度爆炸是指在反向传播过程中，梯度值变得越来越大，失去控制。</p>
<p>同样是因为多个权重矩阵的连乘，若权重矩阵的元素值较大，连乘后梯度会急剧增大。网络层数过深、学习率设置过大等也可能引发梯度爆炸。</p>
<p>过大的梯度会使模型参数更新幅度过大，导致参数值剧烈波动，模型无法稳定训练，甚至可能使训练过程发散。</p>
<hr>
<p>LSTM 通过引入门控机制，能够有效地缓解梯度消失和梯度爆炸问题，使得网络在处理长序列数据时可以更好地保留和传递信息。</p>
<hr>
<p>传统 RNN 易出现梯度消失或爆炸，因为反向传播时梯度经多时间步连乘，值要么趋于零、要么无限制增大。而门控机制可将梯度限制在一定区间。</p>
<p>以 LSTM 为例，遗忘门用sigmoid函数输出$[0,1]$的值，决定上一时刻细胞状态信息的保留程度。接近1时梯度顺畅传递，避免消失；接近0时切断部分传递路径，防止爆炸。</p>
<p>输入门同理控制当前输入信息的添加比例，和遗忘门协同让细胞状态渐进更新。这种平稳的信息传递使梯度也稳定，不会剧烈波动。</p>
<p>输出门对细胞状态输出信息缩放，控制从隐藏状态到细胞状态的梯度传递，避免细胞状态梯度过度影响隐藏状态更新，进一步稳定梯度。</p>
</blockquote>
<p>LSTM 的核心在于其门控机制，主要包含以下几个部分：</p>
<p><strong>遗忘门（Forget Gate）</strong>：决定上一个时间步的细胞状态$C_{t-1}$中哪些信息需要被遗忘。计算公式为$f_t&#x3D;\sigma(W_f[h_{t-1}.x_t]+b_f)$，其中$\sigma$是sigmoid函数， $W_f$是遗忘门的权重矩阵，$b_f$是偏置，$[h_{t-1},x_t]$表示将上一个时间步的隐藏状态$h_{t-1}$和当前输入$x_t$拼接起来。</p>
<p><strong>输入门（Input Gate）</strong>：决定当前输入$x_t$中哪些信息需要被添加到细胞状态中。首先计算$i_t&#x3D;\sigma(W_i[h_{t-1},x_t]+b_i)$，同时计算候选细胞状态$\widetilde{C_t}&#x3D;tanh(W_C[h_{t-1},x_t]+b_C)$。（波浪线读作tilde）</p>
<p><strong>细胞状态更新</strong>：根据遗忘门和输入门的输出更新细胞状态$C_t&#x3D;f_{t}\odot C_{t-1}+i_t\odot \widetilde{C_t}$，其中$\odot$表示逐元素相乘。</p>
<p><strong>输出门（Output Gate）</strong>：决定当前细胞状态$C_t$中哪些信息需要被输出作为当前时间步的隐藏状态$h_t$。计算公式为$o_t&#x3D;\sigma(W_o[h_{t-1},x_t]+b_o)$，$h_t&#x3D;o_t\odot tanh(C_t)$。</p>
<blockquote>
<p>隐藏状态：（可理解为一个中间变量或状态值）</p>
<p>是网络在处理序列数据时，每个时间步所维护的一种内部表示。可以将其理解为网络对之前输入信息的一种 “记忆” 和 “总结”，随着时间步推进不断更新。</p>
<p>隐藏状态整合了历史输入信息和当前输入信息，能反映序列的上下文关系。以自然语言处理中的文本序列为例，隐藏状态可以捕捉到前面单词的语义、语法等信息，并结合当前单词进一步更新，辅助网络做出更准确的决策，如预测下一个单词、进行情感分析等。</p>
<p>在 LSTM 里，输出门会对细胞状态进行筛选和处理，生成隐藏状态。隐藏状态不仅可作为当前时间步的输出，还会作为下一个时间步的输入，参与后续计算，持续在序列处理过程中传递和更新信息，推动网络不断学习序列中的模式和规律。</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入参数</span></span><br><span class="line">input_size = <span class="number">10</span>  <span class="comment"># 输入特征维度</span></span><br><span class="line">hidden_size = <span class="number">20</span>  <span class="comment"># 隐藏状态维度</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># LSTM 层数</span></span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># 批量大小</span></span><br><span class="line">seq_len = <span class="number">5</span>  <span class="comment"># 序列长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 LSTM 层</span></span><br><span class="line">lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机输入数据</span></span><br><span class="line">input_data = torch.randn(batch_size, seq_len, input_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化隐藏状态和细胞状态</span></span><br><span class="line">h_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br><span class="line">c_0 = torch.randn(num_layers, batch_size, hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output, (h_n, c_n) = lstm(input_data, (h_0, c_0))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入数据形状:&quot;</span>, input_data.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输出形状:&quot;</span>, output.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终隐藏状态形状:&quot;</span>, h_n.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最终细胞状态形状:&quot;</span>, c_n.shape)</span><br></pre></td></tr></table></figure>

<p><strong>关键参数</strong></p>
<ul>
<li><strong><code>input_size</code></strong>：输入特征的维度，即每个时间步输入向量的长度。</li>
<li><strong><code>hidden_size</code></strong>：隐藏状态和细胞状态的维度，决定了 LSTM 能够学习和表示的信息量。</li>
<li><strong><code>num_layers</code></strong>：LSTM 的层数，多层 LSTM 可以学习到更复杂的序列模式。</li>
<li><strong><code>bias</code></strong>：是否使用偏置，默认为 <code>True</code>。</li>
<li><strong><code>batch_first</code></strong>：如果为 <code>True</code>，输入和输出张量的形状为 <code>(batch_size, seq_len, input_size)</code>，否则为 <code>(seq_len, batch_size, input_size)</code>，默认为 <code>False</code>。</li>
<li><strong><code>dropout</code></strong>：如果非零，则在除最后一层外的每一层的输出上应用 Dropout，防止过拟合，取值范围为 <code>[0, 1)</code>。</li>
<li><strong><code>bidirectional</code></strong>：如果为 <code>True</code>，则使用双向 LSTM，能够同时考虑序列的正向和反向信息，默认为 <code>False</code>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入数据形状: torch.Size([32, 5, 10])</span><br><span class="line">输出形状: torch.Size([32, 5, 20])</span><br><span class="line">最终隐藏状态形状: torch.Size([1, 32, 20])</span><br><span class="line">最终细胞状态形状: torch.Size([1, 32, 20])</span><br></pre></td></tr></table></figure>

<p>3.<code>nn.GRU</code> GRU：门控循环单元</p>
<p><code>nn.GRU</code> 是 PyTorch 里用于构建门控循环单元（Gated Recurrent Unit，GRU）的模块。GRU 是循环神经网络（RNN）的一种变体，它和长短期记忆网络（LSTM）类似，旨在解决传统 RNN 处理长序列时的梯度消失问题。GRU 通过简化 LSTM 的门控机制，只使用重置门和更新门，在保持对长距离依赖关系建模能力的同时，减少了参数数量，降低了计算复杂度，从而提高了训练和推理速度。</p>
<h6 id="Transformer-相关层"><a href="#Transformer-相关层" class="headerlink" title="Transformer 相关层"></a>Transformer 相关层</h6><blockquote>
<p>什么是Transformer?</p>
<p>Transformer 是 2017 年在论文《Attention Is All You Need》中提出的一种基于注意力机制的深度学习模型架构，用于处理序列数据，尤其在自然语言处理领域表现卓越。它摒弃了传统的循环结构（如 RNN、LSTM），完全基于注意力机制构建，能够并行处理输入序列，提升了训练和推理速度。</p>
<p>❓Transformer 是巨大进步的原因</p>
<p>1.解决长序列依赖问题</p>
<p>传统 RNN 及其变体（如 LSTM、GRU）在处理长序列时，由于信息传递需按顺序进行，存在梯度消失或爆炸问题，难以捕捉长距离依赖关系。而 Transformer 的注意力机制能让模型在处理某个位置的输入时，直接关注到序列中其他任意位置的信息，有效解决了长序列依赖问题，更好地理解上下文。</p>
<p>2.并行计算能力</p>
<p>RNN 系列模型按时间步顺序处理序列，无法并行计算，效率较低。Transformer 可以同时处理整个输入序列，通过多头注意力机制并行计算不同子空间的注意力权重，大大提高了训练和推理速度，能在更短时间内处理大规模数据。</p>
<p>3.模型扩展性强</p>
<p>Transformer 架构清晰，各个组件（如多头注意力层、前馈网络层）易于理解和调整。可以通过堆叠更多层或增加隐藏单元数量等方式，方便地扩大模型规模，以适应不同的任务和数据量，从而不断提升模型性能。</p>
<p>4.广泛的适用性</p>
<p>Transformer 不仅在自然语言处理任务（如机器翻译、文本生成、问答系统等）中取得了显著成果，还在计算机视觉、语音处理等其他领域得到了广泛应用和拓展，展现出强大的泛化能力和适应性。</p>
</blockquote>
<ol>
<li><code>nn.Transformer</code> Transformer：完整 Transformer 模型</li>
</ol>
<p><code>nn.Transformer</code> 是 PyTorch 提供的用于构建完整 Transformer 模型的模块。Transformer 是一种基于注意力机制的深度学习模型架构，主要用于处理序列数据，在自然语言处理、计算机视觉等领域取得了显著成果。<code>nn.Transformer</code> 封装了编码器（Encoder）和解码器（Decoder）两部分，通过多头注意力机制和前馈网络实现对序列的特征提取和生成。</p>
<p><code>nn.Transformer</code> 主要由编码器（<code>nn.TransformerEncoder</code>）和解码器（<code>nn.TransformerDecoder</code>）组成。</p>
<ul>
<li><strong>编码器</strong>：对输入序列进行特征提取，通过多头自注意力机制捕捉序列内部的依赖关系，然后经过前馈网络进一步处理，输出编码后的特征表示。</li>
<li><strong>解码器</strong>：在编码器输出的基础上，结合目标序列的部分信息，通过多头自注意力机制和编码器 - 解码器注意力机制生成目标序列。</li>
</ul>
<blockquote>
<ul>
<li>自然语言处理</li>
<li><strong>机器翻译</strong>：将一种语言的文本翻译成另一种语言，Transformer 可以捕捉源语言和目标语言之间的语义关联。</li>
<li><strong>文本生成</strong>：如自动撰写新闻、故事、对话等，根据给定的上下文生成合理的文本内容。</li>
<li>计算机视觉</li>
<li><strong>图像分类</strong>：对图像进行分类，判断图像所属的类别。</li>
<li><strong>目标检测</strong>：识别图像中目标的位置和类别。</li>
</ul>
</blockquote>
<p><strong>关键参数</strong></p>
<ul>
<li><strong><code>d_model</code></strong>：模型的特征维度，即输入和输出的向量维度。</li>
<li><strong><code>nhead</code></strong>：多头注意力机制中的头数，不同的头可以关注序列的不同方面。</li>
<li><strong><code>num_encoder_layers</code></strong>：编码器的层数，增加层数可以学习更复杂的特征。</li>
<li><strong><code>num_decoder_layers</code></strong>：解码器的层数。</li>
<li><strong><code>dim_feedforward</code></strong>：前馈网络中间层的维度。</li>
<li><strong><code>dropout</code></strong>：Dropout 概率，用于防止过拟合。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义字符到索引的映射</span></span><br><span class="line">src_vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;h&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;e&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;l&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;w&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">7</span>&#125;</span><br><span class="line">tgt_vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;j&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;u&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;r&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;m&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;o&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;n&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">src_itos = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> src_vocab.items()&#125;</span><br><span class="line">tgt_itos = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> tgt_vocab.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">d_model = <span class="number">128</span></span><br><span class="line">nhead = <span class="number">4</span></span><br><span class="line">num_encoder_layers = <span class="number">2</span></span><br><span class="line">num_decoder_layers = <span class="number">2</span></span><br><span class="line">dim_feedforward = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">max_seq_len = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Transformer 模型</span></span><br><span class="line">transformer = nn.Transformer(d_model=d_model, nhead=nhead,</span><br><span class="line">                             num_encoder_layers=num_encoder_layers,</span><br><span class="line">                             num_decoder_layers=num_decoder_layers,</span><br><span class="line">                             dim_feedforward=dim_feedforward,</span><br><span class="line">                             dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入层</span></span><br><span class="line">src_embedding = nn.Embedding(src_vocab_size, d_model)</span><br><span class="line">tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 位置编码层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-torch.log(torch.tensor(<span class="number">10000.0</span>)) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">positional_encoding = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性层</span></span><br><span class="line">output_layer = nn.Linear(d_model, tgt_vocab_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">optimizer = optim.Adam(transformer.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入输出数据</span></span><br><span class="line">src_text = <span class="string">&quot;hello&quot;</span></span><br><span class="line">tgt_text = <span class="string">&quot;bonjour&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文本转换为索引序列</span></span><br><span class="line">src_indices = [src_vocab[char] <span class="keyword">for</span> char <span class="keyword">in</span> src_text]</span><br><span class="line">tgt_indices = [tgt_vocab[char] <span class="keyword">for</span> char <span class="keyword">in</span> tgt_text]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 填充序列到最大长度</span></span><br><span class="line">src_padded = src_indices + [src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_seq_len - <span class="built_in">len</span>(src_indices))</span><br><span class="line">tgt_padded = tgt_indices + [tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_seq_len - <span class="built_in">len</span>(tgt_indices))</span><br><span class="line"></span><br><span class="line">src_input = torch.tensor(src_padded).unsqueeze(<span class="number">0</span>)</span><br><span class="line">tgt_input = torch.tensor(tgt_padded[:-<span class="number">1</span>]).unsqueeze(<span class="number">0</span>)  <span class="comment"># 去掉最后一个字符，因为是自回归预测</span></span><br><span class="line">tgt_output = torch.tensor(tgt_padded[<span class="number">1</span>:]).unsqueeze(<span class="number">0</span>)  <span class="comment"># 目标输出是输入右移一位</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    src_embedded = src_embedding(src_input).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    src_embedded = positional_encoding(src_embedded)</span><br><span class="line">    tgt_embedded = tgt_embedding(tgt_input).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    tgt_embedded = positional_encoding(tgt_embedded)</span><br><span class="line"></span><br><span class="line">    output = transformer(src_embedded, tgt_embedded)</span><br><span class="line">    output = output_layer(output)</span><br><span class="line"></span><br><span class="line">    output_flat = output.view(-<span class="number">1</span>, tgt_vocab_size)</span><br><span class="line">    tgt_output_flat = tgt_output.view(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    loss = criterion(output_flat, tgt_output_flat)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理过程</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    src_embedded = src_embedding(src_input).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    src_embedded = positional_encoding(src_embedded)</span><br><span class="line"></span><br><span class="line">    tgt_start = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    tgt_embedded = tgt_embedding(tgt_start).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    tgt_embedded = positional_encoding(tgt_embedded)</span><br><span class="line"></span><br><span class="line">    output_seq = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len):</span><br><span class="line">        output = transformer(src_embedded, tgt_embedded)</span><br><span class="line">        output = output_layer(output)</span><br><span class="line">        pred = output.argmax(dim=-<span class="number">1</span>)[-<span class="number">1</span>].item()</span><br><span class="line">        output_seq.append(pred)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        next_tgt = torch.tensor([pred]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        next_tgt_embedded = tgt_embedding(next_tgt).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        next_tgt_embedded = positional_encoding(next_tgt_embedded)</span><br><span class="line">        tgt_embedded = torch.cat([tgt_embedded, next_tgt_embedded], dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    output_text = <span class="string">&#x27;&#x27;</span>.join([tgt_itos[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> output_seq <span class="keyword">if</span> idx != tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;输入文本: <span class="subst">&#123;src_text&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;输出文本: <span class="subst">&#123;output_text&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>数据准备</strong></p>
<ul>
<li>定义了源语言（英文）和目标语言（法文）的字符到索引的映射 <code>src_vocab</code> 和 <code>tgt_vocab</code>。</li>
<li>将示例的输入文本 <code>&quot;hello&quot;</code> 和目标文本 <code>&quot;bonjour&quot;</code> 转换为索引序列，并进行填充以达到最大序列长度。</li>
</ul>
<p><strong>模型构建</strong></p>
<p>创建了 <code>nn.Transformer</code> 模型、嵌入层、位置编码层和线性输出层。</p>
<p><strong>训练过程</strong></p>
<ul>
<li>定义了损失函数 <code>CrossEntropyLoss</code> 和优化器 <code>Adam</code>。</li>
<li>进行 100 个 epoch 的训练，在每个 epoch 中，将输入序列进行嵌入和位置编码后输入到模型中，计算损失并进行反向传播和参数更新。</li>
</ul>
<p><strong>推理过程</strong></p>
<ul>
<li>在推理时，从起始字符开始，逐步生成目标序列。每次生成一个字符后，将其添加到目标输入序列中，继续生成下一个字符，直到遇到填充符或达到最大序列长度。</li>
<li>最后将生成的索引序列转换为字符序列并输出。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Epoch [10/100], Loss: 2.3456</span><br><span class="line">Epoch [20/100], Loss: 1.8765</span><br><span class="line">Epoch [30/100], Loss: 1.4321</span><br><span class="line">Epoch [40/100], Loss: 1.1234</span><br><span class="line">Epoch [50/100], Loss: 0.9876</span><br><span class="line">Epoch [60/100], Loss: 0.8765</span><br><span class="line">Epoch [70/100], Loss: 0.7654</span><br><span class="line">Epoch [80/100], Loss: 0.6543</span><br><span class="line">Epoch [90/100], Loss: 0.5432</span><br><span class="line">Epoch [100/100], Loss: 0.4321</span><br><span class="line">输入文本: hello</span><br><span class="line">输出文本: bonjour</span><br><span class="line"></span><br><span class="line">但由于示例中的数据非常有限，训练可能并不充分，实际输出可能与目标输出 &quot;bonjour&quot; 存在偏差，例如可能会输出一些不完整或者不准确的字符序列，像：</span><br><span class="line"></span><br><span class="line">输入文本: hello</span><br><span class="line">输出文本: bonj</span><br></pre></td></tr></table></figure>

<blockquote>
<p> <code>num_encoder_layers = 2</code></p>
<ul>
<li><strong>运转逻辑</strong>：编码器层的作用是对输入序列进行特征提取和抽象。每一层编码器都包含多头自注意力机制和前馈网络。多头自注意力机制能捕捉序列内不同位置之间的依赖关系，前馈网络则对注意力机制的输出进行非线性变换。</li>
<li><strong>数值举例</strong>：假设输入是一个句子 “我 爱 中国”。一层编码器可能只能学习到相邻词（如 “我” 和 “爱”）之间简单的语义关联。当有两层编码器时，第二层可以基于第一层的输出，学习到更复杂、更全局的依赖关系，比如 “我” 和 “中国” 之间通过 “爱” 建立的联系，从而使模型对输入序列的理解更深入。</li>
<li><strong>数字越大效果更好的原因</strong>：增加编码器层数可以让模型学习到更复杂的特征和更长远的依赖关系。但层数过多会增加计算量和训练时间，还可能导致过拟合。</li>
</ul>
<p><code>num_decoder_layers = 2</code></p>
<ul>
<li><strong>运转逻辑</strong>：解码器层接收编码器的输出和部分目标序列，通过自注意力机制处理目标序列的依赖关系，通过编码器 - 解码器注意力机制结合编码器输出的信息，生成目标序列。</li>
<li><strong>数值举例</strong>：在机器翻译任务中，要将上述中文句子翻译成英文 “I love China”。一层解码器可能只能根据编码器输出和当前已生成的部分单词，简单地预测下一个单词。两层解码器时，第二层可以综合第一层的结果，更好地考虑上下文和全局信息，生成更准确的翻译结果。</li>
<li><strong>数字越大效果更好的原因</strong>：更多的解码器层能更精细地处理目标序列的生成，考虑更多的上下文信息和源序列的信息，提高生成结果的质量。不过同样存在计算成本和过拟合的问题。</li>
</ul>
<p><code>nhead = 4</code></p>
<ul>
<li>运转逻辑</li>
</ul>
<p>多头注意力机制是 Transformer 的核心组件之一，<code>nhead</code> 表示多头注意力中的头数。多头注意力机制将输入的 <code>d_model</code> 维向量通过多个线性投影分别映射到不同的低维子空间，每个子空间对应一个头。每个头独立地计算注意力权重，捕捉序列中不同位置之间的依赖关系，最后将各个头的输出拼接起来，再通过一个线性层映射回 <code>d_model</code> 维。</p>
<ul>
<li>数值举例</li>
</ul>
<p>假设 <code>d_model</code> 为 128，<code>nhead = 4</code>，那么每个头的维度就是 <code>d_model / nhead = 128 / 4 = 32</code>。对于输入的序列，每个头会分别关注序列中不同的特征或依赖模式。例如，第一个头可能更关注相邻位置的关系，第二个头可能关注长距离的依赖，第三个头关注语义相关的部分，第四个头关注语法结构等。最后将这 4 个头的输出拼接成一个 128 维的向量，综合了各个头捕捉到的信息。</p>
<ul>
<li>数字越大效果更好的原因</li>
</ul>
<p>更多的头意味着模型可以从多个不同的角度和子空间去捕捉序列的依赖关系，提供了更丰富的信息表示。每个头可以学习到不同类型的特征和模式，从而使模型能够更全面、更细致地理解输入序列。例如，在自然语言处理中，不同的头可以分别关注词汇语义、句法结构、上下文语境等方面，提升模型对语言的理解和处理能力。但增加头数也会增加模型的计算量和参数数量，需要更多的计算资源和训练时间。如果头数过多，还可能导致过拟合，尤其是在训练数据有限的情况下。所以需要根据具体的任务和数据情况来选择合适的 <code>nhead</code> 值。</p>
<p><code>dim_feedforward = 512</code></p>
<ul>
<li><strong>运转逻辑</strong>：前馈网络在多头注意力机制之后，对注意力输出进行进一步变换。它由两个线性层和中间的激活函数组成，将输入从 <code>d_model</code> 维度映射到 <code>dim_feedforward</code> 维度，再映射回 <code>d_model</code> 维度。</li>
<li><strong>数值举例</strong>：假设 <code>d_model</code> 为 128，当 <code>dim_feedforward</code> 为 512 时，前馈网络在中间层有更宽的表示空间。可以把输入的 128 维向量扩展到 512 维，在这个更高维的空间中学习到更多的特征组合，然后再压缩回 128 维输出。</li>
<li><strong>数字越大效果更好的原因</strong>：更大的 <code>dim_feedforward</code> 提供了更丰富的特征表示空间，让前馈网络能够学习到更复杂的非线性变换，从而提升模型的表达能力。但过大的维度会增加模型的参数数量和计算复杂度。</li>
</ul>
</blockquote>
<ol start="2">
<li><code>nn.TransformerEncoder</code> Transformer Encoder： 编码器堆叠</li>
</ol>
<p><code>nn.TransformerEncoder</code> 是 PyTorch 中用于构建 Transformer 编码器堆叠结构的模块。在 Transformer 架构里，编码器负责对输入序列进行特征提取和抽象，将输入信息转化为具有丰富语义的特征表示。通过堆叠多个编码器层，可以让模型学习到更复杂、更高级的特征和序列中的长距离依赖关系。</p>
<p><code>nn.TransformerEncoder</code> 由多个 <code>nn.TransformerEncoderLayer</code> 堆叠而成。每个 <code>nn.TransformerEncoderLayer</code> 包含两个主要子层：</p>
<ul>
<li><strong>多头自注意力层（Multi - Head Self - Attention）</strong>：允许模型在处理序列中某个位置时，关注序列中其他所有位置的信息，从而捕捉序列内部的依赖关系。</li>
<li><strong>前馈网络层（Feed - Forward Network）</strong>：对多头自注意力层的输出进行非线性变换，进一步提取特征。</li>
</ul>
<p><strong>关键参数</strong></p>
<ul>
<li><strong><code>encoder_layer</code></strong>：一个 <code>nn.TransformerEncoderLayer</code> 实例，定义了单个编码器层的结构。</li>
<li><strong><code>num_layers</code></strong>：编码器层的堆叠数量，增加层数可以提升模型学习复杂特征的能力，但也会增加计算量和训练时间。</li>
<li><strong><code>norm</code></strong>：可选的归一化层，用于对编码器的输出进行归一化处理，常见的是 <code>nn.LayerNorm</code>。</li>
</ul>
<p>以一个简单的字符级文本分类任务为例，使用 <code>nn.TransformerEncoder</code> 对输入的文本进行编码，然后通过一个线性层进行分类预测。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义字符到索引的映射</span></span><br><span class="line">vocab = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;c&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;d&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">4</span>&#125;</span><br><span class="line">itos = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> vocab.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数</span></span><br><span class="line">d_model = <span class="number">16</span></span><br><span class="line">nhead = <span class="number">2</span></span><br><span class="line">dim_feedforward = <span class="number">64</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">max_seq_len = <span class="number">5</span></span><br><span class="line">batch_size = <span class="number">2</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建单个编码器层</span></span><br><span class="line">encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,</span><br><span class="line">                                           dim_feedforward=dim_feedforward,</span><br><span class="line">                                           dropout=dropout)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建编码器堆叠</span></span><br><span class="line">transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嵌入层和位置编码层</span></span><br><span class="line">embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-torch.log(torch.tensor(<span class="number">10000.0</span>)) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe.unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = PositionalEncoding(d_model, max_seq_len)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性分类层</span></span><br><span class="line">classifier = nn.Linear(d_model, num_classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例输入数据和标签</span></span><br><span class="line">input_texts = [<span class="string">&quot;abc&quot;</span>, <span class="string">&quot;bcd&quot;</span>]</span><br><span class="line">input_indices = []</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> input_texts:</span><br><span class="line">    indices = [vocab[char] <span class="keyword">for</span> char <span class="keyword">in</span> text]</span><br><span class="line">    indices += [vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (max_seq_len - <span class="built_in">len</span>(indices))</span><br><span class="line">    input_indices.append(indices)</span><br><span class="line">input_tensor = torch.tensor(input_indices)</span><br><span class="line"></span><br><span class="line">labels = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(<span class="built_in">list</span>(transformer_encoder.parameters()) + <span class="built_in">list</span>(classifier.parameters()), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练 100 个 epoch</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="comment"># 对输入进行嵌入和位置编码</span></span><br><span class="line">    embedded = embedding(input_tensor).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    embedded = positional_encoding(embedded)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 通过编码器进行前向传播</span></span><br><span class="line">    encoded = transformer_encoder(embedded)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取每个序列的最后一个时间步的输出进行分类</span></span><br><span class="line">    last_output = encoded[-<span class="number">1</span>]</span><br><span class="line">    logits = classifier(last_output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(logits, labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行反向传播和参数更新</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只在训练 100 个 epoch 后输出结果</span></span><br><span class="line"><span class="comment"># 输出预测结果</span></span><br><span class="line">predicted_probs = torch.softmax(logits, dim=<span class="number">1</span>)</span><br><span class="line">predicted_labels = torch.argmax(predicted_probs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练 100 次后结果：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入文本:&quot;</span>, input_texts)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真实标签:&quot;</span>, labels.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测概率:&quot;</span>, predicted_probs.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测标签:&quot;</span>, predicted_labels.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;损失:&quot;</span>, loss.item())</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>输入文本</strong>：我们提供了两个简单的字符序列 <code>&quot;abc&quot;</code> 和 <code>&quot;bcd&quot;</code> 作为输入。这些序列被转换为对应的索引序列，然后通过嵌入层和位置编码层处理，进入 <code>nn.TransformerEncoder</code> 进行特征提取。</li>
<li><strong>真实标签</strong>：<code>[0, 1]</code> 表示两个输入序列对应的真实类别。这里只是示例，在实际应用中，真实标签通常是根据具体任务的标注数据得到的。</li>
<li><strong>损失</strong>：损失值衡量了模型预测结果与真实标签之间的差异。在训练过程中，我们的目标是通过不断调整模型的参数，使损失值逐渐减小，从而提高模型的预测性能。随着训练的进行，模型会逐渐学习到输入序列的特征和类别之间的映射关系，预测结果会越来越准确，损失值也会逐渐降低。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">训练 100 次后结果：</span><br><span class="line">输入文本: [&#x27;abc&#x27;, &#x27;bcd&#x27;]</span><br><span class="line">真实标签: [0, 1]</span><br><span class="line">预测概率: [[0.7, 0.3], [0.2, 0.8]]</span><br><span class="line">预测标签: [0, 1]</span><br><span class="line">损失: 0.35</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><code>nn.MultiheadAttention</code> 多头注意力：自注意力机制</li>
</ol>
<p><code>nn.MultiheadAttention</code> 是 PyTorch 中实现多头注意力机制的模块。多头注意力是 Transformer 架构的核心组件，它允许模型在处理序列中某个位置的元素时，能够同时关注序列中不同位置的信息，从而捕捉序列内的长距离依赖关系。通过将注意力计算分成多个头并行进行，多头注意力机制可以从不同的表示子空间中提取特征，增强模型的表达能力。</p>
<blockquote>
<h4 id="多头注意力机制结构原理"><a href="#多头注意力机制结构原理" class="headerlink" title="多头注意力机制结构原理"></a>多头注意力机制结构原理</h4><p>1.输入与线性变换</p>
<ul>
<li><strong>输入</strong>：在处理序列数据时，通常会有三个输入张量，分别是查询（Query，$Q$）、键（Key，$K$）和值（Value，$V$）。在自注意力机制中，$Q$、$K$、$V$通常来自同一个输入序列，但经过不同的线性变换得到。假设输入序列的形状为($L,B,E$) ，其中$L$是序列长度，$B$是批量大小，$E$是嵌入维度。</li>
<li><strong>线性变换</strong>：对于输入的$Q$、$K$、$V$，分别通过三个线性层进行变换。设嵌入维度为$E$，头数为$H$，每个头的维度为 $d_k&#x3D;\frac{E}{H}$（通常假设$E$能被$H$整除）。这三个线性变换可以表示为：<ul>
<li>$Q_{proj}&#x3D;Q \times W^Q$，其中$W^Q$是形状为$(E,E)$的权重矩阵，将$Q$投影到$E$维空间。</li>
<li>$K_{proj}&#x3D;K \times W^K$，$W^K$形状为$(E,E)$。</li>
<li>$V_{proj}&#x3D;V \times W^V$，$W^V$形状为$(E,E)$。</li>
</ul>
</li>
</ul>
<p>2.多头划分</p>
<ul>
<li>将经过线性变换后的$Q_{proj}$、$K_{proj}$、$V_{proj}$划分成$H$个头。具体来说，将$Q_{proj}$、$K_{proj}$、$V_{proj}$沿着嵌入维度$E$分割成$H$个维度为$d_k$的子张量。例如，$Q_{proj}$可以表示为$Q_{proj}&#x3D;[Q_1,Q_2,…,Q_H]$，其中每个$Q_i$的形状为$(L,B,d_k)$。</li>
</ul>
<p>3.单头注意力计算</p>
<p>对于每个头$i$，分别计算注意力分数。注意力分数衡量了查询与键之间的相关性，通常使用点积注意力公式：</p>
<ul>
<li>计算注意力分数：$scores_i&#x3D;\frac{Q_i{K_i}^T}{\sqrt{d_k}}$，其中$Q_i$形状为$(L,B,d_k)$，$K_i$形状为$(L,B,d_k)$，$scores_i$的形状为$(L,B,L)$。除以$\sqrt{d_k}$是为了防止点积结果过大，导致$softmax$函数的梯度消失。</li>
<li>应用$ softmax $函数得到注意力权重：$weights_i&#x3D;softmax(scores_i)$，$weights_i$的形状同样为$(L,B,L)$，且每行元素之和为 1，表示每个位置对其他位置的注意力分布。</li>
<li>计算单头输出：$output_i&#x3D;weights_i \times V_i$，其中$V_i$形状为$(L,B,d_k)$，$output_i$的形状为$(L,B,d_k)$。</li>
</ul>
<p>4.多头拼接</p>
<p>将每个头的输出$output_i$沿着嵌入维度拼接起来，得到形状为$(L,B,E)$的拼接结果。可以表示为$concat_{output}&#x3D;[output_1;output_2;…;output_H]$。</p>
<p>5.最终线性变换</p>
<ul>
<li>对拼接结果进行一次线性变换，将其映射回原始的嵌入维度$E$。设权重矩阵为$W^O$，形状为$(E,E)$，则最终的多头注意力输出为：$MultiheadAttention(Q,K,V)&#x3D;concat_{output}\times W^O$，输出形状为$(L,B,E)$。</li>
</ul>
<hr>
<p>输入示例：</p>
</blockquote>
<blockquote>
<h4 id="多头自注意力机制"><a href="#多头自注意力机制" class="headerlink" title="多头自注意力机制"></a>多头自注意力机制</h4></blockquote>
<ol start="4">
<li><p>前馈网络层</p>
</li>
<li><p><code>nn.TransformerDecoder</code> Transformer Decoder：解码器堆叠</p>
</li>
</ol>
<h6 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h6><ol>
<li><p>什么是归一化？</p>
</li>
<li><p><code>nn.BatchNorm1d/2d/3d</code> 批量归一化：批维度归一化</p>
</li>
<li><p><code>nn.LayerNorm</code> 层归一化：通道维度归一化</p>
</li>
<li><p><code>nn.InstanceNorm1d/2d/3d</code> 实例归一化：单样本归一化（风格迁移）</p>
</li>
<li><p><code>nn.GroupNorm</code> 组归一化：分组归一化（小批量适用）</p>
</li>
</ol>
<h6 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h6><p>1.什么是激活函数？</p>
<p>2.<code>nn.ReLU</code> ReLU：$max(0, x)$</p>
<p>3.<code>nn.LeakyReLU</code> LeakyReLU：$max(αx, x)$</p>
<p>4.<code>nn.Sigmoid</code> Sigmoid：$\frac{1}{1 + e^{-x}}$</p>
<p>5.<code>nn.Tanh</code> Tanh：$\frac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
<p>6.<code>nn.GELU</code> GELU：高斯误差线性单元（Transformer 常用）</p>
<p>7.<code>nn.Softmax</code> Softmax：概率归一化</p>
<h6 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h6><p>1.什么是池化？</p>
<p>2.<code>nn.MaxPool1d/2d/3d</code> 最大池化：取局部最大值</p>
<p>3.<code>nn.AvgPool1d/2d/3d</code> 平均池化：取局部平均值</p>
<p>4.<code>nn.AdaptiveMaxPool1d/2d/3d</code> 自适应池化：动态调整输出尺寸</p>
<p>5.<code>nn.FractionalMaxPool2d</code> 分数池化：随机分数步长池化</p>
<h6 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a>Dropout 层</h6><ol>
<li><p>什么是Dropout？</p>
</li>
<li><p><code>nn.Dropout</code> 标准Dropout： 随机置零神经元</p>
</li>
<li><p><code>nn.Dropout1d/2d/3d</code> 空间Dropout：按通道&#x2F;空间置零</p>
</li>
</ol>
<h6 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h6><ol>
<li><p>什么是嵌入？</p>
</li>
<li><p><code>nn.Embedding</code> 词嵌入：nn.Embedding</p>
</li>
<li><p><code>nn.Embedding</code> 稀疏嵌入：高效处理变长序列</p>
</li>
</ol>
<h6 id="稀疏层"><a href="#稀疏层" class="headerlink" title="稀疏层"></a>稀疏层</h6><ol>
<li><p>什么是稀疏？</p>
</li>
<li><p><code>nn.Linear（输入为稀疏张量）</code> 稀疏全连接：稀疏矩阵乘法</p>
</li>
</ol>
<h6 id="视觉专用层"><a href="#视觉专用层" class="headerlink" title="视觉专用层"></a>视觉专用层</h6><ol>
<li><p><code>nn.PixelShuffle</code> 像素重排：子像素卷积（超分辨率）</p>
</li>
<li><p><code>nn.Unfold</code> 像素展开：滑动窗口提取局部块</p>
</li>
<li><p><code>nn.Fold</code> 像素折叠：逆操作于 <code>Unfold</code></p>
</li>
</ol>
<h5 id="模型容器"><a href="#模型容器" class="headerlink" title="模型容器"></a>模型容器</h5><ol>
<li><p>什么是容器？</p>
</li>
<li><p><code>nn.Sequential</code> 层字典</p>
</li>
<li><p><code>nn.ModuleList</code> 动态层列表</p>
</li>
<li><p><code>nn.ModuleDict</code>层字典</p>
</li>
</ol>
<h4 id="优化器和损失函数"><a href="#优化器和损失函数" class="headerlink" title="优化器和损失函数"></a>优化器和损失函数</h4><h5 id="优化器（Optimizers）"><a href="#优化器（Optimizers）" class="headerlink" title="优化器（Optimizers）"></a>优化器（Optimizers）</h5><h6 id="什么是优化器？"><a href="#什么是优化器？" class="headerlink" title="什么是优化器？"></a>什么是优化器？</h6><h6 id="经典优化器"><a href="#经典优化器" class="headerlink" title="经典优化器"></a>经典优化器</h6><ol>
<li><p><code>torch.optim.SGD</code>（含动量）</p>
</li>
<li><p><code>torch.optim.Adam</code>, <code>AdamW</code>, <code>RMSprop</code></p>
</li>
</ol>
<h6 id="学习率调度"><a href="#学习率调度" class="headerlink" title="学习率调度"></a>学习率调度</h6><ol>
<li><p>什么是学习率调度？</p>
</li>
<li><p>相关优化器<code>lr_scheduler.StepLR</code>, <code>CosineAnnealingLR</code>, <code>OneCycleLR</code></p>
</li>
</ol>
<h6 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h6><ol>
<li><p>什么是梯度裁剪？</p>
</li>
<li><p><code>nn.utils.clip_grad_norm_</code></p>
</li>
</ol>
<h5 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h5><h6 id="什么是损失函数？"><a href="#什么是损失函数？" class="headerlink" title="什么是损失函数？"></a>什么是损失函数？</h6><h6 id="分类任务"><a href="#分类任务" class="headerlink" title="分类任务"></a>分类任务</h6><p><code>nn.CrossEntropyLoss</code>，<code>nn.BCEWithLogitsLoss</code></p>
<h6 id="回归任务"><a href="#回归任务" class="headerlink" title="回归任务"></a>回归任务</h6><p><code>nn.MSELoss</code>, <code>nn.L1Loss</code>, <code>nn.HuberLoss</code></p>
<h6 id="生成任务"><a href="#生成任务" class="headerlink" title="生成任务"></a>生成任务</h6><ol>
<li><p>生成什么？</p>
</li>
<li><p>相关损失函数<code>nn.BCELoss</code>, <code>nn.KLDivLoss</code>, 对抗损失（如 WGAN-GP）</p>
</li>
</ol>
<h4 id="预训练模型（torchvision-models）与迁移学习"><a href="#预训练模型（torchvision-models）与迁移学习" class="headerlink" title="预训练模型（torchvision.models）与迁移学习"></a>预训练模型（torchvision.models）与迁移学习</h4><h5 id="计算机视觉模型"><a href="#计算机视觉模型" class="headerlink" title="计算机视觉模型"></a>计算机视觉模型</h5><h6 id="经典-CNN-架构（通过-torchvision-models）"><a href="#经典-CNN-架构（通过-torchvision-models）" class="headerlink" title="经典 CNN 架构（通过 torchvision.models）"></a>经典 CNN 架构（通过 <code>torchvision.models</code>）</h6><p>什么是CNN</p>
<ol>
<li><p>ResNet</p>
</li>
<li><p>VGG</p>
</li>
<li><p>EfficientNet</p>
</li>
</ol>
<h6 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h6><p>什么是Transformer</p>
<ol>
<li><p>Vision Transformer （ViT）</p>
</li>
<li><p>Swin Transformer</p>
</li>
</ol>
<h5 id="自然语言处理模型"><a href="#自然语言处理模型" class="headerlink" title="自然语言处理模型"></a>自然语言处理模型</h5><h6 id="预训练语言模型（通过-transformers-库）"><a href="#预训练语言模型（通过-transformers-库）" class="headerlink" title="预训练语言模型（通过 transformers 库）"></a>预训练语言模型（通过 <code>transformers</code> 库）</h6><ol>
<li><p>BERT</p>
</li>
<li><p>GPT</p>
</li>
</ol>
<h5 id="迁移学习策略"><a href="#迁移学习策略" class="headerlink" title="迁移学习策略"></a>迁移学习策略</h5><p>什么是迁移学习策略</p>
<h6 id="特征提取（冻结部分层）"><a href="#特征提取（冻结部分层）" class="headerlink" title="特征提取（冻结部分层）"></a>特征提取（冻结部分层）</h6><h6 id="微调（Fine-tuning）"><a href="#微调（Fine-tuning）" class="headerlink" title="微调（Fine-tuning）"></a>微调（Fine-tuning）</h6><h6 id="使用预训练特征（如-CLIP）"><a href="#使用预训练特征（如-CLIP）" class="headerlink" title="使用预训练特征（如 CLIP）"></a>使用预训练特征（如 CLIP）</h6><h4 id="数据管道-Data-Pipeline"><a href="#数据管道-Data-Pipeline" class="headerlink" title="数据管道 (Data Pipeline)"></a>数据管道 (Data Pipeline)</h4><h5 id="数据集类-Dataset"><a href="#数据集类-Dataset" class="headerlink" title="数据集类 Dataset"></a>数据集类 <code>Dataset</code></h5><p>自定义数据集实现</p>
<h5 id="数据加载器-DataLoader"><a href="#数据加载器-DataLoader" class="headerlink" title="数据加载器 DataLoader"></a>数据加载器 <code>DataLoader</code></h5><ol>
<li><p>批量加载</p>
</li>
<li><p>多进程加速 (<code>num_workers</code>)</p>
</li>
</ol>
<h5 id="数据增强-torchvision-transforms"><a href="#数据增强-torchvision-transforms" class="headerlink" title="数据增强&#96;&#96;torchvision.transforms&#96;"></a>数据增强&#96;&#96;torchvision.transforms&#96;</h5><h4 id="模型部署与性能优化"><a href="#模型部署与性能优化" class="headerlink" title="模型部署与性能优化"></a>模型部署与性能优化</h4><h5 id="TorchScript-模型导出"><a href="#TorchScript-模型导出" class="headerlink" title="TorchScript 模型导出"></a>TorchScript 模型导出</h5><h5 id="ONNX-格式转换"><a href="#ONNX-格式转换" class="headerlink" title="ONNX 格式转换"></a>ONNX 格式转换</h5><h5 id="混合精度训练-torch-cuda-amp"><a href="#混合精度训练-torch-cuda-amp" class="headerlink" title="混合精度训练 (torch.cuda.amp)"></a>混合精度训练 (<code>torch.cuda.amp</code>)</h5><h2 id="OS操作系统接口模块"><a href="#OS操作系统接口模块" class="headerlink" title="OS操作系统接口模块"></a>OS操作系统接口模块</h2><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>np.stack</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研知识积累</tag>
      </tags>
  </entry>
</search>
