<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法基础课(持续更新中)</title>
    <url>/algorithm_learning/</url>
    <content><![CDATA[<p>算法基础课</p>
<h1 id="算法基础课"><a href="#算法基础课" class="headerlink" title="算法基础课"></a>算法基础课</h1><h2 id="基础算法（一）"><a href="#基础算法（一）" class="headerlink" title="基础算法（一）"></a>基础算法（一）</h2><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> i=l<span class="number">-1</span>,j=r<span class="number">+1</span>,x=q[l+r&gt;&gt;<span class="number">1</span>];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;j)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">do</span> i++;<span class="keyword">while</span>(q[i]&lt;x);</span><br><span class="line">        <span class="keyword">do</span> j--;<span class="keyword">while</span>(q[j]&gt;x);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">quick_sort</span>(q,l,j);</span><br><span class="line">    <span class="built_in">quick_sort</span>(q,j<span class="number">+1</span>,r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">merge_sort</span><span class="params">(<span class="type">int</span> q[],<span class="type">int</span> l,<span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l&gt;=r)<span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> mid=l+r&gt;&gt;<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">merge_sort</span>(q,l,mid),<span class="built_in">merge_sort</span>(q,mid<span class="number">+1</span>,r)；</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> k=<span class="number">0</span>,i=l,j=mid<span class="number">+1</span>;</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid&amp;&amp;j&lt;=r)</span><br><span class="line">        <span class="keyword">if</span>(q[i]&lt;=q[j])tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">else</span> tmp[k++]=q[j++];</span><br><span class="line">    <span class="keyword">while</span>(i&lt;=mid)tmp[k++]=q[i++];</span><br><span class="line">    <span class="keyword">while</span>(j&lt;=r)tmp[k++]=q[j++];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i=l,j=<span class="number">0</span>;i&lt;=r;i++,j++)q[i]=tmp[j];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="二分"><a href="#二分" class="headerlink" title="二分"></a>二分</h3><h4 id="整数二分"><a href="#整数二分" class="headerlink" title="整数二分"></a>整数二分</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_1</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;    <span class="comment">// check()判断mid是否满足性质</span></span><br><span class="line">        <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">bsearch_2</span><span class="params">(<span class="type">int</span> l, <span class="type">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) l = mid;</span><br><span class="line">        <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="浮点数二分"><a href="#浮点数二分" class="headerlink" title="浮点数二分"></a>浮点数二分</h4><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">double</span> x)</span> </span>&#123;<span class="comment">/* ... */</span>&#125; <span class="comment">// 检查x是否满足某种性质</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">bsearch_3</span><span class="params">(<span class="type">double</span> l, <span class="type">double</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> eps = <span class="number">1e-6</span>;   <span class="comment">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class="line">    <span class="keyword">while</span> (r - l &gt; eps)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">double</span> mid = (l + r) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid)) r = mid;</span><br><span class="line">        <span class="keyword">else</span> l = mid;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="基础算法（二）"><a href="#基础算法（二）" class="headerlink" title="基础算法（二）"></a>基础算法（二）</h2><h3 id="高精度"><a href="#高精度" class="headerlink" title="高精度"></a>高精度</h3><h4 id="高精度加法"><a href="#高精度加法" class="headerlink" title="高精度加法"></a>高精度加法</h4><p>本质上是模拟人进行列竖式加法的过程，即$a_1$+…+$a_n$+t，其中当$a$的退一位加起来不超过10则$t&#x3D;0$否则$t&#x3D;1$，以此类推</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line">vector&lt;<span class="type">int</span>&gt;<span class="built_in">add</span>(vector&lt;<span class="type">int</span>&gt;&amp;A,vector&lt;<span class="type">int</span>&gt;&amp;B)</span><br><span class="line">&#123;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;C;</span><br><span class="line">    <span class="type">int</span> t=<span class="number">0</span>;<span class="comment">//存储每一位相加后的数，同时完成是否需要进位的考量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;A.<span class="built_in">size</span>()||i&lt;B.<span class="built_in">size</span>();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(i&lt;A.<span class="built_in">size</span>())t+=A[i];</span><br><span class="line">        <span class="keyword">if</span>(i&lt;B.<span class="built_in">size</span>())t+=B[i];</span><br><span class="line">        C.<span class="built_in">push_back</span>(t%<span class="number">10</span>);</span><br><span class="line">        t/=<span class="number">10</span>;</span><br><span class="line">	&#125;    </span><br><span class="line">    <span class="keyword">if</span>(t)C.<span class="built_in">push_back</span>(<span class="number">1</span>);<span class="comment">//最后一个t代表最高位，因此等for循环运行完后再用判断看看进位情况，是的话就在vector后面再加一个1</span></span><br><span class="line">    <span class="keyword">return</span> C;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string a,b;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt;A,B;</span><br><span class="line">    cin&gt;&gt;a&gt;&gt;b;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=a.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)A.<span class="built_in">push_back</span>(a[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=b.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)B.<span class="built_in">push_back</span>(b[i]-<span class="string">&#x27;0&#x27;</span>);</span><br><span class="line"><span class="comment">//A、B元素进栈都是逆序进栈，比如a=&#x27;123456&#x27;,那A=[6.5,4,3,2,1],目的是方便进位多了1的时候整体数组挪动空间不大，否则要是最高位在前边ta进位后面数组所有数字都要往后挪</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">auto</span> C=<span class="built_in">add</span>(A,B);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=C.<span class="built_in">size</span>()<span class="number">-1</span>;i&gt;=<span class="number">0</span>;i--)cout&lt;&lt;C[i];</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="vector相关知识点"><a href="#vector相关知识点" class="headerlink" title="vector相关知识点"></a>vector相关知识点</h5><ul>
<li><p><strong>vector定义</strong> <code>vector</code> 是 C++ 标准模板库（STL）中的一个容器，它可以看作是一个动态数组。它能存储一系列相同类型的元素，并且可以根据需要自动调整大小。使用 <code>vector</code> 需要包含 <code>&lt;vector&gt;</code> 头文件。</p>
</li>
<li><p><strong>vector数组相关操作</strong>（最后面）增加用<code>.push_back(一个数)</code>，（最后面）删除<code>.pop_back(一个数)</code>，指定位置插入<code>.insert(向量,指定位置)</code>，指定位置删除<code>.erase(向量，指定位置)</code>，清空<code>.clear()</code>，查看大小<code>.size()</code></p>
</li>
<li><p><strong>vector元素的索引和数组一样，也是从 0 开始存储的</strong></p>
</li>
<li><p><strong>vector和普通数组区别</strong></p>
<p>1.大小灵活性</p>
<ul>
<li><strong>普通数组</strong>：大小在定义时就必须确定，且在程序运行过程中不能改变。例如 <code>int arr[10];</code> 定义了一个大小为 10 的整数数组，之后无法再改变其大小。</li>
<li><strong>vector</strong>：可以动态调整大小。可以使用 <code>push_back()</code> 方法在末尾添加元素，当空间不足时，<code>vector</code> 会自动分配更大的内存空间来存储元素。</li>
</ul>
<p>2.内存管理</p>
<ul>
<li><strong>普通数组</strong>：由程序员手动管理内存。如果数组在栈上分配，其生命周期受限于所在的代码块；如果在堆上分配（使用 <code>new</code>），则需要手动使用 <code>delete</code> 释放内存，否则会造成内存泄漏。</li>
<li><strong>vector</strong>：自动管理内存。当 <code>vector</code> 不再使用时，其占用的内存会自动释放，无需程序员手动干预。</li>
</ul>
<p>3.功能丰富度</p>
<ul>
<li><strong>普通数组</strong>：只提供基本的元素访问功能，操作相对有限。</li>
<li><strong>vector</strong>：提供了丰富的成员函数，如 <code>size()</code> 获取元素数量、<code>empty()</code> 判断是否为空、<code>clear()</code> 清空元素等，使用起来更加方便。</li>
</ul>
</li>
<li><p><strong>使用 vector 而不用数组的原因</strong></p>
<p>在大整数相加的场景中，输入的数字长度是不确定的。如果使用普通数组，需要预先定义一个足够大的数组来存储数字的每一位，但这样可能会浪费大量的内存空间。而 <code>vector</code> 可以根据输入数字的实际长度动态调整大小，避免了内存的浪费，并且其提供的 <code>push_back()</code> 方法可以方便地将数字的每一位添加到容器中，同时在处理进位时也更加方便。因此，使用 <code>vector</code> 能更好地适应这种动态长度的需求。</p>
</li>
</ul>
<h5 id="add函数运行示例"><a href="#add函数运行示例" class="headerlink" title="add函数运行示例"></a>add函数运行示例</h5><p>假设我们要计算两个四位数 <code>a = 1234</code> 和 <code>b = 5678</code> 的和。</p>
<p>在代码中，输入的数字以字符串形式存储，然后将其逆序存储到 <code>vector&lt;int&gt;</code> 中，这样做是为了方便处理进位。对于 <code>a = 1234</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>A = &#123;4, 3, 2, 1&#125;</code>；对于 <code>b = 5678</code>，存储在 <code>vector&lt;int&gt;</code> 中为 <code>B = &#123;8, 7, 6, 5&#125;</code>。</p>
<p>具体步骤：</p>
<ul>
<li><p><input checked="" disabled="" type="checkbox"> 
初始化</p>
</li>
<li><p><code>vector&lt;int&gt; C;</code>：用于存储相加结果的向量，初始为空。</p>
</li>
<li><p><code>int t = 0;</code>：用于存储每一位相加后的结果以及进位信息，初始值为 0。</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
循环处理每一位</p>
</li>
</ul>
<p>循环条件为 <code>i &lt; A.size() || i &lt; B.size()</code>，这意味着只要 <code>A</code> 或 <code>B</code> 还有未处理的位，就会继续循环。</p>
<ul>
<li><p><input disabled="" type="checkbox"> 
第一次循环（<code>i = 0</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：因为 <code>i = 0</code> 小于 <code>A.size()</code>（4），所以 <code>t = t + A[0] = 0 + 4 = 4</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：因为 <code>i = 0</code> 小于 <code>B.size()</code>（4），所以 <code>t = t + B[0] = 4 + 8 = 12</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 2）添加到 <code>C</code> 中，此时 <code>C = &#123;2&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第二次循环（<code>i = 1</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[1] = 1 + 3 = 4</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[1] = 4 + 7 = 11</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 1）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 1，即 <code>t = 1</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第三次循环（<code>i = 2</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[2] = 1 + 2 = 3</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[2] = 3 + 6 = 9</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 9）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li><p><input disabled="" type="checkbox"> 
第四次循环（<code>i = 3</code>）</p>
</li>
<li><p><code>if(i &lt; A.size()) t += A[i];</code>：<code>t = t + A[3] = 0 + 1 = 1</code>。</p>
</li>
<li><p><code>if(i &lt; B.size()) t += B[i];</code>：<code>t = t + B[3] = 1 + 5 = 6</code>。</p>
</li>
<li><p><code>C.push_back(t % 10);</code>：将 <code>t % 10</code>（即 6）添加到 <code>C</code> 中，此时 <code>C = &#123;2, 1, 9, 6&#125;</code>。</p>
</li>
<li><p><code>t /= 10;</code>：<code>t</code> 除以 10，得到进位 0，即 <code>t = 0</code>。</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
处理最后可能的进位</p>
</li>
</ul>
<p><code>if(t) C.push_back(1);</code></p>
<p>因为此时 <code>t = 0</code>，所以不需要添加额外的进位，<code>C</code> 仍然为 <code>&#123;2, 1, 9, 6&#125;</code>。</p>
<ul>
<li><input checked="" disabled="" type="checkbox"> 返回结果</li>
</ul>
<p><code>return C;</code>：返回存储相加结果的向量 <code>C</code>。</p>
<p>在 <code>main</code> 函数中，将 <code>C</code> 逆序输出，得到最终结果 <code>6912</code>，即 <code>1234 + 5678 = 6912</code>。</p>
<h5 id="auto的用法"><a href="#auto的用法" class="headerlink" title="auto的用法"></a>auto的用法</h5><p><code>auto</code>是C++11的新特性，可以自动识别变量属性。比如<code>auto C=add(A,B);</code>就自动识别了add返回的类型，因此<code>auto C &lt;=&gt; vector&lt;int&gt;c</code></p>
<h5 id="a-i-0-的作用：将字符串每一位由ASCII码转为整数"><a href="#a-i-0-的作用：将字符串每一位由ASCII码转为整数" class="headerlink" title="a[i] - &#39;0&#39; 的作用：将字符串每一位由ASCII码转为整数"></a><code>a[i] - &#39;0&#39;</code> 的作用：将字符串每一位由ASCII码转为整数</h5><p>在 C++ 中，当你从输入读取一个数字字符串时，比如 <code>&quot;123&quot;</code>，字符串中的每个字符（如 <code>&#39;1&#39;</code>、<code>&#39;2&#39;</code>、<code>&#39;3&#39;</code>）实际上存储的是字符的 ASCII 码值，而不是对应的数值。字符 <code>&#39;0&#39;</code> 到 <code>&#39;9&#39;</code> 的 ASCII 码是连续的，<code>&#39;0&#39;</code> 的 ASCII 码值是 48，<code>&#39;1&#39;</code> 是 49，以此类推，<code>&#39;9&#39;</code> 是 57。</p>
<p><code>b[i] - &#39;0&#39;</code> 的作用就是将字符形式的数字转换为对应的整数值。例如，当 <code>b[i]</code> 为 <code>&#39;1&#39;</code> 时，<code>&#39;1&#39;</code> 的 ASCII 码值是 49，<code>&#39;0&#39;</code> 的 ASCII 码值是 48，那么 <code>&#39;1&#39; - &#39;0&#39;</code> 就等于 <code>49 - 48 = 1</code>，这样就把字符 <code>&#39;1&#39;</code> 转换为了整数 1。</p>
<p>如果没有 <code>b[i] - &#39;0&#39;</code> 这个操作，直接将字符存储到 <code>vector&lt;int&gt;</code> 中，那么存储的是字符的 ASCII 码值，而不是对应的数值，这会导致后续的计算出现错误。</p>
<h4 id="高精度减法"><a href="#高精度减法" class="headerlink" title="高精度减法"></a>高精度减法</h4><h4 id="高精度乘法"><a href="#高精度乘法" class="headerlink" title="高精度乘法"></a>高精度乘法</h4><h4 id="高精度除法"><a href="#高精度除法" class="headerlink" title="高精度除法"></a>高精度除法</h4>]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>ACWing</tag>
      </tags>
  </entry>
  <entry>
    <title>如何用hexo新建文件并上传/如何用hexo插入图片（next主题）</title>
    <url>/hexo_maintanance/</url>
    <content><![CDATA[<p>hexo（博主采用next主题）日常维护教程</p>
<h1 id="修改scaffolds-post-md（默认标题文件）"><a href="#修改scaffolds-post-md（默认标题文件）" class="headerlink" title="修改scaffolds&#x2F;post.md（默认标题文件）"></a>修改scaffolds&#x2F;post.md（默认标题文件）</h1><p>注意修改时不要把中文加进去，在此只起到注释作用</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:([1,2]设置同时属于不同类别可以这样)</span><br><span class="line">categories:</span><br><span class="line">top:(表示置顶情况，不置顶不填即可数字大小代表置顶顺序，数字越大排序越前)</span><br></pre></td></tr></table></figure>



<h1 id="新建md文件"><a href="#新建md文件" class="headerlink" title="新建md文件"></a>新建md文件</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new/hexo n &quot;文件名&quot;</span><br></pre></td></tr></table></figure>

<h1 id="查看新建文件"><a href="#查看新建文件" class="headerlink" title="查看新建文件"></a>查看新建文件</h1><p>进入目录是source&#x2F;_posts</p>
<h1 id="完善标题对应信息，填写md"><a href="#完善标题对应信息，填写md" class="headerlink" title="完善标题对应信息，填写md"></a>完善标题对应信息，填写md</h1><p>也就是刚才上面那堆东西</p>
<h1 id="在blog目录下（因人而异）打开git-bash"><a href="#在blog目录下（因人而异）打开git-bash" class="headerlink" title="在blog目录下（因人而异）打开git bash"></a>在blog目录下（因人而异）打开git bash</h1><p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g -d</span><br></pre></td></tr></table></figure>

<p>即可上传</p>
<p>输入</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure>

<p>即可实时查看网页</p>
<h1 id="插入图片操作（图片和md文件最好均为英文名）"><a href="#插入图片操作（图片和md文件最好均为英文名）" class="headerlink" title="插入图片操作（图片和md文件最好均为英文名）"></a>插入图片操作（图片和md文件最好均为英文名）</h1><h2 id="下插件"><a href="#下插件" class="headerlink" title="下插件"></a>下插件</h2><p>见<a href="https://github.com/yiyungent/hexo-asset-img">yiyungent&#x2F;hexo-asset-img：🍰 Hexo 本地图片插件。|Hexo 本地图片插件：转换图片相对路径为asset_img</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-asset-img --save</span><br><span class="line">或者</span><br><span class="line">npm install git://github.com/yiyungent/hexo-asset-img.git#main</span><br></pre></td></tr></table></figure>

<h2 id="修改host-config-yml"><a href="#修改host-config-yml" class="headerlink" title="修改host&#x2F;_config.yml"></a>修改host&#x2F;_config.yml</h2><p>permalink控制了永久域名的样式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.post_asset_folder: true</span><br><span class="line">2.permalink: :title/（我的自带了日期导致图片一直不行:year/:month/:date/:title/）</span><br></pre></td></tr></table></figure>

<h2 id="直接粘贴图片"><a href="#直接粘贴图片" class="headerlink" title="直接粘贴图片"></a>直接粘贴图片</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">![1](./hexo_maintanance/1.png)</span><br></pre></td></tr></table></figure>

<p>上面是下面图片的路径。能看到下面的图片就能说明这个方法就是成功的。</p>
<p><img src="/./hexo_maintanance/1.png" alt="1"></p>
<h1 id="实现侧边栏标题全展开"><a href="#实现侧边栏标题全展开" class="headerlink" title="实现侧边栏标题全展开"></a>实现侧边栏标题全展开</h1><p>有些文件目录很长，不全展开不方便看。可以修改<code>blog\themes\next\source\css\_common\outline\sidebar\sidebar-toc.styl</code></p>
<p>文件，查找修改<code>.nav-child</code>对应代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">.<span class="property">nav</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (not hexo-<span class="title function_">config</span>(<span class="string">&#x27;toc.expand_all&#x27;</span>)) &#123;</span><br><span class="line">    .<span class="property">nav</span>-child &#123;</span><br><span class="line">      --<span class="attr">height</span>: auto;          <span class="comment">/* 取消高度限制 */</span></span><br><span class="line">      <span class="attr">height</span>: auto;            <span class="comment">/* 启用自动高度适应内容 */</span></span><br><span class="line">      <span class="attr">opacity</span>: <span class="number">1</span>;              <span class="comment">/* 取消透明度隐藏 */</span></span><br><span class="line">      <span class="attr">overflow</span>: visible;       <span class="comment">/* 允许内容溢出显示 */</span></span><br><span class="line">      transition-<span class="attr">property</span>: opacity;  <span class="comment">/* 仅保留透明度过渡 */</span></span><br><span class="line">      <span class="attr">visibility</span>: visible;     <span class="comment">/* 确保元素可见 */</span></span><br><span class="line">      <span class="attr">transition</span>: $transition-ease;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>原配置为：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line">原配置--<span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">height</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">opacity</span>: <span class="number">0</span>;</span><br><span class="line"><span class="attr">overflow</span>: hidden;</span><br><span class="line">transition-<span class="attr">property</span>: height, opacity, visibility;</span><br><span class="line"><span class="attr">transition</span>: $transition-ease;</span><br><span class="line"><span class="attr">visibility</span>: hidden;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>博客维护</category>
      </categories>
      <tags>
        <tag>博客维护</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕OCC-NeRF:Occlusion-Free Scene Recovery via Neural Radiance Fields</title>
    <url>/%E6%89%8B%E6%92%95nerfmm/</url>
    <content><![CDATA[<p>复现论文：OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields</p>
<h1 id="复现论文：OCC-NeRF-Occlusion-Free-Scene-Recovery-via-Neural-Radiance-Fields"><a href="#复现论文：OCC-NeRF-Occlusion-Free-Scene-Recovery-via-Neural-Radiance-Fields" class="headerlink" title="复现论文：OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields"></a>复现论文：OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields</h1><p>链接：<a href="https://freebutuselesssoul.github.io/occnerf/">OCC-NeRF: Occlusion-Free Scene Recovery via Neural Radiance Fields</a></p>
<h2 id="文件夹目录"><a href="#文件夹目录" class="headerlink" title="文件夹目录"></a>文件夹目录</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">occ-nerf/</span><br><span class="line">├── .gitignore</span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── environment.yml</span><br><span class="line">├── local1.txt</span><br><span class="line">├── dataloader/</span><br><span class="line">│   ├── any_folder.py</span><br><span class="line">│   ├── local_save</span><br><span class="line">│   ├── with_colmap.py</span><br><span class="line">│   ├── with_feature.py</span><br><span class="line">│   ├── with_feature_colmap.py</span><br><span class="line">│   └── with_mask.py</span><br><span class="line">├── models/</span><br><span class="line">│   ├── depth_decoder.py</span><br><span class="line">│   ├── intrinsics.py</span><br><span class="line">│   ├── layers.py</span><br><span class="line">│   ├── nerf_feature.py</span><br><span class="line">│   ├── nerf_mask.py</span><br><span class="line">│   ├── nerf_models.py</span><br><span class="line">│   └── poses.py</span><br><span class="line">├── utils/</span><br><span class="line">│   ├── align_traj.py</span><br><span class="line">│   ├── comp_ate.py</span><br><span class="line">│   ├── comp_ray_dir.py</span><br><span class="line">│   ├── lie_group_helper.py</span><br><span class="line">│   ├── pos_enc.py</span><br><span class="line">│   ├── pose_utils.py</span><br><span class="line">│   ├── split_dataset.py</span><br><span class="line">│   ├── training_utils.py</span><br><span class="line">│   ├── vgg.py</span><br><span class="line">│   ├── vis_cam_traj.py</span><br><span class="line">│   └── volume_op.py</span><br><span class="line">├── tasks/</span><br><span class="line">│   └── ...</span><br><span class="line">└── third_party/</span><br><span class="line">    ├── ATE/</span><br><span class="line">    │   └── README.md</span><br><span class="line">    └── pytorch_ssim/</span><br></pre></td></tr></table></figure>

<h2 id="DEBUG代码"><a href="#DEBUG代码" class="headerlink" title="DEBUG代码"></a>DEBUG代码</h2><h3 id="dataloader"><a href="#dataloader" class="headerlink" title="dataloader"></a>dataloader</h3><h4 id="any-folder-py"><a href="#any-folder-py" class="headerlink" title="any_folder.py"></a>any_folder.py</h4><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os                                       <span class="comment"># 操作系统接口模块</span></span><br><span class="line"><span class="keyword">import</span> torch                                    <span class="comment"># PyTorch深度学习框架</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                              <span class="comment"># 科学计算库</span></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm                           <span class="comment"># 进度条显示模块</span></span><br><span class="line"><span class="keyword">import</span> imageio                                  <span class="comment"># 图像IO处理库</span></span><br><span class="line"><span class="keyword">from</span> dataloader.with_colmap <span class="keyword">import</span> resize_imgs  <span class="comment"># 自定义图像缩放函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_imgs</span>(<span class="params">image_dir, num_img_to_load, start, end, skip, load_sorted, load_img</span>):</span><br><span class="line">    img_names = np.array(<span class="built_in">sorted</span>(os.listdir(image_dir)))  <span class="comment"># 获取并排序目录下所有文件名</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> end == -<span class="number">1</span>:                                 <span class="comment"># 从start开始按间隔skip取图</span></span><br><span class="line">        img_names = img_names[start::skip]</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 取start到end区间按间隔skip取图</span></span><br><span class="line">        img_names = img_names[start:end:skip]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> load_sorted:                           <span class="comment"># 是否打乱图像顺序</span></span><br><span class="line">        np.random.shuffle(img_names)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> num_img_to_load &gt; <span class="built_in">len</span>(img_names):          <span class="comment"># 检查请求数量是否超出范围</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;图像请求数<span class="subst">&#123;num_img_to_load&#125;</span>超过可用数<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>&#x27;</span>)</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">elif</span> num_img_to_load == -<span class="number">1</span>:                   <span class="comment"># 加载全部可用图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;加载全部<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张图像&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:                                         <span class="comment"># 截取指定数量的图像</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;从<span class="subst">&#123;<span class="built_in">len</span>(img_names)&#125;</span>张中加载<span class="subst">&#123;num_img_to_load&#125;</span>张&#x27;</span>)</span><br><span class="line">        img_names = img_names[:num_img_to_load]</span><br><span class="line">    </span><br><span class="line">    img_paths = [os.path.join(image_dir, n) <span class="keyword">for</span> n <span class="keyword">in</span> img_names]  <span class="comment"># 构建完整文件路径</span></span><br><span class="line">    N_imgs = <span class="built_in">len</span>(img_paths)                         <span class="comment"># 计算实际加载数量</span></span><br><span class="line">    </span><br><span class="line">    img_list = []</span><br><span class="line">    <span class="keyword">if</span> load_img:                                     <span class="comment"># 实际加载图像数据</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> tqdm(img_paths):</span><br><span class="line">            img = imageio.imread(p)[:, :, :<span class="number">3</span>]        <span class="comment"># 读取RGB三通道图像</span></span><br><span class="line">            img_list.append(img)</span><br><span class="line">        img_list = np.stack(img_list)                <span class="comment"># 堆叠为4D数组</span></span><br><span class="line">        img_list = torch.from_numpy(img_list).<span class="built_in">float</span>() / <span class="number">255</span>  <span class="comment"># 转换为浮点张量并归一化</span></span><br><span class="line">        H, W = img_list.shape[<span class="number">1</span>], img_list.shape[<span class="number">2</span>]  <span class="comment"># 获取图像尺寸</span></span><br><span class="line">    <span class="keyword">else</span>:                                            <span class="comment"># 仅获取图像尺寸</span></span><br><span class="line">        tmp_img = imageio.imread(img_paths[<span class="number">0</span>])</span><br><span class="line">        H, W = tmp_img.shape[<span class="number">0</span>], tmp_img.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;                                         <span class="comment"># 返回结构化数据</span></span><br><span class="line">        <span class="string">&#x27;imgs&#x27;</span>: img_list,        <span class="comment"># 图像张量 (N, H, W, 3)</span></span><br><span class="line">        <span class="string">&#x27;img_names&#x27;</span>: img_names,  <span class="comment"># 图像文件名数组</span></span><br><span class="line">        <span class="string">&#x27;N_imgs&#x27;</span>: N_imgs,        <span class="comment"># 总图像数</span></span><br><span class="line">        <span class="string">&#x27;H&#x27;</span>: H,                  <span class="comment"># 图像高度</span></span><br><span class="line">        <span class="string">&#x27;W&#x27;</span>: W,                  <span class="comment"># 图像宽度</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataLoaderAnyFolder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_dir, scene_name, res_ratio, num_img_to_load, </span></span><br><span class="line"><span class="params">                 start, end, skip, load_sorted, load_img=<span class="literal">True</span></span>):  <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="variable language_">self</span>.base_dir = base_dir                  <span class="comment"># 数据根目录</span></span><br><span class="line">        <span class="variable language_">self</span>.scene_name = scene_name              <span class="comment"># 场景名称</span></span><br><span class="line">        <span class="variable language_">self</span>.res_ratio = res_ratio                <span class="comment"># 分辨率缩放比例</span></span><br><span class="line">        <span class="variable language_">self</span>.num_img_to_load = num_img_to_load    <span class="comment"># 最大加载数量</span></span><br><span class="line">        <span class="variable language_">self</span>.start = start                        <span class="comment"># 起始索引</span></span><br><span class="line">        <span class="variable language_">self</span>.end = end                            <span class="comment"># 结束索引</span></span><br><span class="line">        <span class="variable language_">self</span>.skip = skip                          <span class="comment"># 采样间隔</span></span><br><span class="line">        <span class="variable language_">self</span>.load_sorted = load_sorted            <span class="comment"># 是否保持顺序</span></span><br><span class="line">        <span class="variable language_">self</span>.load_img = load_img                  <span class="comment"># 是否实际加载图像</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs_dir = os.path.join(<span class="variable language_">self</span>.base_dir, <span class="variable language_">self</span>.scene_name)  <span class="comment"># 构建图像目录路径</span></span><br><span class="line">        </span><br><span class="line">        image_data = load_imgs(<span class="variable language_">self</span>.imgs_dir, <span class="variable language_">self</span>.num_img_to_load,  <span class="comment"># 加载图像数据</span></span><br><span class="line">                              <span class="variable language_">self</span>.start, <span class="variable language_">self</span>.end, <span class="variable language_">self</span>.skip,</span><br><span class="line">                              <span class="variable language_">self</span>.load_sorted, <span class="variable language_">self</span>.load_img)</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.imgs = image_data[<span class="string">&#x27;imgs&#x27;</span>]             <span class="comment"># 图像张量</span></span><br><span class="line">        <span class="variable language_">self</span>.img_names = image_data[<span class="string">&#x27;img_names&#x27;</span>]   <span class="comment"># 文件名列表</span></span><br><span class="line">        <span class="variable language_">self</span>.N_imgs = image_data[<span class="string">&#x27;N_imgs&#x27;</span>]         <span class="comment"># 图像总数</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_H = image_data[<span class="string">&#x27;H&#x27;</span>]               <span class="comment"># 原始高度</span></span><br><span class="line">        <span class="variable language_">self</span>.ori_W = image_data[<span class="string">&#x27;W&#x27;</span>]               <span class="comment"># 原始宽度</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.near = <span class="number">0.0</span>                            <span class="comment"># 近裁剪面(NDC坐标系)</span></span><br><span class="line">        <span class="variable language_">self</span>.far = <span class="number">1.0</span>                             <span class="comment"># 远裁剪面(NDC坐标系)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.res_ratio &gt; <span class="number">1</span>:                     <span class="comment"># 计算实际使用分辨率</span></span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W // <span class="variable language_">self</span>.res_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.H = <span class="variable language_">self</span>.ori_H</span><br><span class="line">            <span class="variable language_">self</span>.W = <span class="variable language_">self</span>.ori_W</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.load_img:                          <span class="comment"># 执行图像缩放</span></span><br><span class="line">            <span class="variable language_">self</span>.imgs = resize_imgs(<span class="variable language_">self</span>.imgs, <span class="variable language_">self</span>.H, <span class="variable language_">self</span>.W)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    base_dir = <span class="string">&#x27;/your/data/path&#x27;</span>                   <span class="comment"># 数据根目录配置示例</span></span><br><span class="line">    scene_name = <span class="string">&#x27;LLFF/fern/images&#x27;</span>                <span class="comment"># 场景路径配置示例</span></span><br><span class="line">    resize_ratio = <span class="number">8</span>                               <span class="comment"># 缩放比例配置</span></span><br><span class="line">    num_img_to_load = -<span class="number">1</span>                           <span class="comment"># 加载全部图像</span></span><br><span class="line">    start, end, skip = <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>                    <span class="comment"># 采样参数初始化</span></span><br><span class="line">    load_sorted, load_img = <span class="literal">True</span>, <span class="literal">True</span>             <span class="comment"># 加载配置参数</span></span><br><span class="line">    </span><br><span class="line">    scene = DataLoaderAnyFolder(                   <span class="comment"># 创建数据加载实例</span></span><br><span class="line">        base_dir=base_dir,</span><br><span class="line">        scene_name=scene_name,</span><br><span class="line">        res_ratio=resize_ratio,</span><br><span class="line">        num_img_to_load=num_img_to_load,</span><br><span class="line">        start=start,</span><br><span class="line">        end=end,</span><br><span class="line">        skip=skip,</span><br><span class="line">        load_sorted=load_sorted,</span><br><span class="line">        load_img=load_img)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>手撕论文知识库</title>
    <url>/%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/</url>
    <content><![CDATA[<p>手撕论文知识库</p>
<h1 id="手撕论文知识库"><a href="#手撕论文知识库" class="headerlink" title="手撕论文知识库"></a>手撕论文知识库</h1><h2 id="Torch"><a href="#Torch" class="headerlink" title="Torch"></a>Torch</h2><h3 id="什么是torch"><a href="#什么是torch" class="headerlink" title="什么是torch"></a>什么是torch</h3><p>Torch 是 PyTorch 深度学习框架的核心库，具备强大的功能与广泛的用途。它提供了丰富的张量操作，可在 CPU 或 GPU 上高效计算，能轻松处理各类数据；其自动求导机制极大简化了深度学习中梯度计算与反向传播的过程，让模型训练更为便捷。借助<code>torch.nn</code>模块可方便构建如 CNN、RNN 等复杂神经网络架构，<code>torch.optim</code>模块提供多种优化算法用于模型参数更新。此外，Torch 还支持预训练模型的使用与微调，结合可视化工具能助力监控训练过程，广泛应用于图像、自然语言处理、推荐系统等诸多领域。</p>
<h3 id="torch常用函数和功能"><a href="#torch常用函数和功能" class="headerlink" title="torch常用函数和功能"></a>torch常用函数和功能</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><h5 id="什么是张量"><a href="#什么是张量" class="headerlink" title="什么是张量"></a>什么是张量</h5><p>张量是多维数组的泛化表示，可理解为一个多维的数据容器，零维张量是标量，一维张量是向量，二维张量是矩阵，三维及以上则是更高阶的张量。在深度学习里，使用张量是因为它能够高效地表示和处理大量的数据，像图像可表示为三维张量（高度、宽度、通道数），视频可表示为四维张量（帧数、高度、宽度、通道数）。并且，深度学习框架（如 PyTorch）针对张量运算进行了高度优化，能利用 GPU 等硬件加速计算，张量还能自然地支持自动求导机制，方便进行模型训练时的梯度计算和参数更新。</p>
<p>张量是 PyTorch 中最基础的数据结构，类似于 NumPy 的多维数组，但它可以在 GPU 上进行加速计算，并且支持自动求导等深度学习所需的特性。</p>
<h5 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h5><p>维度（也称为轴）是指张量在某个方向上的延伸。可以将维度理解为数据组织的一个方向或一个层次，类似于在地理坐标系统中，经度和纬度分别代表了不同的方向，张量的每个维度也代表了数据的一个特定方向的排列。维度的数量被称为张量的阶（rank），零阶张量是标量（一个单独的数值），一阶张量是向量（一维数组），二阶张量是矩阵（二维数组），三阶及以上的张量则用于表示更复杂的数据结构。</p>
<p><strong>注：维度从0开始算起，比如对于二阶张量，维度0代表行，维度1代表列</strong></p>
<p><strong>另：维度排列遵循（$a_n$, $a_{n-1}$, …, $a_1$）的形式，数字越前代表越高维的堆叠。比如<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起</strong></p>
<p><strong>1.零阶张量（标量）</strong></p>
<p>零阶张量只有一个数值，它没有方向的概念，维度数量为 0。例如这里的 <code>scalar</code> 就是一个零阶张量，它代表一个单一的数值，不涉及方向或多个元素的排列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scalar = torch.tensor(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;标量的维度数量:&quot;</span>, scalar.dim()) </span><br></pre></td></tr></table></figure>

<p><strong>2.一阶张量（向量）</strong></p>
<p>一阶张量可以看作是一个向量，它有一个维度。这个维度代表了向量中元素的排列方向，向量的长度就是这个维度的大小。例如<code>vector</code> 是一个一阶张量，维度数量为 1，该维度的大小为 4，表示向量中有 4 个元素。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">vector = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量的维度数量:&quot;</span>, vector.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;向量在该维度的大小:&quot;</span>, vector.size(<span class="number">0</span>)) </span><br></pre></td></tr></table></figure>

<p><strong>3.二阶张量（矩阵）</strong></p>
<p>二阶张量是一个矩阵，有两个维度，通常称为行和列。第一个维度代表矩阵的行方向，第二个维度代表矩阵的列方向。例如<code>matrix</code> 是一个 2 行 3 列的矩阵，第一个维度的大小为 2 表示有 2 行，第二个维度的大小为 3 表示有 3 列。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">matrix = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵的维度数量:&quot;</span>, matrix.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第一个维度（行）的大小:&quot;</span>, matrix.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;矩阵第二个维度（列）的大小:&quot;</span>, matrix.size(<span class="number">1</span>)) </span><br></pre></td></tr></table></figure>

<p><strong>4.高阶张量（图像、视频等）</strong></p>
<p>对于三阶及以上的张量，维度的含义更加丰富，通常与具体的数据类型和应用场景相关。</p>
<p><strong>图像数据</strong>：在处理图像时，通常使用三阶张量。例如，一张彩色图像可以表示为一个形状为 <code>(高度, 宽度, 通道数)</code> 的三阶张量。这里的第一个维度代表图像的高度方向，第二个维度代表图像的宽度方向，第三个维度代表图像的通道（如 RGB 三个通道）。</p>
<p><code>image = torch.randn(224, 224, 3)</code> 这行代码能够随机生成一个形状为 <code>(224, 224, 3)</code> 的张量来模拟图像的三通道数值。</p>
<p>由于 <code>torch.randn()</code> 生成的是服从标准正态分布的随机数，这些数值可能为负数，也可能超出了常见图像像素值的范围（通常是 0 - 255 或者 0 - 1）。在实际的图像处理任务中，如果需要模拟真实图像，可能需要对这些随机值进行进一步的处理，例如通过归一化或裁剪操作将其限制在合适的范围内。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">image = torch.randn(<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像张量的维度数量:&quot;</span>, image.dim()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像高度维度的大小:&quot;</span>, image.size(<span class="number">0</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像宽度维度的大小:&quot;</span>, image.size(<span class="number">1</span>)) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;图像通道维度的大小:&quot;</span>, image.size(<span class="number">2</span>)) </span><br></pre></td></tr></table></figure>

<p><img src="/./%E6%89%8B%E6%92%95%E7%9F%A5%E8%AF%86%E5%BA%93/image-20250219032451598.png" alt="image-20250219032451598"></p>
<p><strong>视频数据</strong>：视频可以看作是一系列的图像帧，因此可以用四阶张量表示，形状通常为 <code>(帧数, 高度, 宽度, 通道数)</code>。第一个维度代表视频中的帧数，其余维度与图像张量的含义相同。</p>
<p>*<strong>5.通道</strong></p>
<p>通道指图像中特定类型信息的集合，图像可含一个或多个通道，各通道存储图像某方面特征数据。像单通道存亮度，RGB 三通道分别存红、绿、蓝颜色信息，四通道还多了透明度通道，以此组合完整呈现图像。</p>
<p>通道能实现颜色表示与混合，如 RGB 三通道通过不同数值组合呈现丰富色彩；可用于特征提取与分析，不同通道提供不同特征，助力图像分析和目标识别；还能用于图像合成与特效制作，借助透明度通道可控制图像透明效果实现合成。</p>
<p>在图片里，灰度图用单通道呈现黑白影像；彩色照片靠 RGB 三通道展示多彩画面；PNG 图片利用四通道含透明度信息实现图像融合。视频是连续的图片帧，同样利用通道来呈现色彩、进行特效处理，如影视中常见的抠图合成场景就借助了通道特性。</p>
<h5 id="张量相关函数"><a href="#张量相关函数" class="headerlink" title="张量相关函数"></a>张量相关函数</h5><p><strong>1.创建</strong></p>
<ul>
<li><p>创建张量<code>torch.tensor()</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>当你有现有的数据存储在 Python 列表或 NumPy 数组中，并且需要将其输入到 PyTorch 模型进行计算时使用。例如，在加载数据集后，将数据转换为张量形式以便后续处理。</p>
<p>从数据存储的角度来看，<code>tensor</code> 存储了 Python 列表 <code>data</code> 中的元素 <code>[1, 2, 3]</code>。它将这些数据以一种高效的、适合计算机处理的方式组织起来，存储在内存中。在这个例子中，<code>tensor</code> 是一个一维张量，形状为 <code>(3,)</code>，这意味着它包含 3 个元素。</p>
<p>在数学运算方面，<code>tensor</code> 可以参与各种数学运算，如加法、乘法、矩阵乘法等。PyTorch 为张量提供了丰富的数学运算函数，这些运算可以在 CPU 或 GPU 上高效执行。</p>
<p>在深度学习的上下文中，<code>tensor</code> 是模型输入、输出以及参数的基本表示形式。例如，在一个简单的全连接神经网络中，输入数据会被转换为张量输入到网络中，网络的权重和偏置也是以张量的形式存储和更新的。在上述例子中，<code>tensor</code> 可以作为一个简单的输入数据示例，如果要构建一个神经网络处理这个输入，可能会进行如下操作（以下是一个简单示例）：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的全连接层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)  <span class="comment"># 输入维度为 3，输出维度为 1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = SimpleNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备输入数据</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(<span class="number">0</span>)  <span class="comment"># 转换为适合输入模型的形状</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">output = model(tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型输出:&quot;</span>, output)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>创建全零张量<code>torch.zeros()</code></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">zeros_tensor = torch.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>常用于初始化某些变量，如在初始化神经网络的偏置项时，可使用全零张量。另外，在需要填充零值进行数据预处理或占位时也会用到。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>传入的参数 <code>(2, 3)</code>对应创建的 <code>zeros_tensor</code> 是一个 2 行 3 列的二维张量。如果使用 <code>torch.zeros((2, 3, 4))</code> 这样的代码，那么创建的就是一个三维张量，其中 <code>2</code> 表示最外层维度的大小（可以想象成有 2 个二维矩阵堆叠在一起），<code>3</code> 表示每个二维矩阵的行数，<code>4</code> 表示每个二维矩阵的列数。依此类推，对于更高维的张量，每个数字都代表对应维度上的大小。相当于高是2，行是3，列是4</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">创建的全零张量：</span><br><span class="line">tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line">张量的形状： torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<p>如果是<code>torch.zeros((2, 3, 4, 5))</code>，那就是两个三维（三层）的4x5矩阵叠在一起，数字越前就代表越高维的堆叠。</p>
<ul>
<li><p>创建全一张量**<code>torch.ones()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">ones_tensor = torch.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>与 <code>torch.zeros()</code> 类似，可用于初始化特定变量。在一些归一化操作或需要特定初始值为 1 的场景中会使用。</p>
<ul>
<li><p>创建指定形状的随机张量，元素值在[0 , 1)之间**<code>torch.rand()</code>**</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">random_tensor = torch.rand((<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li>
</ul>
<p>在初始化神经网络的权重时，随机初始化是常见的做法，可使用 <code>torch.rand()</code> 生成初始权重张量。</p>
<p><strong>2.操作</strong></p>
<ul>
<li><p><strong><code>torch.cat()</code></strong>：用于在指定维度上拼接多个张量。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">c = torch.cat((a, b), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>当需要将多个张量合并为一个更大的张量时使用。例如，在处理多模态数据时，将不同模态的特征张量拼接在一起。</p>
<p><code>torch.cat((a, b), dim=1)</code> 表示在维度 1（列方向）上对张量 <code>a</code> 和 <code>b</code> 进行拼接。可以看到，拼接后的张量 <code>c</code> 是将 <code>a</code> 和 <code>b</code> 的列进行了合并，行数不变，列数变为原来两个张量列数之和。在处理多模态数据时，比如一个模态的数据特征用张量 <code>a</code> 表示，另一个模态的数据特征用张量 <code>b</code> 表示，通过这种拼接操作可以将不同模态的特征合并在一起，方便后续的处理。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>

<p>在二维张量的语境下，维度 0 代表行方向。<code>torch.cat((a, b), dim=0)</code> 会将张量 <code>b</code> 按行的顺序拼接到张量 <code>a</code> 的下方，拼接后的张量列数不变，行数为原来两个张量行数之和。在实际应用中，若 <code>a</code> 和 <code>b</code> 分别表示两组样本数据，在维度 0 上拼接就相当于将这两组样本合并成一组更大的样本集。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">张量 a：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">张量 b：</span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">在维度 <span class="number">0</span> 上拼接后的张量 c：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.reshape()</code></strong>：改变张量的形状。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([1, 2, 3, 4])</span><br><span class="line">reshaped_x = torch.reshape(x, (2, 2))</span><br></pre></td></tr></table></figure>

<p>在神经网络中，不同层之间的数据形状可能需要进行调整，使用 <code>torch.reshape()</code> 可以方便地改变张量形状以满足层的输入要求。</p>
<p><code>torch.reshape(x, (2, 2))</code> 是将一维张量 <code>x</code> 重塑为二维张量 <code>reshaped_x</code>，形状为 <code>(2, 2)</code>。在神经网络中，不同层之间的数据形状可能不匹配，例如某一层的输出是一维向量，而后续层需要二维矩阵作为输入，这时就可以使用 <code>torch.reshape()</code> 来调整数据的形状，使其满足层的输入要求。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">重塑后的张量 reshaped_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.transpose()</code></strong>：交换张量的两个维度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([[1, 2], [3, 4]])</span><br><span class="line">transposed_x = torch.transpose(x, 0, 1)</span><br></pre></td></tr></table></figure>

<p>在矩阵运算中，有时需要对矩阵进行转置操作。在图像处理中，可能需要调整图像张量的维度顺序。</p>
<p><code>torch.transpose(x, 0, 1)</code> 表示交换张量 <code>x</code> 的第 0 维和第 1 维。在这个二维矩阵的例子中，就是对矩阵进行了转置操作，原来的行变成了列，列变成了行。在矩阵运算中，矩阵转置是一个常见的操作，例如在计算矩阵乘法时可能需要对矩阵进行转置。在图像处理中，图像张量的维度顺序可能需要调整，比如将 <code>(高度, 宽度, 通道数)</code> 调整为 <code>(通道数, 高度, 宽度)</code> 以适应某些模型的输入要求，这时就可以使用 <code>torch.transpose()</code> 来实现。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">原始张量 x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">转置后的张量 transposed_x：</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h4><p><code>requires_grad</code> 和 <code>backward()</code></p>
<p>设置张量的 <code>requires_grad=True</code> 来跟踪其操作，使用 <code>backward()</code> 计算梯度。</p>
<p>在深度学习模型训练中，需要计算损失函数关于模型参数的梯度，以便使用优化算法更新参数。通过设置参数张量的 <code>requires_grad=True</code>，可以利用自动求导机制自动计算梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">2.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出 4.0</span></span><br></pre></td></tr></table></figure>



<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p>1.<code>torch.nn.Linear</code>定义全连接层。全连接层常用于多层感知机（MLP）中，用于将输入特征映射到输出特征。</p>
<p>输入张量 <code>input_tensor = torch.randn(1, 10)</code> ，形状为 <code>(1, 10)</code>，这里的 <code>1</code> 表示样本数量为 1，<code>10</code> 表示每个样本具有 10 个特征，这与全连接层定义的输入维度 <code>in_features = 10</code> 相匹配。可以把维度理解为10个特征，因此<code>torch.randn(1, 10)</code>就是一个10维向量（有10个数对应10个特征）</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">linear_layer = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)  <span class="comment"># 输入维度为 10，输出维度为 5</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">output = linear_layer(input_tensor)</span><br></pre></td></tr></table></figure>



<p>2.<code>torch.nn.Conv2d</code>定义二维卷积层。在计算机视觉领域，卷积层广泛应用于图像分类、目标检测、语义分割等任务中，用于提取图像的特征。</p>
<p>①<code>out_channels = 16</code> 的含义</p>
<ul>
<li><strong>输出通道的数量</strong>：<code>out_channels</code> 表示卷积层输出特征图的通道数量。在卷积神经网络（CNN）中，卷积层通过一系列卷积核在输入特征图上滑动进行卷积操作，每个卷积核会生成一个对应的输出特征图。这里 <code>out_channels = 16</code> 意味着该卷积层使用了 16 个不同的卷积核，每个卷积核会对输入特征图进行卷积计算，最终会输出 16 个特征图，这些特征图共同构成了卷积层的输出，且输出的通道数为 16。</li>
<li><strong>特征提取的多样性</strong>：不同的卷积核可以学习到输入图像的不同特征，例如边缘、纹理、颜色等。通过设置多个输出通道，卷积层能够从输入图像中提取多种不同类型的特征，从而丰富了模型对图像特征的表达能力。在后续的网络层中，这些提取到的特征会被进一步处理和组合，用于完成图像分类、目标检测等任务。</li>
</ul>
<p>②<code>kernel_size = 3</code> 的含义</p>
<ul>
<li><strong>卷积核的大小</strong>：<code>kernel_size</code> 定义了卷积核的尺寸。<code>kernel_size = 3</code> 表示使用的是一个 3x3 的卷积核。卷积核是一个小的二维矩阵，在卷积操作中，它会在输入特征图上逐行逐列地滑动，与输入特征图的对应区域进行元素相乘并求和，从而得到输出特征图的一个元素。</li>
<li><strong>特征感受野</strong>：卷积核的大小决定了卷积层的感受野，即输出特征图上的一个元素受到输入特征图上多大区域的影响。一个 3x3 的卷积核意味着输出特征图上的每个元素是由输入特征图上一个 3x3 的区域计算得到的，它能够捕捉到输入图像中 3x3 邻域内的局部特征。较大的卷积核可以捕捉到更大范围的特征，但同时也会增加模型的参数数量和计算量；较小的卷积核则更侧重于捕捉局部细节特征，并且计算效率更高。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">conv_layer = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">16</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">input_image = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># 假设输入为 1 张 3 通道、224x224 的图像</span></span><br><span class="line">output = conv_layer(input_image)</span><br></pre></td></tr></table></figure>



<p>3.<code>torch.nn.ReLU</code>定义 ReLU 激活函数层。激活函数用于引入非线性，使得神经网络能够学习更复杂的函数。ReLU 是一种常用的激活函数，在许多神经网络架构中都有应用。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">relu = nn.ReLU()</span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">output = relu(input_tensor)</span><br></pre></td></tr></table></figure>



<h4 id="优化器和损失函数"><a href="#优化器和损失函数" class="headerlink" title="优化器和损失函数"></a>优化器和损失函数</h4><p>1.<code>torch.optim.SGD</code>定义随机梯度下降优化器。在模型训练过程中，需要使用优化器来更新模型的参数。SGD 是一种基本的优化算法，适用于大多数场景。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">model = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<p>2.<code>torch.nn.MSELoss</code>定义均方误差损失函数。均方误差损失函数常用于回归问题，用于衡量预测值与真实值之间的差异。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">predictions = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">targets = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">loss = criterion(predictions, targets)</span><br></pre></td></tr></table></figure>

<p>3.<code>torch.nn.CrossEntropyLoss</code>定义交叉熵损失函数。交叉熵损失函数常用于分类问题，用于衡量模型输出的概率分布与真实标签之间的差异。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">logits = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>])</span><br><span class="line">loss = criterion(logits, targets)</span><br></pre></td></tr></table></figure>

<h2 id="OS操作系统接口模块"><a href="#OS操作系统接口模块" class="headerlink" title="OS操作系统接口模块"></a>OS操作系统接口模块</h2>]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>科研知识积累</tag>
      </tags>
  </entry>
</search>
